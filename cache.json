{"2025-05-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.13258v3","updated":"2025-05-15T17:59:42Z","published":"2024-10-17T06:30:55Z","title":"How Does Knowledge Selection Help Retrieval Augmented Generation?","summary":"  Retrieval-augmented generation (RAG) is a powerful method for enhancing\nnatural language generation by integrating external knowledge into a model's\noutput. While prior work has demonstrated the importance of improving knowledge\nretrieval for boosting generation quality, the role of knowledge selection\nremains less clear. This paper empirically analyzes how knowledge selection\ninfluences downstream generation performance in RAG systems. By simulating\ndifferent retrieval and selection conditions through a controlled mixture of\ngold and distractor knowledge, we assess the impact of these factors on\ngeneration outcomes. Our findings indicate that the downstream generator\nmodel's capability, as well as the complexity of the task and dataset,\nsignificantly influence the impact of knowledge selection on the overall RAG\nsystem performance. In typical scenarios, improving the knowledge recall score\nis key to enhancing generation outcomes, with the knowledge selector providing\nlimited benefit when a strong generator model is used on clear, well-defined\ntasks. For weaker generator models or more ambiguous tasks and datasets, the\nknowledge F1 score becomes a critical factor, and the knowledge selector plays\na more prominent role in improving overall performance.\n","authors":["Xiangci Li","Jessica Ouyang"],"pdf_url":"https://arxiv.org/pdf/2410.13258v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10557v1","updated":"2025-05-15T17:59:21Z","published":"2025-05-15T17:59:21Z","title":"MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal\n  Mathematical Reasoning","summary":"  Natural language image-caption datasets, widely used for training Large\nMultimodal Models, mainly focus on natural scenarios and overlook the intricate\ndetails of mathematical figures that are critical for problem-solving,\nhindering the advancement of current LMMs in multimodal mathematical reasoning.\nTo this end, we propose leveraging code as supervision for cross-modal\nalignment, since code inherently encodes all information needed to generate\ncorresponding figures, establishing a precise connection between the two\nmodalities. Specifically, we co-develop our image-to-code model and dataset\nwith model-in-the-loop approach, resulting in an image-to-code model,\nFigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.\nFurthermore, we utilize FigCodifier to synthesize novel mathematical figures\nand then construct MM-MathInstruct-3M, a high-quality multimodal math\ninstruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with\nImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on\nMM-MathInstruct-3M for multimodal math problem solving. Our model achieves a\nnew open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and\nClaude 3.5 Sonnet in the geometry problem-solving subset of MathVista,\nachieving improvements of 8.9% and 9.2%. The dataset and models will be\nreleased at https://github.com/mathllm/MathCoder.\n","authors":["Ke Wang","Junting Pan","Linda Wei","Aojun Zhou","Weikang Shi","Zimu Lu","Han Xiao","Yunqiao Yang","Houxing Ren","Mingjie Zhan","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2505.10557v1.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.10554v1","updated":"2025-05-15T17:58:33Z","published":"2025-05-15T17:58:33Z","title":"Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models","summary":"  Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment\n","authors":["Zhiyuan Hu","Yibo Wang","Hanze Dong","Yuhui Xu","Amrita Saha","Caiming Xiong","Bryan Hooi","Junnan Li"],"pdf_url":"https://arxiv.org/pdf/2505.10554v1.pdf","comment":"In Progress"},{"id":"http://arxiv.org/abs/2505.10543v1","updated":"2025-05-15T17:53:47Z","published":"2025-05-15T17:53:47Z","title":"Towards a Deeper Understanding of Reasoning Capabilities in Large\n  Language Models","summary":"  While large language models demonstrate impressive performance on static\nbenchmarks, the true potential of large language models as self-learning and\nreasoning agents in dynamic environments remains unclear. This study\nsystematically evaluates the efficacy of self-reflection, heuristic mutation,\nand planning as prompting techniques to test the adaptive capabilities of\nagents. We conduct experiments with various open-source language models in\ndynamic environments and find that larger models generally outperform smaller\nones, but that strategic prompting can close this performance gap. Second, a\ntoo-long prompt can negatively impact smaller models on basic reactive tasks,\nwhile larger models show more robust behaviour. Third, advanced prompting\ntechniques primarily benefit smaller models on complex games, but offer less\nimprovement for already high-performing large language models. Yet, we find\nthat advanced reasoning methods yield highly variable outcomes: while capable\nof significantly improving performance when reasoning and decision-making\nalign, they also introduce instability and can lead to big performance drops.\nCompared to human performance, our findings reveal little evidence of true\nemergent reasoning. Instead, large language model performance exhibits\npersistent limitations in crucial areas such as planning, reasoning, and\nspatial coordination, suggesting that current-generation large language models\nstill suffer fundamental shortcomings that may not be fully overcome through\nself-reflective prompting alone. Reasoning is a multi-faceted task, and while\nreasoning methods like Chain of thought improves multi-step reasoning on math\nword problems, our findings using dynamic benchmarks highlight important\nshortcomings in general reasoning capabilities, indicating a need to move\nbeyond static benchmarks to capture the complexity of reasoning.\n","authors":["Annie Wong","Thomas BÃ¤ck","Aske Plaat","Niki van Stein","Anna V. Kononova"],"pdf_url":"https://arxiv.org/pdf/2505.10543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04689v3","updated":"2025-05-15T17:52:51Z","published":"2025-02-07T06:30:33Z","title":"ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning","summary":"  Large language models (LLMs) have demonstrated impressive capabilities on\ncomplex evaluation benchmarks, many of which are formulated as\nquestion-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts\nis becoming increasingly vital for advancing their development and\napplicability. This paper introduces ARR, an intuitive, effective, and general\nQA solving method that explicitly incorporates three key steps: analyzing the\nintent of the question, retrieving relevant information, and reasoning step by\nstep. Notably, this paper is the first to introduce intent analysis in QA,\nwhich plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA\ntasks demonstrate that ARR consistently outperforms the baseline methods.\nAblation and case studies further validate the positive contributions of each\nARR component. Furthermore, experiments involving variations in prompt design\nindicate that ARR maintains its effectiveness regardless of the specific prompt\nformulation. Additionally, extensive evaluations across various model sizes,\nLLM series, and generation settings solidify the effectiveness, robustness, and\ngeneralizability of ARR.\n","authors":["Yuwei Yin","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2502.04689v3.pdf","comment":"21 pages. Code: https://github.com/YuweiYin/ARR"},{"id":"http://arxiv.org/abs/2505.10527v1","updated":"2025-05-15T17:38:37Z","published":"2025-05-15T17:38:37Z","title":"WorldPM: Scaling Human Preference Modeling","summary":"  Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.\n","authors":["Binghai Wang","Runji Lin","Keming Lu","Le Yu","Zhenru Zhang","Fei Huang","Chujie Zheng","Kai Dang","Yang Fan","Xingzhang Ren","An Yang","Binyuan Hui","Dayiheng Liu","Tao Gui","Qi Zhang","Xuanjing Huang","Yu-Gang Jiang","Bowen Yu","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2505.10527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10526v1","updated":"2025-05-15T17:37:00Z","published":"2025-05-15T17:37:00Z","title":"MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative\n  Decoding of Vision-Language Models","summary":"  Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs.\n","authors":["Mugilan Ganesan","Shane Segal","Ankur Aggarwal","Nish Sinnadurai","Sean Lie","Vithursan Thangarasa"],"pdf_url":"https://arxiv.org/pdf/2505.10526v1.pdf","comment":"Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp"},{"id":"http://arxiv.org/abs/2505.10518v1","updated":"2025-05-15T17:25:03Z","published":"2025-05-15T17:25:03Z","title":"Multi-Token Prediction Needs Registers","summary":"  Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.\n","authors":["Anastasios Gerontopoulos","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2505.10518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02069v4","updated":"2025-05-15T17:18:12Z","published":"2024-06-04T07:51:30Z","title":"PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling","summary":"  In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance.\n","authors":["Zefan Cai","Yichi Zhang","Bofei Gao","Yuliang Liu","Yucheng Li","Tianyu Liu","Keming Lu","Wayne Xiong","Yue Dong","Junjie Hu","Wen Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.02069v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10507v1","updated":"2025-05-15T17:10:50Z","published":"2025-05-15T17:10:50Z","title":"The Devil Is in the Word Alignment Details: On Translation-Based\n  Cross-Lingual Transfer for Token Classification Tasks","summary":"  Translation-based strategies for cross-lingual transfer XLT such as\ntranslate-train -- training on noisy target language data translated from the\nsource language -- and translate-test -- evaluating on noisy source language\ndata translated from the target language -- are competitive XLT baselines. In\nXLT for token classification tasks, however, these strategies include label\nprojection, the challenging step of mapping the labels from each token in the\noriginal sentence to its counterpart(s) in the translation. Although word\naligners (WAs) are commonly used for label projection, the low-level design\ndecisions for applying them to translation-based XLT have not been\nsystematically investigated. Moreover, recent marker-based methods, which\nproject labeled spans by inserting tags around them before (or after)\ntranslation, claim to outperform WAs in label projection for XLT. In this work,\nwe revisit WAs for label projection, systematically investigating the effects\nof low-level design decisions on token-level XLT: (i) the algorithm for\nprojecting labels between (multi-)token spans, (ii) filtering strategies to\nreduce the number of noisily mapped labels, and (iii) the pre-tokenization of\nthe translated sentences. We find that all of these substantially impact\ntranslation-based XLT performance and show that, with optimized choices, XLT\nwith WA offers performance at least comparable to that of marker-based methods.\nWe then introduce a new projection strategy that ensembles translate-train and\ntranslate-test predictions and demonstrate that it substantially outperforms\nthe marker-based projection. Crucially, we show that our proposed ensembling\nalso reduces sensitivity to low-level WA design choices, resulting in more\nrobust XLT for token classification tasks.\n","authors":["Benedikt Ebing","Goran GlavaÅ¡"],"pdf_url":"https://arxiv.org/pdf/2505.10507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13957v2","updated":"2025-05-15T17:09:21Z","published":"2025-01-21T04:05:45Z","title":"Benchmarking Generative AI for Scoring Medical Student Interviews in\n  Objective Structured Clinical Examinations (OSCEs)","summary":"  Objective Structured Clinical Examinations (OSCEs) are widely used to assess\nmedical students' communication skills, but scoring interview-based assessments\nis time-consuming and potentially subject to human bias. This study explored\nthe potential of large language models (LLMs) to automate OSCE evaluations\nusing the Master Interview Rating Scale (MIRS). We compared the performance of\nfour state-of-the-art LLMs (GPT-4o, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro)\nin evaluating OSCE transcripts across all 28 items of the MIRS under the\nconditions of zero-shot, chain-of-thought (CoT), few-shot, and multi-step\nprompting. The models were benchmarked against a dataset of 10 OSCE cases with\n174 expert consensus scores available. Model performance was measured using\nthree accuracy metrics (exact, off-by-one, thresholded). Averaging across all\nMIRS items and OSCE cases, LLMs performed with low exact accuracy (0.27 to\n0.44), and moderate to high off-by-one accuracy (0.67 to 0.87) and thresholded\naccuracy (0.75 to 0.88). A zero temperature parameter ensured high intra-rater\nreliability ({\\alpha} = 0.98 for GPT-4o). CoT, few-shot, and multi-step\ntechniques proved valuable when tailored to specific assessment items. The\nperformance was consistent across MIRS items, independent of encounter phases\nand communication domains. We demonstrated the feasibility of AI-assisted OSCE\nevaluation and provided benchmarking of multiple LLMs across multiple prompt\ntechniques. Our work provides a baseline performance assessment for LLMs that\nlays a foundation for future research into automated assessment of clinical\ncommunication skills.\n","authors":["Jadon Geathers","Yann Hicke","Colleen Chan","Niroop Rajashekar","Justin Sewell","Susannah Cornes","Rene F. Kizilcec","Dennis Shung"],"pdf_url":"https://arxiv.org/pdf/2501.13957v2.pdf","comment":"12 pages + 3 pages of references, 4 figures"},{"id":"http://arxiv.org/abs/2411.13504v3","updated":"2025-05-15T17:05:43Z","published":"2024-11-20T17:55:38Z","title":"Disentangling Memory and Reasoning Ability in Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.\n","authors":["Mingyu Jin","Weidi Luo","Sitao Cheng","Xinyi Wang","Wenyue Hua","Ruixiang Tang","William Yang Wang","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.13504v3.pdf","comment":"Accepted by ACL 2025"},{"id":"http://arxiv.org/abs/2505.10495v1","updated":"2025-05-15T16:53:45Z","published":"2025-05-15T16:53:45Z","title":"RouteNator: A Router-Based Multi-Modal Architecture for Generating\n  Synthetic Training Data for Function Calling LLMs","summary":"  This paper addresses fine-tuning Large Language Models (LLMs) for function\ncalling tasks when real user interaction data is unavailable. In digital\ncontent creation tools, where users express their needs through natural\nlanguage queries that must be mapped to API calls, the lack of real-world\ntask-specific data and privacy constraints for training on it necessitate\nsynthetic data generation. Existing approaches to synthetic data generation\nfall short in diversity and complexity, failing to replicate real-world data\ndistributions and leading to suboptimal performance after LLM fine-tuning. We\npresent a novel router-based architecture that leverages domain resources like\ncontent metadata and structured knowledge graphs, along with text-to-text and\nvision-to-text language models to generate high-quality synthetic training\ndata. Our architecture's flexible routing mechanism enables synthetic data\ngeneration that matches observed real-world distributions, addressing a\nfundamental limitation of traditional approaches. Evaluation on a comprehensive\nset of real user queries demonstrates significant improvements in both function\nclassification accuracy and API parameter selection. Models fine-tuned with our\nsynthetic data consistently outperform traditional approaches, establishing new\nbenchmarks for function calling tasks.\n","authors":["Vibha Belavadi","Tushar Vatsa","Dewang Sultania","Suhas Suresha","Ishita Verma","Cheng Chen","Tracy Holloway King","Michael Friedrich"],"pdf_url":"https://arxiv.org/pdf/2505.10495v1.pdf","comment":"Proceedings of the 4th International Workshop on Knowledge-Augmented\n  Methods for Natural Language Processing"},{"id":"http://arxiv.org/abs/2505.10494v1","updated":"2025-05-15T16:53:41Z","published":"2025-05-15T16:53:41Z","title":"Can You Really Trust Code Copilots? Evaluating Large Language Models\n  from a Code Security Perspective","summary":"  Code security and usability are both essential for various coding assistant\napplications driven by large language models (LLMs). Current code security\nbenchmarks focus solely on single evaluation task and paradigm, such as code\ncompletion and generation, lacking comprehensive assessment across dimensions\nlike secure code generation, vulnerability repair and discrimination. In this\npaper, we first propose CoV-Eval, a multi-task benchmark covering various tasks\nsuch as code completion, vulnerability repair, vulnerability detection and\nclassification, for comprehensive evaluation of LLM code security. Besides, we\ndeveloped VC-Judge, an improved judgment model that aligns closely with human\nexperts and can review LLM-generated programs for vulnerabilities in a more\nefficient and reliable way. We conduct a comprehensive evaluation of 20\nproprietary and open-source LLMs. Overall, while most LLMs identify vulnerable\ncodes well, they still tend to generate insecure codes and struggle with\nrecognizing specific vulnerability types and performing repairs. Extensive\nexperiments and qualitative analyses reveal key challenges and optimization\ndirections, offering insights for future research in LLM code security.\n","authors":["Yutao Mou","Xiao Deng","Yuxiao Luo","Shikun Zhang","Wei Ye"],"pdf_url":"https://arxiv.org/pdf/2505.10494v1.pdf","comment":"Accepted by ACL2025 Main Conference"},{"id":"http://arxiv.org/abs/2505.10493v1","updated":"2025-05-15T16:53:04Z","published":"2025-05-15T16:53:04Z","title":"CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with\n  Curriculum Learning","summary":"  Retrieval-Augmented Generation (RAG) is an effective method to enhance the\ncapabilities of large language models (LLMs). Existing methods focus on\noptimizing the retriever or generator in the RAG system by directly utilizing\nthe top-k retrieved documents. However, the documents effectiveness are various\nsignificantly across user queries, i.e. some documents provide valuable\nknowledge while others totally lack critical information. It hinders the\nretriever and generator's adaptation during training. Inspired by human\ncognitive learning, curriculum learning trains models using samples progressing\nfrom easy to difficult, thus enhancing their generalization ability, and we\nintegrate this effective paradigm to the training of the RAG system. In this\npaper, we propose a multi-stage Curriculum Learning based RAG system training\nframework, named CL-RAG. We first construct training data with multiple\ndifficulty levels for the retriever and generator separately through sample\nevolution. Then, we train the model in stages based on the curriculum learning\napproach, thereby optimizing the overall performance and generalization of the\nRAG system more effectively. Our CL-RAG framework demonstrates consistent\neffectiveness across four open-domain QA datasets, achieving performance gains\nof 2% to 4% over multiple advanced methods.\n","authors":["Shaohan Wang","Licheng Zhang","Zheren Fu","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2505.10493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21909v2","updated":"2025-05-15T16:40:39Z","published":"2024-10-29T10:01:40Z","title":"SceneGenAgent: Precise Industrial Scene Generation with Coding Agent","summary":"  The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent .\n","authors":["Xiao Xia","Dan Zhang","Zibo Liao","Zhenyu Hou","Tianrui Sun","Jing Li","Ling Fu","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2410.21909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.17671v3","updated":"2025-05-15T16:24:49Z","published":"2025-04-24T15:39:46Z","title":"Data-Driven Calibration of Prediction Sets in Large Vision-Language\n  Models Based on Inductive Conformal Prediction","summary":"  This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.\n","authors":["Yuanchang Ye","Weiyan Wen"],"pdf_url":"https://arxiv.org/pdf/2504.17671v3.pdf","comment":"Accepted by ICIPCA 2025"},{"id":"http://arxiv.org/abs/2505.10475v1","updated":"2025-05-15T16:24:45Z","published":"2025-05-15T16:24:45Z","title":"Parallel Scaling Law for Language Models","summary":"  It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply $P$ diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the $P$ outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with $P$ parallel\nstreams is similar to scaling the parameters by $O(\\log P)$ while showing\nsuperior inference efficiency. For example, ParScale can use up to 22$\\times$\nless memory increase and 6$\\times$ less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.\n","authors":["Mouxiang Chen","Binyuan Hui","Zeyu Cui","Jiaxi Yang","Dayiheng Liu","Jianling Sun","Junyang Lin","Zhongxin Liu"],"pdf_url":"https://arxiv.org/pdf/2505.10475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10465v1","updated":"2025-05-15T16:18:13Z","published":"2025-05-15T16:18:13Z","title":"Superposition Yields Robust Neural Scaling","summary":"  The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters.\n","authors":["Yizhou liu","Ziming Liu","Jeff Gore"],"pdf_url":"https://arxiv.org/pdf/2505.10465v1.pdf","comment":"30 pages, 23 figures"},{"id":"http://arxiv.org/abs/2502.18036v4","updated":"2025-05-15T16:08:51Z","published":"2025-02-25T09:48:53Z","title":"Harnessing Multiple Large Language Models: A Survey on LLM Ensemble","summary":"  LLM Ensemble -- which involves the comprehensive use of multiple large\nlanguage models (LLMs), each aimed at handling user queries during downstream\ninference, to benefit from their individual strengths -- has gained substantial\nattention recently. The widespread availability of LLMs, coupled with their\nvarying strengths and out-of-the-box usability, has profoundly advanced the\nfield of LLM Ensemble. This paper presents the first systematic review of\nrecent developments in LLM Ensemble. First, we introduce our taxonomy of LLM\nEnsemble and discuss several related research problems. Then, we provide a more\nin-depth classification of the methods under the broad categories of\n\"ensemble-before-inference, ensemble-during-inference,\nensemble-after-inference'', and review all relevant methods. Finally, we\nintroduce related benchmarks and applications, summarize existing studies, and\nsuggest several future research directions. A curated list of papers on LLM\nEnsemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.\n","authors":["Zhijun Chen","Jingzheng Li","Pengpeng Chen","Zhuoran Li","Kai Sun","Yuankai Luo","Qianren Mao","Dingqi Yang","Hailong Sun","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.18036v4.pdf","comment":"9 pages, 2 figures, codebase:\n  https://github.com/junchenzhi/Awesome-LLM-Ensemble"},{"id":"http://arxiv.org/abs/2505.10446v1","updated":"2025-05-15T16:06:32Z","published":"2025-05-15T16:06:32Z","title":"Reinforcing the Diffusion Chain of Lateral Thought with Diffusion\n  Language Models","summary":"  We introduce the \\emph{Diffusion Chain of Lateral Thought (DCoLT)}, a\nreasoning framework for diffusion language models. DCoLT treats each\nintermediate step in the reverse diffusion process as a latent \"thinking\"\naction and optimizes the entire reasoning trajectory to maximize the reward on\nthe correctness of the final answer with outcome-based Reinforcement Learning\n(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,\nlinear thinking process, DCoLT allows bidirectional, non-linear reasoning with\nno strict rule on grammatical correctness amid its intermediate steps of\nthought. We implement DCoLT on two representative Diffusion Language Models\n(DLMs). First, we choose SEDD as a representative continuous-time discrete\ndiffusion model, where its concrete score derives a probabilistic policy to\nmaximize the RL reward over the entire sequence of intermediate diffusion\nsteps. We further consider the discrete-time masked diffusion language model --\nLLaDA, and find that the order to predict and unmask tokens plays an essential\nrole to optimize its RL action resulting from the ranking-based Unmasking\nPolicy Module (UPM) defined by the Plackett-Luce model. Experiments on both\nmath and code generation tasks show that using only public data and 16 H800\nGPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even\nboth. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,\n+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.\n","authors":["Zemin Huang","Zhiyang Chen","Zijun Wang","Tiancheng Li","Guo-Jun Qi"],"pdf_url":"https://arxiv.org/pdf/2505.10446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17067v2","updated":"2025-05-15T15:57:32Z","published":"2024-05-27T11:39:59Z","title":"Tokenization Matters! Degrading Large Language Models through\n  Challenging Their Tokenization","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. Nonetheless, it was also witnessed that LLMs tend\nto produce inaccurate responses to specific queries. This deficiency can be\ntraced to the tokenization step LLMs must undergo, which is an inevitable\nlimitation inherent to all LLMs. In fact, incorrect tokenization is the\ncritical point that hinders LLMs in understanding the input precisely, thus\nleading to unsatisfactory output. This defect is more obvious in Chinese\nscenarios. To demonstrate this flaw of LLMs, we construct an adversarial\ndataset, named as $\\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which\ndraws upon the vocabularies of various open-source LLMs to challenge LLMs'\ntokenization. ADT consists of two subsets: the manually constructed ADT-Human\nand the automatically generated ADT-Auto. Our empirical results reveal that our\nADT is highly effective on challenging the tokenization of leading LLMs,\nincluding GPT-4o, Llama-3, Deepseek-R1 and so on, thus degrading these LLMs'\ncapabilities. Moreover, our method of automatic data generation has been proven\nefficient and robust, which can be applied to any open-source LLMs. In this\npaper, we substantially investigate LLMs' vulnerability in terms of challenging\ntheir token segmentation, which will shed light on the subsequent research of\nimproving LLMs' capabilities through optimizing their tokenization process and\nalgorithms.\n","authors":["Dixuan Wang","Yanda Li","Junyuan Jiang","Zepeng Ding","Ziqin Luo","Guochao Jiang","Jiaqing Liang","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2405.17067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10413v1","updated":"2025-05-15T15:34:15Z","published":"2025-05-15T15:34:15Z","title":"Hierarchical Document Refinement for Long-context Retrieval-augmented\n  Generation","summary":"  Real-world RAG applications often encounter long-context input scenarios,\nwhere redundant information and noise results in higher inference costs and\nreduced performance. To address these challenges, we propose LongRefiner, an\nefficient plug-and-play refiner that leverages the inherent structural\ncharacteristics of long documents. LongRefiner employs dual-level query\nanalysis, hierarchical document structuring, and adaptive refinement through\nmulti-task learning on a single foundation model. Experiments on seven QA\ndatasets demonstrate that LongRefiner achieves competitive performance in\nvarious scenarios while using 10x fewer computational costs and latency\ncompared to the best baseline. Further analysis validates that LongRefiner is\nscalable, efficient, and effective, providing practical insights for real-world\nlong-text RAG applications. Our code is available at\nhttps://github.com/ignorejjj/LongRefiner.\n","authors":["Jiajie Jin","Xiaoxi Li","Guanting Dong","Yuyao Zhang","Yutao Zhu","Yongkang Wu","Zhonghua Li","Qi Ye","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2505.10413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10409v1","updated":"2025-05-15T15:31:17Z","published":"2025-05-15T15:31:17Z","title":"Are LLM-generated plain language summaries truly understandable? A\n  large-scale crowdsourced evaluation","summary":"  Plain language summaries (PLSs) are essential for facilitating effective\ncommunication between clinicians and patients by making complex medical\ninformation easier for laypeople to understand and act upon. Large language\nmodels (LLMs) have recently shown promise in automating PLS generation, but\ntheir effectiveness in supporting health information comprehension remains\nunclear. Prior evaluations have generally relied on automated scores that do\nnot measure understandability directly, or subjective Likert-scale ratings from\nconvenience samples with limited generalizability. To address these gaps, we\nconducted a large-scale crowdsourced evaluation of LLM-generated PLSs using\nAmazon Mechanical Turk with 150 participants. We assessed PLS quality through\nsubjective Likert-scale ratings focusing on simplicity, informativeness,\ncoherence, and faithfulness; and objective multiple-choice comprehension and\nrecall measures of reader understanding. Additionally, we examined the\nalignment between 10 automated evaluation metrics and human judgments. Our\nfindings indicate that while LLMs can generate PLSs that appear\nindistinguishable from human-written ones in subjective evaluations,\nhuman-written PLSs lead to significantly better comprehension. Furthermore,\nautomated evaluation metrics fail to reflect human judgment, calling into\nquestion their suitability for evaluating PLSs. This is the first study to\nsystematically evaluate LLM-generated PLSs based on both reader preferences and\ncomprehension outcomes. Our findings highlight the need for evaluation\nframeworks that move beyond surface-level quality and for generation methods\nthat explicitly optimize for layperson comprehension.\n","authors":["Yue Guo","Jae Ho Sohn","Gondy Leroy","Trevor Cohen"],"pdf_url":"https://arxiv.org/pdf/2505.10409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10402v1","updated":"2025-05-15T15:26:32Z","published":"2025-05-15T15:26:32Z","title":"Rethinking Repetition Problems of LLMs in Code Generation","summary":"  With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code.\n","authors":["Yihong Dong","Yuchen Liu","Xue Jiang","Zhi Jin","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2505.10402v1.pdf","comment":"Accepted to ACL 2025 (main)"},{"id":"http://arxiv.org/abs/2505.06046v2","updated":"2025-05-15T15:14:47Z","published":"2025-05-09T13:42:59Z","title":"Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information","summary":"  As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries. To create PubHealthBench we extract free\ntext from 687 current UK government guidance documents and implement an\nautomated pipeline for generating MCQA samples. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% accuracy in the MCQA setup, and\noutperform humans with cursory search engine use. However, in the free form\nsetup we see lower performance with no model scoring >75%. Importantly we find\nin both setups LLMs have higher accuracy on guidance intended for the general\npublic. Therefore, there are promising signs that state of the art (SOTA) LLMs\nare an increasingly accurate source of public health information, but\nadditional safeguards or tools may still be needed when providing free form\nresponses on public health topics.\n","authors":["Joshua Harris","Fan Grayson","Felix Feldman","Timothy Laurence","Toby Nonnenmacher","Oliver Higgins","Leo Loman","Selina Patel","Thomas Finnie","Samuel Collins","Michael Borowitz"],"pdf_url":"https://arxiv.org/pdf/2505.06046v2.pdf","comment":"24 pages, 10 pages main text"},{"id":"http://arxiv.org/abs/2505.10389v1","updated":"2025-05-15T15:11:48Z","published":"2025-05-15T15:11:48Z","title":"Multi-domain Multilingual Sentiment Analysis in Industry: Predicting\n  Aspect-based Opinion Quadruples","summary":"  This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. Using internal datasets, we investigate whether a single fine-tuned\nmodel can effectively handle multiple domain-specific taxonomies\nsimultaneously. We demonstrate that a combined multi-domain model achieves\nperformance comparable to specialized single-domain models while reducing\noperational complexity. We also share lessons learned for handling\nnon-extractive predictions and evaluating various failure modes when developing\nLLM-based systems for structured prediction tasks.\n","authors":["Benjamin White","Anastasia Shimorina"],"pdf_url":"https://arxiv.org/pdf/2505.10389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10356v1","updated":"2025-05-15T14:46:45Z","published":"2025-05-15T14:46:45Z","title":"Coherent Language Reconstruction from Brain Recordings with Flexible\n  Multi-Modal Input Stimuli","summary":"  Decoding thoughts from brain activity offers valuable insights into human\ncognition and enables promising applications in brain-computer interaction.\nWhile prior studies have explored language reconstruction from fMRI data, they\nare typically limited to single-modality inputs such as images or audio. In\ncontrast, human thought is inherently multimodal. To bridge this gap, we\npropose a unified and flexible framework for reconstructing coherent language\nfrom brain recordings elicited by diverse input modalities-visual, auditory,\nand textual. Our approach leverages visual-language models (VLMs), using\nmodality-specific experts to jointly interpret information across modalities.\nExperiments demonstrate that our method achieves performance comparable to\nstate-of-the-art systems while remaining adaptable and extensible. This work\nadvances toward more ecologically valid and generalizable mind decoding.\n","authors":["Chunyu Ye","Shaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10354v1","updated":"2025-05-15T14:45:45Z","published":"2025-05-15T14:45:45Z","title":"LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with\n  Relative Representations","summary":"  Semantic text representation is a fundamental task in the field of natural\nlanguage processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have\ndemonstrated excellent performance, but the values of each dimension are\ndifficult to trace and interpret. Bag-of-words, as classic sparse interpretable\nembeddings, suffers from poor performance. Recently, Benara et al. (2024)\npropose interpretable text embeddings using large language models, which forms\n\"0/1\" embeddings based on responses to a series of questions. These\ninterpretable text embeddings are typically high-dimensional (larger than\n10,000). In this work, we propose Low-dimensional (lower than 500) Dense and\nInterpretable text embeddings with Relative representations (LDIR). The\nnumerical values of its dimensions indicate semantic relatedness to different\nanchor texts through farthest point sampling, offering both semantic\nrepresentation as well as a certain level of traceability and interpretability.\nWe validate LDIR on multiple semantic textual similarity, retrieval, and\nclustering tasks. Extensive experimental results show that LDIR performs close\nto the black-box baseline models and outperforms the interpretable embeddings\nbaselines with much fewer dimensions. Code is available at\nhttps://github.com/szu-tera/LDIR.\n","authors":["Yile Wang","Zhanyu Shen","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2505.10354v1.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2412.03587v2","updated":"2025-05-15T14:39:45Z","published":"2024-11-26T08:41:45Z","title":"Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient\n  Fine-Tuning of Language Models","summary":"  Transformer-based large-scale pre-trained models achieve great success.\nFine-tuning is the standard practice for leveraging these models in downstream\ntasks. Among the fine-tuning methods, adapter-tuning provides a\nparameter-efficient fine-tuning by introducing lightweight trainable modules\nwhile keeping most pre-trained parameters frozen. However, existing\nadapter-tuning methods still impose substantial resource usage. Through our\ninvestigation, we show that each adapter unequally contributes to both task\nperformance and resource usage. Motivated by this insight, we propose Selective\nAdapter FrEezing (SAFE), which gradually freezes less important adapters early\nto reduce unnecessary resource usage while maintaining performance. In our\nexperiments, SAFE reduces memory usage, computation amount, and training time\nby 42.85\\%, 34.59\\%, and 11.82\\%, respectively, while achieving comparable or\nbetter task performance compared to the baseline. We also demonstrate that SAFE\ninduces regularization effect, thereby smoothing the loss landscape, which\nenables the model to generalize better by avoiding sharp minima.\n","authors":["Hyegang Son","Yonglak Son","Changhoon Kim","Young Geun Kim"],"pdf_url":"https://arxiv.org/pdf/2412.03587v2.pdf","comment":"URL: https://aclanthology.org/2025.naacl-long.480/ Volume:\n  Proceedings of the 2025 Conference of the Nations of the Americas Chapter of\n  the Association for Computational Linguistics: Human Language Technologies\n  (Volume 1: Long Papers) Year: 2025 Address: Albuquerque, New Mexico"},{"id":"http://arxiv.org/abs/2503.06899v2","updated":"2025-05-15T14:37:01Z","published":"2025-03-10T04:05:38Z","title":"KwaiChat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue\n  Corpus","summary":"  Video-based dialogue systems, such as education assistants, have compelling\napplication value, thereby garnering growing interest. However, the current\nvideo-based dialogue systems are limited by their reliance on a single dialogue\ntype, which hinders their versatility in practical applications across a range\nof scenarios, including question-answering, emotional dialog, etc. In this\npaper, we identify this challenge as how to generate video-driven multilingual\nmixed-type dialogues. To mitigate this challenge, we propose a novel task and\ncreate a human-to-human video-driven multilingual mixed-type dialogue corpus,\ntermed KwaiChat, containing a total of 93,209 videos and 246,080 dialogues,\nacross 4 dialogue types, 30 domains, 4 languages, and 13 topics. Additionally,\nwe establish baseline models on KwaiChat. An extensive analysis of 7 distinct\nLLMs on KwaiChat reveals that GPT-4o achieves the best performance but still\ncannot perform well in this situation even with the help of in-context learning\nand fine-tuning, which indicates that the task is not trivial and needs further\nresearch.\n","authors":["Xiaoming Shi","Zeming Liu","Yiming Lei","Chenkai Zhang","Haitao Leng","Chuan Wang","Qingjie Liu","Wanxiang Che","Shaoguo Liu","Size Li","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2503.06899v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03689v2","updated":"2025-05-15T14:35:43Z","published":"2024-05-06T17:59:36Z","title":"Pose Priors from Language Models","summary":"  Language is often used to describe physical interaction, yet most 3D human\npose estimation methods overlook this rich source of information. We bridge\nthis gap by leveraging large multimodal models (LMMs) as priors for\nreconstructing contact poses, offering a scalable alternative to traditional\nmethods that rely on human annotations or motion capture data. Our approach\nextracts contact-relevant descriptors from an LMM and translates them into\ntractable losses to constrain 3D human pose optimization. Despite its\nsimplicity, our method produces compelling reconstructions for both two-person\ninteractions and self-contact scenarios, accurately capturing the semantics of\nphysical and social interactions. Our results demonstrate that LMMs can serve\nas powerful tools for contact prediction and pose estimation, offering an\nalternative to costly manual human annotations or motion capture data. Our code\nis publicly available at https://prosepose.github.io.\n","authors":["Sanjay Subramanian","Evonne Ng","Lea MÃ¼ller","Dan Klein","Shiry Ginosar","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2405.03689v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2407.12363v5","updated":"2025-05-15T14:27:17Z","published":"2024-07-17T07:39:16Z","title":"Conversational Query Reformulation with the Guidance of Retrieved\n  Documents","summary":"  Conversational search seeks to retrieve relevant passages for the given\nquestions in conversational question answering. Conversational Query\nReformulation (CQR) improves conversational search by refining the original\nqueries into de-contextualized forms to resolve the issues in the original\nqueries, such as omissions and coreferences. Previous CQR methods focus on\nimitating human written queries which may not always yield meaningful search\nresults for the retriever. In this paper, we introduce GuideCQR, a framework\nthat refines queries for CQR by leveraging key information from the initially\nretrieved documents. Specifically, GuideCQR extracts keywords and generates\nexpected answers from the retrieved documents, then unifies them with the\nqueries after filtering to add useful information that enhances the search\nprocess. Experimental results demonstrate that our proposed method achieves\nstate-of-the-art performance across multiple datasets, outperforming previous\nCQR methods. Additionally, we show that GuideCQR can get additional performance\ngains in conversational search using various types of queries, even for queries\nwritten by humans.\n","authors":["Jeonghyun Park","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2407.12363v5.pdf","comment":"18 pages, 3 figures, 16 tables"},{"id":"http://arxiv.org/abs/2501.00777v2","updated":"2025-05-15T14:18:58Z","published":"2025-01-01T09:00:10Z","title":"FitCF: A Framework for Automatic Feature Importance-guided\n  Counterfactual Example Generation","summary":"  Counterfactual examples are widely used in natural language processing (NLP)\nas valuable data to improve models, and in explainable artificial intelligence\n(XAI) to understand model behavior. The automated generation of counterfactual\nexamples remains a challenging task even for large language models (LLMs),\ndespite their impressive performance on many tasks. In this paper, we first\nintroduce ZeroCF, a faithful approach for leveraging important words derived\nfrom feature attribution methods to generate counterfactual examples in a\nzero-shot setting. Second, we present a new framework, FitCF, which further\nverifies aforementioned counterfactuals by label flip verification and then\ninserts them as demonstrations for few-shot prompting, outperforming two\nstate-of-the-art baselines. Through ablation studies, we identify the\nimportance of each of FitCF's core components in improving the quality of\ncounterfactuals, as assessed through flip rate, perplexity, and similarity\nmeasures. Furthermore, we show the effectiveness of LIME and Integrated\nGradients as backbone attribution methods for FitCF and find that the number of\ndemonstrations has the largest effect on performance. Finally, we reveal a\nstrong correlation between the faithfulness of feature attribution scores and\nthe quality of generated counterfactuals.\n","authors":["Qianli Wang","Nils Feldhus","Simon Ostermann","Luis Felipe Villa-Arenas","Sebastian MÃ¶ller","Vera Schmitt"],"pdf_url":"https://arxiv.org/pdf/2501.00777v2.pdf","comment":"ACL 2025 Findings; camera-ready version"},{"id":"http://arxiv.org/abs/2505.00551v3","updated":"2025-05-15T14:16:03Z","published":"2025-05-01T14:28:35Z","title":"100 Days After DeepSeek-R1: A Survey on Replication Studies and More\n  Directions for Reasoning Language Models","summary":"  The recent development of reasoning language models (RLMs) represents a novel\nevolution in large language models. In particular, the recent release of\nDeepSeek-R1 has generated widespread social impact and sparked enthusiasm in\nthe research community for exploring the explicit reasoning paradigm of\nlanguage models. However, the implementation details of the released models\nhave not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero,\nDeepSeek-R1, and the distilled small models. As a result, many replication\nstudies have emerged aiming to reproduce the strong performance achieved by\nDeepSeek-R1, reaching comparable performance through similar training\nprocedures and fully open-source data resources. These works have investigated\nfeasible strategies for supervised fine-tuning (SFT) and reinforcement learning\nfrom verifiable rewards (RLVR), focusing on data preparation and method design,\nyielding various valuable insights. In this report, we provide a summary of\nrecent replication studies to inspire future research. We primarily focus on\nSFT and RLVR as two main directions, introducing the details for data\nconstruction, method design and training procedure of current replication\nstudies. Moreover, we conclude key findings from the implementation details and\nexperimental results reported by these studies, anticipating to inspire future\nresearch. We also discuss additional techniques of enhancing RLMs, highlighting\nthe potential of expanding the application scope of these models, and\ndiscussing the challenges in development. By this survey, we aim to help\nresearchers and developers of RLMs stay updated with the latest advancements,\nand seek to inspire new ideas to further enhance RLMs.\n","authors":["Chong Zhang","Yue Deng","Xiang Lin","Bin Wang","Dianwen Ng","Hai Ye","Xingxuan Li","Yao Xiao","Zhanfeng Mo","Qi Zhang","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2505.00551v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13338v3","updated":"2025-05-15T14:13:36Z","published":"2024-09-20T08:57:20Z","title":"Time Awareness in Large Language Models: Benchmarking Fact Recall Across\n  Time","summary":"  Who is the US President? The answer changes depending on when the question is\nasked. While large language models (LLMs) are evaluated on various reasoning\ntasks, they often miss a crucial dimension: time. In real-world scenarios, the\ncorrectness of answers is frequently tied to temporal context. To address this\ngap, we present a novel framework and dataset spanning over 8,000 events from\n2018 to 2024, annotated with day-level granularity and sourced globally across\ndomains such as politics, science, and business. Our TimeShift evaluation\nmethod systematically probes LLMs for temporal reasoning, revealing that base\nmodels often outperform instruction-tuned and synthetic-trained counterparts on\ntime-sensitive recall. Additionally, we find that even large-scale models\nexhibit brittleness in handling paraphrased facts, highlighting unresolved\nchallenges in temporal consistency. By identifying these limitations, our work\nprovides a significant step toward advancing time-aware language models capable\nof adapting to the dynamic nature of real-world knowledge.\n","authors":["David Herel","Vojtech Bartek","Jiri Jirak","Tomas Mikolov"],"pdf_url":"https://arxiv.org/pdf/2409.13338v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19477v3","updated":"2025-05-15T14:06:27Z","published":"2024-11-29T05:29:47Z","title":"Simple and Provable Scaling Laws for the Test-Time Compute of Large\n  Language Models","summary":"  We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms.\n","authors":["Yanxi Chen","Xuchen Pan","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.19477v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10320v1","updated":"2025-05-15T14:05:15Z","published":"2025-05-15T14:05:15Z","title":"J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning","summary":"  The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.\n","authors":["Chenxi Whitehouse","Tianlu Wang","Ping Yu","Xian Li","Jason Weston","Ilia Kulikov","Swarnadeep Saha"],"pdf_url":"https://arxiv.org/pdf/2505.10320v1.pdf","comment":"10 pages, 8 tables, 11 figures"},{"id":"http://arxiv.org/abs/2410.11516v3","updated":"2025-05-15T13:50:00Z","published":"2024-10-15T11:37:21Z","title":"TopoLM: brain-like spatio-functional organization in a topographic\n  language model","summary":"  Neurons in the brain are spatially organized such that neighbors on tissue\noften exhibit similar response profiles. In the human language system,\nexperimental studies have observed clusters for syntactic and semantic\ncategories, but the mechanisms underlying this functional organization remain\nunclear. Here, building on work from the vision literature, we develop TopoLM,\na transformer language model with an explicit two-dimensional spatial\nrepresentation of model units. By combining a next-token prediction objective\nwith a spatial smoothness loss, representations in this model assemble into\nclusters that correspond to semantically interpretable groupings of text and\nclosely match the functional organization in the brain's language system.\nTopoLM successfully predicts the emergence of the spatio-functional\norganization of a cortical language system as well as the organization of\nfunctional clusters selective for fine-grained linguistic features empirically\nobserved in human cortex. Our results suggest that the functional organization\nof the human language system is driven by a unified spatial objective, and\nprovide a functionally and spatially aligned model of language processing in\nthe brain.\n","authors":["Neil Rathi","Johannes Mehrer","Badr AlKhamissi","Taha Binhuraib","Nicholas M. Blauch","Martin Schrimpf"],"pdf_url":"https://arxiv.org/pdf/2410.11516v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10292v1","updated":"2025-05-15T13:42:14Z","published":"2025-05-15T13:42:14Z","title":"StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding\n  and Grounded Story Generation","summary":"  Visual storytelling systems struggle to maintain character identity across\nframes and link actions to appropriate subjects, frequently leading to\nreferential hallucinations. These issues can be addressed through grounding of\ncharacters, objects, and other entities on the visual elements. We propose\nStoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie\nimages, with both structured scene analyses and grounded stories. Each story\nmaintains character and object consistency across frames while explicitly\nmodeling multi-frame relationships through structured tabular representations.\nOur approach features cross-frame object re-identification using visual\nsimilarity and face recognition, chain-of-thought reasoning for explicit\nnarrative modeling, and a grounding scheme that links textual elements to\nvisual entities across multiple frames. We establish baseline performance by\nfine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end\nobject detection, re-identification, and landmark detection while maintaining\nconsistent object references throughout the story. Evaluation demonstrates a\nreduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when\ncompared to a non-fine-tuned model.\n","authors":["Daniel A. P. Oliveira","David Martins de Matos"],"pdf_url":"https://arxiv.org/pdf/2505.10292v1.pdf","comment":"31 pages, 14 figures"},{"id":"http://arxiv.org/abs/2505.10282v1","updated":"2025-05-15T13:30:39Z","published":"2025-05-15T13:30:39Z","title":"From Questions to Clinical Recommendations: Large Language Models\n  Driving Evidence-Based Clinical Decision Making","summary":"  Clinical evidence, derived from rigorous research and data analysis, provides\nhealthcare professionals with reliable scientific foundations for informed\ndecision-making. Integrating clinical evidence into real-time practice is\nchallenging due to the enormous workload, complex professional processes, and\ntime constraints. This highlights the need for tools that automate evidence\nsynthesis to support more efficient and accurate decision making in clinical\nsettings. This study introduces Quicker, an evidence-based clinical decision\nsupport system powered by large language models (LLMs), designed to automate\nevidence synthesis and generate clinical recommendations modeled after standard\nclinical guideline development processes. Quicker implements a fully automated\nchain that covers all phases, from questions to clinical recommendations, and\nfurther enables customized decision-making through integrated tools and\ninteractive user interfaces. To evaluate Quicker's capabilities, we developed\nthe Q2CRBench-3 benchmark dataset, based on clinical guideline development\nrecords for three different diseases. Experimental results highlighted\nQuicker's strong performance, with fine-grained question decomposition tailored\nto user preferences, retrieval sensitivities comparable to human experts, and\nliterature screening performance approaching comprehensive inclusion of\nrelevant studies. In addition, Quicker-assisted evidence assessment effectively\nsupported human reviewers, while Quicker's recommendations were more\ncomprehensive and logically coherent than those of clinicians. In system-level\ntesting, collaboration between a single reviewer and Quicker reduced the time\nrequired for recommendation development to 20-40 minutes. In general, our\nfindings affirm the potential of Quicker to help physicians make quicker and\nmore reliable evidence-based clinical decisions.\n","authors":["Dubai Li","Nan Jiang","Kangping Huang","Ruiqi Tu","Shuyu Ouyang","Huayu Yu","Lin Qiao","Chen Yu","Tianshu Zhou","Danyang Tong","Qian Wang","Mengtao Li","Xiaofeng Zeng","Yu Tian","Xinping Tian","Jingsong Li"],"pdf_url":"https://arxiv.org/pdf/2505.10282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10261v1","updated":"2025-05-15T13:11:14Z","published":"2025-05-15T13:11:14Z","title":"The Evolving Landscape of Generative Large Language Models and\n  Traditional Natural Language Processing in Medicine","summary":"  Natural language processing (NLP) has been traditionally applied to medicine,\nand generative large language models (LLMs) have become prominent recently.\nHowever, the differences between them across different medical tasks remain\nunderexplored. We analyzed 19,123 studies, finding that generative LLMs\ndemonstrate advantages in open-ended tasks, while traditional NLP dominates in\ninformation extraction and analysis tasks. As these technologies advance,\nethical use of them is essential to ensure their potential in medical\napplications.\n","authors":["Rui Yang","Huitao Li","Matthew Yu Heng Wong","Yuhe Ke","Xin Li","Kunyu Yu","Jingchi Liao","Jonathan Chong Kai Liew","Sabarinath Vinod Nair","Jasmine Chiat Ling Ong","Irene Li","Douglas Teodoro","Chuan Hong","Daniel Shu Wei Ting","Nan Liu"],"pdf_url":"https://arxiv.org/pdf/2505.10261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10260v1","updated":"2025-05-15T13:10:47Z","published":"2025-05-15T13:10:47Z","title":"Comparing LLM Text Annotation Skills: A Study on Human Rights Violations\n  in Social Media Data","summary":"  In the era of increasingly sophisticated natural language processing (NLP)\nsystems, large language models (LLMs) have demonstrated remarkable potential\nfor diverse applications, including tasks requiring nuanced textual\nunderstanding and contextual reasoning. This study investigates the\ncapabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,\nMistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex\ntextual dataset comprising social media posts in Russian and Ukrainian.\nSpecifically, the focus is on the binary classification task of identifying\nreferences to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared\nagainst a gold standard set of human double-annotated labels across 1000\nsamples. The analysis includes assessing annotation performance under different\nprompting conditions, with prompts provided in both English and Russian.\nAdditionally, the study explores the unique patterns of errors and\ndisagreements exhibited by each model, offering insights into their strengths,\nlimitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes\nto understanding the reliability and applicability of LLMs for sensitive,\ndomain-specific tasks in multilingual contexts. It also sheds light on how\nlanguage models handle inherently subjective and context-dependent judgments, a\ncritical consideration for their deployment in real-world scenarios.\n","authors":["Poli Apollinaire Nemkova","Solomon Ubani","Mark V. Albert"],"pdf_url":"https://arxiv.org/pdf/2505.10260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14790v4","updated":"2025-05-15T13:02:21Z","published":"2024-11-22T08:21:03Z","title":"KBAlign: Efficient Self Adaptation on Specific Knowledge Bases","summary":"  Although retrieval-augmented generation (RAG) remains essential for\nknowledge-based question answering (KBQA), current paradigms face critical\nchallenges under specific domains. Existing methods struggle with targeted\nadaptation on small-scale KBs: vanilla unsupervised training exhibits poor\neffectiveness, while fine-tuning incurs prohibitive costs of external signals.\nWe present KBAlign, a self-supervised framework that enhances RAG systems\nthrough efficient model adaptation. Our key insight is to leverage the model's\nintrinsic capabilities for knowledge alignment through two innovative\nmechanisms: multi-grained self-annotation that captures global knowledge for\ndata construction, and iterative tuning that accelerates convergence through\nself verification. This framework enables cost-effective model adaptation to\nspecific textual KBs, without human supervision or external model assistance.\nExperiments demonstrate that KBAlign can achieve 90\\% of the performance gain\nobtained through GPT-4-supervised adaptation, while relying entirely on\nself-annotation of much smaller models. KBAlign significantly improves\ndownstream QA accuracy across multiple domains with tiny costs, particularly\nbenefiting scenarios requiring deep knowledge integration from specialized\ncorpora. We release our experimental data, models, and process analyses to the\ncommunity for further exploration (https://github.com/thunlp/KBAlign).\n","authors":["Zheni Zeng","Yuxuan Chen","Shi Yu","Ruobing Wang","Yukun Yan","Zhenghao Liu","Shuo Wang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2411.14790v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10231v1","updated":"2025-05-15T12:43:23Z","published":"2025-05-15T12:43:23Z","title":"On the Interplay of Human-AI Alignment,Fairness, and Performance\n  Trade-offs in Medical Imaging","summary":"  Deep neural networks excel in medical imaging but remain prone to biases,\nleading to fairness gaps across demographic groups. We provide the first\nsystematic exploration of Human-AI alignment and fairness in this domain. Our\nresults show that incorporating human insights consistently reduces fairness\ngaps and enhances out-of-domain generalization, though excessive alignment can\nintroduce performance trade-offs, emphasizing the need for calibrated\nstrategies. These findings highlight Human-AI alignment as a promising approach\nfor developing fair, robust, and generalizable medical AI systems, striking a\nbalance between expert guidance and automated efficiency. Our code is available\nat https://github.com/Roypic/Aligner.\n","authors":["Haozhe Luo","Ziyu Zhou","Zixin Shu","AurÃ©lie Pahud de Mortanges","Robert Berke","Mauricio Reyes"],"pdf_url":"https://arxiv.org/pdf/2505.10231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15674v2","updated":"2025-05-15T12:42:44Z","published":"2025-01-26T21:05:16Z","title":"TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and\n  Compression in LLMs","summary":"  The reasoning abilities of Large Language Models (LLMs) can be improved by\nstructurally denoising their weights, yet existing techniques primarily focus\non denoising the feed-forward network (FFN) of the transformer block, and can\nnot efficiently utilise the Multi-head Attention (MHA) block, which is the core\nof transformer architectures. To address this issue, we propose a novel\nintuitive framework that, at its very core, performs MHA compression through a\nmulti-head tensorisation process and the Tucker decomposition. This enables\nboth higher-dimensional structured denoising and compression of the MHA\nweights, by enforcing a shared higher-dimensional subspace across the weights\nof the multiple attention heads. We demonstrate that this approach consistently\nenhances the reasoning capabilities of LLMs across multiple benchmark datasets,\nand for both encoder-only and decoder-only architectures, while achieving\ncompression rates of up to $\\sim 250$ times in the MHA weights, all without\nrequiring any additional data, training, or fine-tuning. Furthermore, we show\nthat the proposed method can be seamlessly combined with existing\nFFN-only-based denoising techniques to achieve further improvements in LLM\nreasoning performance.\n","authors":["Yuxuan Gu","Wuyang Zhou","Giorgos Iacovides","Danilo Mandic"],"pdf_url":"https://arxiv.org/pdf/2501.15674v2.pdf","comment":"Accpeted for IEEE International Joint Conference on Neural Networks\n  (IJCNN 2025). The code is available at https://github.com/guyuxuan9/TensorLLM"},{"id":"http://arxiv.org/abs/2505.10222v1","updated":"2025-05-15T12:30:33Z","published":"2025-05-15T12:30:33Z","title":"ComplexFormer: Disruptively Advancing Transformer Inference Ability via\n  Head-Specific Complex Vector Attention","summary":"  Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism.\n","authors":["Jintian Shao","Hongyi Huang","Jiayi Wu","Beiwen Zhang","ZhiYu Wu","You Shan","MingKai Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.10222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10218v1","updated":"2025-05-15T12:22:10Z","published":"2025-05-15T12:22:10Z","title":"RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable\n  Reward","summary":"  Role-playing conversational agents (RPCAs) face persistent challenges in\nmaintaining role consistency. To address this, we propose RAIDEN-R1, a novel\nreinforcement learning framework that integrates Verifiable Role-Awareness\nReward (VRAR). The method introduces both singular and multi-term mining\nstrategies to generate quantifiable rewards by assessing role-specific keys.\nAdditionally, we construct a high-quality, role-aware Chain-of-Thought dataset\nthrough multi-LLM collaboration, and implement experiments to enhance reasoning\ncoherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's\nsuperiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on\nScript-Based Knowledge and Conversation Memory metrics, respectively,\noutperforming baseline models while maintaining robustness. Case analyses\nfurther reveal the model's enhanced ability to resolve conflicting contextual\ncues and sustain first-person narrative consistency. This work bridges the\nnon-quantifiability gap in RPCA training and provides insights into role-aware\nreasoning patterns, advancing the development of RPCAs.\n","authors":["Zongsheng Wang","Kaili Sun","Bowen Wu","Qun Yu","Ying Li","Baoxun Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11758v2","updated":"2025-05-15T12:13:37Z","published":"2024-10-15T16:28:09Z","title":"Latent Action Pretraining from Videos","summary":"  We introduce Latent Action Pretraining for general Action models (LAPA), an\nunsupervised method for pretraining Vision-Language-Action (VLA) models without\nground-truth robot action labels. Existing Vision-Language-Action models\nrequire action labels typically collected by human teleoperators during\npretraining, which significantly limits possible data sources and scale. In\nthis work, we propose a method to learn from internet-scale videos that do not\nhave robot action labels. We first train an action quantization model\nleveraging VQ-VAE-based objective to learn discrete latent actions between\nimage frames, then pretrain a latent VLA model to predict these latent actions\nfrom observations and task descriptions, and finally finetune the VLA on\nsmall-scale robot manipulation data to map from latent to robot actions.\nExperimental results demonstrate that our method significantly outperforms\nexisting techniques that train robot manipulation policies from large-scale\nvideos. Furthermore, it outperforms the state-of-the-art VLA model trained with\nrobotic action labels on real-world manipulation tasks that require language\nconditioning, generalization to unseen objects, and semantic generalization to\nunseen instructions. Training only on human manipulation videos also shows\npositive transfer, opening up the potential for leveraging web-scale data for\nrobotics foundation model.\n","authors":["Seonghyeon Ye","Joel Jang","Byeongguk Jeon","Sejune Joo","Jianwei Yang","Baolin Peng","Ajay Mandlekar","Reuben Tan","Yu-Wei Chao","Bill Yuchen Lin","Lars Liden","Kimin Lee","Jianfeng Gao","Luke Zettlemoyer","Dieter Fox","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2410.11758v2.pdf","comment":"ICLR 2025 Website: https://latentactionpretraining.github.io"},{"id":"http://arxiv.org/abs/2505.10202v1","updated":"2025-05-15T11:58:04Z","published":"2025-05-15T11:58:04Z","title":"VQ-Logits: Compressing the Output Bottleneck of Large Language Models\n  via Vector Quantized Logits","summary":"  Large Language Models (LLMs) have achieved remarkable success but face\nsignificant computational and memory challenges, particularly due to their\nextensive output vocabularies. The final linear projection layer, mapping\nhidden states to vocabulary-sized logits, often constitutes a substantial\nportion of the model's parameters and computational cost during inference.\nExisting methods like adaptive softmax or hierarchical softmax introduce\nstructural complexities. In this paper, we propose VQ-Logits, a novel approach\nthat leverages Vector Quantization (VQ) to drastically reduce the parameter\ncount and computational load of the LLM output layer. VQ-Logits replaces the\nlarge V * dmodel output embedding matrix with a small, shared codebook of K\nembedding vectors (K << V ). Each token in the vocabulary is mapped to one of\nthese K codebook vectors. The LLM predicts logits over this compact codebook,\nwhich are then efficiently \"scattered\" to the full vocabulary space using the\nlearned or preassigned mapping. We demonstrate through extensive experiments on\nstandard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits\ncan achieve up to 99% parameter reduction in the output layer and 6x speedup in\nlogit computation, with only a marginal 4% increase in perplexity compared to\nfull softmax baselines. We further provide detailed ablation studies on\ncodebook size, initialization, and learning strategies, showcasing the\nrobustness and effectiveness of our approach.\n","authors":["Jintian Shao","Hongyi Huang","Jiayi Wu","YiMing Cheng","ZhiYu Wu","You Shan","MingKai Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.10202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10185v1","updated":"2025-05-15T11:31:02Z","published":"2025-05-15T11:31:02Z","title":"The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think","summary":"  Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.\n","authors":["Seongyun Lee","Seungone Kim","Minju Seo","Yongrae Jo","Dongyoung Go","Hyeonbin Hwang","Jinho Park","Xiang Yue","Sean Welleck","Graham Neubig","Moontae Lee","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2505.10185v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2505.10182v1","updated":"2025-05-15T11:29:01Z","published":"2025-05-15T11:29:01Z","title":"Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with\n  Synthetic Data for LLM Reasoning","summary":"  Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty.\n","authors":["Yoichi Ishibashi","Taro Yano","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2505.10182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00646v2","updated":"2025-05-15T11:25:54Z","published":"2024-11-01T15:04:37Z","title":"Phase Diagram of Vision Large Language Models Inference: A Perspective\n  from Interaction across Image and Instruction","summary":"  Vision Large Language Models (VLLMs) usually take input as a concatenation of\nimage token embeddings and text token embeddings and conduct causal modeling.\nHowever, their internal behaviors remain underexplored, raising the question of\ninteraction among two types of tokens. To investigate such multimodal\ninteraction during model inference, in this paper, we measure the\ncontextualization among the hidden state vectors of tokens from different\nmodalities. Our experiments uncover a four-phase inference dynamics of VLLMs\nagainst the depth of Transformer-based LMs, including (I) Alignment: In very\nearly layers, contextualization emerges between modalities, suggesting a\nfeature space alignment. (II) Intra-modal Encoding: In early layers,\nintra-modal contextualization is enhanced while inter-modal interaction is\nsuppressed, suggesting a local encoding within modalities. (III) Inter-modal\nEncoding: In later layers, contextualization across modalities is enhanced,\nsuggesting a deeper fusion across modalities. (IV) Output Preparation: In very\nlate layers, contextualization is reduced globally, and hidden states are\naligned towards the unembedding space.\n","authors":["Houjing Wei","Yuting Shi","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2411.00646v2.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.07247v2","updated":"2025-05-15T11:01:45Z","published":"2025-05-12T05:43:21Z","title":"SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring\n  with Large Language Models","summary":"  Subjective Answer Grading (SAG) plays a crucial role in education,\nstandardized testing, and automated assessment systems, particularly for\nevaluating short-form responses in Short Answer Scoring (SAS). However,\nexisting approaches often produce coarse-grained scores and lack detailed\nreasoning. Although large language models (LLMs) have demonstrated potential as\nzero-shot evaluators, they remain susceptible to bias, inconsistencies with\nhuman judgment, and limited transparency in scoring decisions. To overcome\nthese limitations, we introduce SAS-Bench, a benchmark specifically designed\nfor LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,\nexpert-annotated error categories, and a diverse range of question types\nderived from real-world subject-specific exams. This benchmark facilitates\ndetailed evaluation of model reasoning processes and explainability. We also\nrelease an open-source dataset containing 1,030 questions and 4,109 student\nresponses, each annotated by domain experts. Furthermore, we conduct\ncomprehensive experiments with various LLMs, identifying major challenges in\nscoring science-related questions and highlighting the effectiveness of\nfew-shot prompting in improving scoring accuracy. Our work offers valuable\ninsights into the development of more robust, fair, and educationally\nmeaningful LLM-based evaluation systems.\n","authors":["Peichao Lai","Kexuan Zhang","Yi Lin","Linyihan Zhang","Feiyang Ye","Jinhao Yan","Yanwei Xu","Conghui He","Yilei Wang","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2505.07247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15523v2","updated":"2025-05-15T10:18:58Z","published":"2024-05-24T13:05:05Z","title":"The Mosaic Memory of Large Language Models","summary":"  As Large Language Models (LLMs) become widely adopted, understanding how they\nlearn from, and memorize, training data becomes crucial. Memorization in LLMs\nis widely assumed to only occur as a result of sequences being repeated in the\ntraining data. Instead, we show that LLMs memorize by assembling information\nfrom similar sequences, a phenomena we call mosaic memory. We show major LLMs\nto exhibit mosaic memory, with fuzzy duplicates contributing to memorization as\nmuch as 0.8 of an exact duplicate and even heavily modified sequences\ncontributing substantially to memorization. Despite models display reasoning\ncapabilities, we somewhat surprisingly show memorization to be predominantly\nsyntactic rather than semantic. We finally show fuzzy duplicates to be\nubiquitous in real-world data, untouched by deduplication techniques. Taken\ntogether, our results challenge widely held beliefs and show memorization to be\na more complex, mosaic process, with real-world implications for privacy,\nconfidentiality, model utility and evaluation.\n","authors":["Igor Shilov","Matthieu Meeus","Yves-Alexandre de Montjoye"],"pdf_url":"https://arxiv.org/pdf/2405.15523v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10143v1","updated":"2025-05-15T10:17:35Z","published":"2025-05-15T10:17:35Z","title":"GE-Chat: A Graph Enhanced RAG Framework for Evidential Response\n  Generation of LLMs","summary":"  Large Language Models are now key assistants in human decision-making\nprocesses. However, a common note always seems to follow: \"LLMs can make\nmistakes. Be careful with important info.\" This points to the reality that not\nall outputs from LLMs are dependable, and users must evaluate them manually.\nThe challenge deepens as hallucinated responses, often presented with seemingly\nplausible explanations, create complications and raise trust issues among\nusers. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph\nenhanced retrieval-augmented generation framework to provide Evidence-based\nresponse generation. Specifically, when the user uploads a material document, a\nknowledge graph will be created, which helps construct a retrieval-augmented\nagent, enhancing the agent's responses with additional knowledge beyond its\ntraining corpus. Then we leverage Chain-of-Thought (CoT) logic generation,\nn-hop sub-graph searching, and entailment-based sentence generation to realize\naccurate evidence retrieval. We demonstrate that our method improves the\nexisting models' performance in terms of identifying the exact evidence in a\nfree-form context, providing a reliable way to examine the resources of LLM's\nconclusion and help with the judgment of the trustworthiness.\n","authors":["Longchao Da","Parth Mitesh Shah","Kuan-Ru Liou","Jiaxing Zhang","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2505.10143v1.pdf","comment":"5 pages, 4 figures, accepted to IJCAI2025 demo track"},{"id":"http://arxiv.org/abs/2505.10118v1","updated":"2025-05-15T09:43:28Z","published":"2025-05-15T09:43:28Z","title":"Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via\n  Multi-Objective Balanced Covering","summary":"  Existing visual token pruning methods target prompt alignment and visual\npreservation with static strategies, overlooking the varying relative\nimportance of these objectives across tasks, which leads to inconsistent\nperformance. To address this, we derive the first closed-form error bound for\nvisual token pruning based on the Hausdorff distance, uniformly characterizing\nthe contributions of both objectives. Moreover, leveraging $\\epsilon$-covering\ntheory, we reveal an intrinsic trade-off between these objectives and quantify\ntheir optimal attainment levels under a fixed budget. To practically handle\nthis trade-off, we propose Multi-Objective Balanced Covering (MoB), which\nreformulates visual token pruning as a bi-objective covering problem. In this\nframework, the attainment trade-off reduces to budget allocation via greedy\nradius trading. MoB offers a provable performance bound and linear scalability\nwith respect to the number of input visual tokens, enabling adaptation to\nchallenging pruning scenarios. Extensive experiments show that MoB preserves\n96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual\ntokens and accelerates LLaVA-Next-7B by 1.3-1.5$\\times$ with negligible\nperformance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm\nthat MoB integrates seamlessly into advanced MLLMs and diverse vision-language\ntasks.\n","authors":["Yangfu Li","Hongjian Zhan","Tianyi Chen","Qi Liu","Yue Lu"],"pdf_url":"https://arxiv.org/pdf/2505.10118v1.pdf","comment":"31 pages,9 figures,conference"},{"id":"http://arxiv.org/abs/2505.10117v1","updated":"2025-05-15T09:42:11Z","published":"2025-05-15T09:42:11Z","title":"Learning Virtual Machine Scheduling in Cloud Computing through Language\n  Agents","summary":"  In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments.\n","authors":["JieHao Wu","Ziwei Wang","Junjie Sheng","Wenhao Li","Xiangfei Wang","Jun Luo"],"pdf_url":"https://arxiv.org/pdf/2505.10117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10113v1","updated":"2025-05-15T09:35:26Z","published":"2025-05-15T09:35:26Z","title":"What Does Neuro Mean to Cardio? Investigating the Role of Clinical\n  Specialty Data in Medical LLMs","summary":"  In this paper, we introduce S-MedQA, an English medical question-answering\n(QA) dataset for benchmarking large language models in fine-grained clinical\nspecialties. We use S-MedQA to check the applicability of a popular hypothesis\nrelated to knowledge injection in the knowledge-intense scenario of medical QA,\nand show that: 1) training on data from a speciality does not necessarily lead\nto best performance on that specialty and 2) regardless of the specialty\nfine-tuned on, token probabilities of clinically relevant terms for all\nspecialties increase consistently. Thus, we believe improvement gains come\nmostly from domain shifting (e.g., general to medical) rather than knowledge\ninjection and suggest rethinking the role of fine-tuning data in the medical\ndomain. We release S-MedQA and all code needed to reproduce all our experiments\nto the research community.\n","authors":["Xinlan Yan","Di Wu","Yibin Lei","Christof Monz","Iacer Calixto"],"pdf_url":"https://arxiv.org/pdf/2505.10113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19458v3","updated":"2025-05-15T09:07:58Z","published":"2025-04-28T03:48:23Z","title":"Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal\n  Perspective","summary":"  Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from\ndifferent Multi-Modal Knowledge Graphs (MMKGs), a critical information\nretrieval task. Existing studies have explored various fusion paradigms and\nconsistency constraints to improve the alignment of equivalent entities, while\noverlooking that the visual modality may not always contribute positively.\nEmpirically, entities with low-similarity images usually generate\nunsatisfactory performance, highlighting the limitation of overly relying on\nvisual features. We believe the model can be biased toward the visual modality,\nleading to a shortcut image-matching task. To address this, we propose a\ncounterfactual debiasing framework for MMEA, termed CDMEA, which investigates\nvisual modality bias from a causal perspective. Our approach aims to leverage\nboth visual and graph modalities to enhance MMEA while suppressing the direct\ncausal effect of the visual modality on model predictions. By estimating the\nTotal Effect (TE) of both modalities and excluding the Natural Direct Effect\n(NDE) of the visual modality, we ensure that the model predicts based on the\nTotal Indirect Effect (TIE), effectively utilizing both modalities and reducing\nvisual modality bias. Extensive experiments on 9 benchmark datasets show that\nCDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,\nhigh-noise, and low-resource data scenarios.\n","authors":["Taoyu Su","Jiawei Sheng","Duohe Ma","Xiaodong Li","Juwei Yue","Mengxiao Song","Yingkai Tang","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2504.19458v3.pdf","comment":"Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,"},{"id":"http://arxiv.org/abs/2505.10093v1","updated":"2025-05-15T08:51:53Z","published":"2025-05-15T08:51:53Z","title":"From Text to Network: Constructing a Knowledge Graph of Taiwan-Based\n  China Studies Using Generative AI","summary":"  Taiwanese China Studies (CS) has developed into a rich, interdisciplinary\nresearch field shaped by the unique geopolitical position and long standing\nacademic engagement with Mainland China. This study responds to the growing\nneed to systematically revisit and reorganize decades of Taiwan based CS\nscholarship by proposing an AI assisted approach that transforms unstructured\nacademic texts into structured, interactive knowledge representations. We apply\ngenerative AI (GAI) techniques and large language models (LLMs) to extract and\nstandardize entity relation triples from 1,367 peer reviewed CS articles\npublished between 1996 and 2019. These triples are then visualized through a\nlightweight D3.js based system, forming the foundation of a domain specific\nknowledge graph and vector database for the field. This infrastructure allows\nusers to explore conceptual nodes and semantic relationships across the corpus,\nrevealing previously uncharted intellectual trajectories, thematic clusters,\nand research gaps. By decomposing textual content into graph structured\nknowledge units, our system enables a paradigm shift from linear text\nconsumption to network based knowledge navigation. In doing so, it enhances\nscholarly access to CS literature while offering a scalable, data driven\nalternative to traditional ontology construction. This work not only\ndemonstrates how generative AI can augment area studies and digital humanities\nbut also highlights its potential to support a reimagined scholarly\ninfrastructure for regional knowledge systems.\n","authors":["Hsuan-Lei Shao"],"pdf_url":"https://arxiv.org/pdf/2505.10093v1.pdf","comment":"4 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.10089v1","updated":"2025-05-15T08:47:55Z","published":"2025-05-15T08:47:55Z","title":"XRAG: Cross-lingual Retrieval-Augmented Generation","summary":"  We propose XRAG, a novel benchmark designed to evaluate the generation\nabilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)\nsettings where the user language does not match the retrieval results. XRAG is\nconstructed from recent news articles to ensure that its questions require\nexternal knowledge to be answered. It covers the real-world scenarios of\nmonolingual and multilingual retrieval, and provides relevancy annotations for\neach retrieved document. Our novel dataset construction pipeline results in\nquestions that require complex reasoning, as evidenced by the significant gap\nbetween human and LLM performance. Consequently, XRAG serves as a valuable\nbenchmark for studying LLM reasoning abilities, even before considering the\nadditional cross-lingual complexity. Experimental results on five LLMs uncover\ntwo previously unreported challenges in cross-lingual RAG: 1) in the\nmonolingual retrieval setting, all evaluated models struggle with response\nlanguage correctness; 2) in the multilingual retrieval setting, the main\nchallenge lies in reasoning over retrieved information across languages rather\nthan generation of non-English text.\n","authors":["Wei Liu","Sony Trenous","Leonardo F. R. Ribeiro","Bill Byrne","Felix Hieber"],"pdf_url":"https://arxiv.org/pdf/2505.10089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10081v1","updated":"2025-05-15T08:35:14Z","published":"2025-05-15T08:35:14Z","title":"Designing and Contextualising Probes for African Languages","summary":"  Pretrained language models (PLMs) for African languages are continually\nimproving, but the reasons behind these advances remain unclear. This paper\npresents the first systematic investigation into probing PLMs for linguistic\nknowledge about African languages. We train layer-wise probes for six\ntypologically diverse African languages to analyse how linguistic features are\ndistributed. We also design control tasks, a way to interpret probe\nperformance, for the MasakhaPOS dataset. We find PLMs adapted for African\nlanguages to encode more linguistic information about target languages than\nmassively multilingual PLMs. Our results reaffirm previous findings that\ntoken-level syntactic information concentrates in middle-to-last layers, while\nsentence-level semantic information is distributed across all layers. Through\ncontrol tasks and probing baselines, we confirm that performance reflects the\ninternal knowledge of PLMs rather than probe memorisation. Our study applies\nestablished interpretability techniques to African-language PLMs. In doing so,\nwe highlight the internal mechanisms underlying the success of strategies like\nactive learning and multilingual adaptation.\n","authors":["Wisdom Aduah","Francois Meyer"],"pdf_url":"https://arxiv.org/pdf/2505.10081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12393v5","updated":"2025-05-15T08:22:08Z","published":"2024-07-17T08:13:22Z","title":"PersLLM: A Personified Training Approach for Large Language Models","summary":"  Large language models (LLMs) exhibit human-like intelligence, enabling them\nto simulate human behavior and support various applications that require both\nhumanized communication and extensive knowledge reserves. Efforts are made to\npersonify LLMs with special training data or hand-crafted prompts, while\ncorrespondingly faced with challenges such as insufficient data usage or rigid\nbehavior patterns. Consequently, personified LLMs fail to capture personified\nknowledge or express persistent opinion. To fully unlock the potential of LLM\npersonification, we propose PersLLM, a framework for better data construction\nand model tuning. For insufficient data usage, we incorporate strategies such\nas Chain-of-Thought prompting and anti-induction, improving the quality of data\nconstruction and capturing the personality experiences, knowledge, and thoughts\nmore comprehensively. For rigid behavior patterns, we design the tuning process\nand introduce automated DPO to enhance the specificity and dynamism of the\nmodels' personalities, which leads to a more natural opinion communication.\nBoth automated metrics and expert human evaluations demonstrate the\neffectiveness of our approach. Case studies in human-machine interactions and\nmulti-agent systems further suggest potential application scenarios and future\ndirections for LLM personification.\n","authors":["Zheni Zeng","Jiayi Chen","Huimin Chen","Yukun Yan","Yuxuan Chen","Zhenghao Liu","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2407.12393v5.pdf","comment":"8 pages for main text, 5 figures"},{"id":"http://arxiv.org/abs/2505.10066v1","updated":"2025-05-15T08:07:04Z","published":"2025-05-15T08:07:04Z","title":"Dark LLMs: The Growing Threat of Unaligned AI Models","summary":"  Large Language Models (LLMs) rapidly reshape modern life, advancing fields\nfrom healthcare to education and beyond. However, alongside their remarkable\ncapabilities lies a significant threat: the susceptibility of these models to\njailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems\nfrom the very data they learn from. As long as this training data includes\nunfiltered, problematic, or 'dark' content, the models can inherently learn\nundesirable patterns or weaknesses that allow users to circumvent their\nintended safety controls. Our research identifies the growing threat posed by\ndark LLMs models deliberately designed without ethical guardrails or modified\nthrough jailbreak techniques. In our research, we uncovered a universal\njailbreak attack that effectively compromises multiple state-of-the-art models,\nenabling them to answer almost any question and produce harmful outputs upon\nrequest. The main idea of our attack was published online over seven months\nago. However, many of the tested LLMs were still vulnerable to this attack.\nDespite our responsible disclosure efforts, responses from major LLM providers\nwere often inadequate, highlighting a concerning gap in industry practices\nregarding AI safety. As model training becomes more accessible and cheaper, and\nas open-source LLMs proliferate, the risk of widespread misuse escalates.\nWithout decisive intervention, LLMs may continue democratizing access to\ndangerous knowledge, posing greater risks than anticipated.\n","authors":["Michael Fire","Yitzhak Elbazis","Adi Wasenstein","Lior Rokach"],"pdf_url":"https://arxiv.org/pdf/2505.10066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10063v1","updated":"2025-05-15T08:05:12Z","published":"2025-05-15T08:05:12Z","title":"CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance\n  Multi-Document QA Capability","summary":"  Advancements in Large Language Models (LLMs) have extended their input\ncontext length, yet they still struggle with retrieval and reasoning in\nlong-context inputs. Existing methods propose to utilize the prompt strategy\nand retrieval head to alleviate this limitation. However, they still face\nchallenges in balancing retrieval precision and recall, impacting their\nefficacy in answering questions. To address this, we introduce $\\textbf{CAFE}$,\na two-stage coarse-to-fine method to enhance multi-document question-answering\ncapacities. By gradually eliminating the negative impacts of background and\ndistracting documents, CAFE makes the responses more reliant on the evidence\ndocuments. Initially, a coarse-grained filtering method leverages retrieval\nheads to identify and rank relevant documents. Then, a fine-grained steering\nmethod guides attention to the most relevant content. Experiments across\nbenchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%\nSubEM improvement over SFT and RAG methods on the Mistral model, respectively.\n","authors":["Han Peng","Jinhao Jiang","Zican Dong","Wayne Xin Zhao","Lei Fang"],"pdf_url":"https://arxiv.org/pdf/2505.10063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05145v2","updated":"2025-05-15T07:19:33Z","published":"2025-05-08T11:32:46Z","title":"Understanding In-context Learning of Addition via Activation Subspaces","summary":"  To perform in-context learning, language models must extract signals from\nindividual few-shot examples, aggregate these into a learned prediction rule,\nand then apply this rule to new examples. How is this implemented in the\nforward pass of modern transformer models? To study this, we consider a\nstructured family of few-shot learning tasks for which the true prediction rule\nis to add an integer $k$ to the input. We find that Llama-3-8B attains high\naccuracy on this task for a range of $k$, and localize its few-shot ability to\njust three attention heads via a novel optimization approach. We further show\nthe extracted signals lie in a six-dimensional subspace, where four of the\ndimensions track the unit digit and the other two dimensions track overall\nmagnitude. We finally examine how these heads extract information from\nindividual few-shot examples, identifying a self-correction mechanism in which\nmistakes from earlier examples are suppressed by later examples. Our results\ndemonstrate how tracking low-dimensional subspaces across a forward pass can\nprovide insight into fine-grained computational structures.\n","authors":["Xinyan Hu","Kayo Yin","Michael I. Jordan","Jacob Steinhardt","Lijie Chen"],"pdf_url":"https://arxiv.org/pdf/2505.05145v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2505.10013v1","updated":"2025-05-15T06:53:37Z","published":"2025-05-15T06:53:37Z","title":"DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs","summary":"  As Large Language Models (LLMs) have risen in prominence over the past few\nyears, there has been concern over the potential biases in LLMs inherited from\nthe training data. Previous studies have examined how LLMs exhibit implicit\nbias, such as when response generation changes when different social contexts\nare introduced. We argue that this implicit bias is not only an ethical, but\nalso a technical issue, as it reveals an inability of LLMs to accommodate\nextraneous information. However, unlike other measures of LLM intelligence,\nthere are no standard methods to benchmark this specific subset of LLM bias. To\nbridge this gap, we developed a method for calculating an easily interpretable\nbenchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM\nlogic and math problem datasets with sociodemographic personas. We demonstrate\nthat this method can statistically validate the presence of implicit bias in\nLLM behavior and find an inverse trend between question answering accuracy and\nimplicit bias, supporting our argument.\n","authors":["Lake Yin","Fan Huang"],"pdf_url":"https://arxiv.org/pdf/2505.10013v1.pdf","comment":"7 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.15508v3","updated":"2025-05-15T05:34:45Z","published":"2024-07-22T09:45:16Z","title":"Compensate Quantization Errors+: Quantized Models Are Inquisitive\n  Learners","summary":"  The quantization of large language models (LLMs) has been a prominent\nresearch area aimed at enabling their lightweight deployment in practice.\nExisting research about LLM's quantization has mainly explored the interplay\nbetween weights and activations, or employing auxiliary components while\nneglecting the necessity of adjusting weights during quantization.\nConsequently, original weight distributions frequently fail to yield desired\nresults after round-to-nearest (RTN) quantization. Even though incorporating\ntechniques such as mixed precision and low-rank error approximation in LLM's\nquantization can yield improved results, they inevitably introduce additional\ncomputational overhead. On the other hand, traditional techniques for weight\nquantization, such as Generative Post-Training Quantization, rely on manually\ntweaking weight distributions to minimize local errors, but they fall short of\nachieving globally optimal outcomes. Although the recently proposed Learnable\nSingular-value Increment improves global weight quantization by modifying\nweight distributions, it disrupts the original distribution considerably. This\nintroduces pronounced bias toward the training data and can degrade downstream\ntask performance. In this paper, we introduce Singular-value Diagonal\nExpansion, a more nuanced approach to refining weight distributions to achieve\nbetter quantization alignment. Furthermore, we introduce Cross-layer Learning\nthat improves overall quantization outcomes by distributing errors more evenly\nacross layers. Our plug-and-play weight-quantization methods demonstrate\nsubstantial performance improvements over state-of-the-art approaches,\nincluding OmniQuant, DuQuant, and PrefixQuant.\n","authors":["Yifei Gao","Jie Ou","Lei Wang","Jun Cheng","Mengchu Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.15508v3.pdf","comment":"Effecient Quantization Methods for LLMs"},{"id":"http://arxiv.org/abs/2407.12665v3","updated":"2025-05-15T05:15:13Z","published":"2024-07-17T15:48:39Z","title":"Beyond Next Token Prediction: Patch-Level Training for Large Language\n  Models","summary":"  The prohibitive training costs of Large Language Models (LLMs) have emerged\nas a significant bottleneck in the development of next-generation LLMs. In this\npaper, we show that it is possible to significantly reduce the training costs\nof LLMs without sacrificing their performance. Specifically, we introduce\npatch-level training for LLMs, in which multiple tokens are aggregated into a\nunit of higher information density, referred to as a `patch', to serve as the\nfundamental text unit for training LLMs. During patch-level training, we feed\nthe language model shorter sequences of patches and train it to predict the\nnext patch, thereby processing the majority of the training data at a\nsignificantly reduced cost. Following this, the model continues token-level\ntraining on the remaining training data to align with the inference mode.\nExperiments on a diverse range of models (370M-2.7B parameters) demonstrate\nthat patch-level training can reduce the overall training costs to 0.5$\\times$,\nwithout compromising the model performance compared to token-level training.\nSource code: https://github.com/shaochenze/PatchTrain.\n","authors":["Chenze Shao","Fandong Meng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.12665v3.pdf","comment":"ICLR 2025 Spotlight"},{"id":"http://arxiv.org/abs/2409.14074v3","updated":"2025-05-15T04:35:00Z","published":"2024-09-21T09:05:48Z","title":"MultiMed: Multilingual Medical Speech Recognition via Attention Encoder\n  Decoder","summary":"  Multilingual automatic speech recognition (ASR) in the medical domain serves\nas a foundational task for various downstream applications such as speech\ntranslation, spoken language understanding, and voice-activated assistants.\nThis technology improves patient care by enabling efficient communication\nacross language barriers, alleviating specialized workforce shortages, and\nfacilitating improved diagnosis and treatment, particularly during pandemics.\nIn this work, we introduce MultiMed, the first multilingual medical ASR\ndataset, along with the first collection of small-to-large end-to-end medical\nASR models, spanning five languages: Vietnamese, English, German, French, and\nMandarin Chinese. To our best knowledge, MultiMed stands as the world's largest\nmedical ASR dataset across all major benchmarks: total duration, number of\nrecording conditions, number of accents, and number of speaking roles.\nFurthermore, we present the first multilinguality study for medical ASR, which\nincludes reproducible empirical baselines, a monolinguality-multilinguality\nanalysis, Attention Encoder Decoder (AED) vs Hybrid comparative study and a\nlinguistic analysis. We present practical ASR end-to-end training schemes\noptimized for a fixed number of trainable parameters that are common in\nindustry settings. All code, data, and models are available online:\nhttps://github.com/leduckhai/MultiMed/tree/master/MultiMed.\n","authors":["Khai Le-Duc","Phuc Phan","Tan-Hanh Pham","Bach Phan Tat","Minh-Huong Ngo","Chris Ngo","Thanh Nguyen-Tang","Truong-Son Hy"],"pdf_url":"https://arxiv.org/pdf/2409.14074v3.pdf","comment":"ACL 2025, 38 pages"},{"id":"http://arxiv.org/abs/2505.08910v2","updated":"2025-05-15T04:24:06Z","published":"2025-05-13T19:01:12Z","title":"Behind Maya: Building a Multilingual Vision Language Model","summary":"  In recent times, we have seen a rapid development of large Vision-Language\nModels (VLMs). They have shown impressive results on academic benchmarks,\nprimarily in widely spoken languages but lack performance on low-resource\nlanguages and varied cultural contexts. To address these limitations, we\nintroduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a\nmultilingual image-text pretraining dataset in eight languages, based on the\nLLaVA pretraining dataset; and 2) a multilingual image-text model supporting\nthese languages, enhancing cultural and linguistic comprehension in\nvision-language tasks. Code available at https://github.com/nahidalam/maya.\n","authors":["Nahid Alam","Karthik Reddy Kanjula","Surya Guthikonda","Timothy Chung","Bala Krishna S Vegesna","Abhipsha Das","Anthony Susevski","Ryan Sze-Yin Chan","S M Iftekhar Uddin","Shayekh Bin Islam","Roshan Santhosh","Snegha A","Drishti Sharma","Chen Liu","Isha Chaturvedi","Genta Indra Winata","Ashvanth. S","Snehanshu Mukherjee","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2505.08910v2.pdf","comment":"Accepted at VLMs4ALL CVPR 2025 Workshop; corrected workshop name\n  spelling"},{"id":"http://arxiv.org/abs/2505.02387v2","updated":"2025-05-15T04:14:49Z","published":"2025-05-05T06:11:12Z","title":"RM-R1: Reward Modeling as Reasoning","summary":"  Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences through reinforcement learning (RL). To provide accurate\nreward signals, a reward model (RM) should stimulate deep thinking and conduct\ninterpretable reasoning before assigning a score or a judgment. Inspired by\nrecent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we\nhypothesize and validate that integrating reasoning capabilities into reward\nmodeling significantly enhances RM's interpretability and performance. To this\nend, we introduce a new class of generative reward models -- Reasoning Reward\nModels (ReasRMs) -- which formulate reward modeling as a reasoning task. We\npropose a reasoning-oriented training pipeline and train a family of ReasRMs,\nRM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism -- self-generating\nsample-level chat rubrics or math/code solutions, and evaluating candidate\nresponses against them. The training of M-R1 consists of two key stages: (1)\ndistillation of high-quality reasoning chains and (2) reinforcement learning\nwith verifiable rewards. Empirically, our models achieve state-of-the-art\nperformance across three reward model benchmarks on average, outperforming much\nlarger open-weight models (e.g., INF-ORM-Llama3.1-70B) and proprietary ones\n(e.g., GPT-4o) by up to 4.9%. Beyond final performance, we perform thorough\nempirical analysis to understand the key ingredients of successful ReasRM\ntraining. To facilitate future research, we release six ReasRM models along\nwith code and data at https://github.com/RM-R1-UIUC/RM-R1.\n","authors":["Xiusi Chen","Gaotang Li","Ziqi Wang","Bowen Jin","Cheng Qian","Yu Wang","Hongru Wang","Yu Zhang","Denghui Zhang","Tong Zhang","Hanghang Tong","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2505.02387v2.pdf","comment":"24 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.09949v1","updated":"2025-05-15T04:07:55Z","published":"2025-05-15T04:07:55Z","title":"Advanced Crash Causation Analysis for Freeway Safety: A Large Language\n  Model Approach to Identifying Key Contributing Factors","summary":"  Understanding the factors contributing to traffic crashes and developing\nstrategies to mitigate their severity is essential. Traditional statistical\nmethods and machine learning models often struggle to capture the complex\ninteractions between various factors and the unique characteristics of each\ncrash. This research leverages large language model (LLM) to analyze freeway\ncrash data and provide crash causation analysis accordingly. By compiling 226\ntraffic safety studies related to freeway crashes, a training dataset\nencompassing environmental, driver, traffic, and geometric design factors was\ncreated. The Llama3 8B model was fine-tuned using QLoRA to enhance its\nunderstanding of freeway crashes and their contributing factors, as covered in\nthese studies. The fine-tuned Llama3 8B model was then used to identify crash\ncausation without pre-labeled data through zero-shot classification, providing\ncomprehensive explanations to ensure that the identified causes were reasonable\nand aligned with existing research. Results demonstrate that LLMs effectively\nidentify primary crash causes such as alcohol-impaired driving, speeding,\naggressive driving, and driver inattention. Incorporating event data, such as\nroad maintenance, offers more profound insights. The model's practical\napplicability and potential to improve traffic safety measures were validated\nby a high level of agreement among researchers in the field of traffic safety,\nas reflected in questionnaire results with 88.89%. This research highlights the\ncomplex nature of traffic crashes and how LLMs can be used for comprehensive\nanalysis of crash causation and other contributing factors. Moreover, it\nprovides valuable insights and potential countermeasures to aid planners and\npolicymakers in developing more effective and efficient traffic safety\npractices.\n","authors":["Ahmed S. Abdelrahman","Mohamed Abdel-Aty","Samgyu Yang","Abdulrahman Faden"],"pdf_url":"https://arxiv.org/pdf/2505.09949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09945v1","updated":"2025-05-15T04:01:58Z","published":"2025-05-15T04:01:58Z","title":"Personalizing Large Language Models using Retrieval Augmented Generation\n  and Knowledge Graph","summary":"  The advent of large language models (LLMs) has allowed numerous applications,\nincluding the generation of queried responses, to be leveraged in chatbots and\nother conversational assistants. Being trained on a plethora of data, LLMs\noften undergo high levels of over-fitting, resulting in the generation of extra\nand incorrect data, thus causing hallucinations in output generation. One of\nthe root causes of such problems is the lack of timely, factual, and\npersonalized information fed to the LLM. In this paper, we propose an approach\nto address these problems by introducing retrieval augmented generation (RAG)\nusing knowledge graphs (KGs) to assist the LLM in personalized response\ngeneration tailored to the users. KGs have the advantage of storing\ncontinuously updated factual information in a structured way. While our KGs can\nbe used for a variety of frequently updated personal data, such as calendar,\ncontact, and location data, we focus on calendar data in this paper. Our\nexperimental results show that our approach works significantly better in\nunderstanding personal information and generating accurate responses compared\nto the baseline LLMs using personal data as text inputs, with a moderate\nreduction in response time.\n","authors":["Deeksha Prahlad","Chanhee Lee","Dongha Kim","Hokeun Kim"],"pdf_url":"https://arxiv.org/pdf/2505.09945v1.pdf","comment":"To appear in the Companion Proceedings of the ACM Web Conference 2025\n  (WWW Companion '25)"},{"id":"http://arxiv.org/abs/2411.14251v2","updated":"2025-05-15T03:35:25Z","published":"2024-11-21T15:57:02Z","title":"Natural Language Reinforcement Learning","summary":"  Reinforcement Learning (RL) mathematically formulates decision-making with\nMarkov Decision Process (MDP). With MDPs, researchers have achieved remarkable\nbreakthroughs across various domains, including games, robotics, and language\nmodels. This paper seeks a new possibility, Natural Language Reinforcement\nLearning (NLRL), by extending traditional MDP to natural language-based\nrepresentation space. Specifically, NLRL innovatively redefines RL principles,\nincluding task objectives, policy, value function, Bellman equation, and policy\niteration, into their language counterparts. With recent advancements in large\nlanguage models (LLMs), NLRL can be practically implemented to achieve RL-like\npolicy and value improvement by either pure prompting or gradient-based\ntraining. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games\ndemonstrate the effectiveness, efficiency, and interpretability of the NLRL\nframework among diverse use cases.\n","authors":["Xidong Feng","Bo Liu","Ziyu Wan","Haotian Fu","Girish A. Koushik","Zhiyuan Hu","Mengyue Yang","Ying Wen","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14251v2.pdf","comment":"Accepted at ICLR 2025 Workshop SSI-FM"},{"id":"http://arxiv.org/abs/2505.09930v1","updated":"2025-05-15T03:31:37Z","published":"2025-05-15T03:31:37Z","title":"Rethinking Prompt Optimizers: From Prompt Merits to Optimization","summary":"  Prompt optimization (PO) offers a practical alternative to fine-tuning large\nlanguage models (LLMs), enabling performance improvements without altering\nmodel weights. Existing methods typically rely on advanced, large-scale LLMs\nlike GPT-4 to generate optimized prompts. However, due to limited downward\ncompatibility, verbose, instruction-heavy prompts from advanced LLMs can\noverwhelm lightweight inference models and degrade response quality. In this\nwork, we rethink prompt optimization through the lens of interpretable design.\nWe first identify a set of model-agnostic prompt quality merits and empirically\nvalidate their effectiveness in enhancing prompt and response quality. We then\nintroduce MePO, a merit-guided, lightweight, and locally deployable prompt\noptimizer trained on our preference dataset built from merit-aligned prompts\ngenerated by a lightweight LLM. Unlike prior work, MePO avoids online\noptimization reliance, reduces cost and privacy concerns, and, by learning\nclear, interpretable merits, generalizes effectively to both large-scale and\nlightweight inference models. Experiments demonstrate that MePO achieves better\nresults across diverse tasks and model types, offering a scalable and robust\nsolution for real-world deployment. Our model and dataset are available at:\nhttps://github.com/MidiyaZhu/MePO\n","authors":["Zixiao Zhu","Hanzhang Zhou","Zijian Feng","Tianjiao Li","Chua Jia Jim Deryl","Mak Lee Onn","Gee Wah Ng","Kezhi Mao"],"pdf_url":"https://arxiv.org/pdf/2505.09930v1.pdf","comment":"20 pages, 14 figures"},{"id":"http://arxiv.org/abs/2504.05185v2","updated":"2025-05-15T03:23:16Z","published":"2025-04-07T15:35:54Z","title":"Concise Reasoning via Reinforcement Learning","summary":"  Despite significant advancements in large language models (LLMs), a major\ndrawback of reasoning models is their enormous token usage, which increases\ncomputational cost, resource requirements, and response time. In this work, we\nrevisit the core principles of reinforcement learning (RL) and, through\nmathematical analysis, demonstrate that the tendency to generate lengthy\nresponses arises inherently from RL-based optimization during training. This\nfinding questions the prevailing assumption that longer responses inherently\nimprove reasoning accuracy. Instead, we uncover a natural correlation between\nconciseness and accuracy that has been largely overlooked. We show that\nintroducing a secondary phase of RL training, using a very small set of\nproblems, can significantly reduce chains of thought while maintaining or even\nenhancing accuracy. Additionally, we demonstrate that, while GRPO shares some\ninteresting properties of PPO, it suffers from collapse modes, which limit its\nreliability for concise reasoning. Finally, we validate our conclusions through\nextensive experimental results.\n","authors":["Mehdi Fatemi","Banafsheh Rafiee","Mingjie Tang","Kartik Talamadupula"],"pdf_url":"https://arxiv.org/pdf/2504.05185v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07440v2","updated":"2025-05-15T03:16:17Z","published":"2025-04-10T04:09:47Z","title":"Model Utility Law: Evaluating LLMs beyond Performance through Mechanism\n  Interpretable Metric","summary":"  Large Language Models (LLMs) have become indispensable across academia,\nindustry, and daily applications, yet current evaluation methods struggle to\nkeep pace with their rapid development. One core challenge of evaluation in the\nlarge language model (LLM) era is the generalization issue: how to infer a\nmodel's near-unbounded abilities from inevitably bounded benchmarks. We address\nthis challenge by proposing Model Utilization Index (MUI), a mechanism\ninterpretability enhanced metric that complements traditional performance\nscores. MUI quantifies the effort a model expends on a task, defined as the\nproportion of activated neurons or features during inference. Intuitively, a\ntruly capable model should achieve higher performance with lower effort.\nExtensive experiments across popular LLMs reveal a consistent inverse\nlogarithmic relationship between MUI and performance, which we formulate as the\nUtility Law. From this law we derive four practical corollaries that (i) guide\ntraining diagnostics, (ii) expose data contamination issue, (iii) enable fairer\nmodel comparisons, and (iv) design model-specific dataset diversity. Our code\ncan be found at https://github.com/ALEX-nlp/MUI-Eva.\n","authors":["Yixin Cao","Jiahao Ying","Yaoning Wang","Xipeng Qiu","Xuanjing Huang","Yugang Jiang"],"pdf_url":"https://arxiv.org/pdf/2504.07440v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09924v1","updated":"2025-05-15T03:12:36Z","published":"2025-05-15T03:12:36Z","title":"From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework\n  for Large Language Models","summary":"  The rise of Large Language Models (LLMs) has heightened concerns about the\nmisuse of AI-generated text, making watermarking a promising solution.\nMainstream watermarking schemes for LLMs fall into two categories: logits-based\nand sampling-based. However, current schemes entail trade-offs among\nrobustness, text quality, and security. To mitigate this, we integrate\nlogits-based and sampling-based schemes, harnessing their respective strengths\nto achieve synergy. In this paper, we propose a versatile symbiotic\nwatermarking framework with three strategies: serial, parallel, and hybrid. The\nhybrid framework adaptively embeds watermarks using token entropy and semantic\nentropy, optimizing the balance between detectability, robustness, text\nquality, and security. Furthermore, we validate our approach through\ncomprehensive experiments on various datasets and models. Experimental results\nindicate that our method outperforms existing baselines and achieves\nstate-of-the-art (SOTA) performance. We believe this framework provides novel\ninsights into diverse watermarking paradigms. Our code is available at\n\\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.\n","authors":["Yidan Wang","Yubing Ren","Yanan Cao","Binxing Fang"],"pdf_url":"https://arxiv.org/pdf/2505.09924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09921v1","updated":"2025-05-15T03:11:57Z","published":"2025-05-15T03:11:57Z","title":"PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative\n  In-Context Optimization","summary":"  Large Language Models (LLMs) excel in various domains but pose inherent\nprivacy risks. Existing methods to evaluate privacy leakage in LLMs often use\nmemorized prefixes or simple instructions to extract data, both of which\nwell-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM\nsafety mechanisms to generate harmful content, but their role in privacy\nscenarios remains underexplored. In this paper, we examine the effectiveness of\njailbreak attacks in extracting sensitive information, bridging privacy leakage\nand jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework\ntargeting Personally Identifiable Information (PII) and addressing the\nlimitations of current jailbreak methods. Specifically, PIG identifies PII\nentities and their types in privacy queries, uses in-context learning to build\na privacy context, and iteratively updates it with three gradient-based\nstrategies to elicit target PII. We evaluate PIG and existing jailbreak methods\nusing two privacy-related datasets. Experiments on four white-box and two\nblack-box LLMs show that PIG outperforms baseline methods and achieves\nstate-of-the-art (SoTA) results. The results underscore significant privacy\nrisks in LLMs, emphasizing the need for stronger safeguards. Our code is\navailble at\n\\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.\n","authors":["Yidan Wang","Yanan Cao","Yubing Ren","Fang Fang","Zheng Lin","Binxing Fang"],"pdf_url":"https://arxiv.org/pdf/2505.09921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17785v3","updated":"2025-05-15T02:48:26Z","published":"2024-04-27T05:49:11Z","title":"Temporal Scaling Law for Large Language Models","summary":"  Recently, Large Language Models (LLMs) have been widely adopted in a wide\nrange of tasks, leading to increasing attention towards the research on how\nscaling LLMs affects their performance. Existing works, termed Scaling Laws,\nhave discovered that the final test loss of LLMs scales as power-laws with\nmodel size, computational budget, and dataset size. However, the temporal\nchange of the test loss of an LLM throughout its pre-training process remains\nunexplored, though it is valuable in many aspects, such as selecting better\nhyperparameters \\textit{directly} on the target LLM. In this paper, we propose\nthe novel concept of Temporal Scaling Law, studying how the test loss of an LLM\nevolves as the training steps scale up. In contrast to modeling the test loss\nas a whole in a coarse-grained manner, we break it down and dive into the\nfine-grained test loss of each token position, and further develop a dynamic\nhyperbolic-law. Afterwards, we derive the much more precise temporal scaling\nlaw by studying the temporal patterns of the parameters in the dynamic\nhyperbolic-law. Results on both in-distribution (ID) and out-of-distribution\n(OOD) validation datasets demonstrate that our temporal scaling law accurately\npredicts the test loss of LLMs across training steps. Our temporal scaling law\nhas broad practical applications. First, it enables direct and efficient\nhyperparameter selection on the target LLM, such as data mixture proportions.\nSecondly, viewing the LLM pre-training dynamics from the token position\ngranularity provides some insights to enhance the understanding of LLM\npre-training.\n","authors":["Yizhe Xiong","Xiansheng Chen","Xin Ye","Hui Chen","Zijia Lin","Haoran Lian","Zhenpeng Su","Wei Huang","Jianwei Niu","Jungong Han","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2404.17785v3.pdf","comment":"Preprint, Currently under review"},{"id":"http://arxiv.org/abs/2410.04526v4","updated":"2025-05-15T02:17:31Z","published":"2024-10-06T15:41:26Z","title":"FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question\n  Answering","summary":"  In this paper, we introduce FAMMA, an open-source benchmark for\n\\underline{f}in\\underline{a}ncial \\underline{m}ultilingual\n\\underline{m}ultimodal question \\underline{a}nswering (QA). Our benchmark aims\nto evaluate the abilities of large language models (LLMs) in answering complex\nreasoning questions that require advanced financial knowledge. The benchmark\nhas two versions: FAMMA-Basic consists of 1,945 questions extracted from\nuniversity textbooks and exams, along with human-annotated answers and\nrationales; FAMMA-LivePro consists of 103 novel questions created by human\ndomain experts, with answers and rationales held out from the public for a\ncontamination-free evaluation. These questions cover advanced knowledge of 8\nmajor subfields in finance (e.g., corporate finance, derivatives, and portfolio\nmanagement). Some are in Chinese or French, while a majority of them are in\nEnglish. Each question has some non-text data such as charts, diagrams, or\ntables. Our experiments reveal that FAMMA poses a significant challenge on\nLLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,\nwe curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,\nand fine-tuned a series of open-source Qwen models using this reasoning data.\nWe found that training a model on these reasoning trajectories can\nsignificantly improve its performance on FAMMA-LivePro. We released our\nleaderboard, data, code, and trained models at\nhttps://famma-bench.github.io/famma/.\n","authors":["Siqiao Xue","Xiaojing Li","Fan Zhou","Qingyang Dai","Zhixuan Chu","Hongyuan Mei"],"pdf_url":"https://arxiv.org/pdf/2410.04526v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09902v1","updated":"2025-05-15T02:09:19Z","published":"2025-05-15T02:09:19Z","title":"Crossing Borders Without Crossing Boundaries: How Sociolinguistic\n  Awareness Can Optimize User Engagement with Localized Spanish AI Models\n  Across Hispanophone Countries","summary":"  Large language models are, by definition, based on language. In an effort to\nunderscore the critical need for regional localized models, this paper examines\nprimary differences between variants of written Spanish across Latin America\nand Spain, with an in-depth sociocultural and linguistic contextualization\ntherein. We argue that these differences effectively constitute significant\ngaps in the quotidian use of Spanish among dialectal groups by creating\nsociolinguistic dissonances, to the extent that locale-sensitive AI models\nwould play a pivotal role in bridging these divides. In doing so, this approach\ninforms better and more efficient localization strategies that also serve to\nmore adequately meet inclusivity goals, while securing sustainable active daily\nuser growth in a major low-risk investment geographic area. Therefore,\nimplementing at least the proposed five sub variants of Spanish addresses two\nlines of action: to foment user trust and reliance on AI language models while\nalso demonstrating a level of cultural, historical, and sociolinguistic\nawareness that reflects positively on any internationalization strategy.\n","authors":["Martin Capdevila","Esteban Villa Turek","Ellen Karina Chumbe Fernandez","Luis Felipe Polo Galvez","Luis Cadavid","Andrea Marroquin","Rebeca Vargas Quesada","Johanna Crew","Nicole Vallejo Galarraga","Christopher Rodriguez","Diego Gutierrez","Radhi Datla"],"pdf_url":"https://arxiv.org/pdf/2505.09902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09901v1","updated":"2025-05-15T02:09:18Z","published":"2025-05-15T02:09:18Z","title":"Comparing Exploration-Exploitation Strategies of LLMs and Humans:\n  Insights from Standard Multi-armed Bandit Tasks","summary":"  Large language models (LLMs) are increasingly used to simulate or automate\nhuman behavior in complex sequential decision-making tasks. A natural question\nis then whether LLMs exhibit similar decision-making behavior to humans, and\ncan achieve comparable (or superior) performance. In this work, we focus on the\nexploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic\ndecision-making under uncertainty. We employ canonical multi-armed bandit (MAB)\ntasks introduced in the cognitive science and psychiatry literature to conduct\na comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.\nWe use interpretable choice models to capture the E&E strategies of the agents\nand investigate how explicit reasoning, through both prompting strategies and\nreasoning-enhanced models, shapes LLM decision-making. We find that reasoning\nshifts LLMs toward more human-like behavior, characterized by a mix of random\nand directed exploration. In simple stationary tasks, reasoning-enabled LLMs\nexhibit similar levels of random and directed exploration compared to humans.\nHowever, in more complex, non-stationary environments, LLMs struggle to match\nhuman adaptability, particularly in effective directed exploration, despite\nachieving similar regret in certain scenarios. Our findings highlight both the\npromise and limits of LLMs as simulators of human behavior and tools for\nautomated decision-making and point to potential areas of improvements.\n","authors":["Ziyuan Zhang","Darcy Wang","Ningyuan Chen","Rodrigo Mansur","Vahid Sarhangian"],"pdf_url":"https://arxiv.org/pdf/2505.09901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03080v5","updated":"2025-05-15T02:03:46Z","published":"2024-04-03T21:46:14Z","title":"Construction and Application of Materials Knowledge Graph in\n  Multidisciplinary Materials Science via Large Language Model","summary":"  Knowledge in materials science is widely dispersed across extensive\nscientific literature, posing significant challenges to the efficient discovery\nand integration of new materials. Traditional methods, often reliant on costly\nand time-consuming experimental approaches, further complicate rapid\ninnovation. Addressing these challenges, the integration of artificial\nintelligence with materials science has opened avenues for accelerating the\ndiscovery process, though it also demands precise annotation, data extraction,\nand traceability of information. To tackle these issues, this article\nintroduces the Materials Knowledge Graph (MKG), which utilizes advanced natural\nlanguage processing techniques integrated with large language models to extract\nand systematically organize a decade's worth of high-quality research into\nstructured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes\ninformation into comprehensive labels such as Name, Formula, and Application,\nstructured around a meticulously designed ontology, thus enhancing data\nusability and integration. By implementing network-based algorithms, MKG not\nonly facilitates efficient link prediction but also significantly reduces\nreliance on traditional experimental methods. This structured approach not only\nstreamlines materials research but also lays the groundwork for more\nsophisticated science knowledge graphs.\n","authors":["Yanpeng Ye","Jie Ren","Shaozhou Wang","Yuwei Wan","Imran Razzak","Bram Hoex","Haofen Wang","Tong Xie","Wenjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.03080v5.pdf","comment":"Accepted by 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.14964v2","updated":"2025-05-15T01:51:31Z","published":"2024-10-19T03:44:19Z","title":"ChronoFact: Timeline-based Temporal Fact Verification","summary":"  Temporal claims, often riddled with inaccuracies, are a significant challenge\nin the digital misinformation landscape. Fact-checking systems that can\naccurately verify such claims are crucial for combating misinformation. Current\nsystems struggle with the complexities of evaluating the accuracy of these\nclaims, especially when they include multiple, overlapping, or recurring\nevents. We introduce a novel timeline-based fact verification framework that\nidentify events from both claim and evidence and organize them into their\nrespective chronological timelines. The framework systematically examines the\nrelationships between the events in both claim and evidence to predict the\nveracity of each claim event and their chronological accuracy. This allows us\nto accurately determine the overall veracity of the claim. We also introduce a\nnew dataset of complex temporal claims involving timeline-based reasoning for\nthe training and evaluation of our proposed framework. Experimental results\ndemonstrate the effectiveness of our approach in handling the intricacies of\ntemporal claim verification.\n","authors":["Anab Maulana Barik","Wynne Hsu","Mong Li Lee"],"pdf_url":"https://arxiv.org/pdf/2410.14964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00367v2","updated":"2025-05-15T01:38:21Z","published":"2024-06-01T08:59:46Z","title":"RoBERTa-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis","summary":"  Effectively analyzing the comments to uncover latent intentions holds immense\nvalue in making strategic decisions across various domains. However, several\nchallenges hinder the process of sentiment analysis including the lexical\ndiversity exhibited in comments, the presence of long dependencies within the\ntext, encountering unknown symbols and words, and dealing with imbalanced\ndatasets. Moreover, existing sentiment analysis tasks mostly leveraged\nsequential models to encode the long dependent texts and it requires longer\nexecution time as it processes the text sequentially. In contrast, the\nTransformer requires less execution time due to its parallel processing nature.\nIn this work, we introduce a novel hybrid deep learning model, RoBERTa-BiLSTM,\nwhich combines the Robustly Optimized BERT Pretraining Approach (RoBERTa) with\nBidirectional Long Short-Term Memory (BiLSTM) networks. RoBERTa is utilized to\ngenerate meaningful word embedding vectors, while BiLSTM effectively captures\nthe contextual semantics of long-dependent texts. The RoBERTa-BiLSTM hybrid\nmodel leverages the strengths of both sequential and Transformer models to\nenhance performance in sentiment analysis. We conducted experiments using\ndatasets from IMDb, Twitter US Airline, and Sentiment140 to evaluate the\nproposed model against existing state-of-the-art methods. Our experimental\nfindings demonstrate that the RoBERTa-BiLSTM model surpasses baseline models\n(e.g., BERT, RoBERTa-base, RoBERTa-GRU, and RoBERTa-LSTM), achieving accuracies\nof 80.74%, 92.36%, and 82.25% on the Twitter US Airline, IMDb, and Sentiment140\ndatasets, respectively. Additionally, the model achieves F1-scores of 80.73%,\n92.35%, and 82.25% on the same datasets, respectively.\n","authors":["Md. Mostafizer Rahman","Ariful Islam Shiplu","Yutaka Watanobe","Md. Ashad Alam"],"pdf_url":"https://arxiv.org/pdf/2406.00367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01257v5","updated":"2025-05-15T01:04:11Z","published":"2024-07-01T13:07:01Z","title":"uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in\n  Low-Data Regimes","summary":"  Recent work on distilling Whisper's knowledge into small models using\npseudo-labels shows promising performance while reducing the size by up to 50%.\nThis results in small, efficient, and dedicated models. However, a critical\nstep of distillation using pseudo-labels involves filtering high-quality\npredictions and using only those during training. This step requires ground\ntruth labels to compare with and filter low-quality examples, making the\nprocess dependent on human labels. Additionally, the distillation process\nrequires a large amount of data thereby limiting its applicability in\nlow-resource settings. To address this, we propose a distillation framework\nthat does not require any labeled data. Through experimentation, we show that\nour best-distilled models outperform the teacher model by 5-7 WER points and\nare on par with or outperform similar supervised data filtering setups. When\nscaling the data, our models significantly outperform all zero-shot and\nsupervised models. Our models are also 25-50% more compute- and\nmemory-efficient while maintaining performance equal to or better than that of\nthe teacher model. For more details about our models, dataset, and other\nresources, please visit our GitHub page:\nhttps://github.com/UBC-NLP/uDistilWhisper.\n","authors":["Abdul Waheed","Karima Kadaoui","Bhiksha Raj","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2407.01257v5.pdf","comment":"Accepted to NAACL'25 main conference"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2505.10566v1","updated":"2025-05-15T17:59:51Z","published":"2025-05-15T17:59:51Z","title":"3D-Fixup: Advancing Photo Editing with 3D Priors","summary":"  Despite significant advances in modeling image priors via diffusion models,\n3D-aware image editing remains challenging, in part because the object is only\nspecified via a single image. To tackle this challenge, we propose 3D-Fixup, a\nnew framework for editing 2D images guided by learned 3D priors. The framework\nsupports difficult editing situations such as object translation and 3D\nrotation. To achieve this, we leverage a training-based approach that harnesses\nthe generative power of diffusion models. As video data naturally encodes\nreal-world physical dynamics, we turn to video data for generating training\ndata pairs, i.e., a source and a target frame. Rather than relying solely on a\nsingle trained model to infer transformations between source and target frames,\nwe incorporate 3D guidance from an Image-to-3D model, which bridges this\nchallenging task by explicitly projecting 2D information into 3D space. We\ndesign a data generation pipeline to ensure high-quality 3D guidance throughout\ntraining. Results show that by integrating these 3D priors, 3D-Fixup\neffectively supports complex, identity coherent 3D-aware edits, achieving\nhigh-quality results and advancing the application of diffusion models in\nrealistic image manipulation. The code is provided at\nhttps://3dfixup.github.io/\n","authors":["Yen-Chi Cheng","Krishna Kumar Singh","Jae Shin Yoon","Alex Schwing","Liangyan Gui","Matheus Gadelha","Paul Guerrero","Nanxuan Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.10566v1.pdf","comment":"SIGGRAPH 2025. Project page: https://3dfixup.github.io/"},{"id":"http://arxiv.org/abs/2505.10565v1","updated":"2025-05-15T17:59:50Z","published":"2025-05-15T17:59:50Z","title":"Depth Anything with Any Prior","summary":"  This work presents Prior Depth Anything, a framework that combines incomplete\nbut precise metric information in depth measurement with relative but complete\ngeometric structures in depth prediction, generating accurate, dense, and\ndetailed metric depth maps for any scene. To this end, we design a\ncoarse-to-fine pipeline to progressively integrate the two complementary depth\nsources. First, we introduce pixel-level metric alignment and distance-aware\nweighting to pre-fill diverse metric priors by explicitly using depth\nprediction. It effectively narrows the domain gap between prior patterns,\nenhancing generalization across varying scenarios. Second, we develop a\nconditioned monocular depth estimation (MDE) model to refine the inherent noise\nof depth priors. By conditioning on the normalized pre-filled prior and\nprediction, the model further implicitly merges the two complementary depth\nsources. Our model showcases impressive zero-shot generalization across depth\ncompletion, super-resolution, and inpainting over 7 real-world datasets,\nmatching or even surpassing previous task-specific methods. More importantly,\nit performs well on challenging, unseen mixed priors and enables test-time\nimprovements by switching prediction models, providing a flexible\naccuracy-efficiency trade-off while evolving with advancements in MDE models.\n","authors":["Zehan Wang","Siyu Chen","Lihe Yang","Jialei Wang","Ziang Zhang","Hengshuang Zhao","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.10565v1.pdf","comment":"Home page: https://prior-depth-anything.github.io/"},{"id":"http://arxiv.org/abs/2505.10562v1","updated":"2025-05-15T17:59:39Z","published":"2025-05-15T17:59:39Z","title":"End-to-End Vision Tokenizer Tuning","summary":"  Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.\n","authors":["Wenxuan Wang","Fan Zhang","Yufeng Cui","Haiwen Diao","Zhuoyan Luo","Huchuan Lu","Jing Liu","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10557v1","updated":"2025-05-15T17:59:21Z","published":"2025-05-15T17:59:21Z","title":"MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal\n  Mathematical Reasoning","summary":"  Natural language image-caption datasets, widely used for training Large\nMultimodal Models, mainly focus on natural scenarios and overlook the intricate\ndetails of mathematical figures that are critical for problem-solving,\nhindering the advancement of current LMMs in multimodal mathematical reasoning.\nTo this end, we propose leveraging code as supervision for cross-modal\nalignment, since code inherently encodes all information needed to generate\ncorresponding figures, establishing a precise connection between the two\nmodalities. Specifically, we co-develop our image-to-code model and dataset\nwith model-in-the-loop approach, resulting in an image-to-code model,\nFigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.\nFurthermore, we utilize FigCodifier to synthesize novel mathematical figures\nand then construct MM-MathInstruct-3M, a high-quality multimodal math\ninstruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with\nImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on\nMM-MathInstruct-3M for multimodal math problem solving. Our model achieves a\nnew open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and\nClaude 3.5 Sonnet in the geometry problem-solving subset of MathVista,\nachieving improvements of 8.9% and 9.2%. The dataset and models will be\nreleased at https://github.com/mathllm/MathCoder.\n","authors":["Ke Wang","Junting Pan","Linda Wei","Aojun Zhou","Weikang Shi","Zimu Lu","Han Xiao","Yunqiao Yang","Houxing Ren","Mingjie Zhan","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2505.10557v1.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.10558v1","updated":"2025-05-15T17:59:21Z","published":"2025-05-15T17:59:21Z","title":"Style Customization of Text-to-Vector Generation with Image Diffusion\n  Priors","summary":"  Scalable Vector Graphics (SVGs) are highly favored by designers due to their\nresolution independence and well-organized layer structure. Although existing\ntext-to-vector (T2V) generation methods can create SVGs from text prompts, they\noften overlook an important need in practical applications: style\ncustomization, which is vital for producing a collection of vector graphics\nwith consistent visual appearance and coherent aesthetics. Extending existing\nT2V methods for style customization poses certain challenges.\nOptimization-based T2V models can utilize the priors of text-to-image (T2I)\nmodels for customization, but struggle with maintaining structural regularity.\nOn the other hand, feed-forward T2V models can ensure structural regularity,\nyet they encounter difficulties in disentangling content and style due to\nlimited SVG training data.\n  To address these challenges, we propose a novel two-stage style customization\npipeline for SVG generation, making use of the advantages of both feed-forward\nT2V models and T2I image priors. In the first stage, we train a T2V diffusion\nmodel with a path-level representation to ensure the structural regularity of\nSVGs while preserving diverse expressive capabilities. In the second stage, we\ncustomize the T2V diffusion model to different styles by distilling customized\nT2I models. By integrating these techniques, our pipeline can generate\nhigh-quality and diverse SVGs in custom styles based on text prompts in an\nefficient feed-forward manner. The effectiveness of our method has been\nvalidated through extensive experiments. The project page is\nhttps://customsvg.github.io.\n","authors":["Peiying Zhang","Nanxuan Zhao","Jing Liao"],"pdf_url":"https://arxiv.org/pdf/2505.10558v1.pdf","comment":"Accepted by SIGGRAPH 2025 (Conference Paper). Project page:\n  https://customsvg.github.io"},{"id":"http://arxiv.org/abs/2505.10551v1","updated":"2025-05-15T17:57:38Z","published":"2025-05-15T17:57:38Z","title":"Does Feasibility Matter? Understanding the Impact of Feasibility on\n  Synthetic Training Data","summary":"  With the development of photorealistic diffusion models, models trained in\npart or fully on synthetic data achieve progressively better results. However,\ndiffusion models still routinely generate images that would not exist in\nreality, such as a dog floating above the ground or with unrealistic texture\nartifacts. We define the concept of feasibility as whether attributes in a\nsynthetic image could realistically exist in the real-world domain; synthetic\nimages containing attributes that violate this criterion are considered\ninfeasible. Intuitively, infeasible images are typically considered\nout-of-distribution; thus, training on such images is expected to hinder a\nmodel's ability to generalize to real-world data, and they should therefore be\nexcluded from the training set whenever possible. However, does feasibility\nreally matter? In this paper, we investigate whether enforcing feasibility is\nnecessary when generating synthetic training data for CLIP-based classifiers,\nfocusing on three target attributes: background, color, and texture. We\nintroduce VariReal, a pipeline that minimally edits a given source image to\ninclude feasible or infeasible attributes given by the textual prompt generated\nby a large language model. Our experiments show that feasibility minimally\naffects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference\nin top-1 accuracy across three fine-grained datasets. Also, the attribute\nmatters on whether the feasible/infeasible images adversarially influence the\nclassification performance. Finally, mixing feasible and infeasible images in\ntraining datasets does not significantly impact performance compared to using\npurely feasible or infeasible datasets.\n","authors":["Yiwen Liu","Jessica Bader","Jae Myung Kim"],"pdf_url":"https://arxiv.org/pdf/2505.10551v1.pdf","comment":"CVPRW 2025"},{"id":"http://arxiv.org/abs/2505.08787v2","updated":"2025-05-15T17:53:41Z","published":"2025-05-13T17:59:22Z","title":"UniSkill: Imitating Human Videos via Cross-Embodiment Skill\n  Representations","summary":"  Mimicry is a fundamental learning mechanism in humans, enabling individuals\nto learn new tasks by observing and imitating experts. However, applying this\nability to robots presents significant challenges due to the inherent\ndifferences between human and robot embodiments in both their visual appearance\nand physical capabilities. While previous methods bridge this gap using\ncross-embodiment datasets with shared scenes and tasks, collecting such aligned\ndata between humans and robots at scale is not trivial. In this paper, we\npropose UniSkill, a novel framework that learns embodiment-agnostic skill\nrepresentations from large-scale cross-embodiment video data without any\nlabels, enabling skills extracted from human video prompts to effectively\ntransfer to robot policies trained only on robot data. Our experiments in both\nsimulation and real-world environments show that our cross-embodiment skills\nsuccessfully guide robots in selecting appropriate actions, even with unseen\nvideo prompts. The project website can be found at:\nhttps://kimhanjung.github.io/UniSkill.\n","authors":["Hanjung Kim","Jaehyun Kang","Hyolim Kang","Meedeum Cho","Seon Joo Kim","Youngwoon Lee"],"pdf_url":"https://arxiv.org/pdf/2505.08787v2.pdf","comment":"Project Page: https://kimhanjung.github.io/UniSkill/"},{"id":"http://arxiv.org/abs/2505.10541v1","updated":"2025-05-15T17:52:40Z","published":"2025-05-15T17:52:40Z","title":"Exploring Implicit Visual Misunderstandings in Multimodal Large Language\n  Models through Attention Analysis","summary":"  Recent advancements have enhanced the capability of Multimodal Large Language\nModels (MLLMs) to comprehend multi-image information. However, existing\nbenchmarks primarily evaluate answer correctness, overlooking whether models\ngenuinely comprehend the visual input. To address this, we define implicit\nvisual misunderstanding (IVM), where MLLMs provide correct answers without\nfully comprehending the visual input. Through our analysis, we decouple the\nvisual and textual modalities within the causal attention module, revealing\nthat attention distribution increasingly converges on the image associated with\nthe correct answer as the network layers deepen. This insight leads to the\nintroduction of a scale-agnostic metric, \\textit{attention accuracy}, and a\nnovel benchmark for quantifying IVMs. Attention accuracy directly evaluates the\nmodel's visual understanding via internal mechanisms, remaining robust to\npositional biases for more reliable assessments. Furthermore, we extend our\napproach to finer granularities and demonstrate its effectiveness in unimodal\nscenarios, underscoring its versatility and generalizability.\n","authors":["Pengfei Wang","Guohai Xu","Weinong Wang","Junjie Yang","Jie Lou","Yunhua Xue"],"pdf_url":"https://arxiv.org/pdf/2505.10541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10533v1","updated":"2025-05-15T17:41:52Z","published":"2025-05-15T17:41:52Z","title":"Enhancing Multi-Image Question Answering via Submodular Subset Selection","summary":"  Large multimodal models (LMMs) have achieved high performance in\nvision-language tasks involving single image but they struggle when presented\nwith a collection of multiple images (Multiple Image Question Answering\nscenario). These tasks, which involve reasoning over large number of images,\npresent issues in scalability (with increasing number of images) and retrieval\nperformance. In this work, we propose an enhancement for retriever framework\nintroduced in MIRAGE model using submodular subset selection techniques. Our\nmethod leverages query-aware submodular functions, such as GraphCut, to\npre-select a subset of semantically relevant images before main retrieval\ncomponent. We demonstrate that using anchor-based queries and augmenting the\ndata improves submodular-retriever pipeline effectiveness, particularly in\nlarge haystack sizes.\n","authors":["Aaryan Sharma","Shivansh Gupta","Samar Agarwal","Vishak Prasad C.","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2505.10533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10526v1","updated":"2025-05-15T17:37:00Z","published":"2025-05-15T17:37:00Z","title":"MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative\n  Decoding of Vision-Language Models","summary":"  Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs.\n","authors":["Mugilan Ganesan","Shane Segal","Ankur Aggarwal","Nish Sinnadurai","Sean Lie","Vithursan Thangarasa"],"pdf_url":"https://arxiv.org/pdf/2505.10526v1.pdf","comment":"Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp"},{"id":"http://arxiv.org/abs/2505.08616v2","updated":"2025-05-15T17:30:10Z","published":"2025-05-13T14:34:46Z","title":"A portable diagnosis model for Keratoconus using a smartphone","summary":"  Keratoconus (KC) is a corneal disorder that results in blurry and distorted\nvision. Traditional diagnostic tools, while effective, are often bulky, costly,\nand require professional operation. In this paper, we present a portable and\ninnovative methodology for diagnosing. Our proposed approach first captures the\nimage reflected on the eye's cornea when a smartphone screen-generated Placido\ndisc sheds its light on an eye, then utilizes a two-stage diagnosis for\nidentifying the KC cornea and pinpointing the location of the KC on the cornea.\nThe first stage estimates the height and width of the Placido disc extracted\nfrom the captured image to identify whether it has KC. In this KC\nidentification, k-means clustering is implemented to discern statistical\ncharacteristics, such as height and width values of extracted Placido discs,\nfrom non-KC (control) and KC-affected groups. The second stage involves the\ncreation of a distance matrix, providing a precise localization of KC on the\ncornea, which is critical for efficient treatment planning. The analysis of\nthese distance matrices, paired with a logistic regression model and robust\nstatistical analysis, reveals a clear distinction between control and KC\ngroups. The logistic regression model, which classifies small areas on the\ncornea as either control or KC-affected based on the corresponding inter-disc\ndistances in the distance matrix, reported a classification accuracy of 96.94%,\nwhich indicates that we can effectively pinpoint the protrusion caused by KC.\nThis comprehensive, smartphone-based method is expected to detect KC and\nstreamline timely treatment.\n","authors":["Yifan Li","Peter Ho","Jo Woon Chong"],"pdf_url":"https://arxiv.org/pdf/2505.08616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08517v2","updated":"2025-05-15T17:28:04Z","published":"2025-05-13T12:48:36Z","title":"A Deep Learning-Driven Inhalation Injury Grading Assistant Using\n  Bronchoscopy Images","summary":"  Inhalation injuries present a challenge in clinical diagnosis and grading due\nto Conventional grading methods such as the Abbreviated Injury Score (AIS)\nbeing subjective and lacking robust correlation with clinical parameters like\nmechanical ventilation duration and patient mortality. This study introduces a\nnovel deep learning-based diagnosis assistant tool for grading inhalation\ninjuries using bronchoscopy images to overcome subjective variability and\nenhance consistency in severity assessment. Our approach leverages data\naugmentation techniques, including graphic transformations, Contrastive\nUnpaired Translation (CUT), and CycleGAN, to address the scarcity of medical\nimaging data. We evaluate the classification performance of two deep learning\nmodels, GoogLeNet and Vision Transformer (ViT), across a dataset significantly\nexpanded through these augmentation methods. The results demonstrate GoogLeNet\ncombined with CUT as the most effective configuration for grading inhalation\ninjuries through bronchoscopy images and achieves a classification accuracy of\n97.8%. The histograms and frequency analysis evaluations reveal variations\ncaused by the augmentation CUT with distribution changes in the histogram and\ntexture details of the frequency spectrum. PCA visualizations underscore the\nCUT substantially enhances class separability in the feature space. Moreover,\nGrad-CAM analyses provide insight into the decision-making process; mean\nintensity for CUT heatmaps is 119.6, which significantly exceeds 98.8 of the\noriginal datasets. Our proposed tool leverages mechanical ventilation periods\nas a novel grading standard, providing comprehensive diagnostic support.\n","authors":["Yifan Li","Alan W Pang","Jo Woon Chong"],"pdf_url":"https://arxiv.org/pdf/2505.08517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10518v1","updated":"2025-05-15T17:25:03Z","published":"2025-05-15T17:25:03Z","title":"Multi-Token Prediction Needs Registers","summary":"  Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.\n","authors":["Anastasios Gerontopoulos","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2505.10518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01482v2","updated":"2025-05-15T17:15:14Z","published":"2025-01-02T18:13:35Z","title":"An unsupervised method for MRI recovery: Deep image prior with\n  structured sparsity","summary":"  Objective: To propose and validate an unsupervised MRI reconstruction method\nthat does not require fully sampled k-space data. Materials and Methods: The\nproposed method, deep image prior with structured sparsity (DISCUS), extends\nthe deep image prior (DIP) by introducing group sparsity to frame-specific code\nvectors, enabling the discovery of a low-dimensional manifold for capturing\ntemporal variations. \\discus was validated using four studies: (I) simulation\nof a dynamic Shepp-Logan phantom to demonstrate its manifold discovery\ncapabilities, (II) comparison with compressed sensing and DIP-based methods\nusing simulated single-shot late gadolinium enhancement (LGE) image series from\nsix distinct digital cardiac phantoms in terms of normalized mean square error\n(NMSE) and structural similarity index measure (SSIM), (III) evaluation on\nretrospectively undersampled single-shot LGE data from eight patients, and (IV)\nevaluation on prospectively undersampled single-shot LGE data from eight\npatients, assessed via blind scoring from two expert readers. Results: DISCUS\noutperformed competing methods, demonstrating superior reconstruction quality\nin terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study\nIV). Discussion: An unsupervised image reconstruction method is presented and\nvalidated on simulated and measured data. These developments can benefit\napplications where acquiring fully sampled data is challenging.\n","authors":["Muhammad Ahmad Sultan","Chong Chen","Yingmin Liu","Katarzyna Gil","Karolina Zareba","Rizwan Ahmad"],"pdf_url":"https://arxiv.org/pdf/2501.01482v2.pdf","comment":"Magn Reson Mater Phy (2025)"},{"id":"http://arxiv.org/abs/2505.10497v1","updated":"2025-05-15T17:00:16Z","published":"2025-05-15T17:00:16Z","title":"MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face\n  Morphing Attacks","summary":"  Face recognition has evolved significantly with the advancement of deep\nlearning techniques, enabling its widespread adoption in various applications\nrequiring secure authentication. However, this progress has also increased its\nexposure to presentation attacks, including face morphing, which poses a\nserious security threat by allowing one identity to impersonate another.\nTherefore, modern face recognition systems must be robust against such attacks.\n  In this work, we propose a novel approach for training deep networks for face\nrecognition with enhanced robustness to face morphing attacks. Our method\nmodifies the classification task by introducing a dual-branch classification\nstrategy that effectively handles the ambiguity in the labeling of face morphs.\nThis adaptation allows the model to incorporate morph images into the training\nprocess, improving its ability to distinguish them from bona fide samples.\n  Our strategy has been validated on public benchmarks, demonstrating its\neffectiveness in enhancing robustness against face morphing attacks.\nFurthermore, our approach is universally applicable and can be integrated into\nexisting face recognition training pipelines to improve classification-based\nrecognition methods.\n","authors":["Iurii Medvedev","Nuno Goncalves"],"pdf_url":"https://arxiv.org/pdf/2505.10497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10496v1","updated":"2025-05-15T16:59:17Z","published":"2025-05-15T16:59:17Z","title":"CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of\n  Synthetic Chest Radiographs","summary":"  We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/\n","authors":["Raman Dutt","Pedro Sanchez","Yongchen Yao","Steven McDonagh","Sotirios A. Tsaftaris","Timothy Hospedales"],"pdf_url":"https://arxiv.org/pdf/2505.10496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10492v1","updated":"2025-05-15T16:47:24Z","published":"2025-05-15T16:47:24Z","title":"Multi-contrast laser endoscopy for in vivo gastrointestinal imaging","summary":"  White light endoscopy is the clinical gold standard for detecting diseases in\nthe gastrointestinal tract. Most applications involve identifying visual\nabnormalities in tissue color, texture, and shape. Unfortunately, the contrast\nof these features is often subtle, causing many clinically relevant cases to go\nundetected. To overcome this challenge, we introduce Multi-contrast Laser\nEndoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable\nspectral, coherent, and directional illumination. We demonstrate three\ncapabilities of MLE: enhancing tissue chromophore contrast with multispectral\ndiffuse reflectance, quantifying blood flow using laser speckle contrast\nimaging, and characterizing mucosal topography using photometric stereo. We\nvalidate MLE with benchtop models, then demonstrate MLE in vivo during clinical\ncolonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold\nimprovement in contrast and a five-fold improvement in color difference\ncompared to white light and narrow band imaging. With the ability to reveal\nmultiple complementary types of tissue contrast while seamlessly integrating\ninto the clinical environment, MLE shows promise as an investigative tool to\nimprove gastrointestinal imaging.\n","authors":["Taylor L. Bobrow","Mayank Golhar","Suchapa Arayakarnkul","Anthony A. Song","Saowanee Ngamruengphong","Nicholas J. Durr"],"pdf_url":"https://arxiv.org/pdf/2505.10492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10483v1","updated":"2025-05-15T16:34:50Z","published":"2025-05-15T16:34:50Z","title":"UniEval: Unified Holistic Evaluation for Unified Multimodal\n  Understanding and Generation","summary":"  The emergence of unified multimodal understanding and generation models is\nrapidly attracting attention because of their ability to enhance\ninstruction-following capabilities while minimizing model redundancy. However,\nthere is a lack of a unified evaluation framework for these models, which would\nenable an elegant, simplified, and overall evaluation. Current models conduct\nevaluations on multiple task-specific benchmarks, but there are significant\nlimitations, such as the lack of overall results, errors from extra evaluation\nmodels, reliance on extensive labeled images, benchmarks that lack diversity,\nand metrics with limited capacity for instruction-following evaluation. To\ntackle these challenges, we introduce UniEval, the first evaluation framework\ndesigned for unified multimodal models without extra models, images, or\nannotations. This facilitates a simplified and unified evaluation process. The\nUniEval framework contains a holistic benchmark, UniBench (supports both\nunified and visual generation models), along with the corresponding UniScore\nmetric. UniBench includes 81 fine-grained tags contributing to high diversity.\nExperimental results indicate that UniBench is more challenging than existing\nbenchmarks, and UniScore aligns closely with human evaluations, surpassing\ncurrent metrics. Moreover, we extensively evaluated SoTA unified and visual\ngeneration models, uncovering new insights into Univeral's unique values.\n","authors":["Yi Li","Haonan Wang","Qixiang Zhang","Boyu Xiao","Chenchang Hu","Hualiang Wang","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2505.10483v1.pdf","comment":"UniEval is the first evaluation framework designed for unified\n  multimodal models, including a holistic benchmark UniBench and the UniScore\n  metric"},{"id":"http://arxiv.org/abs/2505.10481v1","updated":"2025-05-15T16:31:49Z","published":"2025-05-15T16:31:49Z","title":"Logos as a Well-Tempered Pre-train for Sign Language Recognition","summary":"  This paper examines two aspects of the isolated sign language recognition\n(ISLR) task. First, despite the availability of a number of datasets, the\namount of data for most individual sign languages is limited. It poses the\nchallenge of cross-language ISLR model training, including transfer learning.\nSecond, similar signs can have different semantic meanings. It leads to\nambiguity in dataset labeling and raises the question of the best policy for\nannotating such signs. To address these issues, this study presents Logos, a\nnovel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by\nthe number of signers and one of the largest available datasets while also the\nlargest RSL dataset in size and vocabulary. It is shown that a model,\npre-trained on the Logos dataset can be used as a universal encoder for other\nlanguage SLR tasks, including few-shot learning. We explore cross-language\ntransfer learning approaches and find that joint training using multiple\nclassification heads benefits accuracy for the target lowresource datasets the\nmost. The key feature of the Logos dataset is explicitly annotated visually\nsimilar sign groups. We show that explicitly labeling visually similar signs\nimproves trained model quality as a visual encoder for downstream tasks. Based\non the proposed contributions, we outperform current state-of-the-art results\nfor the WLASL dataset and get competitive results for the AUTSL dataset, with a\nsingle stream model processing solely RGB video. The source code, dataset, and\npre-trained models are publicly available.\n","authors":["Ilya Ovodov","Petr Surovtsev","Karina Kvanchiani","Alexander Kapitanov","Alexander Nagaev"],"pdf_url":"https://arxiv.org/pdf/2505.10481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02147v2","updated":"2025-05-15T16:26:53Z","published":"2024-06-04T09:34:46Z","title":"S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object\n  Tracking","summary":"  3D multiple object tracking (MOT) plays a crucial role in autonomous driving\nperception. Recent end-to-end query-based trackers simultaneously detect and\ntrack objects, which have shown promising potential for the 3D MOT task.\nHowever, existing methods are still in the early stages of development and lack\nsystematic improvements, failing to track objects in certain complex scenarios,\nlike occlusions and the small size of target object's situations. In this\npaper, we first summarize the current end-to-end 3D MOT framework by\ndecomposing it into three constituent parts: query initialization, query\npropagation, and query matching. Then we propose corresponding improvements,\nwhich lead to a strong yet simple tracker: S2-Track. Specifically, for query\ninitialization, we present 2D-Prompted Query Initialization, which leverages\npredicted 2D object and depth information to prompt an initial estimate of the\nobject's 3D location. For query propagation, we introduce an Uncertainty-aware\nProbabilistic Decoder to capture the uncertainty of complex environment in\nobject prediction with probabilistic attention. For query matching, we propose\na Hierarchical Query Denoising strategy to enhance training robustness and\nconvergence. As a result, our S2-Track achieves state-of-the-art performance on\nnuScenes benchmark, i.e., 66.3% AMOTA on test split, surpassing the previous\nbest end-to-end solution by a significant margin of 8.9% AMOTA. We achieve 1st\nplace on the nuScenes tracking task leaderboard.\n","authors":["Tao Tang","Lijun Zhou","Pengkun Hao","Zihang He","Kalok Ho","Shuo Gu","Zhihui Hao","Haiyang Sun","Kun Zhan","Peng Jia","XianPeng Lang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2406.02147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10473v1","updated":"2025-05-15T16:23:51Z","published":"2025-05-15T16:23:51Z","title":"Consistent Quantity-Quality Control across Scenes for Deployment-Aware\n  Gaussian Splatting","summary":"  To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks\nto minimize the number of Gaussians used while preserving high rendering\nquality, introducing an inherent trade-off between Gaussian quantity and\nrendering quality. Existing methods strive for better quantity-quality\nperformance, but lack the ability for users to intuitively adjust this\ntrade-off to suit practical needs such as model deployment under diverse\nhardware and communication constraints. Here, we present ControlGS, a 3DGS\noptimization method that achieves semantically meaningful and cross-scene\nconsistent quantity-quality control while maintaining strong quantity-quality\nperformance. Through a single training run using a fixed setup and a\nuser-specified hyperparameter reflecting quantity-quality preference, ControlGS\ncan automatically find desirable quantity-quality trade-off points across\ndiverse scenes, from compact objects to large outdoor scenes. It also\noutperforms baselines by achieving higher rendering quality with fewer\nGaussians, and supports a broad adjustment range with stepless control over the\ntrade-off.\n","authors":["Fengdi Zhang","Hongkun Cao","Ruqi Huang"],"pdf_url":"https://arxiv.org/pdf/2505.10473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10464v1","updated":"2025-05-15T16:18:00Z","published":"2025-05-15T16:18:00Z","title":"HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric\n  Lesion Segmentation","summary":"  Multimodal medical image segmentation faces significant challenges in the\ncontext of gastric cancer lesion analysis. This clinical context is defined by\nthe scarcity of independent multimodal datasets and the imperative to\namalgamate inherently misaligned modalities. As a result, algorithms are\nconstrained to train on approximate data and depend on application migration,\nleading to substantial resource expenditure and a potential decline in analysis\naccuracy. To address those challenges, we have made two major contributions:\nFirst, we publicly disseminate the GCM 2025 dataset, which serves as the first\nlarge-scale, open-source collection of gastric cancer multimodal MRI scans,\nfeaturing professionally annotated FS-T2W, CE-T1W, and ADC images from 500\npatients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework\nthat employs an original HWA block with learnable window aggregation layers to\nestablish dynamic feature correspondences between different modalities'\nanatomical structures, and leverages the innovative tri-orientated fusion mamba\nmechanism for context modeling and capturing long-range spatial dependencies.\nExtensive experiments on our GCM 2025 dataset and the publicly BraTS 2021\ndataset validate the performance of our framework, demonstrating that the new\napproach surpasses existing methods by up to 1.68\\% in the Dice score while\nmaintaining solid robustness. The dataset and code are public via\nhttps://github.com/JeMing-creater/HWA-UNETR.\n","authors":["Jiaming Liang","Lihuan Dai","Xiaoqi Sheng","Xiangguang Chen","Chun Yao","Guihua Tao","Qibin Leng","Honming Cai","Xi Zhong"],"pdf_url":"https://arxiv.org/pdf/2505.10464v1.pdf","comment":"This work has been provisionally accepted for MICCAI 2025"},{"id":"http://arxiv.org/abs/2505.10457v1","updated":"2025-05-15T16:14:18Z","published":"2025-05-15T16:14:18Z","title":"SEAL: Searching Expandable Architectures for Incremental Learning","summary":"  Incremental learning is a machine learning paradigm where a model learns from\na sequential stream of tasks. This setting poses a key challenge: balancing\nplasticity (learning new tasks) and stability (preserving past knowledge).\nNeural Architecture Search (NAS), a branch of AutoML, automates the design of\nthe architecture of Deep Neural Networks and has shown success in static\nsettings. However, existing NAS-based approaches to incremental learning often\nrely on expanding the model at every task, making them impractical in\nresource-constrained environments. In this work, we introduce SEAL, a NAS-based\nframework tailored for data-incremental learning, a scenario where disjoint\ndata samples arrive sequentially and are not stored for future access. SEAL\nadapts the model structure dynamically by expanding it only when necessary,\nbased on a capacity estimation metric. Stability is preserved through\ncross-distillation training after each expansion step. The NAS component\njointly searches for both the architecture and the optimal expansion policy.\nExperiments across multiple benchmarks demonstrate that SEAL effectively\nreduces forgetting and enhances accuracy while maintaining a lower model size\ncompared to prior methods. These results highlight the promise of combining NAS\nand selective expansion for efficient, adaptive learning in incremental\nscenarios.\n","authors":["Matteo Gambella","Vicente Javier Castro Solar","Manuel Roveri"],"pdf_url":"https://arxiv.org/pdf/2505.10457v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.10453v1","updated":"2025-05-15T16:11:33Z","published":"2025-05-15T16:11:33Z","title":"Vision language models have difficulty recognizing virtual objects","summary":"  Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.\n","authors":["Tyler Tran","Sangeet Khemlani","J. G. Trafton"],"pdf_url":"https://arxiv.org/pdf/2505.10453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10441v1","updated":"2025-05-15T16:00:31Z","published":"2025-05-15T16:00:31Z","title":"PIF: Anomaly detection via preference embedding","summary":"  We address the problem of detecting anomalies with respect to structured\npatterns. To this end, we conceive a novel anomaly detection method called PIF,\nthat combines the advantages of adaptive isolation methods with the flexibility\nof preference embedding. Specifically, we propose to embed the data in a high\ndimensional space where an efficient tree-based method, PI-Forest, is employed\nto compute an anomaly score. Experiments on synthetic and real datasets\ndemonstrate that PIF favorably compares with state-of-the-art anomaly detection\ntechniques, and confirm that PI-Forest is better at measuring arbitrary\ndistances and isolate points in the preference space.\n","authors":["Filippo Leveni","Luca Magri","Giacomo Boracchi","Cesare Alippi"],"pdf_url":"https://arxiv.org/pdf/2505.10441v1.pdf","comment":"Accepted at International Conference on Pattern Recognition (ICPR\n  2020)"},{"id":"http://arxiv.org/abs/2411.14347v3","updated":"2025-05-15T15:52:39Z","published":"2024-11-21T17:42:20Z","title":"DINO-X: A Unified Vision Model for Open-World Object Detection and\n  Understanding","summary":"  In this paper, we introduce DINO-X, which is a unified object-centric vision\nmodel developed by IDEA Research with the best open-world object detection\nperformance to date. DINO-X employs the same Transformer-based encoder-decoder\narchitecture as Grounding DINO 1.5 to pursue an object-level representation for\nopen-world object understanding. To make long-tailed object detection easy,\nDINO-X extends its input options to support text prompt, visual prompt, and\ncustomized prompt. With such flexible prompt options, we develop a universal\nobject prompt to support prompt-free open-world detection, making it possible\nto detect anything in an image without requiring users to provide any prompt.\nTo enhance the model's core grounding capability, we have constructed a\nlarge-scale dataset with over 100 million high-quality grounding samples,\nreferred to as Grounding-100M, for advancing the model's open-vocabulary\ndetection performance. Pre-training on such a large-scale grounding dataset\nleads to a foundational object-level representation, which enables DINO-X to\nintegrate multiple perception heads to simultaneously support multiple object\nperception and understanding tasks, including detection, segmentation, pose\nestimation, object captioning, object-based QA, etc. Experimental results\ndemonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro\nmodel achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and\nLVIS-val zero-shot object detection benchmarks, respectively. Notably, it\nscores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val\nbenchmarks, improving the previous SOTA performance by 5.8 AP and 5.0 AP. Such\na result underscores its significantly improved capacity for recognizing\nlong-tailed objects.\n","authors":["Tianhe Ren","Yihao Chen","Qing Jiang","Zhaoyang Zeng","Yuda Xiong","Wenlong Liu","Zhengyu Ma","Junyi Shen","Yuan Gao","Xiaoke Jiang","Xingyu Chen","Zhuheng Song","Yuhong Zhang","Hongjie Huang","Han Gao","Shilong Liu","Hao Zhang","Feng Li","Kent Yu","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14347v3.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2505.05901v2","updated":"2025-05-15T15:46:43Z","published":"2025-05-09T09:09:08Z","title":"Examining the Source of Defects from a Mechanical Perspective for 3D\n  Anomaly Detection","summary":"  In this paper, we explore a novel approach to 3D anomaly detection (AD) that\ngoes beyond merely identifying anomalies based on structural characteristics.\nOur primary perspective is that most anomalies arise from unpredictable\ndefective forces originating from both internal and external sources. To\naddress these anomalies, we seek out opposing forces that can help correct\nthem. Therefore, we introduce the Mechanics Complementary Model-based Framework\nfor the 3D-AD task (MC4AD), which generates internal and external corrective\nforces for each point. We first propose a Diverse Anomaly-Generation (DA-Gen)\nmodule designed to simulate various types of anomalies. Next, we present the\nCorrective Force Prediction Network (CFP-Net), which uses complementary\nrepresentations for point-level analysis to simulate the different\ncontributions from internal and external corrective forces. To ensure the\ncorrective forces are constrained effectively, we have developed a combined\nloss function that includes a new symmetric loss and an overall loss. Notably,\nwe implement a Hierarchical Quality Control (HQC) strategy based on a three-way\ndecision process and contribute a dataset titled Anomaly-IntraVariance, which\nincorporates intraclass variance to evaluate our model. As a result, the\nproposed MC4AD has been proven effective through theory and experimentation.\nThe experimental results demonstrate that our approach yields nine\nstate-of-the-art performances, achieving optimal results with minimal\nparameters and the fastest inference speed across five existing datasets, in\naddition to the proposed Anomaly-IntraVariance dataset. The source is available\nat https://github.com/hzzzzzhappy/MC4AD\n","authors":["Hanzhe Liang","Aoran Wang","Jie Zhou","Xin Jin","Can Gao","Jinbao Wang"],"pdf_url":"https://arxiv.org/pdf/2505.05901v2.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2505.10420v1","updated":"2025-05-15T15:37:51Z","published":"2025-05-15T15:37:51Z","title":"Learned Lightweight Smartphone ISP with Unpaired Data","summary":"  The Image Signal Processor (ISP) is a fundamental component in modern\nsmartphone cameras responsible for conversion of RAW sensor image data to RGB\nimages with a strong focus on perceptual quality. Recent work highlights the\npotential of deep learning approaches and their ability to capture details with\na quality increasingly close to that of professional cameras. A difficult and\ncostly step when developing a learned ISP is the acquisition of pixel-wise\naligned paired data that maps the raw captured by a smartphone camera sensor to\nhigh-quality reference images. In this work, we address this challenge by\nproposing a novel training method for a learnable ISP that eliminates the need\nfor direct correspondences between raw images and ground-truth data with\nmatching content. Our unpaired approach employs a multi-term loss function\nguided by adversarial training with multiple discriminators processing feature\nmaps from pre-trained networks to maintain content structure while learning\ncolor and texture characteristics from the target RGB dataset. Using\nlightweight neural network architectures suitable for mobile devices as\nbackbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm\nUltraISP datasets. Compared to paired training methods, our unpaired learning\nstrategy shows strong potential and achieves high fidelity across multiple\nevaluation metrics. The code and pre-trained models are available at\nhttps://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .\n","authors":["Andrei Arhire","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2505.10420v1.pdf","comment":"Accepted at CVPRW 2025"},{"id":"http://arxiv.org/abs/2411.05731v3","updated":"2025-05-15T15:32:07Z","published":"2024-11-08T17:42:02Z","title":"PEP-GS: Perceptually-Enhanced Precise Structured 3D Gaussians for\n  View-Adaptive Rendering","summary":"  Recently, 3D Gaussian Splatting (3D-GS) has achieved significant success in\nreal-time, high-quality 3D scene rendering. However, it faces several\nchallenges, including Gaussian redundancy, limited ability to capture\nview-dependent effects, and difficulties in handling complex lighting and\nspecular reflections. Additionally, methods that use spherical harmonics for\ncolor representation often struggle to effectively capture anisotropic\ncomponents, especially when modeling view-dependent colors under complex\nlighting conditions, leading to insufficient contrast and unnatural color\nsaturation. To address these limitations, we introduce PEP-GS, a\nperceptually-enhanced framework that dynamically predicts Gaussian attributes,\nincluding opacity, color, and covariance. We replace traditional spherical\nharmonics with a Hierarchical Granular-Structural Attention mechanism, which\nenables more accurate modeling of complex view-dependent color effects. By\nemploying a stable and interpretable framework for opacity and covariance\nestimation, PEP-GS avoids the removal of essential Gaussians prematurely,\nensuring a more accurate scene representation. Furthermore, perceptual\noptimization is applied to the final rendered images, enhancing perceptual\nconsistency across different views and ensuring high-quality renderings with\nimproved texture fidelity and fine-scale detail preservation. Experimental\nresults demonstrate that PEP-GS outperforms state-of-the-art methods,\nparticularly in challenging scenarios involving view-dependent effects and\nfine-scale details.\n","authors":["Junxi Jin","Xiulai Li","Haiping Huang","Lianjun Liu","Yujie Sun","Logan Liu"],"pdf_url":"https://arxiv.org/pdf/2411.05731v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10405v1","updated":"2025-05-15T15:28:32Z","published":"2025-05-15T15:28:32Z","title":"Visual Fidelity Index for Generative Semantic Communications with\n  Critical Information Embedding","summary":"  Generative semantic communication (Gen-SemCom) with large artificial\nintelligence (AI) model promises a transformative paradigm for 6G networks,\nwhich reduces communication costs by transmitting low-dimensional prompts\nrather than raw data. However, purely prompt-driven generation loses\nfine-grained visual details. Additionally, there is a lack of systematic\nmetrics to evaluate the performance of Gen-SemCom systems. To address these\nissues, we develop a hybrid Gen-SemCom system with a critical information\nembedding (CIE) framework, where both text prompts and semantically critical\nfeatures are extracted for transmissions. First, a novel approach of semantic\nfiltering is proposed to select and transmit the semantically critical features\nof images relevant to semantic label. By integrating the text prompt and\ncritical features, the receiver reconstructs high-fidelity images using a\ndiffusion-based generative model. Next, we propose the generative visual\ninformation fidelity (GVIF) metric to evaluate the visual quality of the\ngenerated image. By characterizing the statistical models of image features,\nthe GVIF metric quantifies the mutual information between the distorted\nfeatures and their original counterparts. By maximizing the GVIF metric, we\ndesign a channel-adaptive Gen-SemCom system that adaptively control the volume\nof features and compression rate according to the channel state. Experimental\nresults validate the GVIF metric's sensitivity to visual fidelity, correlating\nwith both the PSNR and critical information volume. In addition, the optimized\nsystem achieves superior performance over benchmarking schemes in terms of\nhigher PSNR and lower FID scores.\n","authors":["Jianhao Huang","Qunsong Zeng","Kaibin Huang"],"pdf_url":"https://arxiv.org/pdf/2505.10405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16147v2","updated":"2025-05-15T15:11:23Z","published":"2024-12-20T18:50:54Z","title":"SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage\n  Estimation in the Wild","summary":"  Seagrass meadows play a crucial role in marine ecosystems, providing benefits\nsuch as carbon sequestration, water quality improvement, and habitat provision.\nMonitoring the distribution and abundance of seagrass is essential for\nenvironmental impact assessments and conservation efforts. However, the current\nmanual methods of analyzing underwater video data to assess seagrass coverage\nare time-consuming and subjective. This work explores the use of deep learning\nmodels to automate the process of seagrass detection and coverage estimation\nfrom underwater video data. We create a new dataset of over 8,300 annotated\nunderwater images, and subsequently evaluate several deep learning\narchitectures, including ResNet, InceptionNetV3, DenseNet, and Vision\nTransformer for the task of binary classification on the presence and absence\nof seagrass by transfer learning. The results demonstrate that deep learning\nmodels, particularly Vision Transformers, can achieve high performance in\npredicting eelgrass presence, with AUROC scores exceeding 0.95 on the final\ntest dataset. The application of underwater image enhancement further improved\nthe models' prediction capabilities. Furthermore, we introduce a novel approach\nfor estimating seagrass coverage from video data, showing promising preliminary\nresults that align with expert manual labels, and indicating potential for\nconsistent and scalable monitoring. The proposed methodology allows for the\nefficient processing of large volumes of video data, enabling the acquisition\nof much more detailed information on seagrass distributions in comparison to\ncurrent manual methods. This information is crucial for environmental impact\nassessments and monitoring programs, as seagrasses are important indicators of\ncoastal ecosystem health. This project demonstrates the value that deep\nlearning can bring to the field of marine ecology and environmental monitoring.\n","authors":["Jannik ElsÃ¤Ãer","Laura Weihl","Veronika Cheplygina","Lisbeth Tangaa Nielsen"],"pdf_url":"https://arxiv.org/pdf/2412.16147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04722v2","updated":"2025-05-15T15:09:00Z","published":"2025-04-07T04:21:31Z","title":"TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile\n  Graphics for Individuals with Vision Impairment","summary":"  Tactile graphics are essential for providing access to visual information for\nthe 43 million people globally living with vision loss. Traditional methods for\ncreating these graphics are labor-intensive and cannot meet growing demand. We\nintroduce TactileNet, the first comprehensive dataset and AI-driven framework\nfor generating embossing-ready 2D tactile templates using text-to-image Stable\nDiffusion (SD) models. By integrating Low-Rank Adaptation (LoRA) and\nDreamBooth, our method fine-tunes SD models to produce high-fidelity,\nguideline-compliant graphics while reducing computational costs. Quantitative\nevaluations with tactile experts show 92.86% adherence to accessibility\nstandards. Structural fidelity analysis revealed near-human design similarity,\nwith an SSIM of 0.538 between generated graphics and expert-designed tactile\nimages. Notably, our method preserves object silhouettes better than human\ndesigns (SSIM = 0.259 vs. 0.215 for binary masks), addressing a key limitation\nof manual tactile abstraction. The framework scales to 32,000 images (7,050\nhigh-quality) across 66 classes, with prompt editing enabling customizable\noutputs (e.g., adding or removing details). By automating the 2D template\ngeneration step-compatible with standard embossing workflows-TactileNet\naccelerates production while preserving design flexibility. This work\ndemonstrates how AI can augment (not replace) human expertise to bridge the\naccessibility gap in education and beyond. Code, data, and models will be\npublicly released to foster further research.\n","authors":["Adnan Khan","Alireza Choubineh","Mai A. Shaaban","Abbas Akkasi","Majid Komeili"],"pdf_url":"https://arxiv.org/pdf/2504.04722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20291v2","updated":"2025-05-15T15:06:46Z","published":"2025-03-26T07:33:36Z","title":"CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at\n  Intermediate Resolution with Structure-Aware Multimodal U-Nets","summary":"  Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at\nintermediate resolution (4-8 {\\AA}) is crucial in protein structure\ndetermination. Recent advances in deep learning have led to the development of\nautomated approaches for enhancing experimental cryo-EM density maps. Yet,\nthese methods are not optimized for intermediate-resolution maps and rely on\nmap density features alone. To address this, we propose CryoSAMU, a novel\nmethod designed to enhance 3D cryo-EM density maps of protein structures using\nstructure-aware multimodal U-Nets and trained on curated\nintermediate-resolution density maps. We comprehensively evaluate CryoSAMU\nacross various metrics and demonstrate its competitive performance compared to\nstate-of-the-art methods. Notably, CryoSAMU achieves significantly faster\nprocessing speed, showing promise for future practical applications. Our code\nis available at https://github.com/chenwei-zhang/CryoSAMU.\n","authors":["Chenwei Zhang","Khanh Dao Duc"],"pdf_url":"https://arxiv.org/pdf/2503.20291v2.pdf","comment":"19 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4\n  supplementary tables"},{"id":"http://arxiv.org/abs/2505.07119v2","updated":"2025-05-15T15:05:10Z","published":"2025-05-11T21:05:33Z","title":"Towards Scalable IoT Deployment for Visual Anomaly Detection via\n  Efficient Compression","summary":"  Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing operational costs is essential. Deploying deep learning models\nwithin Internet of Things (IoT) environments introduces specific challenges due\nto limited computational power and bandwidth of edge devices. This study\ninvestigates how to perform VAD effectively under such constraints by\nleveraging compact, efficient processing strategies. We evaluate several data\ncompression techniques, examining the tradeoff between system latency and\ndetection accuracy. Experiments on the MVTec AD benchmark demonstrate that\nsignificant compression can be achieved with minimal loss in anomaly detection\nperformance compared to uncompressed data. Current results show up to 80%\nreduction in end-to-end inference time, including edge processing,\ntransmission, and server computation.\n","authors":["Arianna Stropeni","Francesco Borsatti","Manuel Barusco","Davide Dalle Pezze","Marco Fabris","Gian Antonio Susto"],"pdf_url":"https://arxiv.org/pdf/2505.07119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08046v2","updated":"2025-05-15T15:00:25Z","published":"2025-04-10T18:04:22Z","title":"Teaching Humans Subtle Differences with DIFFusion","summary":"  Scientific expertise often requires recognizing subtle visual differences\nthat remain challenging to articulate even for domain experts. We present a\nsystem that leverages generative models to automatically discover and visualize\nminimal discriminative features between categories while preserving instance\nidentity. Our method generates counterfactual visualizations with subtle,\ntargeted transformations between classes, performing well even in domains where\ndata is sparse, examples are unpaired, and category boundaries resist verbal\ndescription. Experiments across six domains, including black hole simulations,\nbutterfly taxonomy, and medical imaging, demonstrate accurate transitions with\nlimited training data, highlighting both established discriminative features\nand novel subtle distinctions that measurably improved category\ndifferentiation. User studies confirm our generated counterfactuals\nsignificantly outperform traditional approaches in teaching humans to correctly\ndifferentiate between fine-grained classes, showing the potential of generative\nmodels to advance visual learning and scientific research.\n","authors":["Mia Chiquier","Orr Avrech","Yossi Gandelsman","Berthy Feng","Katherine Bouman","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2504.08046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09681v5","updated":"2025-05-15T14:51:10Z","published":"2023-03-16T22:56:12Z","title":"Highly Efficient 3D Human Pose Tracking from Events with Spiking\n  Spatiotemporal Transformer","summary":"  Event camera, as an asynchronous vision sensor capturing scene dynamics,\npresents new opportunities for highly efficient 3D human pose tracking.\nExisting approaches typically adopt modern-day Artificial Neural Networks\n(ANNs), such as CNNs or Transformer, where sparse events are converted into\ndense images or paired with additional gray-scale images as input. Such\npractices, however, ignore the inherent sparsity of events, resulting in\nredundant computations, increased energy consumption, and potentially degraded\nperformance. Motivated by these observations, we introduce the first sparse\nSpiking Neural Networks (SNNs) framework for 3D human pose tracking based\nsolely on events. Our approach eliminates the need to convert sparse data to\ndense formats or incorporate additional images, thereby fully exploiting the\ninnate sparsity of input events. Central to our framework is a novel Spiking\nSpatiotemporal Transformer, which enables bi-directional spatiotemporal fusion\nof spike pose features and provides a guaranteed similarity measurement between\nbinary spike features in spiking attention. Moreover, we have constructed a\nlarge-scale synthetic dataset, SynEventHPD, that features a broad and diverse\nset of 3D human motions, as well as much longer hours of event streams.\nEmpirical experiments demonstrate the superiority of our approach over existing\nstate-of-the-art (SOTA) ANN-based methods, requiring only 19.1% FLOPs and 3.6%\nenergy cost. Furthermore, our approach outperforms existing SNN-based\nbenchmarks in this task, highlighting the effectiveness of our proposed SNN\nframework. The dataset will be released upon acceptance, and code can be found\nat https://github.com/JimmyZou/HumanPoseTracking_SNN.\n","authors":["Shihao Zou","Yuxuan Mu","Wei Ji","Zi-An Wang","Xinxin Zuo","Sen Wang","Weixin Si","Li Cheng"],"pdf_url":"https://arxiv.org/pdf/2303.09681v5.pdf","comment":"Accepted by IEEE TCSVT"},{"id":"http://arxiv.org/abs/2504.13231v2","updated":"2025-05-15T14:47:02Z","published":"2025-04-17T14:43:56Z","title":"WildFireCan-MMD: A Multimodal Dataset for Classification of\n  User-Generated Content During Wildfires in Canada","summary":"  Rapid information access is vital during wildfires, yet traditional data\nsources are slow and costly. Social media offers real-time updates, but\nextracting relevant insights remains a challenge. We present WildFireCan-MMD, a\nnew multimodal dataset of X posts from recent Canadian wildfires, annotated\nacross twelve key themes. Evaluating both vision-language models and\ncustom-trained classifiers, we show that while zero-shot prompting offers quick\ndeployment, even simple trained models outperform them when labelled data is\navailable. Our best-performing transformer-based fine-tuned model reaches 83%\nf-score, outperforming gpt4 by 23%. As a use case, we demonstrate how this\nmodel can be used to uncover trends during wildfires. Our findings highlight\nthe enduring importance of tailored datasets and task-specific training.\nImportantly, such datasets should be localized, as disaster response\nrequirements vary across regions and contexts.\n","authors":["Braeden Sherritt","Isar Nejadgholi","Marzieh Amini"],"pdf_url":"https://arxiv.org/pdf/2504.13231v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10352v1","updated":"2025-05-15T14:43:35Z","published":"2025-05-15T14:43:35Z","title":"SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with\n  Hamming Attention and $\\mathcal{O}(T)$ Complexity","summary":"  Spiking Neural Networks (SNNs) have shown competitive performance to\nArtificial Neural Networks (ANNs) in various vision tasks, while offering\nsuperior energy efficiency. However, existing SNN-based Transformers primarily\nfocus on single-image tasks, emphasizing spatial features while not effectively\nleveraging SNNs' efficiency in video-based vision tasks. In this paper, we\nintroduce SpikeVideoFormer, an efficient spike-driven video Transformer,\nfeaturing linear temporal complexity $\\mathcal{O}(T)$. Specifically, we design\na spike-driven Hamming attention (SDHA) which provides a theoretically guided\nadaptation from traditional real-valued attention to spike-driven attention.\nBuilding on SDHA, we further analyze various spike-driven space-time attention\ndesigns and identify an optimal scheme that delivers appealing performance for\nvideo tasks, while maintaining only linear temporal complexity. The\ngeneralization ability and efficiency of our model are demonstrated across\ndiverse downstream video tasks, including classification, human pose tracking,\nand semantic segmentation. Empirical results show our method achieves\nstate-of-the-art (SOTA) performance compared to existing SNN approaches, with\nover 15\\% improvement on the latter two tasks. Additionally, it matches the\nperformance of recent ANN-based methods while offering significant efficiency\ngains, achieving $\\times 16$, $\\times 10$ and $\\times 5$ improvements on the\nthree tasks. https://github.com/JimmyZou/SpikeVideoFormer\n","authors":["Shihao Zou","Qingfeng Li","Wei Ji","Jingjing Li","Yongkui Yang","Guoqi Li","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2505.10352v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2505.10351v1","updated":"2025-05-15T14:43:34Z","published":"2025-05-15T14:43:34Z","title":"A Unified and Scalable Membership Inference Method for Visual\n  Self-supervised Encoder via Part-aware Capability","summary":"  Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we perform membership inference on visual self-supervised models\nin a more realistic setting: self-supervised training method and details are\nunknown for an adversary when attacking as he usually faces a black-box system\nin practice. In this setting, considering that self-supervised model could be\ntrained by completely different self-supervised paradigms, e.g., masked image\nmodeling and contrastive learning, with complex training details, we propose a\nunified membership inference method called PartCrop. It is motivated by the\nshared part-aware capability among models and stronger part response on the\ntraining data. Specifically, PartCrop crops parts of objects in an image to\nquery responses within the image in representation space. We conduct extensive\nattacks on self-supervised models with different training protocols and\nstructures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Finally, besides\nprototype testing on toy visual encoders and small-scale image datasets, we\nquantitatively study the impacts of scaling from both data and model aspects in\na realistic scenario and propose a scalable PartCrop-v2 by introducing two\nstructural improvements to PartCrop. Our code is at\nhttps://github.com/JiePKU/PartCrop.\n","authors":["Jie Zhu","Jirong Zha","Ding Li","Leye Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10351v1.pdf","comment":"An extension of our ACM CCS2024 conference paper (arXiv:2404.02462).\n  We show the impacts of scaling from both data and model aspects on membership\n  inference for self-supervised visual encoders"},{"id":"http://arxiv.org/abs/2405.03689v2","updated":"2025-05-15T14:35:43Z","published":"2024-05-06T17:59:36Z","title":"Pose Priors from Language Models","summary":"  Language is often used to describe physical interaction, yet most 3D human\npose estimation methods overlook this rich source of information. We bridge\nthis gap by leveraging large multimodal models (LMMs) as priors for\nreconstructing contact poses, offering a scalable alternative to traditional\nmethods that rely on human annotations or motion capture data. Our approach\nextracts contact-relevant descriptors from an LMM and translates them into\ntractable losses to constrain 3D human pose optimization. Despite its\nsimplicity, our method produces compelling reconstructions for both two-person\ninteractions and self-contact scenarios, accurately capturing the semantics of\nphysical and social interactions. Our results demonstrate that LMMs can serve\nas powerful tools for contact prediction and pose estimation, offering an\nalternative to costly manual human annotations or motion capture data. Our code\nis publicly available at https://prosepose.github.io.\n","authors":["Sanjay Subramanian","Evonne Ng","Lea MÃ¼ller","Dan Klein","Shiry Ginosar","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2405.03689v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2307.09420v2","updated":"2025-05-15T14:30:03Z","published":"2023-07-18T16:37:37Z","title":"Measuring Student Behavioral Engagement using Histogram of Actions","summary":"  In this paper, we propose a novel technique for measuring behavioral\nengagement through students' actions recognition. The proposed approach\nrecognizes student actions then predicts the student behavioral engagement\nlevel. For student action recognition, we use human skeletons to model student\npostures and upper body movements. To learn the dynamics of student upper body,\na 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions\nwithin every 2minute video segment then these actions are used to build a\nhistogram of actions which encodes the student actions and their frequencies.\nThis histogram is utilized as an input to SVM classifier to classify whether\nthe student is engaged or disengaged. To evaluate the proposed framework, we\nbuild a dataset consisting of 1414 2-minute video segments annotated with 13\nactions and 112 video segments annotated with two engagement levels.\nExperimental results indicate that student actions can be recognized with top 1\naccuracy 83.63% and the proposed framework can capture the average engagement\nof the class.\n","authors":["Ahmed Abdelkawy","Aly Farag","Islam Alkabbany","Asem Ali","Chris Foreman","Thomas Tretter","Nicholas Hindy"],"pdf_url":"https://arxiv.org/pdf/2307.09420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03093v2","updated":"2025-05-15T14:24:44Z","published":"2025-05-06T01:09:07Z","title":"Estimating the Diameter at Breast Height of Trees in a Forest With a\n  Single 360 Camera","summary":"  Forest inventories rely on accurate measurements of the diameter at breast\nheight (DBH) for ecological monitoring, resource management, and carbon\naccounting. While LiDAR-based techniques can achieve centimeter-level\nprecision, they are cost-prohibitive and operationally complex. We present a\nlow-cost alternative that only needs a consumer-grade 360 video camera. Our\nsemi-automated pipeline comprises of (i) a dense point cloud reconstruction\nusing Structure from Motion (SfM) photogrammetry software called Agisoft\nMetashape, (ii) semantic trunk segmentation by projecting Grounded Segment\nAnything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based\ntechnique to estimate cross section shape and DBH. We introduce an interactive\nvisualization tool for inspecting segmented trees and their estimated DBH. On\n61 acquisitions of 43 trees under a variety of conditions, our method attains\nmedian absolute relative errors of 5-9% with respect to \"ground-truth\" manual\nmeasurements. This is only 2-4% higher than LiDAR-based estimates, while\nemploying a single 360 camera that costs orders of magnitude less, requires\nminimal setup, and is widely available.\n","authors":["Siming He","Zachary Osman","Fernando Cladera","Dexter Ong","Nitant Rai","Patrick Corey Green","Vijay Kumar","Pratik Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2505.03093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10312v1","updated":"2025-05-15T13:56:14Z","published":"2025-05-15T13:56:14Z","title":"SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human\n  Activity Recognition","summary":"  In the realm of Human Activity Recognition (HAR), obtaining high quality and\nvariance data is still a persistent challenge due to high costs and the\ninherent variability of real-world activities. This study introduces a\ngeneration dataset by deep learning approaches (Attention Autoencoder and\nconditional Generative Adversarial Networks). Another problem that data\nheterogeneity is a critical challenge, one of the solutions is to shuffle the\ndata to homogenize the distribution. Experimental results demonstrate that the\nrandom sequence strategy significantly improves classification performance,\nachieving an accuracy of up to 0.70 $\\pm$ 0.03 and a macro F1 score of 0.64\n$\\pm$ 0.01. For that, disrupting temporal dependencies through random sequence\nreordering compels the model to focus on instantaneous recognition, thereby\nimproving robustness against activity transitions. This approach not only\nbroadens the effective training dataset but also offers promising avenues for\nenhancing HAR systems in complex, real-world scenarios.\n","authors":["Anh Tuan Ha","Hoang Khang Phan","Thai Minh Tien Ngo","Anh Phan Truong","Nhat Tan Le"],"pdf_url":"https://arxiv.org/pdf/2505.10312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10294v1","updated":"2025-05-15T13:42:48Z","published":"2025-05-15T13:42:48Z","title":"MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images\n  using ViT Foundation Models","summary":"  Histopathological analysis is a cornerstone of cancer diagnosis, with\nHematoxylin and Eosin (H&E) staining routinely acquired for every patient to\nvisualize cell morphology and tissue architecture. On the other hand, multiplex\nimmunofluorescence (mIF) enables more precise cell type identification via\nproteomic markers, but has yet to achieve widespread clinical adoption due to\ncost and logistical constraints. To bridge this gap, we introduce MIPHEI\n(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired\narchitecture that integrates state-of-the-art ViT foundation models as encoders\nto predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of\nmarkers spanning nuclear content, immune lineages (T cells, B cells, myeloid),\nepithelium, stroma, vasculature, and proliferation. We train our model using\nthe publicly available ORION dataset of restained H&E and mIF images from\ncolorectal cancer tissue, and validate it on two independent datasets. MIPHEI\nachieves accurate cell-type classification from H&E alone, with F1 scores of\n0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,\nsubstantially outperforming both a state-of-the-art baseline and a random\nclassifier for most markers. Our results indicate that our model effectively\ncaptures the complex relationships between nuclear morphologies in their tissue\ncontext, as visible in H&E images and molecular markers defining specific cell\ntypes. MIPHEI offers a promising step toward enabling cell-type-aware analysis\nof large-scale H&E datasets, in view of uncovering relationships between\nspatial cellular organization and patient outcomes.\n","authors":["Guillaume Balezo","Roger Trullo","Albert Pla Planas","Etienne Decenciere","Thomas Walter"],"pdf_url":"https://arxiv.org/pdf/2505.10294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10292v1","updated":"2025-05-15T13:42:14Z","published":"2025-05-15T13:42:14Z","title":"StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding\n  and Grounded Story Generation","summary":"  Visual storytelling systems struggle to maintain character identity across\nframes and link actions to appropriate subjects, frequently leading to\nreferential hallucinations. These issues can be addressed through grounding of\ncharacters, objects, and other entities on the visual elements. We propose\nStoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie\nimages, with both structured scene analyses and grounded stories. Each story\nmaintains character and object consistency across frames while explicitly\nmodeling multi-frame relationships through structured tabular representations.\nOur approach features cross-frame object re-identification using visual\nsimilarity and face recognition, chain-of-thought reasoning for explicit\nnarrative modeling, and a grounding scheme that links textual elements to\nvisual entities across multiple frames. We establish baseline performance by\nfine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end\nobject detection, re-identification, and landmark detection while maintaining\nconsistent object references throughout the story. Evaluation demonstrates a\nreduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when\ncompared to a non-fine-tuned model.\n","authors":["Daniel A. P. Oliveira","David Martins de Matos"],"pdf_url":"https://arxiv.org/pdf/2505.10292v1.pdf","comment":"31 pages, 14 figures"},{"id":"http://arxiv.org/abs/2505.10289v1","updated":"2025-05-15T13:36:42Z","published":"2025-05-15T13:36:42Z","title":"MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot\n  Learning","summary":"  Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object\ncombinations by leveraging known combinations. Existing studies basically rely\non the cross-modal alignment capabilities of CLIP but tend to overlook its\nlimitations in capturing fine-grained local features, which arise from its\narchitectural and training paradigm. To address this issue, we propose a\nMulti-Stage Cross-modal Interaction (MSCI) model that effectively explores and\nutilizes intermediate-layer information from CLIP's visual encoder.\nSpecifically, we design two self-adaptive aggregators to extract local\ninformation from low-level visual features and integrate global information\nfrom high-level visual features, respectively. These key information are\nprogressively incorporated into textual representations through a\nstage-by-stage interaction mechanism, significantly enhancing the model's\nperception capability for fine-grained local visual information. Additionally,\nMSCI dynamically adjusts the attention weights between global and local visual\ninformation based on different combinations, as well as different elements\nwithin the same combination, allowing it to flexibly adapt to diverse\nscenarios. Experiments on three widely used datasets fully validate the\neffectiveness and superiority of the proposed model. Data and code are\navailable at https://github.com/ltpwy/MSCI.\n","authors":["Yue Wang","Shuai Xu","Xuelin Zhu","Yicong Li"],"pdf_url":"https://arxiv.org/pdf/2505.10289v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.10281v1","updated":"2025-05-15T13:29:40Z","published":"2025-05-15T13:29:40Z","title":"MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global\n  Marine Fog Detection and Forecasting","summary":"  Deep learning approaches for marine fog detection and forecasting have\noutperformed traditional methods, demonstrating significant scientific and\npractical importance. However, the limited availability of open-source datasets\nremains a major challenge. Existing datasets, often focused on a single region\nor satellite, restrict the ability to evaluate model performance across diverse\nconditions and hinder the exploration of intrinsic marine fog characteristics.\nTo address these limitations, we introduce \\textbf{MFogHub}, the first\nmulti-regional and multi-satellite dataset to integrate annotated marine fog\nobservations from 15 coastal fog-prone regions and six geostationary\nsatellites, comprising over 68,000 high-resolution samples. By encompassing\ndiverse regions and satellite perspectives, MFogHub facilitates rigorous\nevaluation of both detection and forecasting methods under varying conditions.\nExtensive experiments with 16 baseline models demonstrate that MFogHub can\nreveal generalization fluctuations due to regional and satellite discrepancy,\nwhile also serving as a valuable resource for the development of targeted and\nscalable fog prediction techniques. Through MFogHub, we aim to advance both the\npractical monitoring and scientific understanding of marine fog dynamics on a\nglobal scale. The dataset and code are at\n\\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.\n","authors":["Mengqiu Xu","Kaixin Chen","Heng Guo","Yixiang Huang","Ming Wu","Zhenwei Shi","Chuang Zhang","Jun Guo"],"pdf_url":"https://arxiv.org/pdf/2505.10281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10271v1","updated":"2025-05-15T13:22:20Z","published":"2025-05-15T13:22:20Z","title":"RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall\n  Probabilities Over 8 Hours","summary":"  We present a deep learning model for high-resolution probabilistic\nprecipitation forecasting over an 8-hour horizon in Europe, overcoming the\nlimitations of radar-only deep learning models with short forecast lead times.\nOur model efficiently integrates multiple data sources - including radar,\nsatellite, and physics-based numerical weather prediction (NWP) - while\ncapturing long-range interactions, resulting in accurate forecasts with robust\nuncertainty quantification through consistent probabilistic maps. Featuring a\ncompact architecture, it enables more efficient training and faster inference\nthan existing models. Extensive experiments demonstrate that our model\nsurpasses current operational NWP systems, extrapolation-based methods, and\ndeep-learning nowcasting models, setting a new standard for high-resolution\nprecipitation forecasting in Europe, ensuring a balance between accuracy,\ninterpretability, and computational efficiency.\n","authors":["Rafael Pablos Sarabia","Joachim Nyborg","Morten Birk","Jeppe Liborius SjÃ¸rup","Anders Lillevang Vesterholt","Ira Assent"],"pdf_url":"https://arxiv.org/pdf/2505.10271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10267v1","updated":"2025-05-15T13:18:37Z","published":"2025-05-15T13:18:37Z","title":"HandReader: Advanced Techniques for Efficient Fingerspelling Recognition","summary":"  Fingerspelling is a significant component of Sign Language (SL), allowing the\ninterpretation of proper names, characterized by fast hand movements during\nsigning. Although previous works on fingerspelling recognition have focused on\nprocessing the temporal dimension of videos, there remains room for improving\nthe accuracy of these approaches. This paper introduces HandReader, a group of\nthree architectures designed to address the fingerspelling recognition task.\nHandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to\nprocess RGB features from videos of varying lengths while preserving important\nsequential information. HandReader$_{KP}$ is built on the proposed Temporal\nPose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition\nin a batch allows the encoder to pass them through 2D and 3D convolution\nlayers, utilizing temporal and spatial information and accumulating keypoints\ncoordinates. We also introduce HandReader_RGB+KP - architecture with a joint\nencoder to benefit from RGB and keypoint modalities. Each HandReader model\npossesses distinct advantages and achieves state-of-the-art results on the\nChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate\nhigh performance on the first open dataset for Russian fingerspelling, Znaki,\npresented in this paper. The Znaki dataset and HandReader pre-trained models\nare publicly available.\n","authors":["Pavel Korotaev","Petr Surovtsev","Alexander Kapitanov","Karina Kvanchiani","Aleksandr Nagaev"],"pdf_url":"https://arxiv.org/pdf/2505.10267v1.pdf","comment":"https://github.com/ai-forever/handreader"},{"id":"http://arxiv.org/abs/2505.10258v1","updated":"2025-05-15T13:09:19Z","published":"2025-05-15T13:09:19Z","title":"Inferring Driving Maps by Deep Learning-based Trail Map Extraction","summary":"  High-definition (HD) maps offer extensive and accurate environmental\ninformation about the driving scene, making them a crucial and essential\nelement for planning within autonomous driving systems. To avoid extensive\nefforts from manual labeling, methods for automating the map creation have\nemerged. Recent trends have moved from offline mapping to online mapping,\nensuring availability and actuality of the utilized maps. While the performance\nhas increased in recent years, online mapping still faces challenges regarding\ntemporal consistency, sensor occlusion, runtime, and generalization. We propose\na novel offline mapping approach that integrates trails - informal routes used\nby drivers - into the map creation process. Our method aggregates trail data\nfrom the ego vehicle and other traffic participants to construct a\ncomprehensive global map using transformer-based deep learning models. Unlike\ntraditional offline mapping, our approach enables continuous updates while\nremaining sensor-agnostic, facilitating efficient data transfer. Our method\ndemonstrates superior performance compared to state-of-the-art online mapping\napproaches, achieving improved generalization to previously unseen environments\nand sensor configurations. We validate our approach on two benchmark datasets,\nhighlighting its robustness and applicability in autonomous driving systems.\n","authors":["Michael Hubbertz","Pascal Colling","Qi Han","Tobias Meisen"],"pdf_url":"https://arxiv.org/pdf/2505.10258v1.pdf","comment":"This paper was accepted at the CVPR WAD 2025 Workshop"},{"id":"http://arxiv.org/abs/2505.10257v1","updated":"2025-05-15T13:08:44Z","published":"2025-05-15T13:08:44Z","title":"Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot","summary":"  The intelligent driving cockpit, an important part of intelligent driving,\nneeds to match different users' comfort, interaction, and safety needs. This\npaper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.\nSage Deer achieves three highlights: (1) Super alignment: It achieves different\nreactions according to different people's preferences and biases. (2)\nGeneralist: It can understand the multi-view and multi-mode inputs to reason\nthe user's physiological indicators, facial emotions, hand movements, body\nmovements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It\ncan elicit implicit thought chains in the language space to further increase\ngeneralist and super-aligned abilities. Besides, we collected multiple data\nsets and built a large-scale benchmark. This benchmark measures the deer's\nperceptual decision-making ability and the super alignment's accuracy.\n","authors":["Hao Lu","Jiaqi Tang","Jiyao Wang","Yunfan LU","Xu Cao","Qingyong Hu","Yin Wang","Yuting Zhang","Tianxin Xie","Yunpeng Zhang","Yong Chen","Jiayu. Gao","Bin Huang","Dengbo He","Shuiguang Deng","Hao Chen","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2505.10257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10250v1","updated":"2025-05-15T13:04:51Z","published":"2025-05-15T13:04:51Z","title":"ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct\n  Preference Optimization","summary":"  Human mesh recovery (HMR) from a single image is inherently ill-posed due to\ndepth ambiguity and occlusions. Probabilistic methods have tried to solve this\nby generating numerous plausible 3D human mesh predictions, but they often\nexhibit misalignment with 2D image observations and weak robustness to\nin-the-wild images. To address these issues, we propose ADHMR, a framework that\nAligns a Diffusion-based HMR model in a preference optimization manner. First,\nwe train a human mesh prediction assessment model, HMR-Scorer, capable of\nevaluating predictions even for in-the-wild images without 3D annotations. We\nthen use HMR-Scorer to create a preference dataset, where each input image has\na pair of winner and loser mesh predictions. This dataset is used to finetune\nthe base model using direct preference optimization. Moreover, HMR-Scorer also\nhelps improve existing HMR models by data cleaning, even with fewer training\nsamples. Extensive experiments show that ADHMR outperforms current\nstate-of-the-art methods. Code is available at:\nhttps://github.com/shenwenhao01/ADHMR.\n","authors":["Wenhao Shen","Wanqi Yin","Xiaofeng Yang","Cheng Chen","Chaoyue Song","Zhongang Cai","Lei Yang","Hao Wang","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2505.10250v1.pdf","comment":"Accepted by ICML 2025. Code: https://github.com/shenwenhao01/ADHMR"},{"id":"http://arxiv.org/abs/2410.01262v3","updated":"2025-05-15T12:59:09Z","published":"2024-10-02T06:16:06Z","title":"Improving Fine-Grained Control via Aggregation of Multiple Diffusion\n  Models","summary":"  While many diffusion models perform well when controlling for particular\naspect among style, character, and interaction, they struggle with fine-grained\ncontrol due to dataset limitations and intricate model architecture design.\nThis paper first introduces a novel training-free algorithm in fine-grained\ngeneration, Aggregation of Multiple Diffusion Models (AMDM), which integrates\nfeatures from multiple diffusion models into a specified model to activate\nspecific features and enable fine-grained control. Experimental results\ndemonstrate that AMDM significantly improves fine-grained control without\ntraining, validating its effectiveness. Additionally, it reveals that diffusion\nmodels initially focus on features such as position, attributes, and style,\nwith later stages improving generation quality and consistency. AMDM offers a\nnew perspective for tackling the challenges of fine-grained conditional control\ngeneration in diffusion models: We can fully utilize existing or develop new\nconditional diffusion models that control specific aspects, and then aggregate\nthem using AMDM algorithm. This eliminates the need for constructing complex\ndatasets, designing intricate model architectures, and incurring high training\ncosts. Code is available at: https://github.com/Hammour-steak/AMDM.\n","authors":["Conghan Yue","Zhengwei Peng","Shiyan Du","Zhi Ji","Chuangjian Cai","Le Wan","Dongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01262v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10238v1","updated":"2025-05-15T12:50:29Z","published":"2025-05-15T12:50:29Z","title":"MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation","summary":"  Human image animation has gained increasing attention and developed rapidly\ndue to its broad applications in digital humans. However, existing methods rely\nlargely on 2D-rendered pose images for motion guidance, which limits\ngeneralization and discards essential 3D information for open-world animation.\nTo tackle this problem, we propose MTVCrafter (Motion Tokenization Video\nCrafter), the first framework that directly models raw 3D motion sequences\n(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT\n(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.\nCompared to 2D-rendered pose images, 4D motion tokens offer more robust\nspatio-temporal cues and avoid strict pixel-level alignment between pose image\nand character, enabling more flexible and disentangled control. Then, we\nintroduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention\nwith 4D positional encodings, MV-DiT can effectively leverage motion tokens as\n4D compact yet expressive context for human image animation in the complex 3D\nworld. Hence, it marks a significant step forward in this field and opens a new\ndirection for pose-guided human video generation. Experiments show that our\nMTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,\nsurpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter\nalso generalizes well to diverse open-world characters (single/multiple,\nfull/half-body) across various styles and scenarios. Our video demos and code\nare provided in the supplementary material and at this anonymous GitHub link:\nhttps://anonymous.4open.science/r/MTVCrafter-1B13.\n","authors":["Yanbo Ding"],"pdf_url":"https://arxiv.org/pdf/2505.10238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09178v2","updated":"2025-05-15T12:49:27Z","published":"2025-05-14T06:21:27Z","title":"UniCAD: Efficient and Extendable Architecture for Multi-Task\n  Computer-Aided Diagnosis System","summary":"  The growing complexity and scale of visual model pre-training have made\ndeveloping and deploying multi-task computer-aided diagnosis (CAD) systems\nincreasingly challenging and resource-intensive. Furthermore, the medical\nimaging community lacks an open-source CAD platform to enable the rapid\ncreation of efficient and extendable diagnostic models. To address these\nissues, we propose UniCAD, a unified architecture that leverages the robust\ncapabilities of pre-trained vision foundation models to seamlessly handle both\n2D and 3D medical images while requiring only minimal task-specific parameters.\nUniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation\nstrategy is employed to adapt a pre-trained visual model to the medical image\ndomain, achieving performance on par with fully fine-tuned counterparts while\nintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular\narchitecture that combines a frozen foundation model with multiple\nplug-and-play experts, enabling diverse tasks and seamless functionality\nexpansion. Building on this unified CAD architecture, we establish an\nopen-source platform where researchers can share and access lightweight CAD\nexperts, fostering a more equitable and efficient research ecosystem.\nComprehensive experiments across 12 diverse medical datasets demonstrate that\nUniCAD consistently outperforms existing methods in both accuracy and\ndeployment efficiency. The source code and project page are available at\nhttps://mii-laboratory.github.io/UniCAD/.\n","authors":["Yitao Zhu","Yuan Yin","Zhenrong Shen","Zihao Zhao","Haiyu Song","Sheng Wang","Dinggang Shen","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2505.09178v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2505.10231v1","updated":"2025-05-15T12:43:23Z","published":"2025-05-15T12:43:23Z","title":"On the Interplay of Human-AI Alignment,Fairness, and Performance\n  Trade-offs in Medical Imaging","summary":"  Deep neural networks excel in medical imaging but remain prone to biases,\nleading to fairness gaps across demographic groups. We provide the first\nsystematic exploration of Human-AI alignment and fairness in this domain. Our\nresults show that incorporating human insights consistently reduces fairness\ngaps and enhances out-of-domain generalization, though excessive alignment can\nintroduce performance trade-offs, emphasizing the need for calibrated\nstrategies. These findings highlight Human-AI alignment as a promising approach\nfor developing fair, robust, and generalizable medical AI systems, striking a\nbalance between expert guidance and automated efficiency. Our code is available\nat https://github.com/Roypic/Aligner.\n","authors":["Haozhe Luo","Ziyu Zhou","Zixin Shu","AurÃ©lie Pahud de Mortanges","Robert Berke","Mauricio Reyes"],"pdf_url":"https://arxiv.org/pdf/2505.10231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10223v1","updated":"2025-05-15T12:32:02Z","published":"2025-05-15T12:32:02Z","title":"Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution\n  Generalisation in MRI Segmentation","summary":"  Medical image segmentation models are often trained on curated datasets,\nleading to performance degradation when deployed in real-world clinical\nsettings due to mismatches between training and test distributions. While data\naugmentation techniques are widely used to address these challenges,\ntraditional visually consistent augmentation strategies lack the robustness\nneeded for diverse real-world scenarios. In this work, we systematically\nevaluate alternative augmentation strategies, focusing on MixUp and Auxiliary\nFourier Augmentation. These methods mitigate the effects of multiple variations\nwithout explicitly targeting specific sources of distribution shifts. We\ndemonstrate how these techniques significantly improve out-of-distribution\ngeneralization and robustness to imaging variations across a wide range of\ntransformations in cardiac cine MRI and prostate MRI segmentation. We\nquantitatively find that these augmentation methods enhance learned feature\nrepresentations by promoting separability and compactness. Additionally, we\nhighlight how their integration into nnU-Net training pipelines provides an\neasy-to-implement, effective solution for enhancing the reliability of medical\nsegmentation models in real-world applications.\n","authors":["Puru Vaish","Felix Meister","Tobias Heimann","Christoph Brune","Jelmer M. Wolterink"],"pdf_url":"https://arxiv.org/pdf/2505.10223v1.pdf","comment":"Accepted at MIDL 2025"},{"id":"http://arxiv.org/abs/2410.11758v2","updated":"2025-05-15T12:13:37Z","published":"2024-10-15T16:28:09Z","title":"Latent Action Pretraining from Videos","summary":"  We introduce Latent Action Pretraining for general Action models (LAPA), an\nunsupervised method for pretraining Vision-Language-Action (VLA) models without\nground-truth robot action labels. Existing Vision-Language-Action models\nrequire action labels typically collected by human teleoperators during\npretraining, which significantly limits possible data sources and scale. In\nthis work, we propose a method to learn from internet-scale videos that do not\nhave robot action labels. We first train an action quantization model\nleveraging VQ-VAE-based objective to learn discrete latent actions between\nimage frames, then pretrain a latent VLA model to predict these latent actions\nfrom observations and task descriptions, and finally finetune the VLA on\nsmall-scale robot manipulation data to map from latent to robot actions.\nExperimental results demonstrate that our method significantly outperforms\nexisting techniques that train robot manipulation policies from large-scale\nvideos. Furthermore, it outperforms the state-of-the-art VLA model trained with\nrobotic action labels on real-world manipulation tasks that require language\nconditioning, generalization to unseen objects, and semantic generalization to\nunseen instructions. Training only on human manipulation videos also shows\npositive transfer, opening up the potential for leveraging web-scale data for\nrobotics foundation model.\n","authors":["Seonghyeon Ye","Joel Jang","Byeongguk Jeon","Sejune Joo","Jianwei Yang","Baolin Peng","Ajay Mandlekar","Reuben Tan","Yu-Wei Chao","Bill Yuchen Lin","Lars Liden","Kimin Lee","Jianfeng Gao","Luke Zettlemoyer","Dieter Fox","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2410.11758v2.pdf","comment":"ICLR 2025 Website: https://latentactionpretraining.github.io"},{"id":"http://arxiv.org/abs/2505.10205v1","updated":"2025-05-15T12:03:05Z","published":"2025-05-15T12:03:05Z","title":"VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume\n  Estimation","summary":"  Accurate food volume estimation is crucial for medical nutrition management\nand health monitoring applications, but current food volume estimation methods\nare often limited by mononuclear data, leveraging single-purpose hardware such\nas 3D scanners, gathering sensor-oriented information such as depth\ninformation, or relying on camera calibration using a reference object. In this\npaper, we present VolE, a novel framework that leverages mobile device-driven\n3D reconstruction to estimate food volume. VolE captures images and camera\nlocations in free motion to generate precise 3D models, thanks to AR-capable\nmobile devices. To achieve real-world measurement, VolE is a reference- and\ndepth-free framework that leverages food video segmentation for food mask\ngeneration. We also introduce a new food dataset encompassing the challenging\nscenarios absent in the previous benchmarks. Our experiments demonstrate that\nVolE outperforms the existing volume estimation techniques across multiple\ndatasets by achieving 2.22 % MAPE, highlighting its superior performance in\nfood volume estimation.\n","authors":["Umair Haroon","Ahmad AlMughrabi","Thanasis Zoumpekas","Ricardo Marques","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2505.10205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10169v1","updated":"2025-05-15T10:55:47Z","published":"2025-05-15T10:55:47Z","title":"Modeling Saliency Dataset Bias","summary":"  Recent advances in image-based saliency prediction are approaching gold\nstandard performance levels on existing benchmarks. Despite this success, we\nshow that predicting fixations across multiple saliency datasets remains\nchallenging due to dataset bias. We find a significant performance drop (around\n40%) when models trained on one dataset are applied to another. Surprisingly,\nincreasing dataset diversity does not resolve this inter-dataset gap, with\nclose to 60% attributed to dataset-specific biases. To address this remaining\ngeneralization gap, we propose a novel architecture extending a mostly\ndataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific\nparameters that govern interpretable mechanisms such as multi-scale structure,\ncenter bias, and fixation spread. Adapting only these parameters to new data\naccounts for more than 75% of the generalization gap, with a large fraction of\nthe improvement achieved with as few as 50 samples. Our model sets a new\nstate-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark\n(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from\nunrelated datasets, but with a substantial boost when adapting to the\nrespective training datasets. The model also provides valuable insights into\nspatial saliency properties, revealing complex multi-scale effects that combine\nboth absolute and relative sizes.\n","authors":["Matthias KÃ¼mmerer","Harneet Khanuja","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2505.10169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05957v2","updated":"2025-05-15T10:43:45Z","published":"2025-05-09T11:09:52Z","title":"Efficient Quantum Convolutional Neural Networks for Image\n  Classification: Overcoming Hardware Constraints","summary":"  While classical convolutional neural networks (CNNs) have revolutionized\nimage classification, the emergence of quantum computing presents new\nopportunities for enhancing neural network architectures. Quantum CNNs (QCNNs)\nleverage quantum mechanical properties and hold potential to outperform\nclassical approaches. However, their implementation on current noisy\nintermediate-scale quantum (NISQ) devices remains challenging due to hardware\nlimitations. In our research, we address this challenge by introducing an\nencoding scheme that significantly reduces the input dimensionality. We\ndemonstrate that a primitive QCNN architecture with 49 qubits is sufficient to\ndirectly process $28\\times 28$ pixel MNIST images, eliminating the need for\nclassical dimensionality reduction pre-processing. Additionally, we propose an\nautomated framework based on expressibility, entanglement, and complexity\ncharacteristics to identify the building blocks of QCNNs, parameterized quantum\ncircuits (PQCs). Our approach demonstrates advantages in accuracy and\nconvergence speed with a similar parameter count compared to both hybrid QCNNs\nand classical CNNs. We validated our experiments on IBM's Heron r2 quantum\nprocessor, achieving $96.08\\%$ classification accuracy, surpassing the\n$71.74\\%$ benchmark of traditional approaches under identical training\nconditions. These results represent one of the first implementations of image\nclassifications on real quantum hardware and validate the potential of quantum\ncomputing in this area.\n","authors":["Peter RÃ¶seler","Oliver Schaudt","Helmut Berg","Christian Bauckhage","Matthias Koch"],"pdf_url":"https://arxiv.org/pdf/2505.05957v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10152v1","updated":"2025-05-15T10:26:17Z","published":"2025-05-15T10:26:17Z","title":"Multi-Source Collaborative Style Augmentation and Domain-Invariant\n  Learning for Federated Domain Generalization","summary":"  Federated domain generalization aims to learn a generalizable model from\nmultiple decentralized source domains for deploying on the unseen target\ndomain. The style augmentation methods have achieved great progress on domain\ngeneralization. However, the existing style augmentation methods either explore\nthe data styles within isolated source domain or interpolate the style\ninformation across existing source domains under the data decentralization\nscenario, which leads to limited style space. To address this issue, we propose\na Multi-source Collaborative Style Augmentation and Domain-invariant learning\nmethod (MCSAD) for federated domain generalization. Specifically, we propose a\nmulti-source collaborative style augmentation module to generate data in the\nbroader style space. Furthermore, we conduct domain-invariant learning between\nthe original data and augmented data by cross-domain feature alignment within\nthe same class and classes relation ensemble distillation between different\nclasses to learn a domain-invariant model. By alternatively conducting\ncollaborative style augmentation and domain-invariant learning, the model can\ngeneralize well on unseen target domain. Extensive experiments on multiple\ndomain generalization datasets indicate that our method significantly\noutperforms the state-of-the-art federated domain generalization methods.\n","authors":["Yikang Wei"],"pdf_url":"https://arxiv.org/pdf/2505.10152v1.pdf","comment":"IJCAI 2025"},{"id":"http://arxiv.org/abs/2505.07344v2","updated":"2025-05-15T10:24:45Z","published":"2025-05-12T08:32:39Z","title":"Generative Pre-trained Autoregressive Diffusion Transformer","summary":"  In this work, we present GPDiT, a Generative Pre-trained Autoregressive\nDiffusion Transformer that unifies the strengths of diffusion and\nautoregressive modeling for long-range video synthesis, within a continuous\nlatent space. Instead of predicting discrete tokens, GPDiT autoregressively\npredicts future latent frames using a diffusion loss, enabling natural modeling\nof motion dynamics and semantic consistency across frames. This continuous\nautoregressive framework not only enhances generation quality but also endows\nthe model with representation capabilities. Additionally, we introduce a\nlightweight causal attention variant and a parameter-free rotation-based\ntime-conditioning mechanism, improving both the training and inference\nefficiency. Extensive experiments demonstrate that GPDiT achieves strong\nperformance in video generation quality, video representation ability, and\nfew-shot learning tasks, highlighting its potential as an effective framework\nfor video modeling in continuous space.\n","authors":["Yuan Zhang","Jiacheng Jiang","Guoqing Ma","Zhiying Lu","Haoyang Huang","Jianlong Yuan","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2505.07344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10144v1","updated":"2025-05-15T10:17:48Z","published":"2025-05-15T10:17:48Z","title":"VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality","summary":"  3D Gaussian Splatting (3DGS) has rapidly become a leading technique for\nnovel-view synthesis, providing exceptional performance through efficient\nsoftware-based GPU rasterization. Its versatility enables real-time\napplications, including on mobile and lower-powered devices. However, 3DGS\nfaces key challenges in virtual reality (VR): (1) temporal artifacts, such as\npopping during head movements, (2) projection-based distortions that result in\ndisturbing and view-inconsistent floaters, and (3) reduced framerates when\nrendering large numbers of Gaussians, falling below the critical threshold for\nVR. Compared to desktop environments, these issues are drastically amplified by\nlarge field-of-view, constant head movements, and high resolution of\nhead-mounted displays (HMDs). In this work, we introduce VRSplat: we combine\nand extend several recent advancements in 3DGS to address challenges of VR\nholistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal\nProjection can complement each other, by modifying the individual techniques\nand core 3DGS rasterizer. Additionally, we propose an efficient foveated\nrasterizer that handles focus and peripheral areas in a single GPU launch,\navoiding redundant computations and improving GPU utilization. Our method also\nincorporates a fine-tuning step that optimizes Gaussian parameters based on\nStopThePop depth evaluations and Optimal Projection. We validate our method\nthrough a controlled user study with 25 participants, showing a strong\npreference for VRSplat over other configurations of Mini-Splatting. VRSplat is\nthe first, systematically evaluated 3DGS approach capable of supporting modern\nVR applications, achieving 72+ FPS while eliminating popping and\nstereo-disrupting floaters.\n","authors":["Xuechang Tu","Lukas Radl","Michael Steiner","Markus Steinberger","Bernhard Kerbl","Fernando de la Torre"],"pdf_url":"https://arxiv.org/pdf/2505.10144v1.pdf","comment":"I3D'25 (PACMCGIT); Project Page: https://cekavis.site/VRSplat/"},{"id":"http://arxiv.org/abs/2401.14066v3","updated":"2025-05-15T10:04:37Z","published":"2024-01-25T10:42:09Z","title":"CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis with\n  Multimodal Diffusion","summary":"  Although remarkable progress has been made in image style transfer, style is\njust one of the components of artistic paintings. Directly transferring\nextracted style features to natural images often results in outputs with\nobvious synthetic traces. This is because key painting attributes including\nlayout, perspective, shape, and semantics often cannot be conveyed and\nexpressed through style transfer. Large-scale pretrained text-to-image\ngeneration models have demonstrated their capability to synthesize a vast\namount of high-quality images. However, even with extensive textual\ndescriptions, it is challenging to fully express the unique visual properties\nand details of paintings. Moreover, generic models often disrupt the overall\nartistic effect when modifying specific areas, making it more complicated to\nachieve a unified aesthetic in artworks. Our main novel idea is to integrate\nmultimodal semantic information as a synthesis guide into artworks, rather than\ntransferring style to the real world. We also aim to reduce the disruption to\nthe harmony of artworks while simplifying the guidance conditions.\nSpecifically, we propose an innovative multi-task unified framework called\nCreativeSynth, based on the diffusion model with the ability to coordinate\nmultimodal inputs. CreativeSynth combines multimodal features with customized\nattention mechanisms to seamlessly integrate real-world semantic content into\nthe art domain through Cross-Art-Attention for aesthetic maintenance and\nsemantic fusion. We demonstrate the results of our method across a wide range\nof different art categories, proving that CreativeSynth bridges the gap between\ngenerative models and artistic expression. Code and results are available at\nhttps://github.com/haha-lisa/CreativeSynth.\n","authors":["Nisha Huang","Weiming Dong","Yuxin Zhang","Fan Tang","Ronghui Li","Chongyang Ma","Xiu Li","Tong-Yee Lee","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2401.14066v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10124v1","updated":"2025-05-15T09:51:05Z","published":"2025-05-15T09:51:05Z","title":"IMITATE: Image Registration with Context for unknown time frame recovery","summary":"  In this paper, we formulate a novel image registration formalism dedicated to\nthe estimation of unknown condition-related images, based on two or more known\nimages and their associated conditions. We show how to practically model this\nformalism by using a new conditional U-Net architecture, which fully takes into\naccount the conditional information and does not need any fixed image. Our\nformalism is then applied to image moving tumors for radiotherapy treatment at\ndifferent breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal\nregions. This driving application is particularly complex as it requires to\nstitch a collection of sequential 2D slices into several 3D volumes at\ndifferent organ positions. Movement interpolation with standard methods then\ngenerates well known reconstruction artefacts in the assembled volumes due to\nirregular patient breathing, hysteresis and poor correlation of breathing\nsignal to internal motion. Results obtained on 4D-CT clinical data showcase\nartefact-free volumes achieved through real-time latencies. The code is\npublicly available at https://github.com/Kheil-Z/IMITATE .\n","authors":["Ziad Kheil","Lucas Robinet","Laurent Risser","Soleakhena Ken"],"pdf_url":"https://arxiv.org/pdf/2505.10124v1.pdf","comment":"IEEE ISBI 2025"},{"id":"http://arxiv.org/abs/2504.02522v2","updated":"2025-05-15T09:48:06Z","published":"2025-04-03T12:19:04Z","title":"Charm: The Missing Piece in ViT fine-tuning for Image Aesthetic\n  Assessment","summary":"  The capacity of Vision transformers (ViTs) to handle variable-sized inputs is\noften constrained by computational complexity and batch processing limitations.\nConsequently, ViTs are typically trained on small, fixed-size images obtained\nthrough downscaling or cropping. While reducing computational burden, these\nmethods result in significant information loss, negatively affecting tasks like\nimage aesthetic assessment. We introduce Charm, a novel tokenization approach\nthat preserves Composition, High-resolution, Aspect Ratio, and Multi-scale\ninformation simultaneously. Charm prioritizes high-resolution details in\nspecific regions while downscaling others, enabling shorter fixed-size input\nsequences for ViTs while incorporating essential information. Charm is designed\nto be compatible with pre-trained ViTs and their learned positional embeddings.\nBy providing multiscale input and introducing variety to input tokens, Charm\nimproves ViT performance and generalizability for image aesthetic assessment.\nWe avoid cropping or changing the aspect ratio to further preserve information.\nExtensive experiments demonstrate significant performance improvements on\nvarious image aesthetic and quality assessment datasets (up to 8.1 %) using a\nlightweight ViT backbone. Code and pre-trained models are available at\nhttps://github.com/FBehrad/Charm.\n","authors":["Fatemeh Behrad","Tinne Tuytelaars","Johan Wagemans"],"pdf_url":"https://arxiv.org/pdf/2504.02522v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2505.10118v1","updated":"2025-05-15T09:43:28Z","published":"2025-05-15T09:43:28Z","title":"Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via\n  Multi-Objective Balanced Covering","summary":"  Existing visual token pruning methods target prompt alignment and visual\npreservation with static strategies, overlooking the varying relative\nimportance of these objectives across tasks, which leads to inconsistent\nperformance. To address this, we derive the first closed-form error bound for\nvisual token pruning based on the Hausdorff distance, uniformly characterizing\nthe contributions of both objectives. Moreover, leveraging $\\epsilon$-covering\ntheory, we reveal an intrinsic trade-off between these objectives and quantify\ntheir optimal attainment levels under a fixed budget. To practically handle\nthis trade-off, we propose Multi-Objective Balanced Covering (MoB), which\nreformulates visual token pruning as a bi-objective covering problem. In this\nframework, the attainment trade-off reduces to budget allocation via greedy\nradius trading. MoB offers a provable performance bound and linear scalability\nwith respect to the number of input visual tokens, enabling adaptation to\nchallenging pruning scenarios. Extensive experiments show that MoB preserves\n96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual\ntokens and accelerates LLaVA-Next-7B by 1.3-1.5$\\times$ with negligible\nperformance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm\nthat MoB integrates seamlessly into advanced MLLMs and diverse vision-language\ntasks.\n","authors":["Yangfu Li","Hongjian Zhan","Tianyi Chen","Qi Liu","Yue Lu"],"pdf_url":"https://arxiv.org/pdf/2505.10118v1.pdf","comment":"31 pages,9 figures,conference"},{"id":"http://arxiv.org/abs/2505.08889v2","updated":"2025-05-15T09:19:01Z","published":"2025-05-13T18:24:15Z","title":"IntrinsicEdit: Precise generative image manipulation in intrinsic space","summary":"  Generative diffusion models have advanced image editing with high-quality\nresults and intuitive interfaces such as prompts and semantic drawing. However,\nthese interfaces lack precise control, and the associated methods typically\nspecialize on a single editing task. We introduce a versatile, generative\nworkflow that operates in an intrinsic-image latent space, enabling semantic,\nlocal manipulation with pixel precision for a range of editing operations.\nBuilding atop the RGB-X diffusion framework, we address key challenges of\nidentity preservation and intrinsic-channel entanglement. By incorporating\nexact diffusion inversion and disentangled channel manipulation, we enable\nprecise, efficient editing with automatic resolution of global illumination\neffects -- all without additional data collection or model fine-tuning. We\ndemonstrate state-of-the-art performance across a variety of tasks on complex\nimages, including color and texture adjustments, object insertion and removal,\nglobal relighting, and their combinations.\n","authors":["Linjie Lyu","Valentin Deschaintre","Yannick Hold-Geoffroy","MiloÅ¡ HaÅ¡an","Jae Shin Yoon","Thomas LeimkÃ¼hler","Christian Theobalt","Iliyan Georgiev"],"pdf_url":"https://arxiv.org/pdf/2505.08889v2.pdf","comment":"SIGGRAPH 2025 Journal track"},{"id":"http://arxiv.org/abs/2403.07547v2","updated":"2025-05-15T08:57:01Z","published":"2024-03-12T11:32:57Z","title":"SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields","summary":"  Neural radiance fields (NeRF) has attracted considerable attention for their\nexceptional ability in synthesizing novel views with high fidelity. However,\nthe presence of motion blur, resulting from slight camera movements during\nextended shutter exposures, poses a significant challenge, potentially\ncompromising the quality of the reconstructed 3D scenes. To effectively handle\nthis issue, we propose sequential motion understanding radiance fields (SMURF),\na novel approach that models continuous camera motion and leverages the\nexplicit volumetric representation method for robustness to motion-blurred\ninput images. The core idea of the SMURF is continuous motion blurring kernel\n(CMBK), a module designed to model a continuous camera movements for processing\nblurry inputs. Our model is evaluated against benchmark datasets and\ndemonstrates state-of-the-art performance both quantitatively and\nqualitatively.\n","authors":["Jungho Lee","Dogyoon Lee","Minhyeok Lee","Donghyung Kim","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.07547v2.pdf","comment":"CVPRW 2025, Neural Fields Beyond Conventional Cameras, Project Page:\n  https://jho-yonsei.github.io/SMURF/"},{"id":"http://arxiv.org/abs/2505.10088v1","updated":"2025-05-15T08:43:53Z","published":"2025-05-15T08:43:53Z","title":"MMRL++: Parameter-Efficient and Interaction-Aware Representation\n  Learning for Vision-Language Models","summary":"  Large-scale pre-trained Vision-Language Models (VLMs) have significantly\nadvanced transfer learning across diverse tasks. However, adapting these models\nwith limited few-shot data often leads to overfitting, undermining their\nability to generalize to new tasks. To address this, we propose Multi-Modal\nRepresentation Learning (MMRL), which introduces a shared, learnable,\nmodality-agnostic representation space. MMRL generates space tokens projected\ninto both text and image encoders as representation tokens, enabling more\neffective cross-modal interactions. Unlike prior methods that mainly optimize\nclass token features, MMRL inserts representation tokens into higher encoder\nlayers--where task-specific features are more prominent--while preserving\ngeneral knowledge in the lower layers. During training, both class and\nrepresentation features are jointly optimized: a trainable projection layer is\napplied to representation tokens for task adaptation, while the projection\nlayer for class token remains frozen to retain pre-trained knowledge. To\nfurther promote generalization, we introduce a regularization term aligning\nclass and text features with the frozen VLM's zero-shot features. At inference,\na decoupling strategy uses both class and representation features for base\ntasks, but only class features for novel tasks due to their stronger\ngeneralization. Building upon this, we propose MMRL++, a parameter-efficient\nand interaction-aware extension that significantly reduces trainable parameters\nand enhances intra-modal interactions--particularly across the layers of\nrepresentation tokens--allowing gradient sharing and instance-specific\ninformation to propagate more effectively through the network. Extensive\nexperiments on 15 datasets demonstrate that MMRL and MMRL++ consistently\noutperform state-of-the-art methods, achieving a strong balance between\ntask-specific adaptation and generalization.\n","authors":["Yuncheng Guo","Xiaodong Gu"],"pdf_url":"https://arxiv.org/pdf/2505.10088v1.pdf","comment":"Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract appearing here is slightly shorter than that in the\n  PDF file"},{"id":"http://arxiv.org/abs/2505.10075v1","updated":"2025-05-15T08:27:16Z","published":"2025-05-15T08:27:16Z","title":"FlowDreamer: A RGB-D World Model with Flow-based Motion Representations\n  for Robot Manipulation","summary":"  This paper investigates training better visual world models for robot\nmanipulation, i.e., models that can predict future visual observations by\nconditioning on past frames and robot actions. Specifically, we consider world\nmodels that operate on RGB-D frames (RGB-D world models). As opposed to\ncanonical approaches that handle dynamics prediction mostly implicitly and\nreconcile it with visual rendering in a single model, we introduce FlowDreamer,\nwhich adopts 3D scene flow as explicit motion representations. FlowDreamer\nfirst predicts 3D scene flow from past frame and action conditions with a\nU-Net, and then a diffusion model will predict the future frame utilizing the\nscene flow. FlowDreamer is trained end-to-end despite its modularized nature.\nWe conduct experiments on 4 different benchmarks, covering both video\nprediction and visual planning tasks. The results demonstrate that FlowDreamer\nachieves better performance compared to other baseline RGB-D world models by 7%\non semantic similarity, 11% on pixel quality, and 6% on success rate in various\nrobot manipulation domains.\n","authors":["Jun Guo","Xiaojian Ma","Yikai Wang","Min Yang","Huaping Liu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2505.10075v1.pdf","comment":"Project page: see https://sharinka0715.github.io/FlowDreamer/"},{"id":"http://arxiv.org/abs/2502.06607v3","updated":"2025-05-15T08:22:44Z","published":"2025-02-10T16:04:54Z","title":"Illegal Waste Detection in Remote Sensing Images: A Case Study","summary":"  Environmental crime is the third largest criminal activity worldwide, with\nsignificant revenues coming from illegal management of solid waste. Thanks to\nthe increasing availability and the decreasing cost of Very High Resolution\nRemote Sensing (VHR RS) images, the fight against environmental crime can\nnowadays rely on modern image-analysis tools to support photo-interpretation\nfor scanning vast territories in search of illegal waste disposal sites. This\npaper illustrates a semi-automatic waste detection pipeline, developed in\ncollaboration with a regional environmental protection agency, for detecting\ncandidate illegal dumping sites in VHR RS images. To optimize the effectiveness\nof the waste detector, extensive experiments evaluate such design choices as\nthe network architecture, the ground resolution and geographic span of the\ninput images, as well as the pretraining procedures. The best model attains\nremarkable performance, achieving 92.02% F1-Score and 94.56% Accuracy. A\ngeneralization study assesses the performance variation when the detector\nprocesses images from a territory substantially different from the one used\nduring training, incurring only a moderate performance loss, i.e., 6.5%\ndecrease in the F1-Score. Finally, an exercise in which photo interpreters\ncompare the territory scanning effort with and without the support of the waste\ndetector assesses the concrete benefit of using a computer-aided image analysis\ntool in a professional environment protection agency. Results show that a\nreduction up to 30% of the time spent for waste site detection can be attained.\n","authors":["Federico Gibellini","Piero Fraternali","Giacomo Boracchi","Luca Morandini","Thomas Martinoli","Andrea Diecidue","Simona Malegori"],"pdf_url":"https://arxiv.org/pdf/2502.06607v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08605v2","updated":"2025-05-15T08:19:15Z","published":"2025-05-13T14:20:11Z","title":"Leveraging Multi-Modal Information to Enhance Dataset Distillation","summary":"  Dataset distillation aims to create a compact and highly representative\nsynthetic dataset that preserves the knowledge of a larger real dataset. While\nexisting methods primarily focus on optimizing visual representations,\nincorporating additional modalities and refining object-level information can\nsignificantly improve the quality of distilled datasets. In this work, we\nintroduce two key enhancements to dataset distillation: caption-guided\nsupervision and object-centric masking. To integrate textual information, we\npropose two strategies for leveraging caption features: the feature\nconcatenation, where caption embeddings are fused with visual features at the\nclassification stage, and caption matching, which introduces a caption-based\nalignment loss during training to ensure semantic coherence between real and\nsynthetic data. Additionally, we apply segmentation masks to isolate target\nobjects and remove background distractions, introducing two loss functions\ndesigned for object-centric learning: masked feature alignment loss and masked\ngradient matching loss. Comprehensive evaluations demonstrate that integrating\ncaption-based guidance and object-centric masking enhances dataset\ndistillation, leading to synthetic datasets that achieve superior performance\non downstream tasks.\n","authors":["Zhe Li","Hadrien Reynaud","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2505.08605v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2411.16782v2","updated":"2025-05-15T08:18:43Z","published":"2024-11-25T08:14:37Z","title":"Scaling Laws for Black box Adversarial Attacks","summary":"  Adversarial examples usually exhibit good cross-model transferability,\nenabling attacks on black-box models with limited information about their\narchitectures and parameters, which are highly threatening in commercial\nblack-box scenarios. Model ensembling is an effective strategy to improve the\ntransferability of adversarial examples by attacking multiple surrogate models.\nHowever, since prior studies usually adopt few models in the ensemble, there\nremains an open question of whether scaling the number of models can further\nimprove black-box attacks. Inspired by the scaling law of large foundation\nmodels, we investigate the scaling laws of black-box adversarial attacks in\nthis work. Through theoretical analysis and empirical evaluations, we conclude\nwith clear scaling laws that using more surrogate models enhances adversarial\ntransferability. Comprehensive experiments verify the claims on standard image\nclassifiers, diverse defended models and multimodal large language models using\nvarious adversarial attack methods. Specifically, by scaling law, we achieve\n90%+ transfer attack success rate on even proprietary models like GPT-4o.\nFurther visualization indicates that there is also a scaling law on the\ninterpretability and semantics of adversarial perturbations.\n","authors":["Chuan Liu","Huanran Chen","Yichi Zhang","Yinpeng Dong","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.16782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10072v1","updated":"2025-05-15T08:16:12Z","published":"2025-05-15T08:16:12Z","title":"ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head\n  Avatars","summary":"  The introduction of 3D Gaussian blendshapes has enabled the real-time\nreconstruction of animatable head avatars from monocular video. Toonify, a\nStyleGAN-based framework, has become widely used for facial image stylization.\nTo extend Toonify for synthesizing diverse stylized 3D head avatars using\nGaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.\nIn Stage 1 (stylized video generation), we employ an improved StyleGAN to\ngenerate the stylized video from the input video frames, which addresses the\nlimitation of cropping aligned faces at a fixed resolution as preprocessing for\nnormal StyleGAN. This process provides a more stable video, which enables\nGaussian blendshapes to better capture the high-frequency details of the video\nframes, and efficiently generate high-quality animation in the next stage. In\nStage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head\nmodel and a set of expression blendshapes from the generated video. By\ncombining the neutral head model with expression blendshapes, ToonifyGB can\nefficiently render stylized avatars with arbitrary expressions. We validate the\neffectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane\nand Pixar.\n","authors":["Rui-Yang Ju","Sheng-Yen Huang","Yi-Ping Hung"],"pdf_url":"https://arxiv.org/pdf/2505.10072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10055v1","updated":"2025-05-15T07:58:38Z","published":"2025-05-15T07:58:38Z","title":"PsOCR: Benchmarking Large Multimodal Models for Optical Character\n  Recognition in Low-resource Pashto Language","summary":"  This paper evaluates the performance of Large Multimodal Models (LMMs) on\nOptical Character Recognition (OCR) in the low-resource Pashto language.\nNatural Language Processing (NLP) in Pashto faces several challenges due to the\ncursive nature of its script and a scarcity of structured datasets. To address\nthis, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one\nmillion images annotated with bounding boxes at word, line, and document\nlevels, suitable for training and evaluating models based on different\narchitectures, including Convolutional Neural Networks (CNNs) and Transformers.\nPsOCR covers variations across 1,000 unique font families, colors, image sizes,\nand layouts. A benchmark subset of 10K images was selected to evaluate the\nperformance of several LMMs, including seven open-source models: DeepSeek's\nJanus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four\nclosed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results\ndemonstrate that Gemini achieves the best performance among all models, whereas\namong open-source models, Qwen-7B stands out. This work provides an insightful\nassessment of the capabilities and limitations of current LMMs for OCR tasks in\nPashto and establishes a foundation for further research not only in Pashto OCR\nbut also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is\navailable at https://github.com/zirak-ai/PashtoOCR.\n","authors":["Ijazul Haq","Yingjie Zhang","Irfan Ali Khan"],"pdf_url":"https://arxiv.org/pdf/2505.10055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16028v2","updated":"2025-05-15T07:55:04Z","published":"2024-12-20T16:25:20Z","title":"CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from\n  Defocused Images","summary":"  3D Gaussian Splatting (3DGS) has attracted significant attention for its\nhigh-quality novel view rendering, inspiring research to address real-world\nchallenges. While conventional methods depend on sharp images for accurate\nscene reconstruction, real-world scenarios are often affected by defocus blur\ndue to finite depth of field, making it essential to account for realistic 3D\nscene representation. In this study, we propose CoCoGaussian, a Circle of\nConfusion-aware Gaussian Splatting that enables precise 3D scene representation\nusing only defocused images. CoCoGaussian addresses the challenge of defocus\nblur by modeling the Circle of Confusion (CoC) through a physically grounded\napproach based on the principles of photographic defocus. Exploiting 3D\nGaussians, we compute the CoC diameter from depth and learnable aperture\ninformation, generating multiple Gaussians to precisely capture the CoC shape.\nFurthermore, we introduce a learnable scaling factor to enhance robustness and\nprovide more flexibility in handling unreliable depth in scenes with reflective\nor refractive surfaces. Experiments on both synthetic and real-world datasets\ndemonstrate that CoCoGaussian achieves state-of-the-art performance across\nmultiple benchmarks.\n","authors":["Jungho Lee","Suhwan Cho","Taeoh Kim","Ho-Deok Jang","Minhyeok Lee","Geonho Cha","Dongyoon Wee","Dogyoon Lee","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2412.16028v2.pdf","comment":"CVPR 2025, Project Page: https://Jho-Yonsei.github.io/CoCoGaussian/"},{"id":"http://arxiv.org/abs/2504.08353v2","updated":"2025-05-15T07:51:50Z","published":"2025-04-11T08:39:18Z","title":"Single View Garment Reconstruction Using Diffusion Mapping Via Pattern\n  Coordinates","summary":"  Reconstructing 3D clothed humans from images is fundamental to applications\nlike virtual try-on, avatar creation, and mixed reality. While recent advances\nhave enhanced human body recovery, accurate reconstruction of garment geometry\n-- especially for loose-fitting clothing -- remains an open challenge. We\npresent a novel method for high-fidelity 3D garment reconstruction from single\nimages that bridges 2D and 3D representations. Our approach combines Implicit\nSewing Patterns (ISP) with a generative diffusion model to learn rich garment\nshape priors in a 2D UV space. A key innovation is our mapping model that\nestablishes correspondences between 2D image pixels, UV pattern coordinates,\nand 3D geometry, enabling joint optimization of both 3D garment meshes and the\ncorresponding 2D patterns by aligning learned priors with image observations.\nDespite training exclusively on synthetically simulated cloth data, our method\ngeneralizes effectively to real-world images, outperforming existing approaches\non both tight- and loose-fitting garments. The reconstructed garments maintain\nphysical plausibility while capturing fine geometric details, enabling\ndownstream applications including garment retargeting and texture manipulation.\n","authors":["Ren Li","Cong Cao","Corentin Dumery","Yingxuan You","Hao Li","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2504.08353v2.pdf","comment":"SIGGRAPH 2025"},{"id":"http://arxiv.org/abs/2505.10049v1","updated":"2025-05-15T07:51:08Z","published":"2025-05-15T07:51:08Z","title":"Advances in Radiance Field for Dynamic Scene: From Neural Field to\n  Gaussian Field","summary":"  Dynamic scene representation and reconstruction have undergone transformative\nadvances in recent years, catalyzed by breakthroughs in neural radiance fields\nand 3D Gaussian splatting techniques. While initially developed for static\nenvironments, these methodologies have rapidly evolved to address the\ncomplexities inherent in 4D dynamic scenes through an expansive body of\nresearch. Coupled with innovations in differentiable volumetric rendering,\nthese approaches have significantly enhanced the quality of motion\nrepresentation and dynamic scene reconstruction, thereby garnering substantial\nattention from the computer vision and graphics communities. This survey\npresents a systematic analysis of over 200 papers focused on dynamic scene\nrepresentation using radiance field, spanning the spectrum from implicit neural\nrepresentations to explicit Gaussian primitives. We categorize and evaluate\nthese works through multiple critical lenses: motion representation paradigms,\nreconstruction techniques for varied scene dynamics, auxiliary information\nintegration strategies, and regularization approaches that ensure temporal\nconsistency and physical plausibility. We organize diverse methodological\napproaches under a unified representational framework, concluding with a\ncritical examination of persistent challenges and promising research\ndirections. By providing this comprehensive overview, we aim to establish a\ndefinitive reference for researchers entering this rapidly evolving field while\noffering experienced practitioners a systematic understanding of both\nconceptual principles and practical frontiers in dynamic scene reconstruction.\n","authors":["Jinlong Fan","Xuepu Zeng","Jing Zhang","Mingming Gong","Yuxiang Yang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2505.10049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10046v1","updated":"2025-05-15T07:43:23Z","published":"2025-05-15T07:43:23Z","title":"Exploring the Deep Fusion of Large Language Models and Diffusion\n  Transformers for Text-to-Image Synthesis","summary":"  This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.\n","authors":["Bingda Tang","Boyang Zheng","Xichen Pan","Sayak Paul","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2505.10046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21776v3","updated":"2025-05-15T07:28:30Z","published":"2025-03-27T17:59:51Z","title":"Video-R1: Reinforcing Video Reasoning in MLLMs","summary":"  Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for incentivizing video\nreasoning within multimodal large language models (MLLMs). However, directly\napplying RL training with the GRPO algorithm to video reasoning presents two\nprimary challenges: (i) a lack of temporal modeling for video reasoning, and\n(ii) the scarcity of high-quality video-reasoning data. To address these\nissues, we first propose the T-GRPO algorithm, which encourages models to\nutilize temporal information in videos for reasoning. Additionally, instead of\nrelying solely on video data, we incorporate high-quality image-reasoning data\ninto the training process. We have constructed two datasets: Video-R1-CoT-165k\nfor SFT cold start and Video-R1-260k for RL training, both comprising image and\nvideo data. Experimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncode, models, and data are released in: https://github.com/tulerfeng/Video-R1.\n","authors":["Kaituo Feng","Kaixiong Gong","Bohao Li","Zonghao Guo","Yibing Wang","Tianshuo Peng","Junfei Wu","Xiaoying Zhang","Benyou Wang","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2503.21776v3.pdf","comment":"Project page: https://github.com/tulerfeng/Video-R1"},{"id":"http://arxiv.org/abs/2505.10030v1","updated":"2025-05-15T07:25:43Z","published":"2025-05-15T07:25:43Z","title":"DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection\n  of Diseases in Cocos nucifera","summary":"  Coconut tree diseases are a serious risk to agricultural yield, particularly\nin developing countries where conventional farming practices restrict early\ndiagnosis and intervention. Current disease identification methods are manual,\nlabor-intensive, and non-scalable. In response to these limitations, we come up\nwith DeepSeqCoco, a deep learning based model for accurate and automatic\ndisease identification from coconut tree images. The model was tested under\nvarious optimizer settings, such as SGD, Adam, and hybrid configurations, to\nidentify the optimal balance between accuracy, minimization of loss, and\ncomputational cost. Results from experiments indicate that DeepSeqCoco can\nachieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than\nexisting models) with the hybrid SGD-Adam showing the lowest validation loss of\n2.81%. It also shows a drop of up to 18% in training time and up to 85% in\nprediction time for input images. The results point out the promise of the\nmodel to improve precision agriculture through an AI-based, scalable, and\nefficient disease monitoring system.\n","authors":["Miit Daga","Dhriti Parikh","Swarna Priya Ramu"],"pdf_url":"https://arxiv.org/pdf/2505.10030v1.pdf","comment":"This paper is accepted for publication in IEEE Access journal and is\n  currently pending revisions before publication"},{"id":"http://arxiv.org/abs/2505.03186v2","updated":"2025-05-15T07:21:04Z","published":"2025-05-06T05:07:11Z","title":"CoGenAV: Versatile Audio-Visual Representation Learning via\n  Contrastive-Generative Synchronization","summary":"  The inherent synchronization between a speaker's lip movements, voice, and\nthe underlying linguistic content offers a rich source of information for\nimproving speech processing tasks, especially in challenging conditions where\ntraditional audio-only systems falter. We introduce CoGenAV, a powerful and\ndata-efficient model designed to learn versatile audio-visual representations\napplicable across a wide range of speech and audio-visual tasks. CoGenAV is\ntrained by optimizing a dual objective derived from natural audio-visual\nsynchrony, contrastive feature alignment and generative text prediction, using\nonly 223 hours of labeled data from the LRS2 dataset. This\ncontrastive-generative synchronization strategy effectively captures\nfundamental cross-modal correlations. We showcase the effectiveness and\nversatility of the learned CoGenAV representations on multiple benchmarks. When\nutilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these\nrepresentations contribute to achieving a state-of-the-art Word Error Rate\n(WER) of 1.27. They also enable strong performance in Visual Speech Recognition\n(VSR) with a WER of 20.5 on LRS2, and significantly improve performance in\nnoisy environments by over 70%. Furthermore, CoGenAV representations benefit\nspeech reconstruction tasks, boosting performance in Speech Enhancement and\nSeparation, and achieve competitive results in audio-visual synchronization\ntasks like Active Speaker Detection (ASD). Our model will be open-sourced to\nfacilitate further development and collaboration within both academia and\nindustry.\n","authors":["Detao Bai","Zhiheng Ma","Xihan Wei","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2505.03186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10027v1","updated":"2025-05-15T07:17:03Z","published":"2025-05-15T07:17:03Z","title":"ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model\n  Super-Resolution Reconstruction","summary":"  With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.\n","authors":["Shijie Lyu"],"pdf_url":"https://arxiv.org/pdf/2505.10027v1.pdf","comment":"Accepted by the 4th International Conference on Computing Innovation\n  and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community\n  Research Series-CORE or Theoretical and Natural Science (TNS)"},{"id":"http://arxiv.org/abs/2502.19159v3","updated":"2025-05-15T07:04:35Z","published":"2025-02-26T14:15:24Z","title":"A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs","summary":"  Compared to width-wise pruning, depth-wise pruning can significantly\naccelerate inference in resource-constrained scenarios. However, treating the\nentire Transformer layer as the minimum pruning unit may degrade model\nperformance by indiscriminately discarding the entire information of the layer.\nThis paper reveals the ``Patch-like'' feature relationship between layers in\nlarge language models by analyzing the correlation of the outputs of different\nlayers in the reproducing kernel Hilbert space. Building on this observation,\nwe propose a sliding layer merging method that dynamically selects and fuses\nconsecutive layers from top to bottom according to a pre-defined similarity\nthreshold, thereby simplifying the model structure while maintaining its\nperformance. Extensive experiments on LLMs with various architectures and\ndifferent parameter scales show that our method outperforms existing pruning\ntechniques in both zero-shot inference performance and retraining recovery\nquality after pruning. In particular, in the experiment with 35% pruning on the\nVicuna-7B model, our method achieved a 1.654% improvement in average\nperformance on zero-shot tasks compared to the existing method. Moreover, we\nfurther reveal the potential of combining depth pruning with width pruning to\nenhance the pruning effect. Our codes are available at\nhttps://github.com/920927/SLM-a-sliding-layer-merging-method.\n","authors":["Xuan Ding","Rui Sun","Yunjian Zhang","Xiu Yan","Yueqi Zhou","Kaihao Huang","Suzhong Fu","Angelica I Aviles-Rivero","Chuanlong Xie","Yao Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.19159v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10016v1","updated":"2025-05-15T06:58:45Z","published":"2025-05-15T06:58:45Z","title":"Application of YOLOv8 in monocular downward multiple Car Target\n  detection","summary":"  Autonomous driving technology is progressively transforming traditional car\ndriving methods, marking a significant milestone in modern transportation.\nObject detection serves as a cornerstone of autonomous systems, playing a vital\nrole in enhancing driving safety, enabling autonomous functionality, improving\ntraffic efficiency, and facilitating effective emergency responses. However,\ncurrent technologies such as radar for environmental perception, cameras for\nroad perception, and vehicle sensor networks face notable challenges, including\nhigh costs, vulnerability to weather and lighting conditions, and limited\nresolution.To address these limitations, this paper presents an improved\nautonomous target detection network based on YOLOv8. By integrating structural\nreparameterization technology, a bidirectional pyramid structure network model,\nand a novel detection pipeline into the YOLOv8 framework, the proposed approach\nachieves highly efficient and precise detection of multi-scale, small, and\nremote objects. Experimental results demonstrate that the enhanced model can\neffectively detect both large and small objects with a detection accuracy of\n65%, showcasing significant advancements over traditional methods.This improved\nmodel holds substantial potential for real-world applications and is\nwell-suited for autonomous driving competitions, such as the Formula Student\nAutonomous China (FSAC), particularly excelling in scenarios involving\nsingle-target and small-object detection.\n","authors":["Shijie Lyu"],"pdf_url":"https://arxiv.org/pdf/2505.10016v1.pdf","comment":"Accepted by the 5th International Conference on Signal Processing and\n  Machine Learning (CONF-SPML 2025), to appear in Applied and Computational\n  Engineering"},{"id":"http://arxiv.org/abs/2505.05513v2","updated":"2025-05-15T06:56:31Z","published":"2025-05-07T10:33:45Z","title":"Exploring Convolutional Neural Networks for Rice Grain Classification:\n  An Explainable AI Approach","summary":"  Rice is an essential staple food worldwide that is important in promoting\ninternational trade, economic growth, and nutrition. Asian countries such as\nChina, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their\nsignificant contribution to the cultivation and utilization of rice. These\nnations are also known for cultivating different rice grains, including short\nand long grains. These sizes are further classified as basmati, jasmine, kainat\nsaila, ipsala, arborio, etc., catering to diverse culinary preferences and\ncultural traditions. For both local and international trade, inspecting and\nmaintaining the quality of rice grains to satisfy customers and preserve a\ncountry's reputation is necessary. Manual quality check and classification is\nquite a laborious and time-consuming process. It is also highly prone to\nmistakes. Therefore, an automatic solution must be proposed for the effective\nand efficient classification of different varieties of rice grains. This\nresearch paper presents an automatic framework based on a convolutional neural\nnetwork (CNN) for classifying different varieties of rice grains. We evaluated\nthe proposed model based on performance metrics such as accuracy, recall,\nprecision, and F1-Score. The CNN model underwent rigorous training and\nvalidation, achieving a remarkable accuracy rate and a perfect area under each\nclass's Receiver Operating Characteristic (ROC) curve. The confusion matrix\nanalysis confirmed the model's effectiveness in distinguishing between the\ndifferent rice varieties, indicating minimal misclassifications. Additionally,\nthe integration of explainability techniques such as LIME (Local Interpretable\nModel-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided\nvaluable insights into the model's decision-making process, revealing how\nspecific features of the rice grains influenced classification outcomes.\n","authors":["Muhammad Junaid Asif","Hamza Khan","Rabia Tehseen","Syed Tahir Hussain Rizvi","Mujtaba Asad","Shazia Saqib","Rana Fayyaz Ahmad"],"pdf_url":"https://arxiv.org/pdf/2505.05513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10543v9","updated":"2025-05-15T06:54:39Z","published":"2023-11-17T14:10:55Z","title":"Unified theory for joint covariance properties under geometric image\n  transformations for spatio-temporal receptive fields according to the\n  generalized Gaussian derivative model for visual receptive fields","summary":"  The influence of natural image transformations on receptive field responses\nis crucial for modelling visual operations in computer vision and biological\nvision. In this regard, covariance properties with respect to geometric image\ntransformations in the earliest layers of the visual hierarchy are essential\nfor expressing robust image operations, and for formulating invariant visual\noperations at higher levels.\n  This paper defines and proves a set of joint covariance properties for\nspatio-temporal receptive fields in terms of spatio-temporal derivative\noperators applied to spatio-temporally smoothed image data under compositions\nof spatial scaling transformations, spatial affine transformations, Galilean\ntransformations and temporal scaling transformations. Specifically, the derived\nrelations show how the parameters of the receptive fields need to be\ntransformed, in order to match the output from spatio-temporal receptive fields\nunder composed spatio-temporal image transformations.\n  For this purpose, we also fundamentally extend the notion of scale-normalized\nderivatives to affine-normalized derivatives, that are computed based on\nspatial smoothing with affine Gaussian kernels, and analyze the covariance\nproperties of the resulting affine-normalized derivatives for the affine group\nas well as for important subgroups thereof.\n  We conclude with a geometric analysis, showing how the derived joint\ncovariance properties make it possible to relate or match spatio-temporal\nreceptive field responses, when observing, possibly moving, local surface\npatches from different views, under locally linearized perspective or\nprojective transformations, as well as when observing different instances of\nspatio-temporal events, that may occur either faster or slower between\ndifferent views of similar spatio-temporal events.\n","authors":["Tony Lindeberg"],"pdf_url":"https://arxiv.org/pdf/2311.10543v9.pdf","comment":"46 pages, 19 figures. Note: From version 4, this paper considers a\n  different form of joint composition of the geometric image transformations\n  than in the earlier versions"},{"id":"http://arxiv.org/abs/2505.09998v1","updated":"2025-05-15T06:22:24Z","published":"2025-05-15T06:22:24Z","title":"From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive\n  3D Sketching","summary":"  In the era of immersive consumer electronics, such as AR/VR headsets and\nsmart devices, people increasingly seek ways to express their identity through\nvirtual fashion. However, existing 3D garment design tools remain inaccessible\nto everyday users due to steep technical barriers and limited data. In this\nwork, we introduce a 3D sketch-driven 3D garment generation framework that\nempowers ordinary users - even those without design experience - to create\nhigh-quality digital clothing through simple 3D sketches in AR/VR environments.\nBy combining a conditional diffusion model, a sketch encoder trained in a\nshared latent space, and an adaptive curriculum learning strategy, our system\ninterprets imprecise, free-hand input and produces realistic, personalized\ngarments. To address the scarcity of training data, we also introduce\nKO3DClothes, a new dataset of paired 3D garments and user-created sketches.\nExtensive experiments and user studies confirm that our method significantly\noutperforms existing baselines in both fidelity and usability, demonstrating\nits promise for democratized fashion design on next-generation consumer\nplatforms.\n","authors":["Ying Zang","Yuanqi Hu","Xinyu Chen","Yuxia Xu","Suhui Wang","Chunan Yu","Lanyun Zhu","Deyi Ji","Xin Xu","Tianrun Chen"],"pdf_url":"https://arxiv.org/pdf/2505.09998v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.09997v1","updated":"2025-05-15T06:21:00Z","published":"2025-05-15T06:21:00Z","title":"Descriptive Image-Text Matching with Graded Contextual Similarity","summary":"  Image-text matching aims to build correspondences between visual and textual\ndata by learning their pairwise similarities. Most existing approaches have\nadopted sparse binary supervision, indicating whether a pair of images and\nsentences matches or not. However, such sparse supervision covers a limited\nsubset of image-text relationships, neglecting their inherent many-to-many\ncorrespondences; an image can be described in numerous texts at different\ndescriptive levels. Moreover, existing approaches overlook the implicit\nconnections from general to specific descriptions, which form the underlying\nrationale for the many-to-many relationships between vision and language. In\nthis work, we propose descriptive image-text matching, called DITM, to learn\nthe graded contextual similarity between image and text by exploring the\ndescriptive flexibility of language. We formulate the descriptiveness score of\neach sentence with cumulative term frequency-inverse document frequency\n(TF-IDF) to balance the pairwise similarity according to the keywords in the\nsentence. Our method leverages sentence descriptiveness to learn robust\nimage-text matching in two key ways: (1) to refine the false negative labeling,\ndynamically relaxing the connectivity between positive and negative pairs, and\n(2) to build more precise matching, aligning a set of relevant sentences in a\ngeneric-to-specific order. By moving beyond rigid binary supervision, DITM\nenhances the discovery of both optimal matches and potential positive pairs.\nExtensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the\neffectiveness of our method in representing complex image-text relationships\ncompared to state-of-the-art approaches. In addition, DITM enhances the\nhierarchical reasoning ability of the model, supported by the extensive\nanalysis on HierarCaps benchmark.\n","authors":["Jinhyun Jang","Jiyeong Lee","Kwanghoon Sohn"],"pdf_url":"https://arxiv.org/pdf/2505.09997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09990v1","updated":"2025-05-15T06:04:42Z","published":"2025-05-15T06:04:42Z","title":"PointArena: Probing Multimodal Grounding Through Language-Guided\n  Pointing","summary":"  Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/\n","authors":["Long Cheng","Jiafei Duan","Yi Ru Wang","Haoquan Fang","Boyang Li","Yushan Huang","Elvis Wang","Ainaz Eftekhar","Jason Lee","Wentao Yuan","Rose Hendrix","Noah A. Smith","Fei Xia","Dieter Fox","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2505.09990v1.pdf","comment":"10 Pages, Dataset and code:https://pointarena.github.io/"},{"id":"http://arxiv.org/abs/2502.19090v2","updated":"2025-05-15T05:59:22Z","published":"2025-02-26T12:36:16Z","title":"EndoMamba: An Efficient Foundation Model for Endoscopic Videos via\n  Hierarchical Pre-training","summary":"  Endoscopic video-based tasks, such as visual navigation and surgical phase\nrecognition, play a crucial role in minimally invasive surgeries by providing\nreal-time assistance. While recent video foundation models have shown promise,\ntheir applications are hindered by (1) computational inefficiencies and (2)\nsuboptimal performance caused by limited data for pre-training in endoscopy. To\naddress these issues, we present EndoMamba, a foundation model designed for\nreal-time inference while learning generalized spatiotemporal representations.\nFirst, to mitigate computational inefficiencies, we propose the EndoMamba\nbackbone, optimized for real-time inference. Inspired by recent advancements in\nstate space models, EndoMamba integrates Bidirectional Mamba blocks for spatial\nmodeling within individual frames and vanilla Mamba blocks for past-to-present\nreasoning across the temporal domain. This design enables both strong\nspatiotemporal modeling and efficient inference in online video streams.\nSecond, we propose a self-supervised hierarchical pre-training diagram to\nenhance EndoMamba's representation learning using endoscopic videos and\nincorporating general video domain knowledge. Specifically, our approach\ncombines masked reconstruction with auxiliary supervision, leveraging low-level\nreconstruction to capture spatial-temporal structures and high-level alignment\nto transfer broader knowledge from a pretrained general-video domain foundation\nmodel. Extensive experiments on four downstream tasks--classification,\nsegmentation, surgical phase recognition, and localization--demonstrate that\nEndoMamba outperforms existing foundation models and task-specific methods\nwhile maintaining real-time inference speed. The source code is available at\nhttps://github.com/TianCuteQY/EndoMamba.\n","authors":["Qingyao Tian","Huai Liao","Xinyan Huang","Bingyu Yang","Dongdong Lei","Sebastien Ourselin","Hongbin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.19090v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13602v2","updated":"2025-05-15T05:56:38Z","published":"2024-11-19T09:09:14Z","title":"Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging\n  Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study AI\n  for ECG to CMR Translation Study","summary":"  Cardiovascular diseases (CVDs) are the leading cause of global mortality,\nnecessitating accessible and accurate diagnostic tools. While cardiac magnetic\nresonance imaging (CMR) provides gold-standard insights into cardiac structure\nand function, its clinical utility is limited by high cost and complexity. In\ncontrast, electrocardiography (ECG) is inexpensive and widely available but\nlacks the granularity of CMR. We propose CardioNets, a deep learning framework\nthat translates 12-lead ECG signals into CMR-level functional parameters and\nsynthetic images, enabling scalable cardiac assessment. CardioNets integrates\ncross-modal contrastive learning and generative pretraining, aligning ECG with\nCMR-derived cardiac phenotypes and synthesizing high-resolution CMR images via\na masked autoregressive model. Trained on 159,819 samples from five cohorts,\nincluding the UK Biobank (n=42,483) and MIMIC-IV-ECG (n=164,550), and\nexternally validated on independent clinical datasets (n=3,767), CardioNets\nachieved strong performance across disease screening and phenotype estimation\ntasks. In the UK Biobank, it improved cardiac phenotype regression R2 by 24.8%\nand cardiomyopathy AUC by up to 39.3% over baseline models. In MIMIC, it\nincreased AUC for pulmonary hypertension detection by 5.6%. Generated CMR\nimages showed 36.6% higher SSIM and 8.7% higher PSNR than prior approaches. In\na reader study, ECG-only CardioNets achieved 13.9% higher accuracy than human\nphysicians using both ECG and real CMR. These results suggest that CardioNets\noffers a promising, low-cost alternative to CMR for large-scale CVD screening,\nparticularly in resource-limited settings. Future efforts will focus on\nclinical deployment and regulatory validation of ECG-based synthetic imaging.\n","authors":["Zhengyao Ding","Ziyu Li","Yujian Hu","Youyao Xu","Chengchen Zhao","Yiheng Mao","Haitao Li","Zhikang Li","Qian Li","Jing Wang","Yue Chen","Mengjia Chen","Longbo Wang","Xuesen Chu","Weichao Pan","Ziyi Liu","Fei Wu","Hongkun Zhang","Ting Chen","Zhengxing Huang"],"pdf_url":"https://arxiv.org/pdf/2411.13602v2.pdf","comment":"27 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.09986v1","updated":"2025-05-15T05:52:11Z","published":"2025-05-15T05:52:11Z","title":"High Quality Underwater Image Compression with Adaptive Correction and\n  Codebook-based Augmentation","summary":"  With the increasing exploration and exploitation of the underwater world,\nunderwater images have become a critical medium for human interaction with\nmarine environments, driving extensive research into their efficient\ntransmission and storage. However, contemporary underwater image compression\nalgorithms fail to fully leverage the unique characteristics distinguishing\nunderwater scenes from terrestrial images, resulting in suboptimal performance.\nTo address this limitation, we introduce HQUIC, designed to exploit\nunderwater-image-specific features for enhanced compression efficiency. HQUIC\nemploys an ALTC module to adaptively predict the attenuation coefficients and\nglobal light information of the images, which effectively mitigates the issues\ncaused by the differences in lighting and tone existing in underwater images.\nSubsequently, HQUIC employs a codebook as an auxiliary branch to extract the\ncommon objects within underwater images and enhances the performance of the\nmain branch. Furthermore, HQUIC dynamically weights multi-scale frequency\ncomponents, prioritizing information critical for distortion quality while\ndiscarding redundant details. Extensive evaluations on diverse underwater\ndatasets demonstrate that HQUIC outperforms state-of-the-art compression\nmethods.\n","authors":["Yimin Zhou","Yichong Xia","Sicheng Pan","Bin Chen","Baoyi An","Haoqian Wang","Zhi Wang","Yaowei Wang","Zikun Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.09986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09985v1","updated":"2025-05-15T05:50:35Z","published":"2025-05-15T05:50:35Z","title":"Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction","summary":"  Score-based diffusion models have shown significant promise in the field of\nsparse-view CT reconstruction. However, the projection dataset is large and\nriddled with redundancy. Consequently, applying the diffusion model to\nunprocessed data results in lower learning effectiveness and higher learning\ndifficulty, frequently leading to reconstructed images that lack fine details.\nTo address these issues, we propose the ordered-subsets multi-diffusion model\n(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT\nprojection data into equal subsets and employs multi-subsets diffusion model\n(MSDM) to learn from each subset independently. This targeted learning approach\nreduces complexity and enhances the reconstruction of fine details.\nFurthermore, the integration of one-whole diffusion model (OWDM) with complete\nsinogram data acts as a global information constraint, which can reduce the\npossibility of generating erroneous or inconsistent sinogram information.\nMoreover, the OSMM's unsupervised learning framework provides strong robustness\nand generalizability, adapting seamlessly to varying sparsity levels of CT\nsinograms. This ensures consistent and reliable performance across different\nclinical scenarios. Experimental results demonstrate that OSMM outperforms\ntraditional diffusion models in terms of image quality and noise resilience,\noffering a powerful and versatile solution for advanced CT imaging in\nsparse-view scenarios.\n","authors":["Pengfei Yu","Bin Huang","Minghui Zhang","Weiwen Wu","Shaoyu Wang","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2505.09985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07214v2","updated":"2025-05-15T05:47:33Z","published":"2025-05-12T03:47:05Z","title":"Towards user-centered interactive medical image segmentation in VR with\n  an assistive AI agent","summary":"  Crucial in disease analysis and surgical planning, manual segmentation of\nvolumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and\nchallenging to master, while fully automatic algorithms can benefit from user\nfeedback. Therefore, with the complementary power of the latest radiological AI\nfoundation models and virtual reality (VR)'s intuitive data interaction, we\npropose SAMIRA, a novel conversational AI agent that assists users with\nlocalizing, segmenting, and visualizing 3D medical concepts in VR. Through\nspeech-based interaction, the agent helps users understand radiological\nfeatures, locate clinical targets, and generate segmentation masks that can be\nrefined with just a few point prompts. The system also supports true-to-scale\n3D visualization of segmented pathology to enhance patient-specific anatomical\nunderstanding. Furthermore, to determine the optimal interaction paradigm under\nnear-far attention-switching for refining segmentation masks in an immersive,\nhuman-in-the-loop workflow, we compare VR controller pointing, head pointing,\nand eye tracking as input modes. With a user study, evaluations demonstrated a\nhigh usability score (SUS=90.0 $\\pm$ 9.0), low overall task load, as well as\nstrong support for the proposed VR system's guidance, training potential, and\nintegration of AI in radiological segmentation tasks.\n","authors":["Pascal Spiegler","Arash Harirpoush","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2505.07214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09971v1","updated":"2025-05-15T05:21:16Z","published":"2025-05-15T05:21:16Z","title":"APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of\n  Airborne LiDAR Point Clouds","summary":"  Airborne laser scanning (ALS) point cloud segmentation is a fundamental task\nfor large-scale 3D scene understanding. In real-world applications, models are\ntypically fixed after training. However, domain shifts caused by changes in the\nenvironment, sensor types, or sensor degradation often lead to a decline in\nmodel performance. Continuous Test-Time Adaptation (CTTA) offers a solution by\nadapting a source-pretrained model to evolving, unlabeled target domains.\nDespite its potential, research on ALS point clouds remains limited, facing\nchallenges such as the absence of standardized datasets and the risk of\ncatastrophic forgetting and error accumulation during prolonged adaptation. To\ntackle these challenges, we propose APCoTTA, the first CTTA method tailored for\nALS point cloud semantic segmentation. We propose a dynamic trainable layer\nselection module. This module utilizes gradient information to select\nlow-confidence layers for training, and the remaining layers are kept frozen,\nmitigating catastrophic forgetting. To further reduce error accumulation, we\npropose an entropy-based consistency loss. By losing such samples based on\nentropy, we apply consistency loss only to the reliable samples, enhancing\nmodel stability. In addition, we propose a random parameter interpolation\nmechanism, which randomly blends parameters from the selected trainable layers\nwith those of the source model. This approach helps balance target adaptation\nand source knowledge retention, further alleviating forgetting. Finally, we\nconstruct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA\nbenchmarks for ALS point cloud segmentation. Experimental results demonstrate\nthat APCoTTA achieves the best performance on two benchmarks, with mIoU\nimprovements of approximately 9% and 14% over direct inference. The new\nbenchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.\n","authors":["Yuan Gao","Shaobo Xia","Sheng Nie","Cheng Wang","Xiaohuan Xi","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2505.09971v1.pdf","comment":"18 pages,12 figures"},{"id":"http://arxiv.org/abs/2406.12632v2","updated":"2025-05-15T05:17:41Z","published":"2024-06-18T13:59:10Z","title":"Cyclic 2.5D Perceptual Loss for Cross-Modal 3D Medical Image Synthesis:\n  T1w MRI to Tau PET","summary":"  There is a demand for medical image synthesis or translation to generate\nsynthetic images of missing modalities from available data. This need stems\nfrom challenges such as restricted access to high-cost imaging devices,\ngovernment regulations, or failure to follow up with patients or study\nparticipants. In medical imaging, preserving high-level semantic features is\noften more critical than achieving pixel-level accuracy. Perceptual loss\nfunctions are widely employed to train medical image synthesis or translation\nmodels, as they quantify differences in high-level image features using a\npre-trained feature extraction network. While 3D and 2.5D perceptual losses are\nused in 3D medical image synthesis, they face challenges, such as the lack of\npre-trained 3D models or difficulties in balancing loss reduction across\ndifferent planes. In this work, we focus on synthesizing 3D tau PET images from\n3D T1-weighted MR images. We propose a cyclic 2.5D perceptual loss that\nsequentially computes the 2D average perceptual loss for each of the axial,\ncoronal, and sagittal planes over epochs, with the cycle duration gradually\ndecreasing. Additionally, we process tau PET images using by-manufacturer\nstandardization to enhance the preservation of high-SUVR regions indicative of\ntau pathology and mitigate SUVR variability caused by inter-manufacturer\ndifferences. We combine the proposed loss with SSIM and MSE losses and\ndemonstrate its effectiveness in improving both quantitative and qualitative\nperformance across various generative models, including U-Net, UNETR,\nSwinUNETR, CycleGAN, and Pix2Pix.\n","authors":["Junho Moon","Symac Kim","Haejun Chung","Ikbeom Jang"],"pdf_url":"https://arxiv.org/pdf/2406.12632v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09967v1","updated":"2025-05-15T05:07:00Z","published":"2025-05-15T05:07:00Z","title":"TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression\n  Recognition","summary":"  Facial expression recognition (FER) in the wild remains a challenging task\ndue to the subtle and localized nature of expression-related features, as well\nas the complex variations in facial appearance. In this paper, we introduce a\nnovel framework that explicitly focuses on Texture Key Driver Factors (TKDF),\nlocalized texture regions that exhibit strong discriminative power across\nemotional categories. By carefully observing facial image patterns, we identify\nthat certain texture cues, such as micro-changes in skin around the brows,\neyes, and mouth, serve as primary indicators of emotional dynamics. To\neffectively capture and leverage these cues, we propose a FER architecture\ncomprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual\nInformation Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced\nwith multi-branch attention to extract fine-grained texture representations,\nwhile DCIF refines these features by filtering context through adaptive pooling\nand attention mechanisms. Experimental results on RAF-DB and KDEF datasets\ndemonstrate that our method achieves state-of-the-art performance, verifying\nthe effectiveness and robustness of incorporating TKDFs into FER pipelines.\n","authors":["Liqian Deng"],"pdf_url":"https://arxiv.org/pdf/2505.09967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05904v2","updated":"2025-05-15T05:01:49Z","published":"2025-04-08T11:02:14Z","title":"Saliency-Motion Guided Trunk-Collateral Network for Unsupervised Video\n  Object Segmentation","summary":"  Recent mainstream unsupervised video object segmentation (UVOS)\nmotion-appearance approaches use either the bi-encoder structure to separately\nencode motion and appearance features, or the uni-encoder structure for joint\nencoding. However, these methods fail to properly balance the motion-appearance\nrelationship. Consequently, even with complex fusion modules for\nmotion-appearance integration, the extracted suboptimal features degrade the\nmodels' overall performance. Moreover, the quality of optical flow varies\nacross scenarios, making it insufficient to rely solely on optical flow to\nachieve high-quality segmentation results. To address these challenges, we\npropose the Saliency-Motion guided Trunk-Collateral Network (SMTC-Net), which\nbetter balances the motion-appearance relationship and incorporates model's\nintrinsic saliency information to enhance segmentation performance.\nSpecifically, considering that optical flow maps are derived from RGB images,\nthey share both commonalities and differences. Accordingly, we propose a novel\nTrunk-Collateral structure for motion-appearance UVOS. The shared trunk\nbackbone captures the motion-appearance commonality, while the collateral\nbranch learns the uniqueness of motion features. Furthermore, an Intrinsic\nSaliency guided Refinement Module (ISRM) is devised to efficiently leverage the\nmodel's intrinsic saliency information to refine high-level features, and\nprovide pixel-level guidance for motion-appearance fusion, thereby enhancing\nperformance without additional input. Experimental results show that SMTC-Net\nachieved state-of-the-art performance on three UVOS datasets ( 89.2% J&F on\nDAVIS-16, 76% J on YouTube-Objects, 86.4% J on FBMS ) and four standard video\nsalient object detection (VSOD) benchmarks with the notable increase,\ndemonstrating its effectiveness and superiority over previous methods.\n","authors":["Xiangyu Zheng","Wanyun Li","Songcheng He","Jianping Fan","Xiaoqiang Li","We Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.05904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09965v1","updated":"2025-05-15T04:59:02Z","published":"2025-05-15T04:59:02Z","title":"MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier\n  Refinement for Diffusion-Based Disease Trajectory Prediction","summary":"  Modelling disease progression in precision medicine requires capturing\ncomplex spatio-temporal dynamics while preserving anatomical integrity.\nExisting methods often struggle with longitudinal dependencies and structural\nconsistency in progressive disorders. To address these limitations, we\nintroduce MambaControl, a novel framework that integrates selective state-space\nmodelling with diffusion processes for high-fidelity prediction of medical\nimage trajectories. To better capture subtle structural changes over time while\nmaintaining anatomical consistency, MambaControl combines Mamba-based\nlong-range modelling with graph-guided anatomical control to more effectively\nrepresent anatomical correlations. Furthermore, we introduce Fourier-enhanced\nspectral graph representations to capture spatial coherence and multiscale\ndetail, enabling MambaControl to achieve state-of-the-art performance in\nAlzheimer's disease prediction. Quantitative and regional evaluations\ndemonstrate improved progression prediction quality and anatomical fidelity,\nhighlighting its potential for personalised prognosis and clinical decision\nsupport.\n","authors":["Hao Yang","Tao Tan","Shuai Tan","Weiqin Yang","Kunyan Cai","Calvin Chen","Yue Sun"],"pdf_url":"https://arxiv.org/pdf/2505.09965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08910v2","updated":"2025-05-15T04:24:06Z","published":"2025-05-13T19:01:12Z","title":"Behind Maya: Building a Multilingual Vision Language Model","summary":"  In recent times, we have seen a rapid development of large Vision-Language\nModels (VLMs). They have shown impressive results on academic benchmarks,\nprimarily in widely spoken languages but lack performance on low-resource\nlanguages and varied cultural contexts. To address these limitations, we\nintroduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a\nmultilingual image-text pretraining dataset in eight languages, based on the\nLLaVA pretraining dataset; and 2) a multilingual image-text model supporting\nthese languages, enhancing cultural and linguistic comprehension in\nvision-language tasks. Code available at https://github.com/nahidalam/maya.\n","authors":["Nahid Alam","Karthik Reddy Kanjula","Surya Guthikonda","Timothy Chung","Bala Krishna S Vegesna","Abhipsha Das","Anthony Susevski","Ryan Sze-Yin Chan","S M Iftekhar Uddin","Shayekh Bin Islam","Roshan Santhosh","Snegha A","Drishti Sharma","Chen Liu","Isha Chaturvedi","Genta Indra Winata","Ashvanth. S","Snehanshu Mukherjee","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2505.08910v2.pdf","comment":"Accepted at VLMs4ALL CVPR 2025 Workshop; corrected workshop name\n  spelling"},{"id":"http://arxiv.org/abs/2501.03021v2","updated":"2025-05-15T04:15:14Z","published":"2025-01-06T14:05:19Z","title":"A Trust-Guided Approach to MR Image Reconstruction with Side Information","summary":"  Reducing MRI scan times can improve patient care and lower healthcare costs.\nMany acceleration methods are designed to reconstruct diagnostic-quality images\nfrom sparse k-space data, via an ill-posed or ill-conditioned linear inverse\nproblem (LIP). To address the resulting ambiguities, it is crucial to\nincorporate prior knowledge into the optimization problem, e.g., in the form of\nregularization. Another form of prior knowledge less commonly used in medical\nimaging is the readily available auxiliary data (a.k.a. side information)\nobtained from sources other than the current acquisition. In this paper, we\npresent the Trust- Guided Variational Network (TGVN), an end-to-end deep\nlearning framework that effectively and reliably integrates side information\ninto LIPs. We demonstrate its effectiveness in multi-coil, multi-contrast MRI\nreconstruction, where incomplete or low-SNR measurements from one contrast are\nused as side information to reconstruct high-quality images of another contrast\nfrom heavily under-sampled data. TGVN is robust across different contrasts,\nanatomies, and field strengths. Compared to baselines utilizing side\ninformation, TGVN achieves superior image quality while preserving subtle\npathological features even at challenging acceleration levels, drastically\nspeeding up acquisition while minimizing hallucinations. Source code and\ndataset splits are available on github.com/sodicksonlab/TGVN.\n","authors":["Arda AtalÄ±k","Sumit Chopra","Daniel K. Sodickson"],"pdf_url":"https://arxiv.org/pdf/2501.03021v2.pdf","comment":"27 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.09943v1","updated":"2025-05-15T03:56:36Z","published":"2025-05-15T03:56:36Z","title":"CSPENet: Contour-Aware and Saliency Priors Embedding Network for\n  Infrared Small Target Detection","summary":"  Infrared small target detection (ISTD) plays a critical role in a wide range\nof civilian and military applications. Existing methods suffer from\ndeficiencies in the localization of dim targets and the perception of contour\ninformation under dense clutter environments, severely limiting their detection\nperformance. To tackle these issues, we propose a contour-aware and saliency\npriors embedding network (CSPENet) for ISTD. We first design a\nsurround-convergent prior extraction module (SCPEM) that effectively captures\nthe intrinsic characteristic of target contour pixel gradients converging\ntoward their center. This module concurrently extracts two collaborative\npriors: a boosted saliency prior for accurate target localization and\nmulti-scale structural priors for comprehensively enriching contour detail\nrepresentation. Building upon this, we propose a dual-branch priors embedding\narchitecture (DBPEA) that establishes differentiated feature fusion pathways,\nembedding these two priors at optimal network positions to achieve performance\nenhancement. Finally, we develop an attention-guided feature enhancement module\n(AGFEM) to refine feature representations and improve saliency estimation\naccuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and\nNUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art\nmethods in detection performance. The code is available at\nhttps://github.com/IDIP2025/CSPENet.\n","authors":["Jiakun Deng","Kexuan Li","Xingye Cui","Jiaxuan Li","Chang Long","Tian Pu","Zhenming Peng"],"pdf_url":"https://arxiv.org/pdf/2505.09943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09939v1","updated":"2025-05-15T03:52:42Z","published":"2025-05-15T03:52:42Z","title":"Non-Registration Change Detection: A Novel Change Detection Task and\n  Benchmark Dataset","summary":"  In this study, we propose a novel remote sensing change detection task,\nnon-registration change detection, to address the increasing number of\nemergencies such as natural disasters, anthropogenic accidents, and military\nstrikes. First, in light of the limited discourse on the issue of\nnon-registration change detection, we systematically propose eight scenarios\nthat could arise in the real world and potentially contribute to the occurrence\nof non-registration problems. Second, we develop distinct image transformation\nschemes tailored to various scenarios to convert the available registration\nchange detection dataset into a non-registration version. Finally, we\ndemonstrate that non-registration change detection can cause catastrophic\ndamage to the state-of-the-art methods. Our code and dataset are available at\nhttps://github.com/ShanZard/NRCD.\n","authors":["Zhe Shan","Lei Zhou","Liu Mao","Shaofan Chen","Chuanqiu Ren","Xia Xie"],"pdf_url":"https://arxiv.org/pdf/2505.09939v1.pdf","comment":"Accepted to IGARSS 2025"},{"id":"http://arxiv.org/abs/2505.09935v1","updated":"2025-05-15T03:40:29Z","published":"2025-05-15T03:40:29Z","title":"VRU-CIPI: Crossing Intention Prediction at Intersections for Improving\n  Vulnerable Road Users Safety","summary":"  Understanding and predicting human behavior in-thewild, particularly at urban\nintersections, remains crucial for enhancing interaction safety between road\nusers. Among the most critical behaviors are crossing intentions of Vulnerable\nRoad Users (VRUs), where misinterpretation may result in dangerous conflicts\nwith oncoming vehicles. In this work, we propose the VRU-CIPI framework with a\nsequential attention-based model designed to predict VRU crossing intentions at\nintersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal\ndynamics in VRU movements, combined with a multi-head Transformer\nself-attention mechanism to encode contextual and spatial dependencies critical\nfor predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed\nachieves state-of-the-art performance with an accuracy of 96.45% and achieving\nreal-time inference speed reaching 33 frames per second. Furthermore, by\nintegrating with Infrastructure-to-Vehicles (I2V) communication, our approach\ncan proactively enhance intersection safety through timely activation of\ncrossing signals and providing early warnings to connected vehicles, ensuring\nsmoother and safer interactions for all road users.\n","authors":["Ahmed S. Abdelrahman","Mohamed Abdel-Aty","Quoc Dai Tran"],"pdf_url":"https://arxiv.org/pdf/2505.09935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09927v1","updated":"2025-05-15T03:24:54Z","published":"2025-05-15T03:24:54Z","title":"DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation\n  of Medical Image Segmentation","summary":"  Domain adaptation addresses the challenge of model performance degradation\ncaused by domain gaps. In the typical setup for unsupervised domain adaptation,\nlabeled data from a source domain and unlabeled data from a target domain are\nused to train a target model. However, access to labeled source domain data,\nparticularly in medical datasets, can be restricted due to privacy policies. As\na result, research has increasingly shifted to source-free domain adaptation\n(SFDA), which requires only a pretrained model from the source domain and\nunlabeled data from the target domain data for adaptation. Existing SFDA\nmethods often rely on domain-specific image style translation and\nself-supervision techniques to bridge the domain gap and train the target\ndomain model. However, the quality of domain-specific style-translated images\nand pseudo-labels produced by these methods still leaves room for improvement.\nMoreover, training the entire model during adaptation can be inefficient under\nlimited supervision. In this paper, we propose a novel SFDA framework to\naddress these challenges. Specifically, to effectively mitigate the impact of\ndomain gap in the initial training phase, we introduce preadaptation to\ngenerate a preadapted model, which serves as an initialization of target model\nand allows for the generation of high-quality enhanced pseudo-labels without\nintroducing extra parameters. Additionally, we propose a data-dependent\nfrequency prompt to more effectively translate target domain images into a\nsource-like style. To further enhance adaptation, we employ a style-related\nlayer fine-tuning strategy, specifically designed for SFDA, to train the target\nmodel using the prompted target domain images and pseudo-labels. Extensive\nexperiments on cross-modality abdominal and cardiac SFDA segmentation tasks\ndemonstrate that our proposed method outperforms existing state-of-the-art\nmethods.\n","authors":["Siqi Yin","Shaolei Liu","Manning Wang"],"pdf_url":"https://arxiv.org/pdf/2505.09927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09926v1","updated":"2025-05-15T03:24:28Z","published":"2025-05-15T03:24:28Z","title":"AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection","summary":"  Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.\n","authors":["Bin-Bin Gao","Yue Zhu","Jiangtao Yan","Yuezhi Cai","Weixi Zhang","Meng Wang","Jun Liu","Yong Liu","Lei Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2505.09926v1.pdf","comment":"27 pages, 15 figures, 22 tables"},{"id":"http://arxiv.org/abs/2505.09915v1","updated":"2025-05-15T03:00:32Z","published":"2025-05-15T03:00:32Z","title":"Large-Scale Gaussian Splatting SLAM","summary":"  The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting (3DGS) have shown encouraging and impressive results for visual SLAM.\nHowever, most representative methods require RGBD sensors and are only\navailable for indoor environments. The robustness of reconstruction in\nlarge-scale outdoor scenarios remains unexplored. This paper introduces a\nlarge-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The\nproposed LSG-SLAM employs a multi-modality strategy to estimate prior poses\nunder large view changes. In tracking, we introduce feature-alignment warping\nconstraints to alleviate the adverse effects of appearance similarity in\nrendering losses. For the scalability of large-scale scenarios, we introduce\ncontinuous Gaussian Splatting submaps to tackle unbounded scenes with limited\nmemory. Loops are detected between GS submaps by place recognition and the\nrelative pose between looped keyframes is optimized utilizing rendering and\nfeature warping losses. After the global optimization of camera poses and\nGaussian points, a structure refinement module enhances the reconstruction\nquality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM\nachieves superior performance over existing Neural, 3DGS-based, and even\ntraditional approaches. Project page: https://lsg-slam.github.io.\n","authors":["Zhe Xin","Chenyang Wu","Penghui Huang","Yanyong Zhang","Yinian Mao","Guoquan Huang"],"pdf_url":"https://arxiv.org/pdf/2505.09915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00496v2","updated":"2025-05-15T03:00:00Z","published":"2025-04-01T07:43:10Z","title":"Learned Image Compression with Dictionary-based Entropy Model","summary":"  Learned image compression methods have attracted great research interest and\nexhibited superior rate-distortion performance to the best classical image\ncompression standards of the present. The entropy model plays a key role in\nlearned image compression, which estimates the probability distribution of the\nlatent representation for further entropy coding. Most existing methods\nemployed hyper-prior and auto-regressive architectures to form their entropy\nmodels. However, they only aimed to explore the internal dependencies of latent\nrepresentation while neglecting the importance of extracting prior from\ntraining data. In this work, we propose a novel entropy model named\nDictionary-based Cross Attention Entropy model, which introduces a learnable\ndictionary to summarize the typical structures occurring in the training\ndataset to enhance the entropy model. Extensive experimental results have\ndemonstrated that the proposed model strikes a better balance between\nperformance and latency, achieving state-of-the-art results on various\nbenchmark datasets.\n","authors":["Jingbo Lu","Leheng Zhang","Xingyu Zhou","Mu Li","Wen Li","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2504.00496v2.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2411.08665v2","updated":"2025-05-15T02:43:56Z","published":"2024-11-13T14:59:00Z","title":"OSMLoc: Single Image-Based Visual Localization in OpenStreetMap with\n  Fused Geometric and Semantic Guidance","summary":"  OpenStreetMap (OSM), a rich and versatile source of volunteered geographic\ninformation (VGI), facilitates human self-localization and scene understanding\nby integrating nearby visual observations with vectorized map data. However,\nthe disparity in modalities and perspectives poses a major challenge for\neffectively matching camera imagery with compact map representations, thereby\nlimiting the full potential of VGI data in real-world localization\napplications.\n  Inspired by the fact that the human brain relies on the fusion of geometric\nand semantic understanding for spatial localization tasks, we propose the\nOSMLoc in this paper. OSMLoc is a brain-inspired visual localization approach\nbased on first-person-view images against the OSM maps. It integrates semantic\nand geometric guidance to significantly improve accuracy, robustness, and\ngeneralization capability. First, we equip the OSMLoc with the visual\nfoundational model to extract powerful image features. Second, a\ngeometry-guided depth distribution adapter is proposed to bridge the monocular\ndepth estimation and camera-to-BEV transform. Thirdly, the semantic embeddings\nfrom the OSM data are utilized as auxiliary guidance for image-to-OSM feature\nmatching. To validate the proposed OSMLoc, we collect a worldwide cross-area\nand cross-condition (CC) benchmark for extensive evaluation. Experiments on the\nMGL dataset, CC validation benchmark, and KITTI dataset have demonstrated the\nsuperiority of our method. Code, pre-trained models, CC validation benchmark,\nand additional results are available at: https://github.com/WHU-USI3DV/OSMLoc.\n","authors":["Youqi Liao","Xieyuanli Chen","Shuhao Kang","Jianping Li","Zhen Dong","Hongchao Fan","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2411.08665v2.pdf","comment":"16 pages, technical report"},{"id":"http://arxiv.org/abs/2505.09193v2","updated":"2025-05-15T01:32:30Z","published":"2025-05-14T06:55:37Z","title":"BiECVC: Gated Diversification of Bidirectional Contexts for Learned\n  Video Compression","summary":"  Recent forward prediction-based learned video compression (LVC) methods have\nachieved impressive results, even surpassing VVC reference software VTM under\nthe Low Delay B (LDB) configuration. In contrast, learned bidirectional video\ncompression (BVC) remains underexplored and still lags behind its forward-only\ncounterparts. This performance gap is mainly due to the limited ability to\nextract diverse and accurate contexts: most existing BVCs primarily exploit\ntemporal motion while neglecting non-local correlations across frames.\nMoreover, they lack the adaptability to dynamically suppress harmful contexts\narising from fast motion or occlusion. To tackle these challenges, we propose\nBiECVC, a BVC framework that incorporates diversified local and non-local\ncontext modeling along with adaptive context gating. For local context\nenhancement, BiECVC reuses high-quality features from lower layers and aligns\nthem using decoded motion vectors without introducing extra motion overhead. To\nmodel non-local dependencies efficiently, we adopt a linear attention mechanism\nthat balances performance and complexity. To further mitigate the impact of\ninaccurate context prediction, we introduce Bidirectional Context Gating,\ninspired by data-dependent decay in recent autoregressive language models, to\ndynamically filter contextual information based on conditional coding results.\nExtensive experiments demonstrate that BiECVC achieves state-of-the-art\nperformance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2\nunder the Random Access (RA) configuration with intra periods of 32 and 64,\nrespectively. To our knowledge, BiECVC is the first learned video codec to\nsurpass VTM 13.2 RA across all standard test datasets. Code will be available\nat https://github.com/JiangWeibeta/ECVC.\n","authors":["Wei Jiang","Junru Li","Kai Zhang","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.09193v2.pdf","comment":"The first learned video codec that surpasses VTM 13.2 RA across all\n  standard test datasets. Code will be available at\n  https://github.com/JiangWeibeta/ECVC"},{"id":"http://arxiv.org/abs/2505.06512v3","updated":"2025-05-15T01:04:26Z","published":"2025-05-10T05:02:58Z","title":"HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image\n  Generation","summary":"  Text-to-image synthesis has progressed to the point where models can generate\nvisually compelling images from natural language prompts. Yet, existing methods\noften fail to reconcile high-level semantic fidelity with explicit spatial\ncontrol, particularly in scenes involving multiple objects, nuanced relations,\nor complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal\nAlignment (HCMA) framework for grounded text-to-image generation. HCMA\nintegrates two alignment modules into each diffusion sampling step: a global\nmodule that continuously aligns latent representations with textual\ndescriptions to ensure scene-level coherence, and a local module that employs\nbounding-box layouts to anchor objects at specified locations, enabling\nfine-grained spatial control. Extensive experiments on the MS-COCO 2014\nvalidation set show that HCMA surpasses state-of-the-art baselines, achieving a\n0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP\nScore. These results demonstrate HCMA's effectiveness in faithfully capturing\nintricate textual semantics while adhering to user-defined spatial constraints,\noffering a robust solution for semantically grounded image generation. Our code\nis available at https://github.com/hwang-cs-ime/HCMA.\n","authors":["Hang Wang","Zhi-Qi Cheng","Chenhao Lin","Chao Shen","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.06512v3.pdf","comment":"10 pages, 4 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.03294v2","updated":"2025-05-15T14:06:05Z","published":"2024-09-05T06:59:56Z","title":"FedPCL-CDR: A Federated Prototype-based Contrastive Learning Framework\n  for Privacy-Preserving Cross-domain Recommendation","summary":"  Cross-domain recommendation (CDR) aims to improve recommendation accuracy in\nsparse domains by transferring knowledge from data-rich domains. However,\nexisting CDR approaches often assume that user-item interaction data across\ndomains is publicly available, neglecting user privacy concerns. Additionally,\nthey experience performance degradation with sparse overlapping users due to\ntheir reliance on a large number of fully shared users for knowledge transfer.\nTo address these challenges, we propose a Federated Prototype-based Contrastive\nLearning (CL) framework for Privacy Preserving CDR, called FedPCL-CDR. This\napproach utilizes non-overlapping user information and differential prototypes\nto improve model performance within a federated learning framework. FedPCL-CDR\ncomprises two key modules: local domain (client) learning and global server\naggregation. In the local domain, FedPCL-CDR first clusters all user data and\nutilizes local differential privacy (LDP) to learn differential prototypes,\neffectively utilizing non-overlapping user information and protecting user\nprivacy. It then conducts knowledge transfer by employing both local and global\nprototypes returned from the server in a CL manner. Meanwhile, the global\nserver aggregates differential prototypes sent from local domains to learn both\nlocal and global prototypes. Extensive experiments on four CDR tasks across\nAmazon and Douban datasets demonstrate that FedPCL-CDR surpasses SOTA\nbaselines. We release our code at https://github.com/Lili1013/FedPCL CDR\n","authors":["Li Wang","Qiang Wu","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2409.03294v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10212v1","updated":"2025-05-15T12:16:36Z","published":"2025-05-15T12:16:36Z","title":"Do LLMs Memorize Recommendation Datasets? A Preliminary Study on\n  MovieLens-1M","summary":"  Large Language Models (LLMs) have become increasingly central to\nrecommendation scenarios due to their remarkable natural language understanding\nand generation capabilities. Although significant research has explored the use\nof LLMs for various recommendation tasks, little effort has been dedicated to\nverifying whether they have memorized public recommendation dataset as part of\ntheir training data. This is undesirable because memorization reduces the\ngeneralizability of research findings, as benchmarking on memorized datasets\ndoes not guarantee generalization to unseen datasets. Furthermore, memorization\ncan amplify biases, for example, some popular items may be recommended more\nfrequently than others.\n  In this work, we investigate whether LLMs have memorized public\nrecommendation datasets. Specifically, we examine two model families (GPT and\nLlama) across multiple sizes, focusing on one of the most widely used dataset\nin recommender systems: MovieLens-1M. First, we define dataset memorization as\nthe extent to which item attributes, user profiles, and user-item interactions\ncan be retrieved by prompting the LLMs. Second, we analyze the impact of\nmemorization on recommendation performance. Lastly, we examine whether\nmemorization varies across model families and model sizes. Our results reveal\nthat all models exhibit some degree of memorization of MovieLens-1M, and that\nrecommendation performance is related to the extent of memorization. We have\nmade all the code publicly available at:\nhttps://github.com/sisinflab/LLM-MemoryInspector\n","authors":["Dario Di Palma","Felice Antonio Merra","Maurizio Sfilio","Vito Walter Anelli","Fedelucio Narducci","Tommaso Di Noia"],"pdf_url":"https://arxiv.org/pdf/2505.10212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19458v3","updated":"2025-05-15T09:07:58Z","published":"2025-04-28T03:48:23Z","title":"Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal\n  Perspective","summary":"  Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from\ndifferent Multi-Modal Knowledge Graphs (MMKGs), a critical information\nretrieval task. Existing studies have explored various fusion paradigms and\nconsistency constraints to improve the alignment of equivalent entities, while\noverlooking that the visual modality may not always contribute positively.\nEmpirically, entities with low-similarity images usually generate\nunsatisfactory performance, highlighting the limitation of overly relying on\nvisual features. We believe the model can be biased toward the visual modality,\nleading to a shortcut image-matching task. To address this, we propose a\ncounterfactual debiasing framework for MMEA, termed CDMEA, which investigates\nvisual modality bias from a causal perspective. Our approach aims to leverage\nboth visual and graph modalities to enhance MMEA while suppressing the direct\ncausal effect of the visual modality on model predictions. By estimating the\nTotal Effect (TE) of both modalities and excluding the Natural Direct Effect\n(NDE) of the visual modality, we ensure that the model predicts based on the\nTotal Indirect Effect (TIE), effectively utilizing both modalities and reducing\nvisual modality bias. Extensive experiments on 9 benchmark datasets show that\nCDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,\nhigh-noise, and low-resource data scenarios.\n","authors":["Taoyu Su","Jiawei Sheng","Duohe Ma","Xiaodong Li","Juwei Yue","Mengxiao Song","Yingkai Tang","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2504.19458v3.pdf","comment":"Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,"},{"id":"http://arxiv.org/abs/2505.10043v1","updated":"2025-05-15T07:41:14Z","published":"2025-05-15T07:41:14Z","title":"Boosting Text-to-Chart Retrieval through Training with Synthesized\n  Semantic Insights","summary":"  Charts are crucial for data analysis and decision-making.Text-to-chart\nretrieval systems have become increasingly important for Business Intelligence\n(BI), where users need to find relevant charts that match their analytical\nneeds. These needs can be categorized into precise queries that are\nwell-specified and fuzzy queries that are more exploratory -- both require\nunderstanding the semantics and context of the charts. However, existing\ntext-to-chart retrieval solutions often fail to capture the semantic content\nand contextual information of charts, primarily due to the lack of\ncomprehensive metadata (or semantic insights). To address this limitation, we\npropose a training data development pipeline that automatically synthesizes\nhierarchical semantic insights for charts, covering visual patterns\n(visual-oriented), statistical properties (statistics-oriented), and practical\napplications (task-oriented), which produces 207,498 semantic insights for\n69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to\nlearn better representations of charts for text-to-chart retrieval. Our method\nleverages rich semantic insights during the training phase to develop a model\nthat understands both visual and semantic aspects of charts.To evaluate\ntext-to-chart retrieval performance, we curate the first benchmark, CRBench,\nfor this task with 21,862 charts and 326 text queries from real-world BI\napplications, with ground-truth labels verified by the crowd\nworkers.Experiments show that ChartFinder significantly outperforms existing\nmethods in text-to-chart retrieval tasks across various settings. For precise\nqueries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than\nstate-of-the-art models. In fuzzy query tasks, our method also demonstrates\nconsistent improvements, with an average increase of 5% across nearly all\nmetrics.\n","authors":["Yifan Wu","Lutao Yan","Yizhang Zhu","Yinan Mei","Jiannan Wang","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2505.10043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21300v4","updated":"2025-05-15T03:22:21Z","published":"2024-07-31T03:00:59Z","title":"SAKR: Enhancing Retrieval-Augmented Generation via Streaming Algorithm\n  and K-Means Clustering","summary":"  Retrieval-augmented generation (RAG) has achieved significant success in\ninformation retrieval to assist large language models LLMs because it builds an\nexternal knowledge database. However, it also has many problems, it consumes a\nlot of memory because of the enormous database, and it cannot update the\nestablished index database in time when confronted with massive streaming data.\nTo reduce the memory required for building the database and maintain accuracy\nsimultaneously, we proposed a new approach integrating a streaming algorithm\nwith k-means clustering into RAG. Our approach applied a streaming algorithm to\nupdate the index dynamically and reduce memory consumption. Additionally, the\nk-means algorithm clusters highly similar documents, and the query time would\nbe shortened. We conducted comparative experiments on four methods, and the\nresults indicated that RAG with streaming algorithm and k-means clusters\noutperforms traditional RAG in accuracy and memory, particularly when dealing\nwith large-scale streaming data.\n","authors":["Haoyu Kang","Yuzhou Zhu","Yukun Zhong","Ke Wang"],"pdf_url":"https://arxiv.org/pdf/2407.21300v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02288v4","updated":"2025-05-15T02:47:32Z","published":"2025-04-03T05:27:55Z","title":"Shallow AutoEncoding Recommender with Cold Start Handling via Side\n  Features","summary":"  User and item cold starts present significant challenges in industrial\napplications of recommendation systems. Supplementing user-item interaction\ndata with metadata is a common solution-but often at the cost of introducing\nadditional biases. In this work, we introduce an augmented EASE model that\nseamlessly integrates both user and item side information to address these cold\nstart issues. Our straightforward, autoencoder-based method produces a\nclosed-form solution that leverages rich content signals for cold items while\nrefining user representations in data-sparse environments. Importantly, our\nmethod strikes a balance by effectively recommending cold start items and\nhandling cold start users without incurring extra bias, and it maintains strong\nperformance in warm settings. Experimental results demonstrate improved\nrecommendation accuracy and robustness compared to previous collaborative\nfiltering approaches. Moreover, our model serves as a strong baseline for\nfuture comparative studies.\n","authors":["Edward DongBo Cui","Lu Zhang","William Ping-hsun Lee"],"pdf_url":"https://arxiv.org/pdf/2504.02288v4.pdf","comment":"Preparing submission to CIKM 2025; 2 Figures; 4 Tables; 13 pages;\n  Python code implementation example"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2505.10559v1","updated":"2025-05-15T17:59:22Z","published":"2025-05-15T17:59:22Z","title":"Neural Thermodynamic Laws for Large Language Model Training","summary":"  Beyond neural scaling laws, little is known about the laws underlying large\nlanguage models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new\nframework that offers fresh insights into LLM training dynamics. On the\ntheoretical side, we demonstrate that key thermodynamic quantities (e.g.,\ntemperature, entropy, heat capacity, thermal conduction) and classical\nthermodynamic principles (e.g., the three laws of thermodynamics and the\nequipartition theorem) naturally emerge under river-valley loss landscape\nassumptions. On the practical side, this scientific perspective yields\nintuitive guidelines for designing learning rate schedules.\n","authors":["Ziming Liu","Yizhou Liu","Jeff Gore","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2505.10559v1.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2505.10556v1","updated":"2025-05-15T17:59:07Z","published":"2025-05-15T17:59:07Z","title":"An AI-driven framework for the prediction of personalised health\n  response to air pollution","summary":"  Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.\n","authors":["Nazanin Zounemat Kermani","Sadjad Naderi","Claire H. Dilliway","Claire E. Heaney","Shrreya Behll","Boyang Chen","Hisham Abubakar-Waziri","Alexandra E. Porter","Marc Chadeau-Hyam","Fangxin Fang","Ian M. Adcock","Kian Fan Chung","Christopher C. Pain"],"pdf_url":"https://arxiv.org/pdf/2505.10556v1.pdf","comment":"Kermani and Naderi share first authorship. 20 pages, 6 figures and 1\n  table"},{"id":"http://arxiv.org/abs/2505.10545v1","updated":"2025-05-15T17:54:29Z","published":"2025-05-15T17:54:29Z","title":"Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug\n  Design","summary":"  Developing bioactive molecules remains a central, time- and cost-heavy\nchallenge in drug discovery, particularly for novel targets lacking structural\nor functional data. Pharmacophore modeling presents an alternative for\ncapturing the key features required for molecular bioactivity against a\nbiological target. In this work, we present PharmaDiff, a\npharmacophore-conditioned diffusion model for 3D molecular generation.\nPharmaDiff employs a transformer-based architecture to integrate an atom-based\nrepresentation of the 3D pharmacophore into the generative process, enabling\nthe precise generation of 3D molecular graphs that align with predefined\npharmacophore hypotheses. Through comprehensive testing, PharmaDiff\ndemonstrates superior performance in matching 3D pharmacophore constraints\ncompared to ligand-based drug design methods. Additionally, it achieves higher\ndocking scores across a range of proteins in structure-based drug design,\nwithout the need for target protein structures. By integrating pharmacophore\nmodeling with 3D generative techniques, PharmaDiff offers a powerful and\nflexible framework for rational drug design.\n","authors":["Amira Alakhdar","Barnabas Poczos","Newell Washburn"],"pdf_url":"https://arxiv.org/pdf/2505.10545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04689v3","updated":"2025-05-15T17:52:51Z","published":"2025-02-07T06:30:33Z","title":"ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning","summary":"  Large language models (LLMs) have demonstrated impressive capabilities on\ncomplex evaluation benchmarks, many of which are formulated as\nquestion-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts\nis becoming increasingly vital for advancing their development and\napplicability. This paper introduces ARR, an intuitive, effective, and general\nQA solving method that explicitly incorporates three key steps: analyzing the\nintent of the question, retrieving relevant information, and reasoning step by\nstep. Notably, this paper is the first to introduce intent analysis in QA,\nwhich plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA\ntasks demonstrate that ARR consistently outperforms the baseline methods.\nAblation and case studies further validate the positive contributions of each\nARR component. Furthermore, experiments involving variations in prompt design\nindicate that ARR maintains its effectiveness regardless of the specific prompt\nformulation. Additionally, extensive evaluations across various model sizes,\nLLM series, and generation settings solidify the effectiveness, robustness, and\ngeneralizability of ARR.\n","authors":["Yuwei Yin","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2502.04689v3.pdf","comment":"21 pages. Code: https://github.com/YuweiYin/ARR"},{"id":"http://arxiv.org/abs/2501.18901v2","updated":"2025-05-15T17:48:47Z","published":"2025-01-31T05:42:58Z","title":"Lightspeed Geometric Dataset Distance via Sliced Optimal Transport","summary":"  We introduce sliced optimal transport dataset distance (s-OTDD), a\nmodel-agnostic, embedding-agnostic approach for dataset comparison that\nrequires no training, is robust to variations in the number of classes, and can\nhandle disjoint label sets. The core innovation is Moment Transform Projection\n(MTP), which maps a label, represented as a distribution over features, to a\nreal number. Using MTP, we derive a data point projection that transforms\ndatasets into one-dimensional distributions. The s-OTDD is defined as the\nexpected Wasserstein distance between the projected distributions, with respect\nto random projection parameters. Leveraging the closed form solution of\none-dimensional optimal transport, s-OTDD achieves (near-)linear computational\ncomplexity in the number of data points and feature dimensions and is\nindependent of the number of classes. With its geometrically meaningful\nprojection, s-OTDD strongly correlates with the optimal transport dataset\ndistance while being more efficient than existing dataset discrepancy measures.\nMoreover, it correlates well with the performance gap in transfer learning and\nclassification accuracy in data augmentation.\n","authors":["Khai Nguyen","Hai Nguyen","Tuan Pham","Nhat Ho"],"pdf_url":"https://arxiv.org/pdf/2501.18901v2.pdf","comment":"Accepted to ICML 2025, 16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2505.10533v1","updated":"2025-05-15T17:41:52Z","published":"2025-05-15T17:41:52Z","title":"Enhancing Multi-Image Question Answering via Submodular Subset Selection","summary":"  Large multimodal models (LMMs) have achieved high performance in\nvision-language tasks involving single image but they struggle when presented\nwith a collection of multiple images (Multiple Image Question Answering\nscenario). These tasks, which involve reasoning over large number of images,\npresent issues in scalability (with increasing number of images) and retrieval\nperformance. In this work, we propose an enhancement for retriever framework\nintroduced in MIRAGE model using submodular subset selection techniques. Our\nmethod leverages query-aware submodular functions, such as GraphCut, to\npre-select a subset of semantically relevant images before main retrieval\ncomponent. We demonstrate that using anchor-based queries and augmenting the\ndata improves submodular-retriever pipeline effectiveness, particularly in\nlarge haystack sizes.\n","authors":["Aaryan Sharma","Shivansh Gupta","Samar Agarwal","Vishak Prasad C.","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2505.10533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10526v1","updated":"2025-05-15T17:37:00Z","published":"2025-05-15T17:37:00Z","title":"MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative\n  Decoding of Vision-Language Models","summary":"  Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs.\n","authors":["Mugilan Ganesan","Shane Segal","Ankur Aggarwal","Nish Sinnadurai","Sean Lie","Vithursan Thangarasa"],"pdf_url":"https://arxiv.org/pdf/2505.10526v1.pdf","comment":"Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp"},{"id":"http://arxiv.org/abs/2505.10522v1","updated":"2025-05-15T17:30:29Z","published":"2025-05-15T17:30:29Z","title":"Knowledge capture, adaptation and composition (KCAC): A framework for\n  cross-task curriculum learning in robotic manipulation","summary":"  Reinforcement learning (RL) has demonstrated remarkable potential in robotic\nmanipulation but faces challenges in sample inefficiency and lack of\ninterpretability, limiting its applicability in real world scenarios. Enabling\nthe agent to gain a deeper understanding and adapt more efficiently to diverse\nworking scenarios is crucial, and strategic knowledge utilization is a key\nfactor in this process. This paper proposes a Knowledge Capture, Adaptation,\nand Composition (KCAC) framework to systematically integrate knowledge transfer\ninto RL through cross-task curriculum learning. KCAC is evaluated using a two\nblock stacking task in the CausalWorld benchmark, a complex robotic\nmanipulation environment. To our knowledge, existing RL approaches fail to\nsolve this task effectively, reflecting deficiencies in knowledge capture. In\nthis work, we redesign the benchmark reward function by removing rigid\nconstraints and strict ordering, allowing the agent to maximize total rewards\nconcurrently and enabling flexible task completion. Furthermore, we define two\nself-designed sub-tasks and implement a structured cross-task curriculum to\nfacilitate efficient learning. As a result, our KCAC approach achieves a 40\npercent reduction in training time while improving task success rates by 10\npercent compared to traditional RL methods. Through extensive evaluation, we\nidentify key curriculum design parameters subtask selection, transition timing,\nand learning rate that optimize learning efficiency and provide conceptual\nguidance for curriculum based RL frameworks. This work offers valuable insights\ninto curriculum design in RL and robotic learning.\n","authors":["Xinrui Wang","Yan Jin"],"pdf_url":"https://arxiv.org/pdf/2505.10522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08616v2","updated":"2025-05-15T17:30:10Z","published":"2025-05-13T14:34:46Z","title":"A portable diagnosis model for Keratoconus using a smartphone","summary":"  Keratoconus (KC) is a corneal disorder that results in blurry and distorted\nvision. Traditional diagnostic tools, while effective, are often bulky, costly,\nand require professional operation. In this paper, we present a portable and\ninnovative methodology for diagnosing. Our proposed approach first captures the\nimage reflected on the eye's cornea when a smartphone screen-generated Placido\ndisc sheds its light on an eye, then utilizes a two-stage diagnosis for\nidentifying the KC cornea and pinpointing the location of the KC on the cornea.\nThe first stage estimates the height and width of the Placido disc extracted\nfrom the captured image to identify whether it has KC. In this KC\nidentification, k-means clustering is implemented to discern statistical\ncharacteristics, such as height and width values of extracted Placido discs,\nfrom non-KC (control) and KC-affected groups. The second stage involves the\ncreation of a distance matrix, providing a precise localization of KC on the\ncornea, which is critical for efficient treatment planning. The analysis of\nthese distance matrices, paired with a logistic regression model and robust\nstatistical analysis, reveals a clear distinction between control and KC\ngroups. The logistic regression model, which classifies small areas on the\ncornea as either control or KC-affected based on the corresponding inter-disc\ndistances in the distance matrix, reported a classification accuracy of 96.94%,\nwhich indicates that we can effectively pinpoint the protrusion caused by KC.\nThis comprehensive, smartphone-based method is expected to detect KC and\nstreamline timely treatment.\n","authors":["Yifan Li","Peter Ho","Jo Woon Chong"],"pdf_url":"https://arxiv.org/pdf/2505.08616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08517v2","updated":"2025-05-15T17:28:04Z","published":"2025-05-13T12:48:36Z","title":"A Deep Learning-Driven Inhalation Injury Grading Assistant Using\n  Bronchoscopy Images","summary":"  Inhalation injuries present a challenge in clinical diagnosis and grading due\nto Conventional grading methods such as the Abbreviated Injury Score (AIS)\nbeing subjective and lacking robust correlation with clinical parameters like\nmechanical ventilation duration and patient mortality. This study introduces a\nnovel deep learning-based diagnosis assistant tool for grading inhalation\ninjuries using bronchoscopy images to overcome subjective variability and\nenhance consistency in severity assessment. Our approach leverages data\naugmentation techniques, including graphic transformations, Contrastive\nUnpaired Translation (CUT), and CycleGAN, to address the scarcity of medical\nimaging data. We evaluate the classification performance of two deep learning\nmodels, GoogLeNet and Vision Transformer (ViT), across a dataset significantly\nexpanded through these augmentation methods. The results demonstrate GoogLeNet\ncombined with CUT as the most effective configuration for grading inhalation\ninjuries through bronchoscopy images and achieves a classification accuracy of\n97.8%. The histograms and frequency analysis evaluations reveal variations\ncaused by the augmentation CUT with distribution changes in the histogram and\ntexture details of the frequency spectrum. PCA visualizations underscore the\nCUT substantially enhances class separability in the feature space. Moreover,\nGrad-CAM analyses provide insight into the decision-making process; mean\nintensity for CUT heatmaps is 119.6, which significantly exceeds 98.8 of the\noriginal datasets. Our proposed tool leverages mechanical ventilation periods\nas a novel grading standard, providing comprehensive diagnostic support.\n","authors":["Yifan Li","Alan W Pang","Jo Woon Chong"],"pdf_url":"https://arxiv.org/pdf/2505.08517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10518v1","updated":"2025-05-15T17:25:03Z","published":"2025-05-15T17:25:03Z","title":"Multi-Token Prediction Needs Registers","summary":"  Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.\n","authors":["Anastasios Gerontopoulos","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2505.10518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10515v1","updated":"2025-05-15T17:21:54Z","published":"2025-05-15T17:21:54Z","title":"PnPXAI: A Universal XAI Framework Providing Automatic Explanations\n  Across Diverse Modalities and Models","summary":"  Recently, post hoc explanation methods have emerged to enhance model\ntransparency by attributing model outputs to input features. However, these\nmethods face challenges due to their specificity to certain neural network\narchitectures and data modalities. Existing explainable artificial intelligence\n(XAI) frameworks have attempted to address these challenges but suffer from\nseveral limitations. These include limited flexibility to diverse model\narchitectures and data modalities due to hard-coded implementations, a\nrestricted number of supported XAI methods because of the requirements for\nlayer-specific operations of attribution methods, and sub-optimal\nrecommendations of explanations due to the lack of evaluation and optimization\nphases. Consequently, these limitations impede the adoption of XAI technology\nin real-world applications, making it difficult for practitioners to select the\noptimal explanation method for their domain. To address these limitations, we\nintroduce \\textbf{PnPXAI}, a universal XAI framework that supports diverse data\nmodalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI\nautomatically detects model architectures, recommends applicable explanation\nmethods, and optimizes hyperparameters for optimal explanations. We validate\nthe framework's effectiveness through user surveys and showcase its versatility\nacross various domains, including medicine and finance.\n","authors":["Seongun Kim","Sol A Kim","Geonhyeong Kim","Enver Menadjiev","Chanwoo Lee","Seongwook Chung","Nari Kim","Jaesik Choi"],"pdf_url":"https://arxiv.org/pdf/2505.10515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10511v1","updated":"2025-05-15T17:17:21Z","published":"2025-05-15T17:17:21Z","title":"Learning Nonlinear Dynamics in Physical Modelling Synthesis using Neural\n  Ordinary Differential Equations","summary":"  Modal synthesis methods are a long-standing approach for modelling\ndistributed musical systems. In some cases extensions are possible in order to\nhandle geometric nonlinearities. One such case is the high-amplitude vibration\nof a string, where geometric nonlinear effects lead to perceptually important\neffects including pitch glides and a dependence of brightness on striking\namplitude. A modal decomposition leads to a coupled nonlinear system of\nordinary differential equations. Recent work in applied machine learning\napproaches (in particular neural ordinary differential equations) has been used\nto model lumped dynamic systems such as electronic circuits automatically from\ndata. In this work, we examine how modal decomposition can be combined with\nneural ordinary differential equations for modelling distributed musical\nsystems. The proposed model leverages the analytical solution for linear\nvibration of system's modes and employs a neural network to account for\nnonlinear dynamic behaviour. Physical parameters of a system remain easily\naccessible after the training without the need for a parameter encoder in the\nnetwork architecture. As an initial proof of concept, we generate synthetic\ndata for a nonlinear transverse string and show that the model can be trained\nto reproduce the nonlinear dynamics of the system. Sound examples are\npresented.\n","authors":["Victor Zheleznov","Stefan Bilbao","Alec Wright","Simon King"],"pdf_url":"https://arxiv.org/pdf/2505.10511v1.pdf","comment":"Accepted for publication in Proceedings of the 28th International\n  Conference on Digital Audio Effects (DAFx25), Ancona, Italy, September 2025"},{"id":"http://arxiv.org/abs/2501.01482v2","updated":"2025-05-15T17:15:14Z","published":"2025-01-02T18:13:35Z","title":"An unsupervised method for MRI recovery: Deep image prior with\n  structured sparsity","summary":"  Objective: To propose and validate an unsupervised MRI reconstruction method\nthat does not require fully sampled k-space data. Materials and Methods: The\nproposed method, deep image prior with structured sparsity (DISCUS), extends\nthe deep image prior (DIP) by introducing group sparsity to frame-specific code\nvectors, enabling the discovery of a low-dimensional manifold for capturing\ntemporal variations. \\discus was validated using four studies: (I) simulation\nof a dynamic Shepp-Logan phantom to demonstrate its manifold discovery\ncapabilities, (II) comparison with compressed sensing and DIP-based methods\nusing simulated single-shot late gadolinium enhancement (LGE) image series from\nsix distinct digital cardiac phantoms in terms of normalized mean square error\n(NMSE) and structural similarity index measure (SSIM), (III) evaluation on\nretrospectively undersampled single-shot LGE data from eight patients, and (IV)\nevaluation on prospectively undersampled single-shot LGE data from eight\npatients, assessed via blind scoring from two expert readers. Results: DISCUS\noutperformed competing methods, demonstrating superior reconstruction quality\nin terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study\nIV). Discussion: An unsupervised image reconstruction method is presented and\nvalidated on simulated and measured data. These developments can benefit\napplications where acquiring fully sampled data is challenging.\n","authors":["Muhammad Ahmad Sultan","Chong Chen","Yingmin Liu","Katarzyna Gil","Karolina Zareba","Rizwan Ahmad"],"pdf_url":"https://arxiv.org/pdf/2501.01482v2.pdf","comment":"Magn Reson Mater Phy (2025)"},{"id":"http://arxiv.org/abs/2505.10498v1","updated":"2025-05-15T17:00:51Z","published":"2025-05-15T17:00:51Z","title":"Batched Nonparametric Bandits via k-Nearest Neighbor UCB","summary":"  We study sequential decision-making in batched nonparametric contextual\nbandits, where actions are selected over a finite horizon divided into a small\nnumber of batches. Motivated by constraints in domains such as medicine and\nmarketing -- where online feedback is limited -- we propose a nonparametric\nalgorithm that combines adaptive k-nearest neighbor (k-NN) regression with the\nupper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully\nnonparametric, adapts to the context dimension, and is simple to implement.\nUnlike prior work relying on parametric or binning-based estimators, BaNk-UCB\nuses local geometry to estimate rewards and adaptively balances exploration and\nexploitation. We provide near-optimal regret guarantees under standard\nLipschitz smoothness and margin assumptions, using a theoretically motivated\nbatch schedule that balances regret across batches and achieves minimax-optimal\nrates. Empirical evaluations on synthetic and real-world datasets demonstrate\nthat BaNk-UCB consistently outperforms binning-based baselines.\n","authors":["Sakshi Arya"],"pdf_url":"https://arxiv.org/pdf/2505.10498v1.pdf","comment":"25 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.10495v1","updated":"2025-05-15T16:53:45Z","published":"2025-05-15T16:53:45Z","title":"RouteNator: A Router-Based Multi-Modal Architecture for Generating\n  Synthetic Training Data for Function Calling LLMs","summary":"  This paper addresses fine-tuning Large Language Models (LLMs) for function\ncalling tasks when real user interaction data is unavailable. In digital\ncontent creation tools, where users express their needs through natural\nlanguage queries that must be mapped to API calls, the lack of real-world\ntask-specific data and privacy constraints for training on it necessitate\nsynthetic data generation. Existing approaches to synthetic data generation\nfall short in diversity and complexity, failing to replicate real-world data\ndistributions and leading to suboptimal performance after LLM fine-tuning. We\npresent a novel router-based architecture that leverages domain resources like\ncontent metadata and structured knowledge graphs, along with text-to-text and\nvision-to-text language models to generate high-quality synthetic training\ndata. Our architecture's flexible routing mechanism enables synthetic data\ngeneration that matches observed real-world distributions, addressing a\nfundamental limitation of traditional approaches. Evaluation on a comprehensive\nset of real user queries demonstrates significant improvements in both function\nclassification accuracy and API parameter selection. Models fine-tuned with our\nsynthetic data consistently outperform traditional approaches, establishing new\nbenchmarks for function calling tasks.\n","authors":["Vibha Belavadi","Tushar Vatsa","Dewang Sultania","Suhas Suresha","Ishita Verma","Cheng Chen","Tracy Holloway King","Michael Friedrich"],"pdf_url":"https://arxiv.org/pdf/2505.10495v1.pdf","comment":"Proceedings of the 4th International Workshop on Knowledge-Augmented\n  Methods for Natural Language Processing"},{"id":"http://arxiv.org/abs/2505.07575v2","updated":"2025-05-15T16:50:52Z","published":"2025-05-12T13:54:55Z","title":"Personalized Federated Learning under Model Dissimilarity Constraints","summary":"  One of the defining challenges in federated learning is that of statistical\nheterogeneity among clients. We address this problem with KARULA, a regularized\nstrategy for personalized federated learning, which constrains the pairwise\nmodel dissimilarities between clients based on the difference in their\ndistributions, as measured by a surrogate for the 1-Wasserstein distance\nadapted for the federated setting. This allows the strategy to adapt to highly\ncomplex interrelations between clients, that e.g., clustered approaches fail to\ncapture. We propose an inexact projected stochastic gradient algorithm to solve\nthe constrained problem that the strategy defines, and show theoretically that\nit converges with smooth, possibly non-convex losses to a neighborhood of a\nstationary point with rate O(1/K). We demonstrate the effectiveness of KARULA\non synthetic and real federated data sets.\n","authors":["Samuel Erickson","Mikael Johansson"],"pdf_url":"https://arxiv.org/pdf/2505.07575v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21909v2","updated":"2025-05-15T16:40:39Z","published":"2024-10-29T10:01:40Z","title":"SceneGenAgent: Precise Industrial Scene Generation with Coding Agent","summary":"  The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent .\n","authors":["Xiao Xia","Dan Zhang","Zibo Liao","Zhenyu Hou","Tianrui Sun","Jing Li","Ling Fu","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2410.21909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10484v1","updated":"2025-05-15T16:36:18Z","published":"2025-05-15T16:36:18Z","title":"Fixing Incomplete Value Function Decomposition for Multi-Agent\n  Reinforcement Learning","summary":"  Value function decomposition methods for cooperative multi-agent\nreinforcement learning compose joint values from individual per-agent\nutilities, and train them using a joint objective. To ensure that the action\nselection process between individual utilities and joint values remains\nconsistent, it is imperative for the composition to satisfy the\nindividual-global max (IGM) property. Although satisfying IGM itself is\nstraightforward, most existing methods (e.g., VDN, QMIX) have limited\nrepresentation capabilities and are unable to represent the full class of IGM\nvalues, and the one exception that has no such limitation (QPLEX) is\nunnecessarily complex. In this work, we present a simple formulation of the\nfull class of IGM values that naturally leads to the derivation of QFIX, a\nnovel family of value function decomposition models that expand the\nrepresentation capabilities of prior models by means of a thin \"fixing\" layer.\nWe derive multiple variants of QFIX, and implement three variants in two\nwell-known multi-agent frameworks. We perform an empirical evaluation on\nmultiple SMACv2 and Overcooked environments, which confirms that QFIX (i)\nsucceeds in enhancing the performance of prior methods, (ii) learns more stably\nand performs better than its main competitor QPLEX, and (iii) achieves this\nwhile employing the simplest and smallest mixing models.\n","authors":["Andrea Baisero","Rupali Bhati","Shuo Liu","Aathira Pillai","Christopher Amato"],"pdf_url":"https://arxiv.org/pdf/2505.10484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10482v1","updated":"2025-05-15T16:33:44Z","published":"2025-05-15T16:33:44Z","title":"Fine-tuning Diffusion Policies with Backpropagation Through Diffusion\n  Timesteps","summary":"  Diffusion policies, widely adopted in decision-making scenarios such as\nrobotics, gaming and autonomous driving, are capable of learning diverse skills\nfrom demonstration data due to their high representation power. However, the\nsub-optimal and limited coverage of demonstration data could lead to diffusion\npolicies that generate sub-optimal trajectories and even catastrophic failures.\nWhile reinforcement learning (RL)-based fine-tuning has emerged as a promising\nsolution to address these limitations, existing approaches struggle to\neffectively adapt Proximal Policy Optimization (PPO) to diffusion models. This\nchallenge stems from the computational intractability of action likelihood\nestimation during the denoising process, which leads to complicated\noptimization objectives. In our experiments starting from randomly initialized\npolicies, we find that online tuning of Diffusion Policies demonstrates much\nlower sample efficiency compared to directly applying PPO on MLP policies\n(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework\nthat reformulates Diffusion Policy as a noise-conditioned deterministic policy.\nBy treating each denoising step as a differentiable transformation conditioned\non pre-sampled noise, NCDPO enables tractable likelihood evaluation and\ngradient backpropagation through all diffusion timesteps. Our experiments\ndemonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when\ntraining from scratch, outperforming existing methods in both sample efficiency\nand final performance across diverse benchmarks, including continuous robot\ncontrol and multi-agent game scenarios. Furthermore, our experimental results\nshow that our method is robust to the number denoising timesteps in the\nDiffusion Policy.\n","authors":["Ningyuan Yang","Jiaxuan Gao","Feng Gao","Yi Wu","Chao Yu"],"pdf_url":"https://arxiv.org/pdf/2505.10482v1.pdf","comment":"9 pages for main text, 23 pages in total, submitted to Neurips, 13\n  figures"},{"id":"http://arxiv.org/abs/2503.12293v2","updated":"2025-05-15T16:29:38Z","published":"2025-03-15T23:20:26Z","title":"Unified Modeling Language Code Generation from Diagram Images Using\n  Multimodal Large Language Models","summary":"  The Unified Modeling Language is a standardized visual language widely used\nfor modeling and documenting the design of software systems. Although many\ntools generate UML diagrams from UML code, generating executable UML code from\nimage-based UML diagrams remains challenging. This paper proposes a new\napproach to generate UML code using a large multimodal language model\nautomatically. Synthetic UML activity and sequence diagram datasets were\ncreated to train and test the model. We compared standard fine-tuning with LoRA\ntechniques to optimize base models. The experiments measured code generation\naccuracy across different model sizes and training strategies. These results\ndemonstrated that domain-adapted MM-LLMs perform for UML code generation\nautomation, whereby, at the best model, it achieved BLEU and SSIM scores of\n0.779 and 0.942 on sequence diagrams. This will enable the modernization of\nlegacy systems and decrease the manual effort in software development\nworkflows.\n","authors":["Averi Bates","Ryan Vavricka","Shane Carleton","Ruosi Shao","Chongle Pan"],"pdf_url":"https://arxiv.org/pdf/2503.12293v2.pdf","comment":"Published in the Journal of Machine Learning with Applications,\n  Author Contributions: Averi Bates: Methodology, Development, Analysis, Data\n  Curation, Drafting, Review. Ryan Vavricka: Data Curation, Development,\n  Review. Shane Carleton: Supervision, Funding. Ruosi Shao: Review. Chongle\n  Pan: Supervision, Review"},{"id":"http://arxiv.org/abs/2504.17671v3","updated":"2025-05-15T16:24:49Z","published":"2025-04-24T15:39:46Z","title":"Data-Driven Calibration of Prediction Sets in Large Vision-Language\n  Models Based on Inductive Conformal Prediction","summary":"  This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.\n","authors":["Yuanchang Ye","Weiyan Wen"],"pdf_url":"https://arxiv.org/pdf/2504.17671v3.pdf","comment":"Accepted by ICIPCA 2025"},{"id":"http://arxiv.org/abs/2505.10475v1","updated":"2025-05-15T16:24:45Z","published":"2025-05-15T16:24:45Z","title":"Parallel Scaling Law for Language Models","summary":"  It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply $P$ diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the $P$ outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with $P$ parallel\nstreams is similar to scaling the parameters by $O(\\log P)$ while showing\nsuperior inference efficiency. For example, ParScale can use up to 22$\\times$\nless memory increase and 6$\\times$ less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.\n","authors":["Mouxiang Chen","Binyuan Hui","Zeyu Cui","Jiaxi Yang","Dayiheng Liu","Jianling Sun","Junyang Lin","Zhongxin Liu"],"pdf_url":"https://arxiv.org/pdf/2505.10475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10472v1","updated":"2025-05-15T16:23:21Z","published":"2025-05-15T16:23:21Z","title":"Large Language Models for Cancer Communication: Evaluating Linguistic\n  Quality, Safety, and Accessibility in Generative AI","summary":"  Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.\n","authors":["Agnik Saha","Victoria Churchill","Anny D. Rodriguez","Ugur Kursuncu","Muhammed Y. Idris"],"pdf_url":"https://arxiv.org/pdf/2505.10472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10466v1","updated":"2025-05-15T16:20:36Z","published":"2025-05-15T16:20:36Z","title":"FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant\n  Tempering","summary":"  Multi-modal and high-dimensional posteriors present significant challenges\nfor variational inference, causing mode-seeking behavior and collapse despite\nthe theoretical expressiveness of normalizing flows. Traditional annealing\nmethods require temperature schedules and hyperparameter tuning, falling short\nof the goal of truly black-box variational inference. We introduce FlowVAT, a\nconditional tempering approach for normalizing flow variational inference that\naddresses these limitations. Our method tempers both the base and target\ndistributions simultaneously, maintaining affine-invariance under tempering. By\nconditioning the normalizing flow on temperature, we leverage overparameterized\nneural networks' generalization capabilities to train a single flow\nrepresenting the posterior across a range of temperatures. This preserves modes\nidentified at higher temperatures when sampling from the variational posterior\nat $T = 1$, mitigating standard variational methods' mode-seeking behavior. In\nexperiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT\noutperforms traditional and adaptive annealing methods, finding more modes and\nachieving better ELBO values, particularly in higher dimensions where existing\napproaches fail. Our method requires minimal hyperparameter tuning and does not\nrequire an annealing schedule, advancing toward fully-automatic black-box\nvariational inference for complicated posteriors.\n","authors":["Juehang Qin","Shixiao Liang","Christopher Tunnell"],"pdf_url":"https://arxiv.org/pdf/2505.10466v1.pdf","comment":"10 pages, 5 figures, and 2 tables in main text, two appendices"},{"id":"http://arxiv.org/abs/2505.10465v1","updated":"2025-05-15T16:18:13Z","published":"2025-05-15T16:18:13Z","title":"Superposition Yields Robust Neural Scaling","summary":"  The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters.\n","authors":["Yizhou liu","Ziming Liu","Jeff Gore"],"pdf_url":"https://arxiv.org/pdf/2505.10465v1.pdf","comment":"30 pages, 23 figures"},{"id":"http://arxiv.org/abs/2505.10457v1","updated":"2025-05-15T16:14:18Z","published":"2025-05-15T16:14:18Z","title":"SEAL: Searching Expandable Architectures for Incremental Learning","summary":"  Incremental learning is a machine learning paradigm where a model learns from\na sequential stream of tasks. This setting poses a key challenge: balancing\nplasticity (learning new tasks) and stability (preserving past knowledge).\nNeural Architecture Search (NAS), a branch of AutoML, automates the design of\nthe architecture of Deep Neural Networks and has shown success in static\nsettings. However, existing NAS-based approaches to incremental learning often\nrely on expanding the model at every task, making them impractical in\nresource-constrained environments. In this work, we introduce SEAL, a NAS-based\nframework tailored for data-incremental learning, a scenario where disjoint\ndata samples arrive sequentially and are not stored for future access. SEAL\nadapts the model structure dynamically by expanding it only when necessary,\nbased on a capacity estimation metric. Stability is preserved through\ncross-distillation training after each expansion step. The NAS component\njointly searches for both the architecture and the optimal expansion policy.\nExperiments across multiple benchmarks demonstrate that SEAL effectively\nreduces forgetting and enhances accuracy while maintaining a lower model size\ncompared to prior methods. These results highlight the promise of combining NAS\nand selective expansion for efficient, adaptive learning in incremental\nscenarios.\n","authors":["Matteo Gambella","Vicente Javier Castro Solar","Manuel Roveri"],"pdf_url":"https://arxiv.org/pdf/2505.10457v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.01002v2","updated":"2025-05-15T16:07:20Z","published":"2025-01-02T01:52:36Z","title":"Multi-Objective Optimization-Based Anonymization of Structured Data for\n  Machine Learning Application","summary":"  Organizations are collecting vast amounts of data, but they often lack the\ncapabilities needed to fully extract insights. As a result, they increasingly\nshare data with external experts, such as analysts or researchers, to gain\nvalue from it. However, this practice introduces significant privacy risks.\nVarious techniques have been proposed to address privacy concerns in data\nsharing. However, these methods often degrade data utility, impacting the\nperformance of machine learning (ML) models. Our research identifies key\nlimitations in existing optimization models for privacy preservation,\nparticularly in handling categorical variables, and evaluating effectiveness\nacross diverse datasets. We propose a novel multi-objective optimization model\nthat simultaneously minimizes information loss and maximizes protection against\nattacks. This model is empirically validated using diverse datasets and\ncompared with two existing algorithms. We assess information loss, the number\nof individuals subject to linkage or homogeneity attacks, and ML performance\nafter anonymization. The results indicate that our model achieves lower\ninformation loss and more effectively mitigates the risk of attacks, reducing\nthe number of individuals susceptible to these attacks compared to alternative\nalgorithms in some cases. Additionally, our model maintains comparable ML\nperformance relative to the original data or data anonymized by other methods.\nOur findings highlight significant improvements in privacy protection and ML\nmodel performance, offering a comprehensive and extensible framework for\nbalancing privacy and utility in data sharing.\n","authors":["Yusi Wei","Hande Y. Benson","Joseph K. Agor","Muge Capan"],"pdf_url":"https://arxiv.org/pdf/2501.01002v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10448v1","updated":"2025-05-15T16:06:44Z","published":"2025-05-15T16:06:44Z","title":"Efficient MCMC Sampling with Expensive-to-Compute and Irregular\n  Likelihoods","summary":"  Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when\nthe likelihood function is irregular and expensive to compute. We explore\nseveral sampling algorithms that make use of subset evaluations to reduce\ncomputational overhead. We adapt the subset samplers for this setting where\ngradient information is not available or is unreliable. To achieve this, we\nintroduce data-driven proxies in place of Taylor expansions and define a novel\ncomputation-cost aware adaptive controller. We undertake an extensive\nevaluation for a challenging disease modelling task and a configurable task\nwith similar irregularity in the likelihood surface. We find our improved\nversion of Hierarchical Importance with Nested Training Samples (HINTS), with\nadaptive proposals and a data-driven proxy, obtains the best sampling error in\na fixed computational budget. We conclude that subset evaluations can provide\ncheap and naturally-tempered exploration, while a data-driven proxy can\npre-screen proposals successfully in explored regions of the state space. These\ntwo elements combine through hierarchical delayed acceptance to achieve\nefficient, exact sampling.\n","authors":["Conor Rosato","Harvinder Lehal","Simon Maskell","Lee Devlin","Malcolm Strens"],"pdf_url":"https://arxiv.org/pdf/2505.10448v1.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2505.10444v1","updated":"2025-05-15T16:05:50Z","published":"2025-05-15T16:05:50Z","title":"Inferring entropy production in many-body systems using nonequilibrium\n  MaxEnt","summary":"  We propose a method for inferring entropy production (EP) in high-dimensional\nstochastic systems, including many-body systems and non-Markovian systems with\nlong memory. Standard techniques for estimating EP become intractable in such\nsystems due to computational and statistical limitations. We infer\ntrajectory-level EP and lower bounds on average EP by exploiting a\nnonequilibrium analogue of the Maximum Entropy principle, along with convex\nduality. Our approach uses only samples of trajectory observables (such as\nspatiotemporal correlation functions). It does not require reconstruction of\nhigh-dimensional probability distributions or rate matrices, nor any special\nassumptions such as discrete states or multipartite dynamics. It may be used to\ncompute a hierarchical decomposition of EP, reflecting contributions from\ndifferent kinds of interactions, and it has an intuitive physical\ninterpretation as a thermodynamic uncertainty relation. We demonstrate its\nnumerical performance on a disordered nonequilibrium spin model with 1000 spins\nand a large neural spike-train dataset.\n","authors":["Miguel Aguilera","Sosuke Ito","Artemy Kolchinsky"],"pdf_url":"https://arxiv.org/pdf/2505.10444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14932v3","updated":"2025-05-15T16:01:20Z","published":"2024-10-19T01:36:51Z","title":"Can AI weather models predict out-of-distribution gray swan tropical\n  cyclones?","summary":"  Predicting gray swan weather extremes, which are possible but so rare that\nthey are absent from the training dataset, is a major concern for AI weather\nmodels and long-term climate emulators. An important open question is whether\nAI models can extrapolate from weaker weather events present in the training\nset to stronger, unseen weather extremes. To test this, we train independent\nversions of the AI model FourCastNet on the 1979-2015 ERA5 dataset with all\ndata, or with Category 3-5 tropical cyclones (TCs) removed, either globally or\nonly over the North Atlantic or Western Pacific basin. We then test these\nversions of FourCastNet on 2018-2023 Category 5 TCs (gray swans). All versions\nyield similar accuracy for global weather, but the one trained without Category\n3-5 TCs cannot accurately forecast Category 5 TCs, indicating that these models\ncannot extrapolate from weaker storms. The versions trained without Category\n3-5 TCs in one basin show some skill forecasting Category 5 TCs in that basin,\nsuggesting that FourCastNet can generalize across tropical basins. This is\nencouraging and surprising because regional information is implicitly encoded\nin inputs. Given that current state-of-the-art AI weather and climate models\nhave similar learning strategies, we expect our findings to apply to other\nmodels. Other types of weather extremes need to be similarly investigated. Our\nwork demonstrates that novel learning strategies are needed for AI models to\nreliably provide early warning or estimated statistics for the rarest, most\nimpactful TCs, and, possibly, other weather extremes.\n","authors":["Y. Qiang Sun","Pedram Hassanzadeh","Mohsen Zand","Ashesh Chattopadhyay","Jonathan Weare","Dorian S. Abbot"],"pdf_url":"https://arxiv.org/pdf/2410.14932v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10441v1","updated":"2025-05-15T16:00:31Z","published":"2025-05-15T16:00:31Z","title":"PIF: Anomaly detection via preference embedding","summary":"  We address the problem of detecting anomalies with respect to structured\npatterns. To this end, we conceive a novel anomaly detection method called PIF,\nthat combines the advantages of adaptive isolation methods with the flexibility\nof preference embedding. Specifically, we propose to embed the data in a high\ndimensional space where an efficient tree-based method, PI-Forest, is employed\nto compute an anomaly score. Experiments on synthetic and real datasets\ndemonstrate that PIF favorably compares with state-of-the-art anomaly detection\ntechniques, and confirm that PI-Forest is better at measuring arbitrary\ndistances and isolate points in the preference space.\n","authors":["Filippo Leveni","Luca Magri","Giacomo Boracchi","Cesare Alippi"],"pdf_url":"https://arxiv.org/pdf/2505.10441v1.pdf","comment":"Accepted at International Conference on Pattern Recognition (ICPR\n  2020)"},{"id":"http://arxiv.org/abs/2505.10438v1","updated":"2025-05-15T15:55:13Z","published":"2025-05-15T15:55:13Z","title":"Identification and Optimal Nonlinear Control of Turbojet Engine Using\n  Koopman Eigenfunction Model","summary":"  Gas turbine engines represent complex highly nonlinear dynamical systems.\nDeriving their physics-based models can be challenging as it requires\nperformance characteristics, that are not always available, and one often has\nto make many simplifying assumptions. In this paper, the limitations of\nconventional experimental methods used to derive component-level and locally\nlinear parameter-varying models are discussed and addressed by employing\nidentification techniques based on data collected from standard engine\noperation under closed-loop control. The rotor dynamics were estimated using\nthe sparse identification of nonlinear dynamics. Subsequently, the autonomous\npart of the dynamics was mapped into an optimally constructed Koopman\neigenfunction space. The process included eigenvalue optimization using\nmetaheuristic algorithms and temporal projection, followed by gradient-based\neigenfunction identification. The resulting Koopman model was validated against\nan in-house reference component-level model. A globally optimal nonlinear\nfeedback controller and a Kalman estimator were then designed in the\neigenfunction space and compared to the classical and gain-scheduled\nproportional-integral controllers, as well as a proposed internal model control\napproach. The eigenmode structure allowed targeting individual modes during the\noptimization process, resulting in a better performance tuning. The results\nshowed that the Koopman-based controller outperformed the other benchmark\ncontrollers in both reference tracking and disturbance rejection, under\nsea-level and varying flight conditions, due to its global nature.\n","authors":["David Grasev"],"pdf_url":"https://arxiv.org/pdf/2505.10438v1.pdf","comment":"51 pages, 28 figures"},{"id":"http://arxiv.org/abs/2505.10432v1","updated":"2025-05-15T15:51:41Z","published":"2025-05-15T15:51:41Z","title":"Score-based diffusion nowcasting of GOES imagery","summary":"  Clouds and precipitation are important for understanding weather and climate.\nSimulating clouds and precipitation with traditional numerical weather\nprediction is challenging because of the sub-grid parameterizations required.\nMachine learning has been explored for forecasting clouds and precipitation,\nbut early machine learning methods often created blurry forecasts. In this\npaper we explore a newer method, named score-based diffusion, to nowcast (zero\nto three hour forecast) clouds and precipitation. We discuss the background and\nintuition of score-based diffusion models - thus providing a starting point for\nthe community - while exploring the methodology's use for nowcasting\ngeostationary infrared imagery. We experiment with three main types of\ndiffusion models: a standard score-based diffusion model (Diff); a residual\ncorrection diffusion model (CorrDiff); and a latent diffusion model (LDM). Our\nresults show that the diffusion models are able to not only advect existing\nclouds, but also generate and decay clouds, including convective initiation.\nThese results are surprising because the forecasts are initiated with only the\npast 20 mins of infrared satellite imagery. A case study qualitatively shows\nthe preservation of high resolution features longer into the forecast than a\nconventional mean-squared error trained U-Net. The best of the three diffusion\nmodels tested was the CorrDiff approach, outperforming all other diffusion\nmodels, the traditional U-Net, and a persistence forecast by one to two kelvin\non root mean squared error. The diffusion models also enable out-of-the-box\nensemble generation, which shows skillful calibration, with the spread of the\nensemble correlating well to the error.\n","authors":["Randy J. Chase","Katherine Haynes","Lander Ver Hoef","Imme Ebert-Uphoff"],"pdf_url":"https://arxiv.org/pdf/2505.10432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10425v1","updated":"2025-05-15T15:40:25Z","published":"2025-05-15T15:40:25Z","title":"Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for\n  LLMs","summary":"  Large language models (LLMs) excel at complex tasks thanks to advances in\nreasoning abilities. However, existing methods overlook the trade-off between\nreasoning effectiveness and computational efficiency, often encouraging\nunnecessarily long reasoning chains and wasting tokens. To address this, we\npropose Learning to Think (L2T), an information-theoretic reinforcement\nfine-tuning framework for LLMs to make the models achieve optimal reasoning\nwith fewer tokens. Specifically, L2T treats each query-response interaction as\na hierarchical session of multiple episodes and proposes a universal dense\nprocess reward, i.e., quantifies the episode-wise information gain in\nparameters, requiring no extra annotations or task-specific evaluators. We\npropose a method to quickly estimate this reward based on PAC-Bayes bounds and\nthe Fisher information matrix. Theoretical analyses show that it significantly\nreduces computational complexity with high estimation accuracy. By immediately\nrewarding each episode's contribution and penalizing excessive updates, L2T\noptimizes the model via reinforcement learning to maximize the use of each\nepisode and achieve effective updates. Empirical results on various reasoning\nbenchmarks and base models demonstrate the advantage of L2T across different\ntasks, boosting both reasoning effectiveness and efficiency.\n","authors":["Jingyao Wang","Wenwen Qiang","Zeen Song","Changwen Zheng","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.10425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10423v1","updated":"2025-05-15T15:39:28Z","published":"2025-05-15T15:39:28Z","title":"The Power of Random Features and the Limits of Distribution-Free\n  Gradient Descent","summary":"  We study the relationship between gradient-based optimization of parametric\nmodels (e.g., neural networks) and optimization of linear combinations of\nrandom features. Our main result shows that if a parametric model can be\nlearned using mini-batch stochastic gradient descent (bSGD) without making\nassumptions about the data distribution, then with high probability, the target\nfunction can also be approximated using a polynomial-sized combination of\nrandom features. The size of this combination depends on the number of gradient\nsteps and numerical precision used in the bSGD process. This finding reveals\nfundamental limitations of distribution-free learning in neural networks\ntrained by gradient descent, highlighting why making assumptions about data\ndistributions is often crucial in practice. Along the way, we also introduce a\nnew theoretical framework called average probabilistic dimension complexity\n(adc), which extends the probabilistic dimension complexity developed by Kamath\net al. (2020). We prove that adc has a polynomial relationship with statistical\nquery dimension, and use this relationship to demonstrate an infinite\nseparation between adc and standard dimension complexity.\n","authors":["Ari Karchmer","Eran Malach"],"pdf_url":"https://arxiv.org/pdf/2505.10423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10422v1","updated":"2025-05-15T15:39:09Z","published":"2025-05-15T15:39:09Z","title":"Decomposed Inductive Procedure Learning: Learning Academic Tasks with\n  Human-Like Data Efficiency","summary":"  Human learning relies on specialization -- distinct cognitive mechanisms\nworking together to enable rapid learning. In contrast, most modern neural\nnetworks rely on a single mechanism: gradient descent over an objective\nfunction. This raises the question: might human learners' relatively rapid\nlearning from just tens of examples instead of tens of thousands in data-driven\ndeep learning arise from our ability to use multiple specialized mechanisms of\nlearning in combination? We investigate this question through an ablation\nanalysis of inductive human learning simulations in online tutoring\nenvironments. Comparing reinforcement learning to a more data-efficient\n3-mechanism symbolic rule induction approach, we find that decomposing learning\ninto multiple distinct mechanisms significantly improves data efficiency,\nbringing it in line with human learning. Furthermore, we show that this\ndecomposition has a greater impact on efficiency than the distinction between\nsymbolic and subsymbolic learning alone. Efforts to align data-driven machine\nlearning with human learning often overlook the stark difference in learning\nefficiency. Our findings suggest that integrating multiple specialized learning\nmechanisms may be key to bridging this gap.\n","authors":["Daniel Weitekamp","Christopher MacLellan","Erik Harpstead","Kenneth Koedinger"],"pdf_url":"https://arxiv.org/pdf/2505.10422v1.pdf","comment":"To appear in CogSci 2025"},{"id":"http://arxiv.org/abs/2505.06761v2","updated":"2025-05-15T15:32:55Z","published":"2025-05-10T21:42:24Z","title":"Learning Graph Representation of Agent Diffusers","summary":"  Diffusion-based generative models have significantly advanced text-to-image\nsynthesis, demonstrating impressive text comprehension and zero-shot\ngeneralization. These models refine images from random noise based on textual\nprompts, with initial reliance on text input shifting towards enhanced visual\nfidelity over time. This transition suggests that static model parameters might\nnot optimally address the distinct phases of generation. We introduce LGR-AD\n(Learning Graph Representation of Agent Diffusers), a novel multi-agent system\ndesigned to improve adaptability in dynamic computer vision tasks. LGR-AD\nmodels the generation process as a distributed system of interacting agents,\neach representing an expert sub-model. These agents dynamically adapt to\nvarying conditions and collaborate through a graph neural network that encodes\ntheir relationships and performance metrics. Our approach employs a\ncoordination mechanism based on top-$k$ maximum spanning trees, optimizing the\ngeneration process. Each agent's decision-making is guided by a meta-model that\nminimizes a novel loss function, balancing accuracy and diversity. Theoretical\nanalysis and extensive empirical evaluations show that LGR-AD outperforms\ntraditional diffusion models across various benchmarks, highlighting its\npotential for scalable and flexible solutions in complex image generation\ntasks. Code is available at: https://github.com/YousIA/LGR_AD\n","authors":["Youcef Djenouri","Nassim Belmecheri","Tomasz Michalak","Jan DubiÅski","Ahmed Nabil Belbachir","Anis Yazidi"],"pdf_url":"https://arxiv.org/pdf/2505.06761v2.pdf","comment":"Accepted at AAMAS2025 International Conference on Autonomous Agents\n  and Multiagent Systems"},{"id":"http://arxiv.org/abs/2505.10407v1","updated":"2025-05-15T15:30:41Z","published":"2025-05-15T15:30:41Z","title":"Two-Stage Generative Model for Intracranial Aneurysm Meshes with\n  Morphological Marker Conditioning","summary":"  A generative model for the mesh geometry of intracranial aneurysms (IA) is\ncrucial for training networks to predict blood flow forces in real time, which\nis a key factor affecting disease progression. This need is necessitated by the\nabsence of a large IA image datasets. Existing shape generation methods\nstruggle to capture realistic IA features and ignore the relationship between\nIA pouches and parent vessels, limiting physiological realism and their\ngeneration cannot be controlled to have specific morphological measurements. We\npropose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh\ngenerator. In the first stage, AneuG generates low-dimensional Graph Harmonic\nDeformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,\nconstrained to morphing energy statistics truths. GHD enables more accurate\nshape encoding than alternatives. In the second stage, AneuG generates parent\nvessels conditioned on GHD tokens, by generating vascular centreline and\npropagating the cross-section. AneuG's IA shape generation can further be\nconditioned to have specific clinically relevant morphological measurements.\nThis is useful for studies to understand shape variations represented by\nclinical measurements, and for flow simulation studies to understand effects of\nspecific clinical shape parameters on fluid dynamics. Source code and\nimplementation details are available at\nhttps://github.com/anonymousaneug/AneuG.\n","authors":["Wenhao Ding","Choon Hwai Yap","Kangjun Ji","SimÃ£o Castro"],"pdf_url":"https://arxiv.org/pdf/2505.10407v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2505.10405v1","updated":"2025-05-15T15:28:32Z","published":"2025-05-15T15:28:32Z","title":"Visual Fidelity Index for Generative Semantic Communications with\n  Critical Information Embedding","summary":"  Generative semantic communication (Gen-SemCom) with large artificial\nintelligence (AI) model promises a transformative paradigm for 6G networks,\nwhich reduces communication costs by transmitting low-dimensional prompts\nrather than raw data. However, purely prompt-driven generation loses\nfine-grained visual details. Additionally, there is a lack of systematic\nmetrics to evaluate the performance of Gen-SemCom systems. To address these\nissues, we develop a hybrid Gen-SemCom system with a critical information\nembedding (CIE) framework, where both text prompts and semantically critical\nfeatures are extracted for transmissions. First, a novel approach of semantic\nfiltering is proposed to select and transmit the semantically critical features\nof images relevant to semantic label. By integrating the text prompt and\ncritical features, the receiver reconstructs high-fidelity images using a\ndiffusion-based generative model. Next, we propose the generative visual\ninformation fidelity (GVIF) metric to evaluate the visual quality of the\ngenerated image. By characterizing the statistical models of image features,\nthe GVIF metric quantifies the mutual information between the distorted\nfeatures and their original counterparts. By maximizing the GVIF metric, we\ndesign a channel-adaptive Gen-SemCom system that adaptively control the volume\nof features and compression rate according to the channel state. Experimental\nresults validate the GVIF metric's sensitivity to visual fidelity, correlating\nwith both the PSNR and critical information volume. In addition, the optimized\nsystem achieves superior performance over benchmarking schemes in terms of\nhigher PSNR and lower FID scores.\n","authors":["Jianhao Huang","Qunsong Zeng","Kaibin Huang"],"pdf_url":"https://arxiv.org/pdf/2505.10405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10402v1","updated":"2025-05-15T15:26:32Z","published":"2025-05-15T15:26:32Z","title":"Rethinking Repetition Problems of LLMs in Code Generation","summary":"  With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code.\n","authors":["Yihong Dong","Yuchen Liu","Xue Jiang","Zhi Jin","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2505.10402v1.pdf","comment":"Accepted to ACL 2025 (main)"},{"id":"http://arxiv.org/abs/2503.17037v2","updated":"2025-05-15T15:22:41Z","published":"2025-03-21T10:46:50Z","title":"Unitless Unrestricted Markov-Consistent SCM Generation: Better Benchmark\n  Datasets for Causal Discovery","summary":"  Causal discovery aims to extract qualitative causal knowledge in the form of\ncausal graphs from data. Because causal ground truth is rarely known in the\nreal world, simulated data plays a vital role in evaluating the performance of\nthe various causal discovery algorithms proposed in the literature. But recent\nwork highlighted certain artifacts of commonly used data generation techniques\nfor a standard class of structural causal models (SCM) that may be nonphysical,\nincluding var- and R2-sortability, where the variables' variance and\ncoefficients of determination (R2) after regressing on all other variables,\nrespectively, increase along the causal order. Some causal methods exploit such\nartifacts, leading to unrealistic expectations for their performance on\nreal-world data. Some modifications have been proposed to remove these\nartifacts; notably, the internally-standardized structural causal model (iSCM)\navoids varsortability and largely alleviates R2-sortability on sparse causal\ngraphs, but exhibits a reversed R2-sortability pattern for denser graphs not\nfeatured in their work. We analyze which sortability patterns we expect to see\nin real data, and propose a method for drawing coefficients that we argue more\neffectively samples the space of SCMs. Finally, we propose a novel extension of\nour SCM generation method to the time series setting.\n","authors":["Rebecca J. Herman","Jonas Wahl","Urmi Ninad","Jakob Runge"],"pdf_url":"https://arxiv.org/pdf/2503.17037v2.pdf","comment":"4th Conference on Causal Learning and Reasoning"},{"id":"http://arxiv.org/abs/2505.10399v1","updated":"2025-05-15T15:22:06Z","published":"2025-05-15T15:22:06Z","title":"Evaluating Model Explanations without Ground Truth","summary":"  There can be many competing and contradictory explanations for a single model\nprediction, making it difficult to select which one to use. Current explanation\nevaluation frameworks measure quality by comparing against ideal \"ground-truth\"\nexplanations, or by verifying model sensitivity to important inputs. We outline\nthe limitations of these approaches, and propose three desirable principles to\nground the future development of explanation evaluation strategies for local\nfeature importance explanations. We propose a ground-truth Agnostic eXplanation\nEvaluation framework (AXE) for evaluating and comparing model explanations that\nsatisfies these principles. Unlike prior approaches, AXE does not require\naccess to ideal ground-truth explanations for comparison, or rely on model\nsensitivity - providing an independent measure of explanation quality. We\nverify AXE by comparing with baselines, and show how it can be used to detect\nexplanation fairwashing. Our code is available at\nhttps://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.\n","authors":["Kaivalya Rawal","Zihao Fu","Eoin Delaney","Chris Russell"],"pdf_url":"https://arxiv.org/pdf/2505.10399v1.pdf","comment":"https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth"},{"id":"http://arxiv.org/abs/2505.10398v1","updated":"2025-05-15T15:21:46Z","published":"2025-05-15T15:21:46Z","title":"AutoCam: Hierarchical Path Planning for an Autonomous Auxiliary Camera\n  in Surgical Robotics","summary":"  Incorporating an autonomous auxiliary camera into robot-assisted minimally\ninvasive surgery (RAMIS) enhances spatial awareness and eliminates manual\nviewpoint control. Existing path planning methods for auxiliary cameras track\ntwo-dimensional surgical features but do not simultaneously account for camera\norientation, workspace constraints, and robot joint limits. This study presents\nAutoCam: an automatic auxiliary camera placement method to improve\nvisualization in RAMIS. Implemented on the da Vinci Research Kit, the system\nuses a priority-based, workspace-constrained control algorithm that combines\nheuristic geometric placement with nonlinear optimization to ensure robust\ncamera tracking. A user study (N=6) demonstrated that the system maintained\n99.84% visibility of a salient feature and achieved a pose error of 4.36 $\\pm$\n2.11 degrees and 1.95 $\\pm$ 5.66 mm. The controller was computationally\nefficient, with a loop time of 6.8 $\\pm$ 12.8 ms. An additional pilot study\n(N=6), where novices completed a Fundamentals of Laparoscopic Surgery training\ntask, suggests that users can teleoperate just as effectively from AutoCam's\nviewpoint as from the endoscope's while still benefiting from AutoCam's\nimproved visual coverage of the scene. These results indicate that an auxiliary\ncamera can be autonomously controlled using the da Vinci patient-side\nmanipulators to track a salient feature, laying the groundwork for new\nmulti-camera visualization methods in RAMIS.\n","authors":["Alexandre Banks","Randy Moore","Sayem Nazmuz Zaman","Alaa Eldin Abdelaal","Septimiu E. Salcudean"],"pdf_url":"https://arxiv.org/pdf/2505.10398v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2409.06356v2","updated":"2025-05-15T15:16:33Z","published":"2024-09-10T09:23:03Z","title":"Double Successive Over-Relaxation Q-Learning with an Extension to Deep\n  Reinforcement Learning","summary":"  Q-learning is a widely used algorithm in reinforcement learning (RL), but its\nconvergence can be slow, especially when the discount factor is close to one.\nSuccessive Over-Relaxation (SOR) Q-learning, which introduces a relaxation\nfactor to speed up convergence, addresses this issue but has two major\nlimitations: In the tabular setting, the relaxation parameter depends on\ntransition probability, making it not entirely model-free, and it suffers from\noverestimation bias. To overcome these limitations, we propose a sample-based,\nmodel-free double SOR Q-learning algorithm. Theoretically and empirically, this\nalgorithm is shown to be less biased than SOR Q-learning. Further, in the\ntabular setting, the convergence analysis under boundedness assumptions on\niterates is discussed. The proposed algorithm is extended to large-scale\nproblems using deep RL. Finally, the tabular version of the proposed algorithm\nis compared using roulette and grid world environments, while the deep RL\nversion is tested on a maximization bias example and OpenAI Gym environments.\n","authors":["Shreyas S R"],"pdf_url":"https://arxiv.org/pdf/2409.06356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06046v2","updated":"2025-05-15T15:14:47Z","published":"2025-05-09T13:42:59Z","title":"Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information","summary":"  As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries. To create PubHealthBench we extract free\ntext from 687 current UK government guidance documents and implement an\nautomated pipeline for generating MCQA samples. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% accuracy in the MCQA setup, and\noutperform humans with cursory search engine use. However, in the free form\nsetup we see lower performance with no model scoring >75%. Importantly we find\nin both setups LLMs have higher accuracy on guidance intended for the general\npublic. Therefore, there are promising signs that state of the art (SOTA) LLMs\nare an increasingly accurate source of public health information, but\nadditional safeguards or tools may still be needed when providing free form\nresponses on public health topics.\n","authors":["Joshua Harris","Fan Grayson","Felix Feldman","Timothy Laurence","Toby Nonnenmacher","Oliver Higgins","Leo Loman","Selina Patel","Thomas Finnie","Samuel Collins","Michael Borowitz"],"pdf_url":"https://arxiv.org/pdf/2505.06046v2.pdf","comment":"24 pages, 10 pages main text"},{"id":"http://arxiv.org/abs/2505.10392v1","updated":"2025-05-15T15:14:02Z","published":"2025-05-15T15:14:02Z","title":"Schreier-Coset Graph Propagation","summary":"  Graph Neural Networks (GNNs) offer a principled framework for learning over\ngraph-structured data, yet their expressive capacity is often hindered by\nover-squashing, wherein information from distant nodes is compressed into\nfixed-size vectors. Existing solutions, including graph rewiring and\nbottleneck-resistant architectures such as Cayley and expander graphs, avoid\nthis problem but introduce scalability bottlenecks. In particular, the Cayley\ngraphs constructed over $SL(2,\\mathbb{Z}_n)$ exhibit strong theoretical\nproperties, yet suffer from cubic node growth $O(n^3)$, leading to high memory\nusage. To address this, this work introduces Schrier-Coset Graph Propagation\n(SCGP), a group-theoretic augmentation method that enriches node features\nthrough Schreier-coset embeddings without altering the input graph topology.\nSCGP embeds bottleneck-free connectivity patterns into a compact feature space,\nimproving long-range message passing while maintaining computational\nefficiency. Empirical evaluations across standard node and graph classification\nbenchmarks demonstrate that SCGP achieves performance comparable to, or\nexceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits\nparticular advantages in processing hierarchical and modular graph structures,\noffering reduced inference latency, improved scalability, and a low memory\nfootprint, making it suitable for real-time and resource-constrained\napplications.\n","authors":["Aryan Mishra","Lizhen Lin"],"pdf_url":"https://arxiv.org/pdf/2505.10392v1.pdf","comment":"9 pages, 1 figure , preprint"},{"id":"http://arxiv.org/abs/2505.04560v2","updated":"2025-05-15T15:13:43Z","published":"2025-05-07T16:48:49Z","title":"ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge\n  Distillation via $Î±$-$Î²$-Divergence","summary":"  Knowledge Distillation (KD) transfers knowledge from a large teacher model to\na smaller student model by minimizing the divergence between their output\ndistributions, typically using forward Kullback-Leibler divergence (FKLD) or\nreverse KLD (RKLD). It has become an effective training paradigm due to the\nbroader supervision information provided by the teacher distribution compared\nto one-hot labels. We identify that the core challenge in KD lies in balancing\ntwo mode-concentration effects: the \\textbf{\\textit{Hardness-Concentration}}\neffect, which refers to focusing on modes with large errors, and the\n\\textbf{\\textit{Confidence-Concentration}} effect, which refers to focusing on\nmodes with high student confidence. Through an analysis of how probabilities\nare reassigned during gradient updates, we observe that these two effects are\nentangled in FKLD and RKLD, but in extreme forms. Specifically, both are too\nweak in FKLD, causing the student to fail to concentrate on the target class.\nIn contrast, both are too strong in RKLD, causing the student to overly\nemphasize the target class while ignoring the broader distributional\ninformation from the teacher. To address this imbalance, we propose ABKD, a\ngeneric framework with $\\alpha$-$\\beta$-divergence. Our theoretical results\nshow that ABKD offers a smooth interpolation between FKLD and RKLD, achieving\nan effective trade-off between these effects. Extensive experiments on 17\nlanguage/vision datasets with 12 teacher-student settings confirm its efficacy.\nThe code is available at https://github.com/ghwang-s/abkd.\n","authors":["Guanghui Wang","Zhiyong Yang","Zitai Wang","Shi Wang","Qianqian Xu","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2505.04560v2.pdf","comment":"ICML 2025 Spotlight"},{"id":"http://arxiv.org/abs/2503.20291v2","updated":"2025-05-15T15:06:46Z","published":"2025-03-26T07:33:36Z","title":"CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at\n  Intermediate Resolution with Structure-Aware Multimodal U-Nets","summary":"  Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at\nintermediate resolution (4-8 {\\AA}) is crucial in protein structure\ndetermination. Recent advances in deep learning have led to the development of\nautomated approaches for enhancing experimental cryo-EM density maps. Yet,\nthese methods are not optimized for intermediate-resolution maps and rely on\nmap density features alone. To address this, we propose CryoSAMU, a novel\nmethod designed to enhance 3D cryo-EM density maps of protein structures using\nstructure-aware multimodal U-Nets and trained on curated\nintermediate-resolution density maps. We comprehensively evaluate CryoSAMU\nacross various metrics and demonstrate its competitive performance compared to\nstate-of-the-art methods. Notably, CryoSAMU achieves significantly faster\nprocessing speed, showing promise for future practical applications. Our code\nis available at https://github.com/chenwei-zhang/CryoSAMU.\n","authors":["Chenwei Zhang","Khanh Dao Duc"],"pdf_url":"https://arxiv.org/pdf/2503.20291v2.pdf","comment":"19 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4\n  supplementary tables"},{"id":"http://arxiv.org/abs/2502.02205v2","updated":"2025-05-15T15:00:10Z","published":"2025-02-04T10:42:30Z","title":"From Uncertain to Safe: Conformal Fine-Tuning of Diffusion Models for\n  Safe PDE Control","summary":"  The application of deep learning for partial differential equation\n(PDE)-constrained control is gaining increasing attention. However, existing\nmethods rarely consider safety requirements crucial in real-world applications.\nTo address this limitation, we propose Safe Diffusion Models for PDE Control\n(SafeDiffCon), which introduce the uncertainty quantile as model uncertainty\nquantification to achieve optimal control under safety constraints through both\npost-training and inference phases. Firstly, our approach post-trains a\npre-trained diffusion model to generate control sequences that better satisfy\nsafety constraints while achieving improved control objectives via a reweighted\ndiffusion loss, which incorporates the uncertainty quantile estimated using\nconformal prediction. Secondly, during inference, the diffusion model\ndynamically adjusts both its generation process and parameters through\niterative guidance and fine-tuning, conditioned on control targets while\nsimultaneously integrating the estimated uncertainty quantile. We evaluate\nSafeDiffCon on three control tasks: 1D Burgers' equation, 2D incompressible\nfluid, and controlled nuclear fusion problem. Results demonstrate that\nSafeDiffCon is the only method that satisfies all safety constraints, whereas\nother classical and deep learning baselines fail. Furthermore, while adhering\nto safety constraints, SafeDiffCon achieves the best control performance.\n","authors":["Peiyan Hu","Xiaowei Qian","Wenhao Deng","Rui Wang","Haodong Feng","Ruiqi Feng","Tao Zhang","Long Wei","Yue Wang","Zhi-Ming Ma","Tailin Wu"],"pdf_url":"https://arxiv.org/pdf/2502.02205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10375v1","updated":"2025-05-15T14:59:17Z","published":"2025-05-15T14:59:17Z","title":"Are Sparse Autoencoders Useful for Java Function Bug Detection?","summary":"  Software vulnerabilities such as buffer overflows and SQL injections are a\nmajor source of security breaches. Traditional methods for vulnerability\ndetection remain essential but are limited by high false positive rates,\nscalability issues, and reliance on manual effort. These constraints have\ndriven interest in AI-based approaches to automated vulnerability detection and\nsecure code generation. While Large Language Models (LLMs) have opened new\navenues for classification tasks, their complexity and opacity pose challenges\nfor interpretability and deployment. Sparse Autoencoder offer a promising\nsolution to this problem. We explore whether SAEs can serve as a lightweight,\ninterpretable alternative for bug detection in Java functions. We evaluate the\neffectiveness of SAEs when applied to representations from GPT-2 Small and\nGemma 2B, examining their capacity to highlight buggy behaviour without\nfine-tuning the underlying LLMs. We found that SAE-derived features enable bug\ndetection with an F1 score of up to 89%, consistently outperforming fine-tuned\ntransformer encoder baselines. Our work provides the first empirical evidence\nthat SAEs can be used to detect software bugs directly from the internal\nrepresentations of pretrained LLMs, without any fine-tuning or task-specific\nsupervision.\n","authors":["Rui Melo","Claudia Mamede","Andre Catarino","Rui Abreu","Henrique Lopes Cardoso"],"pdf_url":"https://arxiv.org/pdf/2505.10375v1.pdf","comment":"10 pages, 10 figures"},{"id":"http://arxiv.org/abs/2505.10371v1","updated":"2025-05-15T14:56:06Z","published":"2025-05-15T14:56:06Z","title":"ILIF: Temporal Inhibitory Leaky Integrate-and-Fire Neuron for\n  Overactivation in Spiking Neural Networks","summary":"  The Spiking Neural Network (SNN) has drawn increasing attention for its\nenergy-efficient, event-driven processing and biological plausibility. To train\nSNNs via backpropagation, surrogate gradients are used to approximate the\nnon-differentiable spike function, but they only maintain nonzero derivatives\nwithin a narrow range of membrane potentials near the firing threshold,\nreferred to as the surrogate gradient support width gamma. We identify a major\nchallenge, termed the dilemma of gamma: a relatively large gamma leads to\noveractivation, characterized by excessive neuron firing, which in turn\nincreases energy consumption, whereas a small gamma causes vanishing gradients\nand weakens temporal dependencies. To address this, we propose a temporal\nInhibitory Leaky Integrate-and-Fire (ILIF) neuron model, inspired by biological\ninhibitory mechanisms. This model incorporates interconnected inhibitory units\nfor membrane potential and current, effectively mitigating overactivation while\npreserving gradient propagation. Theoretical analysis demonstrates ILIF\neffectiveness in overcoming the gamma dilemma, and extensive experiments on\nmultiple datasets show that ILIF improves energy efficiency by reducing firing\nrates, stabilizes training, and enhances accuracy. The code is available at\ngithub.com/kaisun1/ILIF.\n","authors":["Kai Sun","Peibo Duan","Levin Kuhlmann","Beilun Wang","Bin Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.10371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10367v1","updated":"2025-05-15T14:55:11Z","published":"2025-05-15T14:55:11Z","title":"A Hybrid Strategy for Aggregated Probabilistic Forecasting and Energy\n  Trading in HEFTCom2024","summary":"  Obtaining accurate probabilistic energy forecasts and making effective\ndecisions amid diverse uncertainties are routine challenges in future energy\nsystems. This paper presents the solution of team GEB, which ranked 3rd in\ntrading, 4th in forecasting, and 1st among student teams in the IEEE Hybrid\nEnergy Forecasting and Trading Competition 2024 (HEFTCom2024). The solution\nprovides accurate probabilistic forecasts for a wind-solar hybrid system, and\nachieves substantial trading revenue in the day-ahead electricity market. Key\ncomponents include: (1) a stacking-based approach combining sister forecasts\nfrom various Numerical Weather Predictions (NWPs) to provide wind power\nforecasts, (2) an online solar post-processing model to address the\ndistribution shift in the online test set caused by increased solar capacity,\n(3) a probabilistic aggregation method for accurate quantile forecasts of\nhybrid generation, and (4) a stochastic trading strategy to maximize expected\ntrading revenue considering uncertainties in electricity prices. This paper\nalso explores the potential of end-to-end learning to further enhance the\ntrading revenue by adjusting the distribution of forecast errors. Detailed case\nstudies are provided to validate the effectiveness of these proposed methods.\nCode for all mentioned methods is available for reproduction and further\nresearch in both industry and academia.\n","authors":["Chuanqing Pu","Feilong Fan","Nengling Tai","Songyuan Liu","Jinming Yu"],"pdf_url":"https://arxiv.org/pdf/2505.10367v1.pdf","comment":"Solution description of IEEE Hybrid Energy Forecasting and Trading\n  Competition (HEFTCom)"},{"id":"http://arxiv.org/abs/2505.10361v1","updated":"2025-05-15T14:52:16Z","published":"2025-05-15T14:52:16Z","title":"Plasticity as the Mirror of Empowerment","summary":"  Agents are minimally entities that are influenced by their past observations\nand act to influence future observations. This latter capacity is captured by\nempowerment, which has served as a vital framing concept across artificial\nintelligence and cognitive science. This former capacity, however, is equally\nfoundational: In what ways, and to what extent, can an agent be influenced by\nwhat it observes? In this paper, we ground this concept in a universal\nagent-centric measure that we refer to as plasticity, and reveal a fundamental\nconnection to empowerment. Following a set of desiderata on a suitable\ndefinition, we define plasticity using a new information-theoretic quantity we\ncall the generalized directed information. We show that this new quantity\nstrictly generalizes the directed information introduced by Massey (1990) while\npreserving all of its desirable properties. Our first finding is that\nplasticity is the mirror of empowerment: The agent's plasticity is identical to\nthe empowerment of the environment, and vice versa. Our second finding\nestablishes a tension between the plasticity and empowerment of an agent,\nsuggesting that agent design needs to be mindful of both characteristics. We\nexplore the implications of these findings, and suggest that plasticity,\nempowerment, and their relationship are essential to understanding agency.\n","authors":["David Abel","Michael Bowling","AndrÃ© Barreto","Will Dabney","Shi Dong","Steven Hansen","Anna Harutyunyan","Khimya Khetarpal","Clare Lyle","Razvan Pascanu","Georgios Piliouras","Doina Precup","Jonathan Richens","Mark Rowland","Tom Schaul","Satinder Singh"],"pdf_url":"https://arxiv.org/pdf/2505.10361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10360v1","updated":"2025-05-15T14:51:22Z","published":"2025-05-15T14:51:22Z","title":"FactsR: A Safer Method for Producing High Quality Healthcare\n  Documentation","summary":"  There are now a multitude of AI-scribing solutions for healthcare promising\nthe utilization of large language models for ambient documentation. However,\nthese AI scribes still rely on one-shot, or few-shot prompts for generating\nnotes after the consultation has ended, employing little to no reasoning. This\nrisks long notes with an increase in hallucinations, misrepresentation of the\nintent of the clinician, and reliance on the proofreading of the clinician to\ncatch errors. A dangerous combination for patient safety if vigilance is\ncompromised by workload and fatigue. In this paper, we introduce a method for\nextracting salient clinical information in real-time alongside the healthcare\nconsultation, denoted Facts, and use that information recursively to generate\nthe final note. The FactsR method results in more accurate and concise notes by\nplacing the clinician-in-the-loop of note generation, while opening up new use\ncases within real-time decision support.\n","authors":["Victor PetrÃ©n Bach Hansen","Lasse KrogsbÃ¸ll","Jonas LyngsÃ¸","Mathias Baltzersen","Andreas Motzfeldt","Kevin Pelgrims","Lars MaalÃ¸e"],"pdf_url":"https://arxiv.org/pdf/2505.10360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10352v1","updated":"2025-05-15T14:43:35Z","published":"2025-05-15T14:43:35Z","title":"SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with\n  Hamming Attention and $\\mathcal{O}(T)$ Complexity","summary":"  Spiking Neural Networks (SNNs) have shown competitive performance to\nArtificial Neural Networks (ANNs) in various vision tasks, while offering\nsuperior energy efficiency. However, existing SNN-based Transformers primarily\nfocus on single-image tasks, emphasizing spatial features while not effectively\nleveraging SNNs' efficiency in video-based vision tasks. In this paper, we\nintroduce SpikeVideoFormer, an efficient spike-driven video Transformer,\nfeaturing linear temporal complexity $\\mathcal{O}(T)$. Specifically, we design\na spike-driven Hamming attention (SDHA) which provides a theoretically guided\nadaptation from traditional real-valued attention to spike-driven attention.\nBuilding on SDHA, we further analyze various spike-driven space-time attention\ndesigns and identify an optimal scheme that delivers appealing performance for\nvideo tasks, while maintaining only linear temporal complexity. The\ngeneralization ability and efficiency of our model are demonstrated across\ndiverse downstream video tasks, including classification, human pose tracking,\nand semantic segmentation. Empirical results show our method achieves\nstate-of-the-art (SOTA) performance compared to existing SNN approaches, with\nover 15\\% improvement on the latter two tasks. Additionally, it matches the\nperformance of recent ANN-based methods while offering significant efficiency\ngains, achieving $\\times 16$, $\\times 10$ and $\\times 5$ improvements on the\nthree tasks. https://github.com/JimmyZou/SpikeVideoFormer\n","authors":["Shihao Zou","Qingfeng Li","Wei Ji","Jingjing Li","Yongkui Yang","Guoqi Li","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2505.10352v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2412.03587v2","updated":"2025-05-15T14:39:45Z","published":"2024-11-26T08:41:45Z","title":"Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient\n  Fine-Tuning of Language Models","summary":"  Transformer-based large-scale pre-trained models achieve great success.\nFine-tuning is the standard practice for leveraging these models in downstream\ntasks. Among the fine-tuning methods, adapter-tuning provides a\nparameter-efficient fine-tuning by introducing lightweight trainable modules\nwhile keeping most pre-trained parameters frozen. However, existing\nadapter-tuning methods still impose substantial resource usage. Through our\ninvestigation, we show that each adapter unequally contributes to both task\nperformance and resource usage. Motivated by this insight, we propose Selective\nAdapter FrEezing (SAFE), which gradually freezes less important adapters early\nto reduce unnecessary resource usage while maintaining performance. In our\nexperiments, SAFE reduces memory usage, computation amount, and training time\nby 42.85\\%, 34.59\\%, and 11.82\\%, respectively, while achieving comparable or\nbetter task performance compared to the baseline. We also demonstrate that SAFE\ninduces regularization effect, thereby smoothing the loss landscape, which\nenables the model to generalize better by avoiding sharp minima.\n","authors":["Hyegang Son","Yonglak Son","Changhoon Kim","Young Geun Kim"],"pdf_url":"https://arxiv.org/pdf/2412.03587v2.pdf","comment":"URL: https://aclanthology.org/2025.naacl-long.480/ Volume:\n  Proceedings of the 2025 Conference of the Nations of the Americas Chapter of\n  the Association for Computational Linguistics: Human Language Technologies\n  (Volume 1: Long Papers) Year: 2025 Address: Albuquerque, New Mexico"},{"id":"http://arxiv.org/abs/2505.10347v1","updated":"2025-05-15T14:34:36Z","published":"2025-05-15T14:34:36Z","title":"Uniform Loss vs. Specialized Optimization: A Comparative Analysis in\n  Multi-Task Learning","summary":"  Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task\nLearning by addressing issues like conflicting gradients and differing gradient\nnorms, which hinder equal-weighted task training. However, recent critiques\nsuggest that equally weighted tasks can achieve competitive results compared to\nSMTOs, arguing that previous SMTO results were influenced by poor\nhyperparameter optimization and lack of regularization. In this work, we\nevaluate these claims through an extensive empirical evaluation of SMTOs,\nincluding some of the latest methods, on more complex multi-task problems to\nclarify this behavior. Our findings indicate that SMTOs perform well compared\nto uniform loss and that fixed weights can achieve competitive performance\ncompared to SMTOs. Furthermore, we demonstrate why uniform loss perform\nsimilarly to SMTOs in some instances. The code will be made publicly available.\n","authors":["Gabriel S. Gama","Valdir Grassi Jr"],"pdf_url":"https://arxiv.org/pdf/2505.10347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10344v1","updated":"2025-05-15T14:33:31Z","published":"2025-05-15T14:33:31Z","title":"An Introduction to Discrete Variational Autoencoders","summary":"  Variational Autoencoders (VAEs) are well-established as a principled approach\nto probabilistic unsupervised learning with neural networks. Typically, an\nencoder network defines the parameters of a Gaussian distributed latent space\nfrom which we can sample and pass realizations to a decoder network. This model\nis trained to reconstruct its inputs and is optimized through the evidence\nlower bound. In recent years, discrete latent spaces have grown in popularity,\nsuggesting that they may be a natural choice for many data modalities (e.g.\ntext). In this tutorial, we provide a rigorous, yet practical, introduction to\ndiscrete variational autoencoders -- specifically, VAEs in which the latent\nspace is made up of latent variables that follow a categorical distribution. We\nassume only a basic mathematical background with which we carefully derive each\nstep from first principles. From there, we develop a concrete training recipe\nand provide an example implementation, hosted at\nhttps://github.com/alanjeffares/discreteVAE.\n","authors":["Alan Jeffares","Liyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2505.10344v1.pdf","comment":"Tutorial paper"},{"id":"http://arxiv.org/abs/2410.12609v2","updated":"2025-05-15T14:27:59Z","published":"2024-10-16T14:26:08Z","title":"Towards Graph Foundation Models: Training on Knowledge Graphs Enables\n  Transferability to General Graphs","summary":"  Inspired by the success of large language models, there is a trend toward\ndeveloping graph foundation models to conduct diverse downstream tasks in\nvarious domains. However, current models often require extra fine-tuning to\napply their learned structural and semantic representations to new graphs,\nwhich limits their versatility. Recent breakthroughs in zero-shot inductive\nreasoning on knowledge graphs (KGs), offer us a new perspective on extending KG\nreasoning to general graph applications. In this paper, we introduce SCR, a\nunified graph reasoning framework designed to train on knowledge graphs and\neffectively generalize across a wide range of graph tasks and domains. We begin\nby designing the task-specific KG structures to establish a unified topology\nfor different task formats. Then we propose semantic-conditioned message\npassing, a novel mechanism addressing the inherent semantic isolation in\ntraditional KG reasoning, by jointly modeling structural and semantic\ninvariance patterns in graph representations. To demonstrate the effectiveness,\nwe evaluate the inductive reasoning capability of SCR using 38 diverse graph\ndatasets, covering node-level, link-level, and graph-level tasks across\nmultiple domains. Our results show substantial performance gains over existing\nfoundation models and supervised baselines, highlighting the efficacy and\nadaptability of our approach.\n","authors":["Kai Wang","Siqiang Luo","Caihua Shan","Yifei Shen"],"pdf_url":"https://arxiv.org/pdf/2410.12609v2.pdf","comment":"25 Pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.08210v2","updated":"2025-05-15T14:22:35Z","published":"2025-04-11T02:27:30Z","title":"Optimizing Power Grid Topologies with Reinforcement Learning: A Survey\n  of Methods and Challenges","summary":"  Power grid operation is becoming increasingly complex due to the rising\nintegration of renewable energy sources and the need for more adaptive control\nstrategies. Reinforcement Learning (RL) has emerged as a promising approach to\npower network control (PNC), offering the potential to enhance decision-making\nin dynamic and uncertain environments. The Learning To Run a Power Network\n(L2RPN) competitions have played a key role in accelerating research by\nproviding standardized benchmarks and problem formulations, leading to rapid\nadvancements in RL-based methods. This survey provides a comprehensive and\nstructured overview of RL applications for power grid topology optimization,\ncategorizing existing techniques, highlighting key design choices, and\nidentifying gaps in current research. Additionally, we present a comparative\nnumerical study evaluating the impact of commonly applied RL-based methods,\noffering insights into their practical effectiveness. By consolidating existing\nresearch and outlining open challenges, this survey aims to provide a\nfoundation for future advancements in RL-driven power grid optimization.\n","authors":["Erica van der Sar","Alessandro Zocca","Sandjai Bhulai"],"pdf_url":"https://arxiv.org/pdf/2504.08210v2.pdf","comment":"60 pages, 26 figures, preprint"},{"id":"http://arxiv.org/abs/2408.09840v2","updated":"2025-05-15T14:20:49Z","published":"2024-08-19T09:36:07Z","title":"Machine Learning with Physics Knowledge for Prediction: A Survey","summary":"  This survey examines the broad suite of methods and models for combining\nmachine learning with physics knowledge for prediction and forecast, with a\nfocus on partial differential equations. These methods have attracted\nsignificant interest due to their potential impact on advancing scientific\nresearch and industrial practices by improving predictive models with small- or\nlarge-scale datasets and expressive predictive models with useful inductive\nbiases. The survey has two parts. The first considers incorporating physics\nknowledge on an architectural level through objective functions, structured\npredictive models, and data augmentation. The second considers data as physics\nknowledge, which motivates looking at multi-task, meta, and contextual learning\nas an alternative approach to incorporating physics knowledge in a data-driven\nfashion. Finally, we also provide an industrial perspective on the application\nof these methods and a survey of the open-source ecosystem for physics-informed\nmachine learning.\n","authors":["Joe Watson","Chen Song","Oliver Weeger","Theo Gruner","An T. Le","Kay Pompetzki","Ahmed Hendawy","Oleg Arenz","Will Trojak","Miles Cranmer","Carlo D'Eramo","Fabian BÃ¼low","Tanmay Goyal","Jan Peters","Martin W. Hoffman"],"pdf_url":"https://arxiv.org/pdf/2408.09840v2.pdf","comment":"61 pages, 8 figures, 2 tables. Accepted at the Transactions of\n  Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2505.10331v1","updated":"2025-05-15T14:20:02Z","published":"2025-05-15T14:20:02Z","title":"Emergence of Structure in Ensembles of Random Neural Networks","summary":"  Randomness is ubiquitous in many applications across data science and machine\nlearning. Remarkably, systems composed of random components often display\nemergent global behaviors that appear deterministic, manifesting a transition\nfrom microscopic disorder to macroscopic organization. In this work, we\nintroduce a theoretical model for studying the emergence of collective\nbehaviors in ensembles of random classifiers. We argue that, if the ensemble is\nweighted through the Gibbs measure defined by adopting the classification loss\nas an energy, then there exists a finite temperature parameter for the\ndistribution such that the classification is optimal, with respect to the loss\n(or the energy). Interestingly, for the case in which samples are generated by\na Gaussian distribution and labels are constructed by employing a teacher\nperceptron, we analytically prove and numerically confirm that such optimal\ntemperature does not depend neither on the teacher classifier (which is, by\nconstruction of the learning problem, unknown), nor on the number of random\nclassifiers, highlighting the universal nature of the observed behavior.\nExperiments on the MNIST dataset underline the relevance of this phenomenon in\nhigh-quality, noiseless, datasets. Finally, a physical analogy allows us to\nshed light on the self-organizing nature of the studied phenomenon.\n","authors":["Luca Muscarnera","Luigi Loreti","Giovanni Todeschini","Alessio Fumagalli","Francesco Regazzoni"],"pdf_url":"https://arxiv.org/pdf/2505.10331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10330v1","updated":"2025-05-15T14:19:01Z","published":"2025-05-15T14:19:01Z","title":"Efficient Adaptation of Reinforcement Learning Agents to Sudden\n  Environmental Change","summary":"  Real-world autonomous decision-making systems, from robots to recommendation\nengines, must operate in environments that change over time. While deep\nreinforcement learning (RL) has shown an impressive ability to learn optimal\npolicies in stationary environments, most methods are data intensive and assume\na world that does not change between training and test time. As a result,\nconventional RL methods struggle to adapt when conditions change. This poses a\nfundamental challenge: how can RL agents efficiently adapt their behavior when\nencountering novel environmental changes during deployment without\ncatastrophically forgetting useful prior knowledge? This dissertation\ndemonstrates that efficient online adaptation requires two key capabilities:\n(1) prioritized exploration and sampling strategies that help identify and\nlearn from relevant experiences, and (2) selective preservation of prior\nknowledge through structured representations that can be updated without\ndisruption to reusable components.\n","authors":["Jonathan Clifford Balloch"],"pdf_url":"https://arxiv.org/pdf/2505.10330v1.pdf","comment":"PhD Dissertation, 131 pages"},{"id":"http://arxiv.org/abs/2501.00777v2","updated":"2025-05-15T14:18:58Z","published":"2025-01-01T09:00:10Z","title":"FitCF: A Framework for Automatic Feature Importance-guided\n  Counterfactual Example Generation","summary":"  Counterfactual examples are widely used in natural language processing (NLP)\nas valuable data to improve models, and in explainable artificial intelligence\n(XAI) to understand model behavior. The automated generation of counterfactual\nexamples remains a challenging task even for large language models (LLMs),\ndespite their impressive performance on many tasks. In this paper, we first\nintroduce ZeroCF, a faithful approach for leveraging important words derived\nfrom feature attribution methods to generate counterfactual examples in a\nzero-shot setting. Second, we present a new framework, FitCF, which further\nverifies aforementioned counterfactuals by label flip verification and then\ninserts them as demonstrations for few-shot prompting, outperforming two\nstate-of-the-art baselines. Through ablation studies, we identify the\nimportance of each of FitCF's core components in improving the quality of\ncounterfactuals, as assessed through flip rate, perplexity, and similarity\nmeasures. Furthermore, we show the effectiveness of LIME and Integrated\nGradients as backbone attribution methods for FitCF and find that the number of\ndemonstrations has the largest effect on performance. Finally, we reveal a\nstrong correlation between the faithfulness of feature attribution scores and\nthe quality of generated counterfactuals.\n","authors":["Qianli Wang","Nils Feldhus","Simon Ostermann","Luis Felipe Villa-Arenas","Sebastian MÃ¶ller","Vera Schmitt"],"pdf_url":"https://arxiv.org/pdf/2501.00777v2.pdf","comment":"ACL 2025 Findings; camera-ready version"},{"id":"http://arxiv.org/abs/2405.11752v3","updated":"2025-05-15T14:08:49Z","published":"2024-05-20T03:26:58Z","title":"Towards Foundation Model for Chemical Reactor Modeling: Meta-Learning\n  with Physics-Informed Adaptation","summary":"  Developing accurate models for chemical reactors is often challenging due to\nthe complexity of reaction kinetics and process dynamics. Traditional\napproaches require retraining models for each new system, limiting\ngeneralizability and efficiency. In this work, we take a step toward foundation\nmodels for chemical reactor modeling by introducing a neural network framework\nthat generalizes across diverse reactor types and rapidly adapts to new\nchemical processes. Our approach leverages meta-learning to pretrain the model\non a broad set of reactor dynamics, enabling efficient adaptation to unseen\nreactions with minimal data. To further enhance generalizability, we\nincorporate physics-informed fine-tuning, ensuring physically consistent\nadaptation to new reactor conditions. Our framework is evaluated across three\ninteger-order fundamental reactor types - continuous stirred tank reactors,\nbatch reactors, and plug flow reactors - demonstrating superior few-shot\nadaptation compared to conventional data-driven, physics-informed, and transfer\nlearning approaches. By combining meta-learning with physics-informed\nadaptation, this work lays the foundation for a generalizable modeling\nframework, advancing the development of foundation models for chemical\nengineering applications. Source code is available at\nhttps://github.com/killingbear999/chemical-reactor-foundation-model.\n","authors":["Zihao Wang","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2405.11752v3.pdf","comment":"Chemical Engineering Research and Design"},{"id":"http://arxiv.org/abs/2505.10325v1","updated":"2025-05-15T14:08:00Z","published":"2025-05-15T14:08:00Z","title":"A Representation Learning Approach to Feature Drift Detection in\n  Wireless Networks","summary":"  AI is foreseen to be a centerpiece in next generation wireless networks\nenabling enabling ubiquitous communication as well as new services. However, in\nreal deployment, feature distribution changes may degrade the performance of AI\nmodels and lead to undesired behaviors. To counter for undetected model\ndegradation, we propose ALERT; a method that can detect feature distribution\nchanges and trigger model re-training that works well on two wireless network\nuse cases: wireless fingerprinting and link anomaly detection. ALERT includes\nthree components: representation learning, statistical testing and utility\nassessment. We rely on MLP for designing the representation learning component,\non Kolmogorov-Smirnov and Population Stability Index tests for designing the\nstatistical testing and a new function for utility assessment. We show the\nsuperiority of the proposed method against ten standard drift detection methods\navailable in the literature on two wireless network use cases.\n","authors":["Athanasios Tziouvaras","Blaz Bertalanic","George Floros","Kostas Kolomvatsos","Panagiotis Sarigiannidis","Carolina Fortuna"],"pdf_url":"https://arxiv.org/pdf/2505.10325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03825v2","updated":"2025-05-15T14:07:22Z","published":"2025-05-03T11:28:13Z","title":"Intelligently Augmented Contrastive Tensor Factorization: Empowering\n  Multi-dimensional Time Series Classification in Low-Data Environments","summary":"  Classification of multi-dimensional time series from real-world systems\nrequire fine-grained learning of complex features such as cross-dimensional\ndependencies and intra-class variations-all under the practical challenge of\nlow training data availability. However, standard deep learning (DL) struggles\nto learn generalizable features in low-data environments due to model\noverfitting. We propose a versatile yet data-efficient framework, Intelligently\nAugmented Contrastive Tensor Factorization (ITA-CTF), to learn effective\nrepresentations from multi-dimensional time series. The CTF module learns core\nexplanatory components of the time series (e.g., sensor factors, temporal\nfactors), and importantly, their joint dependencies. Notably, unlike standard\ntensor factorization (TF), the CTF module incorporates a new contrastive loss\noptimization to induce similarity learning and class-awareness into the learnt\nrepresentations for better classification performance. To strengthen this\ncontrastive learning, the preceding ITA module generates targeted but\ninformative augmentations that highlight realistic intra-class patterns in the\noriginal data, while preserving class-wise properties. This is achieved by\ndynamically sampling a \"soft\" class prototype to guide the warping of each\nquery data sample, which results in an augmentation that is intelligently\npattern-mixed between the \"soft\" class prototype and the query sample. These\naugmentations enable the CTF module to recognize complex intra-class variations\ndespite the limited original training data, and seek out invariant class-wise\nproperties for accurate classification performance. The proposed method is\ncomprehensively evaluated on five different classification tasks. Compared to\nstandard TF and several DL benchmarks, notable performance improvements up to\n18.7% were achieved.\n","authors":["Anushiya Arunan","Yan Qin","Xiaoli Li","Yuen Chau"],"pdf_url":"https://arxiv.org/pdf/2505.03825v2.pdf","comment":"Accepted in Expert Systems with Applications\n  (DOI:https://doi.org/10.1016/j.eswa.2025.127889)"},{"id":"http://arxiv.org/abs/2505.10322v1","updated":"2025-05-15T14:06:38Z","published":"2025-05-15T14:06:38Z","title":"Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate\n  Descent Framework","summary":"  Decentralized optimization has become vital for leveraging distributed data\nwithout central control, enhancing scalability and privacy. However, practical\ndeployments face fundamental challenges due to heterogeneous computation speeds\nand unpredictable communication delays. This paper introduces a refined model\nof Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under\npractical assumptions of bounded computation and communication times. To\nunderstand the convergence of ADSGD, we first analyze Asynchronous Stochastic\nBlock Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges\nunder computation-delay-independent step sizes. The convergence result is\nestablished without assuming bounded data heterogeneity. Empirical experiments\nreveal that ADSGD outperforms existing methods in wall-clock convergence time\nacross various scenarios. With its simplicity, efficiency in memory and\ncommunication, and resilience to communication and computation delays, ADSGD is\nwell-suited for real-world decentralized learning tasks.\n","authors":["Yijie Zhou","Shi Pu"],"pdf_url":"https://arxiv.org/pdf/2505.10322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19477v3","updated":"2025-05-15T14:06:27Z","published":"2024-11-29T05:29:47Z","title":"Simple and Provable Scaling Laws for the Test-Time Compute of Large\n  Language Models","summary":"  We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms.\n","authors":["Yanxi Chen","Xuchen Pan","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.19477v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10320v1","updated":"2025-05-15T14:05:15Z","published":"2025-05-15T14:05:15Z","title":"J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning","summary":"  The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.\n","authors":["Chenxi Whitehouse","Tianlu Wang","Ping Yu","Xian Li","Jason Weston","Ilia Kulikov","Swarnadeep Saha"],"pdf_url":"https://arxiv.org/pdf/2505.10320v1.pdf","comment":"10 pages, 8 tables, 11 figures"},{"id":"http://arxiv.org/abs/2505.10319v1","updated":"2025-05-15T14:04:02Z","published":"2025-05-15T14:04:02Z","title":"Deconstructing Subset Construction -- Reducing While Determinizing","summary":"  We present a novel perspective on the NFA canonization problem, which\nintroduces intermediate minimization steps to reduce the exploration space\non-the-fly. Essential to our approach are so-called equivalence registries\nwhich manage information about equivalent states and allow for incorporating\nfurther optimization techniques such as convexity closures or simulation to\nboost performance. Due to the generality of our approach, these concepts can be\nembedded in classic subset construction or Brzozowski's approach. We evaluate\nour approach on a set of real-world examples from automatic sequences and\nobserve that we are able to improve especially worst-case scenarios. We\nimplement our approach in an open-source library for users to experiment with.\n","authors":["John Nicol","Markus Frohme"],"pdf_url":"https://arxiv.org/pdf/2505.10319v1.pdf","comment":"19 pages, 2 figures"},{"id":"http://arxiv.org/abs/2505.10307v1","updated":"2025-05-15T13:53:48Z","published":"2025-05-15T13:53:48Z","title":"Negative Metric Learning for Graphs","summary":"  Graph contrastive learning (GCL) often suffers from false negatives, which\ndegrades the performance on downstream tasks. The existing methods addressing\nthe false negative issue usually rely on human prior knowledge, still leading\nGCL to suboptimal results. In this paper, we propose a novel Negative Metric\nLearning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative\nMetric Network (NMN) to build a negative metric space, in which false negatives\ncan be distinguished better from true negatives based on their distance to\nanchor node. To overcome the lack of explicit supervision signals for NML, we\npropose a joint training scheme with bi-level optimization objective, which\nimplicitly utilizes the self-supervision signals to iteratively optimize the\nencoder and the negative metric network. The solid theoretical analysis and the\nextensive experiments conducted on widely used benchmarks verify the\nsuperiority of the proposed method.\n","authors":["Yiyang Zhao","Chengpei Wu","Lilin Zhang","Ning Yang"],"pdf_url":"https://arxiv.org/pdf/2505.10307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10297v1","updated":"2025-05-15T13:44:32Z","published":"2025-05-15T13:44:32Z","title":"Defending the Edge: Representative-Attention for Mitigating Backdoor\n  Attacks in Federated Learning","summary":"  Federated learning (FL) enhances privacy and reduces communication cost for\nresource-constrained edge clients by supporting distributed model training at\nthe edge. However, the heterogeneous nature of such devices produces diverse,\nnon-independent, and identically distributed (non-IID) data, making the\ndetection of backdoor attacks more challenging. In this paper, we propose a\nnovel federated representative-attention-based defense mechanism, named FeRA,\nthat leverages cross-client attention over internal feature representations to\ndistinguish benign from malicious clients. FeRA computes an anomaly score based\non representation reconstruction errors, effectively identifying clients whose\ninternal activations significantly deviate from the group consensus. Our\nevaluation demonstrates FeRA's robustness across various FL scenarios,\nincluding challenging non-IID data distributions typical of edge devices.\nExperimental results show that it effectively reduces backdoor attack success\nrates while maintaining high accuracy on the main task. The method is\nmodel-agnostic, attack-agnostic, and does not require labeled reference data,\nmaking it well suited to heterogeneous and resource-limited edge deployments.\n","authors":["Chibueze Peace Obioma","Youcheng Sun","Mustafa A. Mustafa"],"pdf_url":"https://arxiv.org/pdf/2505.10297v1.pdf","comment":"Submitted to ESORICS 2025"},{"id":"http://arxiv.org/abs/2505.10296v1","updated":"2025-05-15T13:44:27Z","published":"2025-05-15T13:44:27Z","title":"Optimizing Electric Bus Charging Scheduling with Uncertainties Using\n  Hierarchical Deep Reinforcement Learning","summary":"  The growing adoption of Electric Buses (EBs) represents a significant step\ntoward sustainable development. By utilizing Internet of Things (IoT) systems,\ncharging stations can autonomously determine charging schedules based on\nreal-time data. However, optimizing EB charging schedules remains a critical\nchallenge due to uncertainties in travel time, energy consumption, and\nfluctuating electricity prices. Moreover, to address real-world complexities,\ncharging policies must make decisions efficiently across multiple time scales\nand remain scalable for large EB fleets. In this paper, we propose a\nHierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the\noriginal Markov Decision Process (MDP) into two augmented MDPs. To solve these\nMDPs and enable multi-timescale decision-making, we introduce a novel HDRL\nalgorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization\nEnhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic\n(DAC) algorithm for large-scale EB fleets are addressed through enhancements at\nboth decision levels. At the high level, we redesign the decentralized actor\nnetwork and integrate an attention mechanism to extract relevant global state\ninformation for each EB, decreasing the size of neural networks. At the low\nlevel, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is\nincorporated into the DAC framework, enabling decentralized and coordinated\ncharging power decisions, reducing computational complexity and enhancing\nconvergence speed. Extensive experiments with real-world data demonstrate the\nsuperior performance and scalability of DAC-MAPPO-E in optimizing EB fleet\ncharging schedules.\n","authors":["Jiaju Qi","Lei Lei","Thorsteinn Jonsson","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2505.10296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01223v5","updated":"2025-05-15T13:37:18Z","published":"2024-10-02T04:02:21Z","title":"Statistical Taylor Expansion","summary":"  Statistical Taylor expansion replaces the input precise variables in a\nconventional Taylor expansion with random variables each with known\ndistribution, to calculate the result mean and deviation. It is based on the\nuncorrelated uncertainty assumption: Each input variable is measured\nindependently with fine enough statistical precision, so that their\nuncertainties are independent of each other. Statistical Taylor expansion\nreviews that the intermediate analytic expressions can no longer be regarded as\nindependent of each other, and the result of analytic expression should be path\nindependent. This conclusion differs fundamentally from the conventional common\napproach in applied mathematics to find the best execution path for a result.\nThis paper also presents an implementation of statistical Taylor expansion\ncalled variance arithmetic, and the tests on variance arithmetic.\n","authors":["Chengpu Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01223v5.pdf","comment":"65 pages, 53 figures"},{"id":"http://arxiv.org/abs/2505.10279v1","updated":"2025-05-15T13:27:32Z","published":"2025-05-15T13:27:32Z","title":"Estimating the number of household TV profiles based in customer\n  behaviour using Gaussian mixture model averaging","summary":"  TV customers today face many choices from many live channels and on-demand\nservices. Providing a personalised experience that saves customers time when\ndiscovering content is essential for TV providers. However, a reliable\nunderstanding of their behaviour and preferences is key. When creating\npersonalised recommendations for TV, the biggest challenge is understanding\nviewing behaviour within households when multiple people are watching. The\nobjective is to detect and combine individual profiles to make\nbetter-personalised recommendations for group viewing. Our challenge is that we\nhave little explicit information about who is watching the devices at any time\n(individuals or groups). Also, we do not have a way to combine more than one\nindividual profile to make better recommendations for group viewing. We propose\na novel framework using a Gaussian mixture model averaging to obtain point\nestimates for the number of household TV profiles and a Bayesian random walk\nmodel to introduce uncertainty. We applied our approach using data from real\ncustomers whose TV-watching data totalled approximately half a million\nobservations. Our results indicate that combining our framework with the\nselected features provides a means to estimate the number of household TV\nprofiles and their characteristics, including shifts over time and\nquantification of uncertainty.\n","authors":["Gabriel R. Palma","Sally McClean","Brahim Allan","Zeeshan Tariq","Rafael A. Moral"],"pdf_url":"https://arxiv.org/pdf/2505.10279v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2504.04877v2","updated":"2025-05-15T13:27:26Z","published":"2025-04-07T09:41:04Z","title":"System Log Parsing with Large Language Models: A Review","summary":"  Log data provides crucial insights for tasks like monitoring, root cause\nanalysis, and anomaly detection. Due to the vast volume of logs, automated log\nparsing is essential to transform semi-structured log messages into structured\nrepresentations. Recent advances in large language models (LLMs) have\nintroduced the new research field of LLM-based log parsing. Despite promising\nresults, there is no structured overview of the approaches in this relatively\nnew research field with the earliest advances published in late 2023. This work\nsystematically reviews 29 LLM-based log parsing methods. We benchmark seven of\nthem on public datasets and critically assess their comparability and the\nreproducibility of their reported results. Our findings summarize the advances\nof this new research field, with insights on how to report results, which data\nsets, metrics and which terminology to use, and which inconsistencies to avoid,\nwith code and results made publicly available for transparency.\n","authors":["Viktor Beck","Max Landauer","Markus Wurzenberger","Florian Skopik","Andreas Rauber"],"pdf_url":"https://arxiv.org/pdf/2504.04877v2.pdf","comment":"36 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.10272v1","updated":"2025-05-15T13:23:16Z","published":"2025-05-15T13:23:16Z","title":"Spike-timing-dependent Hebbian learning as noisy gradient descent","summary":"  Hebbian learning is a key principle underlying learning in biological neural\nnetworks. It postulates that synaptic changes occur locally, depending on the\nactivities of pre- and postsynaptic neurons. While Hebbian learning based on\nneuronal firing rates is well explored, much less is known about learning rules\nthat account for precise spike-timing. We relate a Hebbian\nspike-timing-dependent plasticity rule to noisy gradient descent with respect\nto a natural loss function on the probability simplex. This connection allows\nus to prove that the learning rule eventually identifies the presynaptic neuron\nwith the highest activity. We also discover an intrinsic connection to noisy\nmirror descent.\n","authors":["Niklas Dexheimer","Sascha Gaudlitz","Johannes Schmidt-Hieber"],"pdf_url":"https://arxiv.org/pdf/2505.10272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10271v1","updated":"2025-05-15T13:22:20Z","published":"2025-05-15T13:22:20Z","title":"RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall\n  Probabilities Over 8 Hours","summary":"  We present a deep learning model for high-resolution probabilistic\nprecipitation forecasting over an 8-hour horizon in Europe, overcoming the\nlimitations of radar-only deep learning models with short forecast lead times.\nOur model efficiently integrates multiple data sources - including radar,\nsatellite, and physics-based numerical weather prediction (NWP) - while\ncapturing long-range interactions, resulting in accurate forecasts with robust\nuncertainty quantification through consistent probabilistic maps. Featuring a\ncompact architecture, it enables more efficient training and faster inference\nthan existing models. Extensive experiments demonstrate that our model\nsurpasses current operational NWP systems, extrapolation-based methods, and\ndeep-learning nowcasting models, setting a new standard for high-resolution\nprecipitation forecasting in Europe, ensuring a balance between accuracy,\ninterpretability, and computational efficiency.\n","authors":["Rafael Pablos Sarabia","Joachim Nyborg","Morten Birk","Jeppe Liborius SjÃ¸rup","Anders Lillevang Vesterholt","Ira Assent"],"pdf_url":"https://arxiv.org/pdf/2505.10271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04442v4","updated":"2025-05-15T13:21:39Z","published":"2024-10-06T10:41:03Z","title":"TimeBridge: Non-Stationarity Matters for Long-term Time Series\n  Forecasting","summary":"  Non-stationarity poses significant challenges for multivariate time series\nforecasting due to the inherent short-term fluctuations and long-term trends\nthat can lead to spurious regressions or obscure essential long-term\nrelationships. Most existing methods either eliminate or retain\nnon-stationarity without adequately addressing its distinct impacts on\nshort-term and long-term modeling. Eliminating non-stationarity is essential\nfor avoiding spurious regressions and capturing local dependencies in\nshort-term modeling, while preserving it is crucial for revealing long-term\ncointegration across variates. In this paper, we propose TimeBridge, a novel\nframework designed to bridge the gap between non-stationarity and dependency\nmodeling in long-term time series forecasting. By segmenting input series into\nsmaller patches, TimeBridge applies Integrated Attention to mitigate short-term\nnon-stationarity and capture stable dependencies within each variate, while\nCointegrated Attention preserves non-stationarity to model long-term\ncointegration across variates. Extensive experiments show that TimeBridge\nconsistently achieves state-of-the-art performance in both short-term and\nlong-term forecasting. Additionally, TimeBridge demonstrates exceptional\nperformance in financial forecasting on the CSI 500 and S&P 500 indices,\nfurther validating its robustness and effectiveness. Code is available at\nhttps://github.com/Hank0626/TimeBridge.\n","authors":["Peiyuan Liu","Beiliang Wu","Yifan Hu","Naiqi Li","Tao Dai","Jigang Bao","Shu-tao Xia"],"pdf_url":"https://arxiv.org/pdf/2410.04442v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10267v1","updated":"2025-05-15T13:18:37Z","published":"2025-05-15T13:18:37Z","title":"HandReader: Advanced Techniques for Efficient Fingerspelling Recognition","summary":"  Fingerspelling is a significant component of Sign Language (SL), allowing the\ninterpretation of proper names, characterized by fast hand movements during\nsigning. Although previous works on fingerspelling recognition have focused on\nprocessing the temporal dimension of videos, there remains room for improving\nthe accuracy of these approaches. This paper introduces HandReader, a group of\nthree architectures designed to address the fingerspelling recognition task.\nHandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to\nprocess RGB features from videos of varying lengths while preserving important\nsequential information. HandReader$_{KP}$ is built on the proposed Temporal\nPose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition\nin a batch allows the encoder to pass them through 2D and 3D convolution\nlayers, utilizing temporal and spatial information and accumulating keypoints\ncoordinates. We also introduce HandReader_RGB+KP - architecture with a joint\nencoder to benefit from RGB and keypoint modalities. Each HandReader model\npossesses distinct advantages and achieves state-of-the-art results on the\nChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate\nhigh performance on the first open dataset for Russian fingerspelling, Znaki,\npresented in this paper. The Znaki dataset and HandReader pre-trained models\nare publicly available.\n","authors":["Pavel Korotaev","Petr Surovtsev","Alexander Kapitanov","Karina Kvanchiani","Aleksandr Nagaev"],"pdf_url":"https://arxiv.org/pdf/2505.10267v1.pdf","comment":"https://github.com/ai-forever/handreader"},{"id":"http://arxiv.org/abs/2505.10264v1","updated":"2025-05-15T13:16:32Z","published":"2025-05-15T13:16:32Z","title":"Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack\n  in Federated Learning","summary":"  Federated Learning (FL) enables collaborative training of machine learning\nmodels across distributed clients without sharing raw data, ostensibly\npreserving data privacy. Nevertheless, recent studies have revealed critical\nvulnerabilities in FL, showing that a malicious central server can manipulate\nmodel updates to reconstruct clients' private training data. Existing data\nreconstruction attacks have important limitations: they often rely on\nassumptions about the clients' data distribution or their efficiency\nsignificantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes\nthese limitations. Our method leverages a new geometric perspective on fully\nconnected layers to craft malicious model parameters, enabling the perfect\nrecovery of arbitrarily large data batches in classification tasks without any\nprior knowledge of clients' data. Through extensive experiments on both image\nand tabular datasets, we demonstrate that our attack outperforms existing\nmethods and achieves perfect reconstruction of data batches two orders of\nmagnitude larger than the state of the art.\n","authors":["Francesco Diana","AndrÃ© Nusser","Chuan Xu","Giovanni Neglia"],"pdf_url":"https://arxiv.org/pdf/2505.10264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10262v1","updated":"2025-05-15T13:13:41Z","published":"2025-05-15T13:13:41Z","title":"Electric Bus Charging Schedules Relying on Real Data-Driven Targets\n  Based on Hierarchical Deep Reinforcement Learning","summary":"  The charging scheduling problem of Electric Buses (EBs) is investigated based\non Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is\nconceived, where the time horizon includes multiple charging and operating\nperiods in a day, while each period is further divided into multiple time\nsteps. To overcome the challenge of long-range multi-phase planning with sparse\nreward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP\ninto a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical\nDouble Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is\nproposed for simultaneously solving the decision problems arising at different\ntemporal resolutions. As a result, the high-level agent learns an effective\npolicy for prescribing the charging targets for every charging period, while\nthe low-level agent learns an optimal policy for setting the charging power of\nevery time step within a single charging period, with the aim of minimizing the\ncharging costs while meeting the charging target. It is proved that the flat\npolicy constructed by superimposing the optimal high-level policy and the\noptimal low-level policy performs as well as the optimal policy of the original\nMDP. Since jointly learning both levels of policies is challenging due to the\nnon-stationarity of the high-level agent and the sampling inefficiency of the\nlow-level agent, we divide the joint learning process into two phases and\nexploit our new HER algorithm to manipulate the experience replay buffers for\nboth levels of agents. Numerical experiments are performed with the aid of\nreal-world data to evaluate the performance of the proposed algorithm.\n","authors":["Jiaju Qi","Lei Lei","Thorsteinn Jonsson","Lajos Hanzo"],"pdf_url":"https://arxiv.org/pdf/2505.10262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10259v1","updated":"2025-05-15T13:10:31Z","published":"2025-05-15T13:10:31Z","title":"SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on\n  Resource-Constrained Devices","summary":"  Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload .\n","authors":["Xiangwen Zhuge","Xu Shen","Zeyu Wang","Fan Dang","Xuan Ding","Danyang Li","Yahui Han","Tianxiang Hao","Zheng Yang"],"pdf_url":"https://arxiv.org/pdf/2505.10259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01262v3","updated":"2025-05-15T12:59:09Z","published":"2024-10-02T06:16:06Z","title":"Improving Fine-Grained Control via Aggregation of Multiple Diffusion\n  Models","summary":"  While many diffusion models perform well when controlling for particular\naspect among style, character, and interaction, they struggle with fine-grained\ncontrol due to dataset limitations and intricate model architecture design.\nThis paper first introduces a novel training-free algorithm in fine-grained\ngeneration, Aggregation of Multiple Diffusion Models (AMDM), which integrates\nfeatures from multiple diffusion models into a specified model to activate\nspecific features and enable fine-grained control. Experimental results\ndemonstrate that AMDM significantly improves fine-grained control without\ntraining, validating its effectiveness. Additionally, it reveals that diffusion\nmodels initially focus on features such as position, attributes, and style,\nwith later stages improving generation quality and consistency. AMDM offers a\nnew perspective for tackling the challenges of fine-grained conditional control\ngeneration in diffusion models: We can fully utilize existing or develop new\nconditional diffusion models that control specific aspects, and then aggregate\nthem using AMDM algorithm. This eliminates the need for constructing complex\ndatasets, designing intricate model architectures, and incurring high training\ncosts. Code is available at: https://github.com/Hammour-steak/AMDM.\n","authors":["Conghan Yue","Zhengwei Peng","Shiyan Du","Zhi Ji","Chuangjian Cai","Le Wan","Dongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01262v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01521v2","updated":"2025-05-15T12:45:16Z","published":"2025-03-03T13:31:02Z","title":"R2VF: A Two-Step Regularization Algorithm to Cluster Categories in GLMs","summary":"  Over recent decades, extensive research has aimed to overcome the restrictive\nunderlying assumptions required for a Generalized Linear Model to generate\naccurate and meaningful predictions. These efforts include regularizing\ncoefficients, selecting features, and clustering ordinal categories, among\nother approaches. Despite these advances, efficiently clustering nominal\ncategories in GLMs without incurring high computational costs remains a\nchallenge. This paper introduces Ranking to Variable Fusion (R2VF), a two-step\nmethod designed to efficiently fuse nominal and ordinal categories in GLMs. By\nfirst transforming nominal features into an ordinal framework via regularized\nregression and then applying variable fusion, R2VF strikes a balance between\nmodel complexity and interpretability. We demonstrate the effectiveness of R2VF\nthrough comparisons with other methods, highlighting its performance in\naddressing overfitting and identifying an appropriate set of covariates.\n","authors":["Yuval Ben Dror"],"pdf_url":"https://arxiv.org/pdf/2503.01521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15674v2","updated":"2025-05-15T12:42:44Z","published":"2025-01-26T21:05:16Z","title":"TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and\n  Compression in LLMs","summary":"  The reasoning abilities of Large Language Models (LLMs) can be improved by\nstructurally denoising their weights, yet existing techniques primarily focus\non denoising the feed-forward network (FFN) of the transformer block, and can\nnot efficiently utilise the Multi-head Attention (MHA) block, which is the core\nof transformer architectures. To address this issue, we propose a novel\nintuitive framework that, at its very core, performs MHA compression through a\nmulti-head tensorisation process and the Tucker decomposition. This enables\nboth higher-dimensional structured denoising and compression of the MHA\nweights, by enforcing a shared higher-dimensional subspace across the weights\nof the multiple attention heads. We demonstrate that this approach consistently\nenhances the reasoning capabilities of LLMs across multiple benchmark datasets,\nand for both encoder-only and decoder-only architectures, while achieving\ncompression rates of up to $\\sim 250$ times in the MHA weights, all without\nrequiring any additional data, training, or fine-tuning. Furthermore, we show\nthat the proposed method can be seamlessly combined with existing\nFFN-only-based denoising techniques to achieve further improvements in LLM\nreasoning performance.\n","authors":["Yuxuan Gu","Wuyang Zhou","Giorgos Iacovides","Danilo Mandic"],"pdf_url":"https://arxiv.org/pdf/2501.15674v2.pdf","comment":"Accpeted for IEEE International Joint Conference on Neural Networks\n  (IJCNN 2025). The code is available at https://github.com/guyuxuan9/TensorLLM"},{"id":"http://arxiv.org/abs/2501.02481v4","updated":"2025-05-15T12:40:27Z","published":"2025-01-05T09:06:17Z","title":"Representation Convergence: Mutual Distillation is Secretly a Form of\n  Regularization","summary":"  In this paper, we argue that mutual distillation between reinforcement\nlearning policies serves as an implicit regularization, preventing them from\noverfitting to irrelevant features. We highlight two key contributions: (a)\nTheoretically, for the first time, we prove that enhancing the policy\nrobustness to irrelevant features leads to improved generalization performance.\n(b) Empirically, we demonstrate that mutual distillation between policies\ncontributes to such robustness, enabling the spontaneous emergence of invariant\nrepresentations over pixel inputs. Overall, our findings challenge the\nconventional view of distillation as merely a means of knowledge transfer,\noffering a novel perspective on the generalization in deep reinforcement\nlearning.\n","authors":["Zhengpeng Xie","Jiahang Cao","Qiang Zhang","Jianxiong Zhang","Changwei Wang","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2501.02481v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10223v1","updated":"2025-05-15T12:32:02Z","published":"2025-05-15T12:32:02Z","title":"Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution\n  Generalisation in MRI Segmentation","summary":"  Medical image segmentation models are often trained on curated datasets,\nleading to performance degradation when deployed in real-world clinical\nsettings due to mismatches between training and test distributions. While data\naugmentation techniques are widely used to address these challenges,\ntraditional visually consistent augmentation strategies lack the robustness\nneeded for diverse real-world scenarios. In this work, we systematically\nevaluate alternative augmentation strategies, focusing on MixUp and Auxiliary\nFourier Augmentation. These methods mitigate the effects of multiple variations\nwithout explicitly targeting specific sources of distribution shifts. We\ndemonstrate how these techniques significantly improve out-of-distribution\ngeneralization and robustness to imaging variations across a wide range of\ntransformations in cardiac cine MRI and prostate MRI segmentation. We\nquantitatively find that these augmentation methods enhance learned feature\nrepresentations by promoting separability and compactness. Additionally, we\nhighlight how their integration into nnU-Net training pipelines provides an\neasy-to-implement, effective solution for enhancing the reliability of medical\nsegmentation models in real-world applications.\n","authors":["Puru Vaish","Felix Meister","Tobias Heimann","Christoph Brune","Jelmer M. Wolterink"],"pdf_url":"https://arxiv.org/pdf/2505.10223v1.pdf","comment":"Accepted at MIDL 2025"},{"id":"http://arxiv.org/abs/2505.10222v1","updated":"2025-05-15T12:30:33Z","published":"2025-05-15T12:30:33Z","title":"ComplexFormer: Disruptively Advancing Transformer Inference Ability via\n  Head-Specific Complex Vector Attention","summary":"  Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism.\n","authors":["Jintian Shao","Hongyi Huang","Jiayi Wu","Beiwen Zhang","ZhiYu Wu","You Shan","MingKai Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.10222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08829v2","updated":"2025-05-15T12:19:18Z","published":"2025-05-13T01:00:25Z","title":"Aggregating Concepts of Accuracy and Fairness in Prediction Algorithms","summary":"  An algorithm that outputs predictions about the state of the world will\nalmost always be designed with the implicit or explicit goal of outputting\naccurate predictions (i.e., predictions that are likely to be true). In\naddition, the rise of increasingly powerful predictive algorithms brought about\nby the recent revolution in artificial intelligence has led to an emphasis on\nbuilding predictive algorithms that are fair, in the sense that their\npredictions do not systematically evince bias or bring about harm to certain\nindividuals or groups. This state of affairs presents two conceptual\nchallenges. First, the goals of accuracy and fairness can sometimes be in\ntension, and there are no obvious normative guidelines for managing the\ntrade-offs between these two desiderata when they arise. Second, there are many\ndistinct ways of measuring both the accuracy and fairness of a predictive\nalgorithm; here too, there are no obvious guidelines on how to aggregate our\npreferences for predictive algorithms that satisfy disparate measures of\nfairness and accuracy to various extents. The goal of this paper is to address\nthese challenges by arguing that there are good reasons for using a linear\ncombination of accuracy and fairness metrics to measure the\nall-things-considered value of a predictive algorithm for agents who care about\nboth accuracy and fairness. My argument depends crucially on a classic result\nin the preference aggregation literature due to Harsanyi. After making this\nformal argument, I apply my result to an analysis of accuracy-fairness\ntrade-offs using the COMPAS dataset compiled by Angwin et al.\n","authors":["David Kinney"],"pdf_url":"https://arxiv.org/pdf/2505.08829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10213v1","updated":"2025-05-15T12:17:52Z","published":"2025-05-15T12:17:52Z","title":"Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM\n  Performance on Time Series Forecasting","summary":"  With the widespread adoption of Large Language Models (LLMs), there is a\ngrowing need to establish best practices for leveraging their capabilities\nbeyond traditional natural language tasks. In this paper, a novel cross-domain\nknowledge transfer framework is proposed to enhance the performance of LLMs in\ntime series forecasting -- a task of increasing relevance in fields such as\nenergy systems, finance, and healthcare. The approach systematically infuses\nLLMs with structured temporal information to improve their forecasting\naccuracy. This study evaluates the proposed method on a real-world time series\ndataset and compares it to a naive baseline where the LLM receives no auxiliary\ninformation. Results show that knowledge-informed forecasting significantly\noutperforms the uninformed baseline in terms of predictive accuracy and\ngeneralization. These findings highlight the potential of knowledge transfer\nstrategies to bridge the gap between LLMs and domain-specific forecasting\ntasks.\n","authors":["Mohammadmahdi Ghasemloo","Alireza Moradi"],"pdf_url":"https://arxiv.org/pdf/2505.10213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11758v2","updated":"2025-05-15T12:13:37Z","published":"2024-10-15T16:28:09Z","title":"Latent Action Pretraining from Videos","summary":"  We introduce Latent Action Pretraining for general Action models (LAPA), an\nunsupervised method for pretraining Vision-Language-Action (VLA) models without\nground-truth robot action labels. Existing Vision-Language-Action models\nrequire action labels typically collected by human teleoperators during\npretraining, which significantly limits possible data sources and scale. In\nthis work, we propose a method to learn from internet-scale videos that do not\nhave robot action labels. We first train an action quantization model\nleveraging VQ-VAE-based objective to learn discrete latent actions between\nimage frames, then pretrain a latent VLA model to predict these latent actions\nfrom observations and task descriptions, and finally finetune the VLA on\nsmall-scale robot manipulation data to map from latent to robot actions.\nExperimental results demonstrate that our method significantly outperforms\nexisting techniques that train robot manipulation policies from large-scale\nvideos. Furthermore, it outperforms the state-of-the-art VLA model trained with\nrobotic action labels on real-world manipulation tasks that require language\nconditioning, generalization to unseen objects, and semantic generalization to\nunseen instructions. Training only on human manipulation videos also shows\npositive transfer, opening up the potential for leveraging web-scale data for\nrobotics foundation model.\n","authors":["Seonghyeon Ye","Joel Jang","Byeongguk Jeon","Sejune Joo","Jianwei Yang","Baolin Peng","Ajay Mandlekar","Reuben Tan","Yu-Wei Chao","Bill Yuchen Lin","Lars Liden","Kimin Lee","Jianfeng Gao","Luke Zettlemoyer","Dieter Fox","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2410.11758v2.pdf","comment":"ICLR 2025 Website: https://latentactionpretraining.github.io"},{"id":"http://arxiv.org/abs/2503.10325v2","updated":"2025-05-15T12:02:56Z","published":"2025-03-13T13:03:38Z","title":"Collaborative Speculative Inference for Efficient LLM Inference Serving","summary":"  Speculative inference is a promising paradigm employing small speculative\nmodels (SSMs) as drafters to generate draft tokens, which are subsequently\nverified in parallel by the target large language model (LLM). This approach\nenhances the efficiency of inference serving by reducing LLM inference latency\nand costs while preserving generation quality. However, existing speculative\nmethods face critical challenges, including inefficient resource utilization\nand limited draft acceptance, which constrain their scalability and overall\neffectiveness. To overcome these obstacles, we present CoSine, a novel\nspeculative inference system that decouples sequential speculative decoding\nfrom parallel verification, enabling efficient collaboration among multiple\nnodes. Specifically, CoSine routes inference requests to specialized drafters\nbased on their expertise and incorporates a confidence-based token fusion\nmechanism to synthesize outputs from cooperating drafters, ensuring\nhigh-quality draft generation. Additionally, CoSine dynamically orchestrates\nthe execution of speculative decoding and verification in a pipelined manner,\nemploying batch scheduling to selectively group requests and adaptive\nspeculation control to minimize idle periods. By optimizing parallel workflows\nthrough heterogeneous node collaboration, CoSine balances draft generation and\nverification throughput in real-time, thereby maximizing resource utilization.\nExperimental results demonstrate that CoSine achieves superior performance\ncompared to state-of-the-art speculative approaches. Notably, with equivalent\nresource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5%\nincrease in throughput compared to baseline methods.\n","authors":["Luyao Gao","Jianchun Liu","Hongli Xu","Xichong Zhang","Yunming Liao","Liusheng Huang"],"pdf_url":"https://arxiv.org/pdf/2503.10325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10198v1","updated":"2025-05-15T11:55:16Z","published":"2025-05-15T11:55:16Z","title":"A multi-head deep fusion model for recognition of cattle foraging events\n  using sound and movement signals","summary":"  Monitoring feeding behaviour is a relevant task for efficient herd management\nand the effective use of available resources in grazing cattle. The ability to\nautomatically recognise animals' feeding activities through the identification\nof specific jaw movements allows for the improvement of diet formulation, as\nwell as early detection of metabolic problems and symptoms of animal\ndiscomfort, among other benefits. The use of sensors to obtain signals for such\nmonitoring has become popular in the last two decades. The most frequently\nemployed sensors include accelerometers, microphones, and cameras, each with\nits own set of advantages and drawbacks. An unexplored aspect is the\nsimultaneous use of multiple sensors with the aim of combining signals in order\nto enhance the precision of the estimations. In this direction, this work\nintroduces a deep neural network based on the fusion of acoustic and inertial\nsignals, composed of convolutional, recurrent, and dense layers. The main\nadvantage of this model is the combination of signals through the automatic\nextraction of features independently from each of them. The model has emerged\nfrom an exploration and comparison of different neural network architectures\nproposed in this work, which carry out information fusion at different levels.\nFeature-level fusion has outperformed data and decision-level fusion by at\nleast a 0.14 based on the F1-score metric. Moreover, a comparison with\nstate-of-the-art machine learning methods is presented, including traditional\nand deep learning approaches. The proposed model yielded an F1-score value of\n0.802, representing a 14% increase compared to previous methods. Finally,\nresults from an ablation study and post-training quantization evaluation are\nalso reported.\n","authors":["Mariano Ferrero","JosÃ© Omar Chelotti","Luciano SebastiÃ¡n Martinez-Rau","Leandro Vignolo","MartÃ­n Pires","Julio Ricardo Galli","Leonardo Luis Giovanini","Hugo Leonardo Rufiner"],"pdf_url":"https://arxiv.org/pdf/2505.10198v1.pdf","comment":"Preprint submitted to Engineering Applications of Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2505.10192v1","updated":"2025-05-15T11:50:02Z","published":"2025-05-15T11:50:02Z","title":"Defect Detection in Photolithographic Patterns Using Deep Learning\n  Models Trained on Synthetic Data","summary":"  In the photolithographic process vital to semiconductor manufacturing,\nvarious types of defects appear during EUV pattering. Due to ever-shrinking\npattern size, these defects are extremely small and cause false or missed\ndetection during inspection. Specifically, the lack of defect-annotated quality\ndata with good representation of smaller defects has prohibited deployment of\ndeep learning based defect detection models in fabrication lines. To resolve\nthe problem of data unavailability, we artificially generate scanning electron\nmicroscopy (SEM) images of line patterns with known distribution of defects and\nautonomously annotate them. We then employ state-of-the-art object detection\nmodels to investigate defect detection performance as a function of defect\nsize, much smaller than the pitch width. We find that the real-time object\ndetector YOLOv8 has the best mean average precision of 96% as compared to\nEfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We\nreport the smallest defect size that can be detected reliably. When tested on\nreal SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and\n78.3% of Break defects across all relevant instances. These promising results\nsuggest that synthetic data can be used as an alternative to real-world data in\norder to develop robust machine-learning models.\n","authors":["Prashant P. Shinde","Priyadarshini P. Pai","Shashishekar P. Adiga","K. Subramanya Mayya","Yongbeom Seo","Myungsoo Hwang","Heeyoung Go","Changmin Park"],"pdf_url":"https://arxiv.org/pdf/2505.10192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10191v1","updated":"2025-05-15T11:47:54Z","published":"2025-05-15T11:47:54Z","title":"LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean\n  Forecasting","summary":"  Mesoscale eddies dominate the spatiotemporal multiscale variability of the\nocean, and their impact on the energy cascade of the global ocean cannot be\nignored. Eddy-resolving ocean forecasting is providing more reliable protection\nfor fisheries and navigational safety, but also presents significant scientific\nchallenges and high computational costs for traditional numerical models.\nArtificial intelligence (AI)-based weather and ocean forecasting systems are\nbecoming powerful tools that balance forecast performance with computational\nefficiency. However, the complex multiscale features in the ocean dynamical\nsystem make AI models still face many challenges in mesoscale eddy forecasting\n(especially regional modelling). Here, we develop LanTu, a regional\neddy-resolving ocean forecasting system based on dynamics-enhanced deep\nlearning. We incorporate cross-scale interactions into LanTu and construct\nmultiscale physical constraint for optimising LanTu guided by knowledge of eddy\ndynamics in order to improve the forecasting skill of LanTu for mesoscale\nevolution. The results show that LanTu outperforms the existing advanced\noperational numerical ocean forecasting system (NOFS) and AI-based ocean\nforecasting system (AI-OFS) in temperature, salinity, sea level anomaly and\ncurrent prediction, with a lead time of more than 10 days. Our study highlights\nthat dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for\neddy-resolving ocean forecasting.\n","authors":["Qingyu Zheng","Qi Shao","Guijun Han","Wei Li","Hong Li","Xuan Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10191v1.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.06945v2","updated":"2025-05-15T11:38:48Z","published":"2025-05-11T11:23:51Z","title":"A systematic review of challenges and proposed solutions in modeling\n  multimodal data","summary":"  Multimodal data modeling has emerged as a powerful approach in clinical\nresearch, enabling the integration of diverse data types such as imaging,\ngenomics, wearable sensors, and electronic health records. Despite its\npotential to improve diagnostic accuracy and support personalized care,\nmodeling such heterogeneous data presents significant technical challenges.\nThis systematic review synthesizes findings from 69 studies to identify common\nobstacles, including missing modalities, limited sample sizes, dimensionality\nimbalance, interpretability issues, and finding the optimal fusion techniques.\nWe highlight recent methodological advances, such as transfer learning,\ngenerative models, attention mechanisms, and neural architecture search that\noffer promising solutions. By mapping current trends and innovations, this\nreview provides a comprehensive overview of the field and offers practical\ninsights to guide future research and development in multimodal modeling for\nmedical applications.\n","authors":["Maryam Farhadizadeh","Maria Weymann","Michael BlaÃ","Johann Kraus","Christopher Gundler","Sebastian Walter","Noah Hempen","Harald Binder","Nadine Binder"],"pdf_url":"https://arxiv.org/pdf/2505.06945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.10016v3","updated":"2025-05-15T11:37:19Z","published":"2022-05-20T08:16:30Z","title":"Learning Progress Driven Multi-Agent Curriculum","summary":"  The number of agents can be an effective curriculum variable for controlling\nthe difficulty of multi-agent reinforcement learning (MARL) tasks. Existing\nwork typically uses manually defined curricula such as linear schemes. We\nidentify two potential flaws while applying existing reward-based automatic\ncurriculum learning methods in MARL: (1) The expected episode return used to\nmeasure task difficulty has high variance; (2) Credit assignment difficulty can\nbe exacerbated in tasks where increasing the number of agents yields higher\nreturns which is common in many MARL tasks. To address these issues, we propose\nto control the curriculum by using a TD-error based *learning progress* measure\nand by letting the curriculum proceed from an initial context distribution to\nthe final task specific one. Since our approach maintains a distribution over\nthe number of agents and measures learning progress rather than absolute\nperformance, which often increases with the number of agents, we alleviate\nproblem (2). Moreover, the learning progress measure naturally alleviates\nproblem (1) by aggregating returns. In three challenging sparse-reward MARL\nbenchmarks, our approach outperforms state-of-the-art baselines.\n","authors":["Wenshuai Zhao","Zhiyuan Li","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2205.10016v3.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2505.10182v1","updated":"2025-05-15T11:29:01Z","published":"2025-05-15T11:29:01Z","title":"Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with\n  Synthetic Data for LLM Reasoning","summary":"  Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty.\n","authors":["Yoichi Ishibashi","Taro Yano","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2505.10182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10172v1","updated":"2025-05-15T11:04:39Z","published":"2025-05-15T11:04:39Z","title":"Does Scaling Law Apply in Time Series Forecasting?","summary":"  Rapid expansion of model size has emerged as a key challenge in time series\nforecasting. From early Transformer with tens of megabytes to recent\narchitectures like TimesNet with thousands of megabytes, performance gains have\noften come at the cost of exponentially increasing parameter counts. But is\nthis scaling truly necessary? To question the applicability of the scaling law\nin time series forecasting, we propose Alinear, an ultra-lightweight\nforecasting model that achieves competitive performance using only k-level\nparameters. We introduce a horizon-aware adaptive decomposition mechanism that\ndynamically rebalances component emphasis across different forecast lengths,\nalongside a progressive frequency attenuation strategy that achieves stable\nprediction in various forecasting horizons without incurring the computational\noverhead of attention mechanisms. Extensive experiments on seven benchmark\ndatasets demonstrate that Alinear consistently outperforms large-scale models\nwhile using less than 1% of their parameters, maintaining strong accuracy\nacross both short and ultra-long forecasting horizons. Moreover, to more fairly\nevaluate model efficiency, we propose a new parameter-aware evaluation metric\nthat highlights the superiority of ALinear under constrained model budgets. Our\nanalysis reveals that the relative importance of trend and seasonal components\nvaries depending on data characteristics rather than following a fixed pattern,\nvalidating the necessity of our adaptive design. This work challenges the\nprevailing belief that larger models are inherently better and suggests a\nparadigm shift toward more efficient time series modeling.\n","authors":["Zeyan Li","Libing Chen","Yin Tang"],"pdf_url":"https://arxiv.org/pdf/2505.10172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10169v1","updated":"2025-05-15T10:55:47Z","published":"2025-05-15T10:55:47Z","title":"Modeling Saliency Dataset Bias","summary":"  Recent advances in image-based saliency prediction are approaching gold\nstandard performance levels on existing benchmarks. Despite this success, we\nshow that predicting fixations across multiple saliency datasets remains\nchallenging due to dataset bias. We find a significant performance drop (around\n40%) when models trained on one dataset are applied to another. Surprisingly,\nincreasing dataset diversity does not resolve this inter-dataset gap, with\nclose to 60% attributed to dataset-specific biases. To address this remaining\ngeneralization gap, we propose a novel architecture extending a mostly\ndataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific\nparameters that govern interpretable mechanisms such as multi-scale structure,\ncenter bias, and fixation spread. Adapting only these parameters to new data\naccounts for more than 75% of the generalization gap, with a large fraction of\nthe improvement achieved with as few as 50 samples. Our model sets a new\nstate-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark\n(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from\nunrelated datasets, but with a substantial boost when adapting to the\nrespective training datasets. The model also provides valuable insights into\nspatial saliency properties, revealing complex multi-scale effects that combine\nboth absolute and relative sizes.\n","authors":["Matthias KÃ¼mmerer","Harneet Khanuja","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2505.10169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10167v1","updated":"2025-05-15T10:51:34Z","published":"2025-05-15T10:51:34Z","title":"QuXAI: Explainers for Hybrid Quantum Machine Learning Models","summary":"  The emergence of hybrid quantum-classical machine learning (HQML) models\nopens new horizons of computational intelligence but their fundamental\ncomplexity frequently leads to black box behavior that undermines transparency\nand reliability in their application. Although XAI for quantum systems still in\nits infancy, a major research gap is evident in robust global and local\nexplainability approaches that are designed for HQML architectures that employ\nquantized feature encoding followed by classical learning. The gap is the focus\nof this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an\nexplainer for explaining feature importance in these hybrid systems. Our model\nentails the creation of HQML models incorporating quantum feature maps, the use\nof Q-MEDLEY, which combines feature based inferences, preserving the quantum\ntransformation stage and visualizing the resulting attributions. Our result\nshows that Q-MEDLEY delineates influential classical aspects in HQML models, as\nwell as separates their noise, and competes well against established XAI\ntechniques in classical validation settings. Ablation studies more\nsignificantly expose the virtues of the composite structure used in Q-MEDLEY.\nThe implications of this work are critically important, as it provides a route\nto improve the interpretability and reliability of HQML models, thus promoting\ngreater confidence and being able to engage in safer and more responsible use\nof quantum-enhanced AI technology.\n","authors":["Saikat Barua","Mostafizur Rahman","Shehenaz Khaled","Md Jafor Sadek","Rafiul Islam","Shahnewaz Siddique"],"pdf_url":"https://arxiv.org/pdf/2505.10167v1.pdf","comment":"16 pages, 6 figures, 7 equations"},{"id":"http://arxiv.org/abs/2501.13018v2","updated":"2025-05-15T10:49:09Z","published":"2025-01-22T17:05:38Z","title":"Multi-Objective Hyperparameter Selection via Hypothesis Testing on\n  Reliability Graphs","summary":"  The selection of hyperparameters, such as prompt templates in large language\nmodels (LLMs), must often strike a balance between reliability and cost. In\nmany cases, structural relationships between the expected reliability levels of\nthe hyperparameters can be inferred from prior information and held-out data --\ne.g., longer prompt templates may be more detailed and thus more reliable.\nHowever, existing hyperparameter selection methods either do not provide formal\nreliability guarantees or are unable to incorporate structured knowledge in the\nhyperparameter space. This paper introduces reliability graph-based Pareto\ntesting (RG-PT), a novel multi-objective hyperparameter selection framework\nthat maintains formal reliability guarantees in terms of false discovery rate\n(FDR), while accounting for known relationships among hyperparameters via a\ndirected acyclic graph. Edges in the graph reflect expected reliability and\ncost trade-offs among hyperparameters, which are inferred via the Bradley-Terry\n(BT) ranking model from prior information and held-out data. Experimental\nevaluations demonstrate that RG-PT significantly outperforms existing methods\nsuch as learn-then-test (LTT) and Pareto testing (PT) through a more efficient\nexploration of the hyperparameter space.\n","authors":["Amirmohammad Farzaneh","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2501.13018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05957v2","updated":"2025-05-15T10:43:45Z","published":"2025-05-09T11:09:52Z","title":"Efficient Quantum Convolutional Neural Networks for Image\n  Classification: Overcoming Hardware Constraints","summary":"  While classical convolutional neural networks (CNNs) have revolutionized\nimage classification, the emergence of quantum computing presents new\nopportunities for enhancing neural network architectures. Quantum CNNs (QCNNs)\nleverage quantum mechanical properties and hold potential to outperform\nclassical approaches. However, their implementation on current noisy\nintermediate-scale quantum (NISQ) devices remains challenging due to hardware\nlimitations. In our research, we address this challenge by introducing an\nencoding scheme that significantly reduces the input dimensionality. We\ndemonstrate that a primitive QCNN architecture with 49 qubits is sufficient to\ndirectly process $28\\times 28$ pixel MNIST images, eliminating the need for\nclassical dimensionality reduction pre-processing. Additionally, we propose an\nautomated framework based on expressibility, entanglement, and complexity\ncharacteristics to identify the building blocks of QCNNs, parameterized quantum\ncircuits (PQCs). Our approach demonstrates advantages in accuracy and\nconvergence speed with a similar parameter count compared to both hybrid QCNNs\nand classical CNNs. We validated our experiments on IBM's Heron r2 quantum\nprocessor, achieving $96.08\\%$ classification accuracy, surpassing the\n$71.74\\%$ benchmark of traditional approaches under identical training\nconditions. These results represent one of the first implementations of image\nclassifications on real quantum hardware and validate the potential of quantum\ncomputing in this area.\n","authors":["Peter RÃ¶seler","Oliver Schaudt","Helmut Berg","Christian Bauckhage","Matthias Koch"],"pdf_url":"https://arxiv.org/pdf/2505.05957v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10160v1","updated":"2025-05-15T10:41:16Z","published":"2025-05-15T10:41:16Z","title":"One-Stage Top-$k$ Learning-to-Defer: Score-Based Surrogates with\n  Theoretical Guarantees","summary":"  We introduce the first one-stage Top-$k$ Learning-to-Defer framework, which\nunifies prediction and deferral by learning a shared score-based model that\nselects the $k$ most cost-effective entities-labels or experts-per input. While\nexisting one-stage L2D methods are limited to deferring to a single expert, our\napproach jointly optimizes prediction and deferral across multiple entities\nthrough a single end-to-end objective. We define a cost-sensitive loss and\nderive a novel convex surrogate that is independent of the cardinality\nparameter $k$, enabling generalization across Top-$k$ regimes without\nretraining. Our formulation recovers the Top-1 deferral policy of prior\nscore-based methods as a special case, and we prove that our surrogate is both\nBayes-consistent and $\\mathcal{H}$-consistent under mild assumptions. We\nfurther introduce an adaptive variant, Top-$k(x)$, which dynamically selects\nthe number of consulted entities per input to balance predictive accuracy and\nconsultation cost. Experiments on CIFAR-10 and SVHN confirm that our one-stage\nTop-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves\nsuperior accuracy-cost trade-offs by tailoring allocations to input complexity.\n","authors":["Yannis Montreuil","Axel Carlier","Lai Xing Ng","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2505.10160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06111v2","updated":"2025-05-15T10:31:45Z","published":"2025-05-09T15:11:13Z","title":"UniVLA: Learning to Act Anywhere with Task-centric Latent Actions","summary":"  A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.\n","authors":["Qingwen Bu","Yanting Yang","Jisong Cai","Shenyuan Gao","Guanghui Ren","Maoqing Yao","Ping Luo","Hongyang Li"],"pdf_url":"https://arxiv.org/pdf/2505.06111v2.pdf","comment":"Accepted to RSS 2025. Code is available at\n  https://github.com/OpenDriveLab/UniVLA"},{"id":"http://arxiv.org/abs/2504.12988v3","updated":"2025-05-15T10:25:18Z","published":"2025-04-17T14:50:40Z","title":"Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to the\n  Top-$k$ Experts","summary":"  Although existing Learning-to-Defer (L2D) frameworks support multiple\nexperts, they allocate each query to a single expert, limiting their ability to\nleverage collective expertise in complex decision-making scenarios. To address\nthis, we introduce the first framework for Top-$k$ Learning-to-Defer, enabling\nsystems to defer each query to the $k$ most cost-effective experts. Our\nformulation strictly generalizes classical two-stage L2D by supporting\nmulti-expert deferral-a capability absent in prior work. We further propose\nTop-$k(x)$ Learning-to-Defer, an adaptive extension that learns the optimal\nnumber of experts per query based on input complexity, expert quality, and\nconsultation cost. We introduce a novel surrogate loss that is\nBayes-consistent, $(\\mathcal{R}, \\mathcal{G})$-consistent, and independent of\nthe cardinality parameter $k$, enabling efficient reuse across different values\nof $k$. We show that classical model cascades arise as a special case of our\nmethod, situating our framework as a strict generalization of both selective\ndeferral and cascaded inference. Experiments on classification and regression\ndemonstrate that Top-$k$ and Top-$k(x)$ yield improved accuracy--cost\ntrade-offs, establishing a new direction for multi-expert deferral in\nLearning-to-Defer.\n","authors":["Yannis Montreuil","Axel Carlier","Lai Xing Ng","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2504.12988v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10147v1","updated":"2025-05-15T10:20:26Z","published":"2025-05-15T10:20:26Z","title":"Near Optimal Best Arm Identification for Clustered Bandits","summary":"  This work investigates the problem of best arm identification for multi-agent\nmulti-armed bandits. We consider $N$ agents grouped into $M$ clusters, where\neach cluster solves a stochastic bandit problem. The mapping between agents and\nbandits is a priori unknown. Each bandit is associated with $K$ arms, and the\ngoal is to identify the best arm for each agent under a $\\delta$-probably\ncorrect ($\\delta$-PC) framework, while minimizing sample complexity and\ncommunication overhead.\n  We propose two novel algorithms: Clustering then Best Arm Identification\n(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a\ntwo-phase approach that first clusters agents based on the bandit problems they\nare learning, followed by identifying the best arm for each cluster. BAI-Cl\nreverses the sequence by identifying the best arms first and then clustering\nagents accordingly. Both algorithms leverage the successive elimination\nframework to ensure computational efficiency and high accuracy.\n  We establish $\\delta$-PC guarantees for both methods, derive bounds on their\nsample complexity, and provide a lower bound for this problem class. Moreover,\nwhen $M$ is small (a constant), we show that the sample complexity of a variant\nof BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic\nand real-world datasets (MovieLens, Yelp) demonstrate the superior performance\nof the proposed algorithms in terms of sample and communication efficiency,\nparticularly in settings where $M \\ll N$.\n","authors":[" Yash","Nikhil Karamchandani","Avishek Ghosh"],"pdf_url":"https://arxiv.org/pdf/2505.10147v1.pdf","comment":"To be published in ICML 2025"},{"id":"http://arxiv.org/abs/2405.15523v2","updated":"2025-05-15T10:18:58Z","published":"2024-05-24T13:05:05Z","title":"The Mosaic Memory of Large Language Models","summary":"  As Large Language Models (LLMs) become widely adopted, understanding how they\nlearn from, and memorize, training data becomes crucial. Memorization in LLMs\nis widely assumed to only occur as a result of sequences being repeated in the\ntraining data. Instead, we show that LLMs memorize by assembling information\nfrom similar sequences, a phenomena we call mosaic memory. We show major LLMs\nto exhibit mosaic memory, with fuzzy duplicates contributing to memorization as\nmuch as 0.8 of an exact duplicate and even heavily modified sequences\ncontributing substantially to memorization. Despite models display reasoning\ncapabilities, we somewhat surprisingly show memorization to be predominantly\nsyntactic rather than semantic. We finally show fuzzy duplicates to be\nubiquitous in real-world data, untouched by deduplication techniques. Taken\ntogether, our results challenge widely held beliefs and show memorization to be\na more complex, mosaic process, with real-world implications for privacy,\nconfidentiality, model utility and evaluation.\n","authors":["Igor Shilov","Matthieu Meeus","Yves-Alexandre de Montjoye"],"pdf_url":"https://arxiv.org/pdf/2405.15523v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10139v1","updated":"2025-05-15T10:13:45Z","published":"2025-05-15T10:13:45Z","title":"Path Gradients after Flow Matching","summary":"  Boltzmann Generators have emerged as a promising machine learning tool for\ngenerating samples from equilibrium distributions of molecular systems using\nNormalizing Flows and importance weighting. Recently, Flow Matching has helped\nspeed up Continuous Normalizing Flows (CNFs), scale them to more complex\nmolecular systems, and minimize the length of the flow integration\ntrajectories. We investigate the benefits of using path gradients to fine-tune\nCNFs initially trained by Flow Matching, in the setting where a target energy\nis known. Our experiments show that this hybrid approach yields up to a\nthreefold increase in sampling efficiency for molecular systems, all while\nusing the same model, a similar computational budget and without the need for\nadditional sampling. Furthermore, by measuring the length of the flow\ntrajectories during fine-tuning, we show that path gradients largely preserve\nthe learned structure of the flow.\n","authors":["Lorenz Vaitl","Leon Klein"],"pdf_url":"https://arxiv.org/pdf/2505.10139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10134v1","updated":"2025-05-15T10:04:44Z","published":"2025-05-15T10:04:44Z","title":"Large Wireless Localization Model (LWLM): A Foundation Model for\n  Positioning in 6G Networks","summary":"  Accurate and robust localization is a critical enabler for emerging 5G and 6G\napplications, including autonomous driving, extended reality (XR), and smart\nmanufacturing. While data-driven approaches have shown promise, most existing\nmodels require large amounts of labeled data and struggle to generalize across\ndeployment scenarios and wireless configurations. To address these limitations,\nwe propose a foundation-model-based solution tailored for wireless\nlocalization. We first analyze how different self-supervised learning (SSL)\ntasks acquire general-purpose and task-specific semantic features based on\ninformation bottleneck (IB) theory. Building on this foundation, we design a\npretraining methodology for the proposed Large Wireless Localization Model\n(LWLM). Specifically, we propose an SSL framework that jointly optimizes three\ncomplementary objectives: (i) spatial-frequency masked channel modeling\n(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)\nposition-invariant contrastive learning (PICL). These objectives jointly\ncapture the underlying semantics of wireless channel from multiple\nperspectives. We further design lightweight decoders for key downstream tasks,\nincluding time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,\nsingle base station (BS) localization, and multiple BS localization.\nComprehensive experimental results confirm that LWLM consistently surpasses\nboth model-based and supervised learning baselines across all localization\ntasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer\nmodels without pretraining, and exhibits strong generalization under\nlabel-limited fine-tuning and unseen BS configurations, confirming its\npotential as a foundation model for wireless localization.\n","authors":["Guangjin Pan","Kaixuan Huang","Hui Chen","Shunqing Zhang","Christian HÃ¤ger","Henk Wymeersch"],"pdf_url":"https://arxiv.org/pdf/2505.10134v1.pdf","comment":"13 pages,16 figures.This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2505.10128v1","updated":"2025-05-15T09:53:14Z","published":"2025-05-15T09:53:14Z","title":"Robust Federated Learning on Edge Devices with Domain Heterogeneity","summary":"  Federated Learning (FL) allows collaborative training while ensuring data\nprivacy across distributed edge devices, making it a popular solution for\nprivacy-sensitive applications. However, FL faces significant challenges due to\nstatistical heterogeneity, particularly domain heterogeneity, which impedes the\nglobal mode's convergence. In this study, we introduce a new framework to\naddress this challenge by improving the generalization ability of the FL global\nmodel under domain heterogeneity, using prototype augmentation. Specifically,\nwe introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a\nprototype-based FL framework designed to enhance feature diversity and model\nrobustness. FedAPC leverages prototypes derived from the mean features of\naugmented data to capture richer representations. By aligning local features\nwith global prototypes, we enable the model to learn meaningful semantic\nfeatures while reducing overfitting to any specific domain. Experimental\nresults on the Office-10 and Digits datasets illustrate that our framework\noutperforms SOTA baselines, demonstrating superior performance.\n","authors":["Huy Q. Le","Latif U. Khan","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2505.10128v1.pdf","comment":"IWCMC 2025"},{"id":"http://arxiv.org/abs/2505.10125v1","updated":"2025-05-15T09:51:47Z","published":"2025-05-15T09:51:47Z","title":"Enhancing the Performance of Global Model by Improving the Adaptability\n  of Local Models in Federated Learning","summary":"  Federated learning enables the clients to collaboratively train a global\nmodel, which is aggregated from local models. Due to the heterogeneous data\ndistributions over clients and data privacy in federated learning, it is\ndifficult to train local models to achieve a well-performed global model. In\nthis paper, we introduce the adaptability of local models, i.e., the average\nperformance of local models on data distributions over clients, and enhance the\nperformance of the global model by improving the adaptability of local models.\nSince each client does not know the data distributions over other clients, the\nadaptability of the local model cannot be directly optimized. First, we provide\nthe property of an appropriate local model which has good adaptability on the\ndata distributions over clients. Then, we formalize the property into the local\ntraining objective with a constraint and propose a feasible solution to train\nthe local model. Extensive experiments on federated learning benchmarks\ndemonstrate that our method significantly improves the adaptability of local\nmodels and achieves a well-performed global model that consistently outperforms\nthe baseline methods.\n","authors":["Wujun Zhou","Shu Ding","ZeLin Li","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10120v1","updated":"2025-05-15T09:46:27Z","published":"2025-05-15T09:46:27Z","title":"All You Need Is Synthetic Task Augmentation","summary":"  Injecting rule-based models like Random Forests into differentiable neural\nnetwork frameworks remains an open challenge in machine learning. Recent\nadvancements have demonstrated that pretrained models can generate efficient\nmolecular embeddings. However, these approaches often require extensive\npretraining and additional techniques, such as incorporating posterior\nprobabilities, to boost performance. In our study, we propose a novel strategy\nthat jointly trains a single Graph Transformer neural network on both sparse\nmultitask molecular property experimental targets and synthetic targets derived\nfrom XGBoost models trained on Osmordred molecular descriptors. These synthetic\ntasks serve as independent auxiliary tasks. Our results show consistent and\nsignificant performance improvement across all 19 molecular property prediction\ntasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms\nthe XGBoost single-task learner. This demonstrates that synthetic task\naugmentation is an effective method for enhancing neural model performance in\nmultitask molecular property prediction without the need for feature injection\nor pretraining.\n","authors":["Guillaume Godin"],"pdf_url":"https://arxiv.org/pdf/2505.10120v1.pdf","comment":"14 pages, 3 Figures, 6 tables"},{"id":"http://arxiv.org/abs/2505.10117v1","updated":"2025-05-15T09:42:11Z","published":"2025-05-15T09:42:11Z","title":"Learning Virtual Machine Scheduling in Cloud Computing through Language\n  Agents","summary":"  In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments.\n","authors":["JieHao Wu","Ziwei Wang","Junjie Sheng","Wenhao Li","Xiangfei Wang","Jun Luo"],"pdf_url":"https://arxiv.org/pdf/2505.10117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05922v2","updated":"2025-05-15T09:31:11Z","published":"2025-05-09T09:54:07Z","title":"Cape: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy","summary":"  Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works.\n","authors":["Haoqi Wu","Wei Dai","Li Wang","Qiang Yan"],"pdf_url":"https://arxiv.org/pdf/2505.05922v2.pdf","comment":"to be published in ICML 2025"},{"id":"http://arxiv.org/abs/2502.00753v2","updated":"2025-05-15T09:09:06Z","published":"2025-02-02T11:23:10Z","title":"Mirror Descent Under Generalized Smoothness","summary":"  Smoothness is crucial for attaining fast rates in first-order optimization.\nHowever, many optimization problems in modern machine learning involve\nnon-smooth objectives. Recent studies relax the smoothness assumption by\nallowing the Lipschitz constant of the gradient to grow with respect to the\ngradient norm, which accommodates a broad range of objectives in practice.\nDespite this progress, existing generalizations of smoothness are restricted to\nEuclidean geometry with $\\ell_2$-norm and only have theoretical guarantees for\noptimization in the Euclidean space. In this paper, we address this limitation\nby introducing a new $\\ell*$-smoothness concept that measures the norm of\nHessians in terms of a general norm and its dual, and establish convergence for\nmirror-descent-type algorithms, matching the rates under the classic\nsmoothness. Notably, we propose a generalized self-bounding property that\nfacilitates bounding the gradients via controlling suboptimality gaps, serving\nas a principal component for convergence analysis. Beyond deterministic\noptimization, we establish an anytime convergence for stochastic mirror descent\nbased on a new bounded noise condition that encompasses the widely adopted\nbounded or affine noise assumptions.\n","authors":["Dingzhi Yu","Wei Jiang","Yuanyu Wan","Lijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.00753v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10099v1","updated":"2025-05-15T09:01:07Z","published":"2025-05-15T09:01:07Z","title":"A Scalable Gradient-Based Optimization Framework for Sparse\n  Minimum-Variance Portfolio Selection","summary":"  Portfolio optimization involves selecting asset weights to minimize a\nrisk-reward objective, such as the portfolio variance in the classical\nminimum-variance framework. Sparse portfolio selection extends this by imposing\na cardinality constraint: only $k$ assets from a universe of $p$ may be\nincluded. The standard approach models this problem as a mixed-integer\nquadratic program and relies on commercial solvers to find the optimal\nsolution. However, the computational costs of such methods increase\nexponentially with $k$ and $p$, making them too slow for problems of even\nmoderate size. We propose a fast and scalable gradient-based approach that\ntransforms the combinatorial sparse selection problem into a constrained\ncontinuous optimization task via Boolean relaxation, while preserving\nequivalence with the original problem on the set of binary points. Our\nalgorithm employs a tunable parameter that transmutes the auxiliary objective\nfrom a convex to a concave function. This allows a stable convex starting\npoint, followed by a controlled path toward a sparse binary solution as the\ntuning parameter increases and the objective moves toward concavity. In\npractice, our method matches commercial solvers in asset selection for most\ninstances and, in rare instances, the solution differs by a few assets whilst\nshowing a negligible error in portfolio variance.\n","authors":["Sarat Moka","Matias Quiroz","Vali Asimit","Samuel Muller"],"pdf_url":"https://arxiv.org/pdf/2505.10099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06533v2","updated":"2025-05-15T08:42:50Z","published":"2025-04-09T02:16:46Z","title":"Flexible Graph Similarity Computation With A Proactive Optimization\n  Strategy","summary":"  Graph Edit Distance (GED) offers a principled and flexible measure of graph\nsimilarity, as it quantifies the minimum cost needed to transform one graph\ninto another with customizable edit operation costs. Despite recent\nlearning-based efforts to approximate GED via vector space representations,\nexisting methods struggle with adapting to varying operation costs.\nFurthermore, they suffer from inefficient, reactive mapping refinements due to\nreliance on isolated node-level distance as guidance. To address these issues,\nwe propose GEN, a novel learning-based approach for flexible GED approximation.\nGEN addresses the varying costs adaptation by integrating operation costs prior\nto match establishment, enabling mappings to dynamically adapt to cost\nvariations. Furthermore, GEN introduces a proactive guidance optimization\nstrategy that captures graph-level dependencies between matches, allowing\ninformed matching decisions in a single step without costly iterative\nrefinements. Extensive evaluations on real-world and synthetic datasets\ndemonstrate that GEN achieves up to 37.8% reduction in GED approximation error\nand 72.7% reduction in inference time compared with state-of-the-art methods,\nwhile consistently maintaining robustness under diverse cost settings and graph\nsizes.\n","authors":["Zhouyang Liu","Ning Liu","Yixin Chen","Jiezhong He","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2504.06533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10083v1","updated":"2025-05-15T08:37:23Z","published":"2025-05-15T08:37:23Z","title":"ChronoSteer: Bridging Large Language Model and Time Series Foundation\n  Model via Synthetic Data","summary":"  Conventional forecasting methods rely on unimodal time series data, limiting\ntheir ability to exploit rich textual information. Recently, large language\nmodels (LLMs) and time series foundation models (TSFMs) have demonstrated\npowerful capability in textual reasoning and temporal modeling, respectively.\nIntegrating the strengths of both to construct a multimodal model that\nconcurrently leverages both temporal and textual information for future\ninference has emerged as a critical research challenge. To address the scarcity\nof event-series paired data, we propose a decoupled framework: an LLM is\nemployed to transform textual events into revision instructions, which are then\nused to steer the output of TSFM. To implement this framework, we introduce\nChronoSteer, a multimodal TSFM that can be steered through textual revision\ninstructions, effectively bridging LLM and TSFM. Moreover, to mitigate the\nshortage of cross-modal instruction-series paired data, we devise a two-stage\ntraining strategy based on synthetic data. In addition, we also construct a\nhigh-quality multimodal time series forecasting benchmark to address the\ninformation leakage concerns during evaluation. After integrating with an LLM,\nChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%\nimprovement in prediction accuracy compared to the unimodal backbone and a\n22.5% gain over the previous state-of-the-art multimodal method.\n","authors":["Chengsen Wang","Qi Qi","Zhongwen Rao","Lujia Pan","Jingyu Wang","Jianxin Liao"],"pdf_url":"https://arxiv.org/pdf/2505.10083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10080v1","updated":"2025-05-15T08:35:10Z","published":"2025-05-15T08:35:10Z","title":"Role of scrambling and noise in temporal information processing with\n  quantum systems","summary":"  Scrambling quantum systems have been demonstrated as effective substrates for\ntemporal information processing. While their role in providing rich feature\nmaps has been widely studied, a theoretical understanding of their performance\nin temporal tasks is still lacking. Here we consider a general quantum\nreservoir processing framework that captures a broad range of physical\ncomputing models with quantum systems. We examine the scalability and memory\nretention of the model with scrambling reservoirs modelled by high-order\nunitary designs in both noiseless and noisy settings. In the former regime, we\nshow that measurement readouts become exponentially concentrated with\nincreasing reservoir size, yet strikingly do not worsen with the reservoir\niterations. Thus, while repeatedly reusing a small scrambling reservoir with\nquantum data might be viable, scaling up the problem size deteriorates\ngeneralization unless one can afford an exponential shot overhead. In contrast,\nthe memory of early inputs and initial states decays exponentially in both\nreservoir size and reservoir iterations. In the noisy regime, we also prove\nexponential memory decays with iterations for local noisy channels. Proving\nthese results required us to introduce new proof techniques for bounding\nconcentration in temporal quantum learning models.\n","authors":["Weijie Xiong","ZoÃ« Holmes","Armando Angrisani","Yudai Suzuki","Thiparat Chotibut","Supanut Thanasilp"],"pdf_url":"https://arxiv.org/pdf/2505.10080v1.pdf","comment":"14+35 pages, 6+5 figures, 1 table"},{"id":"http://arxiv.org/abs/2411.16782v2","updated":"2025-05-15T08:18:43Z","published":"2024-11-25T08:14:37Z","title":"Scaling Laws for Black box Adversarial Attacks","summary":"  Adversarial examples usually exhibit good cross-model transferability,\nenabling attacks on black-box models with limited information about their\narchitectures and parameters, which are highly threatening in commercial\nblack-box scenarios. Model ensembling is an effective strategy to improve the\ntransferability of adversarial examples by attacking multiple surrogate models.\nHowever, since prior studies usually adopt few models in the ensemble, there\nremains an open question of whether scaling the number of models can further\nimprove black-box attacks. Inspired by the scaling law of large foundation\nmodels, we investigate the scaling laws of black-box adversarial attacks in\nthis work. Through theoretical analysis and empirical evaluations, we conclude\nwith clear scaling laws that using more surrogate models enhances adversarial\ntransferability. Comprehensive experiments verify the claims on standard image\nclassifiers, diverse defended models and multimodal large language models using\nvarious adversarial attack methods. Specifically, by scaling law, we achieve\n90%+ transfer attack success rate on even proprietary models like GPT-4o.\nFurther visualization indicates that there is also a scaling law on the\ninterpretability and semantics of adversarial perturbations.\n","authors":["Chuan Liu","Huanran Chen","Yichi Zhang","Yinpeng Dong","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.16782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16917v2","updated":"2025-05-15T08:12:52Z","published":"2025-03-21T07:27:10Z","title":"Malliavin Calculus for Score-based Diffusion Models","summary":"  We introduce a new framework based on Malliavin calculus to derive exact\nanalytical expressions for the score function $\\nabla \\log p_t(x)$, i.e., the\ngradient of the log-density associated with the solution to stochastic\ndifferential equations (SDEs). Our approach combines classical\nintegration-by-parts techniques with modern stochastic analysis tools, such as\nBismut's formula and Malliavin calculus, and it works for both linear and\nnonlinear SDEs. In doing so, we establish a rigorous connection between the\nMalliavin derivative, its adjoint, the Malliavin divergence (Skorokhod\nintegral), and diffusion generative models, thereby providing a systematic\nmethod for computing $\\nabla \\log p_t(x)$. In the linear case, we present a\ndetailed analysis showing that our formula coincides with the analytical score\nfunction derived from the solution of the Fokker--Planck equation. For\nnonlinear SDEs with state-independent diffusion coefficients, we derive a\nclosed-form expression for $\\nabla \\log p_t(x)$. We evaluate the proposed\nframework across multiple generative tasks and find that its performance is\ncomparable to state-of-the-art methods. These results can be generalised to\nbroader classes of SDEs, paving the way for new score-based diffusion\ngenerative models.\n","authors":["Ehsan Mirafzali","Utkarsh Gupta","Patrick Wyrod","Frank Proske","Daniele Venturi","Razvan Marinescu"],"pdf_url":"https://arxiv.org/pdf/2503.16917v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10066v1","updated":"2025-05-15T08:07:04Z","published":"2025-05-15T08:07:04Z","title":"Dark LLMs: The Growing Threat of Unaligned AI Models","summary":"  Large Language Models (LLMs) rapidly reshape modern life, advancing fields\nfrom healthcare to education and beyond. However, alongside their remarkable\ncapabilities lies a significant threat: the susceptibility of these models to\njailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems\nfrom the very data they learn from. As long as this training data includes\nunfiltered, problematic, or 'dark' content, the models can inherently learn\nundesirable patterns or weaknesses that allow users to circumvent their\nintended safety controls. Our research identifies the growing threat posed by\ndark LLMs models deliberately designed without ethical guardrails or modified\nthrough jailbreak techniques. In our research, we uncovered a universal\njailbreak attack that effectively compromises multiple state-of-the-art models,\nenabling them to answer almost any question and produce harmful outputs upon\nrequest. The main idea of our attack was published online over seven months\nago. However, many of the tested LLMs were still vulnerable to this attack.\nDespite our responsible disclosure efforts, responses from major LLM providers\nwere often inadequate, highlighting a concerning gap in industry practices\nregarding AI safety. As model training becomes more accessible and cheaper, and\nas open-source LLMs proliferate, the risk of widespread misuse escalates.\nWithout decisive intervention, LLMs may continue democratizing access to\ndangerous knowledge, posing greater risks than anticipated.\n","authors":["Michael Fire","Yitzhak Elbazis","Adi Wasenstein","Lior Rokach"],"pdf_url":"https://arxiv.org/pdf/2505.10066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10057v1","updated":"2025-05-15T08:00:48Z","published":"2025-05-15T08:00:48Z","title":"JointDistill: Adaptive Multi-Task Distillation for Joint Depth\n  Estimation and Scene Segmentation","summary":"  Depth estimation and scene segmentation are two important tasks in\nintelligent transportation systems. A joint modeling of these two tasks will\nreduce the requirement for both the storage and training efforts. This work\nexplores how the multi-task distillation could be used to improve such unified\nmodeling. While existing solutions transfer multiple teachers' knowledge in a\nstatic way, we propose a self-adaptive distillation method that can dynamically\nadjust the knowledge amount from each teacher according to the student's\ncurrent learning ability. Furthermore, as multiple teachers exist, the\nstudent's gradient update direction in the distillation is more prone to be\nerroneous where knowledge forgetting may occur. To avoid this, we propose a\nknowledge trajectory to record the most essential information that a model has\nlearnt in the past, based on which a trajectory-based distillation loss is\ndesigned to guide the student to follow the learning curve similarly in a\ncost-effective way. We evaluate our method on multiple benchmarking datasets\nincluding Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,\nour method achieves a clearly improvement. The code is provided in the\nsupplementary materials.\n","authors":["Tiancong Cheng","Ying Zhang","Yuxuan Liang","Roger Zimmermann","Zhiwen Yu","Bin Guo"],"pdf_url":"https://arxiv.org/pdf/2505.10057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10050v1","updated":"2025-05-15T07:53:02Z","published":"2025-05-15T07:53:02Z","title":"Financial Fraud Detection Using Explainable AI and Stacking Ensemble\n  Methods","summary":"  Traditional machine learning models often prioritize predictive accuracy,\noften at the expense of model transparency and interpretability. The lack of\ntransparency makes it difficult for organizations to comply with regulatory\nrequirements and gain stakeholders trust. In this research, we propose a fraud\ndetection framework that combines a stacking ensemble of well-known gradient\nboosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable\nartificial intelligence (XAI) techniques are used to enhance the transparency\nand interpretability of the model's decisions. We used SHAP (SHapley Additive\nExplanations) for feature selection to identify the most important features.\nFurther efforts were made to explain the model's predictions using Local\nInterpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots\n(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection\ndataset, which includes more than 590,000 real transaction records, was used to\nevaluate the proposed model. The model achieved a high performance with an\naccuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent\nrelated approaches. These results indicate that combining high prediction\naccuracy with transparent interpretability is possible and could lead to a more\nethical and trustworthy solution in financial fraud detection.\n","authors":["Fahad Almalki","Mehedi Masud"],"pdf_url":"https://arxiv.org/pdf/2505.10050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08353v2","updated":"2025-05-15T07:51:50Z","published":"2025-04-11T08:39:18Z","title":"Single View Garment Reconstruction Using Diffusion Mapping Via Pattern\n  Coordinates","summary":"  Reconstructing 3D clothed humans from images is fundamental to applications\nlike virtual try-on, avatar creation, and mixed reality. While recent advances\nhave enhanced human body recovery, accurate reconstruction of garment geometry\n-- especially for loose-fitting clothing -- remains an open challenge. We\npresent a novel method for high-fidelity 3D garment reconstruction from single\nimages that bridges 2D and 3D representations. Our approach combines Implicit\nSewing Patterns (ISP) with a generative diffusion model to learn rich garment\nshape priors in a 2D UV space. A key innovation is our mapping model that\nestablishes correspondences between 2D image pixels, UV pattern coordinates,\nand 3D geometry, enabling joint optimization of both 3D garment meshes and the\ncorresponding 2D patterns by aligning learned priors with image observations.\nDespite training exclusively on synthetically simulated cloth data, our method\ngeneralizes effectively to real-world images, outperforming existing approaches\non both tight- and loose-fitting garments. The reconstructed garments maintain\nphysical plausibility while capturing fine geometric details, enabling\ndownstream applications including garment retargeting and texture manipulation.\n","authors":["Ren Li","Cong Cao","Corentin Dumery","Yingxuan You","Hao Li","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2504.08353v2.pdf","comment":"SIGGRAPH 2025"},{"id":"http://arxiv.org/abs/2412.03483v2","updated":"2025-05-15T07:47:48Z","published":"2024-12-04T17:20:01Z","title":"Convolutional Neural Networks and Mixture of Experts for Intrusion\n  Detection in 5G Networks and beyond","summary":"  The advent of 6G/NextG networks comes along with a series of benefits,\nincluding extreme capacity, reliability, and efficiency. However, these\nnetworks may become vulnerable to new security threats. Therefore, 6G/NextG\nnetworks must be equipped with advanced Artificial Intelligence algorithms, in\norder to evade these attacks. Existing studies on the intrusion detection task\nrely on the train of shallow machine learning classifiers, including Logistic\nRegression, Decision Trees, and so on, yielding suboptimal performance. Others\nare based on deep neural networks consisting of static components, which are\nnot conditional on the input. This limits their representation power and\nefficiency. To resolve these issues, we present the first study integrating\nMixture of Experts (MoE) for identifying malicious traffic. Specifically, we\nuse network traffic data and convert the 1D array of features into a 2D matrix.\nNext, we pass this matrix through convolutional neural network (CNN) layers\nfollowed by batch normalization and max pooling layers. After obtaining the\nrepresentation vector via the CNN layers, a sparsely gated MoE layer is used.\nThis layer consists of a set of experts (dense layers) and a router, where the\nrouter assigns weights to the output of each expert. Sparsity is achieved by\nchoosing the most relevant experts of the total ones. Finally, we perform a\nseries of ablation experiments to prove the effectiveness of our proposed\nmodel. Experiments are conducted on the 5G-NIDD dataset, a network intrusion\ndetection dataset generated from a real 5G test network. Results show that our\nintroduced approach reaches weighted F1-score up to 99.95% achieving comparable\nperformance to existing approaches. Findings also show that our proposed model\nachieves multiple advantages over state-of-the-art approaches.\n","authors":["Loukas Ilias","George Doukas","Vangelis Lamprou","Christos Ntanos","Dimitris Askounis"],"pdf_url":"https://arxiv.org/pdf/2412.03483v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10040v1","updated":"2025-05-15T07:35:27Z","published":"2025-05-15T07:35:27Z","title":"Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph\n  Learning","summary":"  Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their\ncapacity to preserve previously acquired knowledge amid the assimilation of\nnovel information. Rehearsal-based techniques revisit historical examples,\nadopted as a principal strategy to alleviate this phenomenon. However, memory\nexplosion and privacy infringements impose significant constraints on their\nutility. Non-Exemplar methods circumvent the prior issues through Prototype\nReplay (PR), yet feature drift presents new challenges. In this paper, our\nempirical findings reveal that Prototype Contrastive Learning (PCL) exhibits\nless pronounced drift than conventional PR. Drawing upon PCL, we propose\nInstance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar\nContinual Graph Learning (NECGL). Exploiting graph structural information, we\nformulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature\ndistributions towards high-impact nodes to augment the model's capacity for\nassimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)\nsafeguards task memory by regularizing discontinuities in class relationships.\nMoreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,\nfostering greater inter-class discriminability. Evaluations on four node\nclassification benchmark datasets demonstrate that our method outperforms\nexisting state-of-the-art methods, achieving a better trade-off between\nplasticity and stability.\n","authors":["Lei Song","Jiaxing Li","Shihan Guan","Youyong Kong"],"pdf_url":"https://arxiv.org/pdf/2505.10040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10039v1","updated":"2025-05-15T07:35:14Z","published":"2025-05-15T07:35:14Z","title":"Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER\n  Gates","summary":"  Circuit discovery has gradually become one of the prominent methods for\nmechanistic interpretability, and research on circuit completeness has also\ngarnered increasing attention. Methods of circuit discovery that do not\nguarantee completeness not only result in circuits that are not fixed across\ndifferent runs but also cause key mechanisms to be omitted. The nature of\nincompleteness arises from the presence of OR gates within the circuit, which\nare often only partially detected in standard circuit discovery methods. To\nthis end, we systematically introduce three types of logic gates: AND, OR, and\nADDER gates, and decompose the circuit into combinations of these logical\ngates. Through the concept of these gates, we derive the minimum requirements\nnecessary to achieve faithfulness and completeness. Furthermore, we propose a\nframework that combines noising-based and denoising-based interventions, which\ncan be easily integrated into existing circuit discovery methods without\nsignificantly increasing computational complexity. This framework is capable of\nfully identifying the logic gates and distinguishing them within the circuit.\nIn addition to the extensive experimental validation of the framework's ability\nto restore the faithfulness, completeness, and sparsity of circuits, using this\nframework, we uncover fundamental properties of the three logic gates, such as\ntheir proportions and contributions to the output, and explore how they behave\namong the functionalities of language models.\n","authors":["Hang Chen","Jiaying Zhu","Xinyu Yang","Wenya Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10039v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2505.10037v1","updated":"2025-05-15T07:33:41Z","published":"2025-05-15T07:33:41Z","title":"Optimal normalization in quantum-classical hybrid models for anti-cancer\n  drug response prediction","summary":"  Quantum-classical Hybrid Machine Learning (QHML) models are recognized for\ntheir robust performance and high generalization ability even for relatively\nsmall datasets. These qualities offer unique advantages for anti-cancer drug\nresponse prediction, where the number of available samples is typically small.\nHowever, such hybrid models appear to be very sensitive to the data encoding\nused at the interface of a neural network and a quantum circuit, with\nsuboptimal choices leading to stability issues. To address this problem, we\npropose a novel strategy that uses a normalization function based on a\nmoderated gradient version of the $\\tanh$. This method transforms the outputs\nof the neural networks without concentrating them at the extreme value ranges.\nOur idea was evaluated on a dataset of gene expression and drug response\nmeasurements for various cancer cell lines, where we compared the prediction\nperformance of a classical deep learning model and several QHML models. These\nresults confirmed that QHML performed better than the classical models when\ndata was optimally normalized. This study opens up new possibilities for\nbiomedical data analysis using quantum computers.\n","authors":["Takafumi Ito","Lysenko Artem","Tatsuhiko Tsunoda"],"pdf_url":"https://arxiv.org/pdf/2505.10037v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.10033v1","updated":"2025-05-15T07:29:16Z","published":"2025-05-15T07:29:16Z","title":"Evaluating Robustness of Deep Reinforcement Learning for Autonomous\n  Surface Vehicle Control in Field Tests","summary":"  Despite significant advancements in Deep Reinforcement Learning (DRL) for\nAutonomous Surface Vehicles (ASVs), their robustness in real-world conditions,\nparticularly under external disturbances, remains insufficiently explored. In\nthis paper, we evaluate the resilience of a DRL-based agent designed to capture\nfloating waste under various perturbations. We train the agent using domain\nrandomization and evaluate its performance in real-world field tests, assessing\nits ability to handle unexpected disturbances such as asymmetric drag and an\noff-center payload. We assess the agent's performance under these perturbations\nin both simulation and real-world experiments, quantifying performance\ndegradation and benchmarking it against an MPC baseline. Results indicate that\nthe DRL agent performs reliably despite significant disturbances. Along with\nthe open-source release of our implementation, we provide insights into\neffective training strategies, real-world challenges, and practical\nconsiderations for deploying DRLbased ASV controllers.\n","authors":["Luis F. W. Batista","StÃ©phanie Aravecchia","Seth Hutchinson","CÃ©dric Pradalier"],"pdf_url":"https://arxiv.org/pdf/2505.10033v1.pdf","comment":"Workshop on Field Robotics at ICRA 2025"},{"id":"http://arxiv.org/abs/2505.10030v1","updated":"2025-05-15T07:25:43Z","published":"2025-05-15T07:25:43Z","title":"DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection\n  of Diseases in Cocos nucifera","summary":"  Coconut tree diseases are a serious risk to agricultural yield, particularly\nin developing countries where conventional farming practices restrict early\ndiagnosis and intervention. Current disease identification methods are manual,\nlabor-intensive, and non-scalable. In response to these limitations, we come up\nwith DeepSeqCoco, a deep learning based model for accurate and automatic\ndisease identification from coconut tree images. The model was tested under\nvarious optimizer settings, such as SGD, Adam, and hybrid configurations, to\nidentify the optimal balance between accuracy, minimization of loss, and\ncomputational cost. Results from experiments indicate that DeepSeqCoco can\nachieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than\nexisting models) with the hybrid SGD-Adam showing the lowest validation loss of\n2.81%. It also shows a drop of up to 18% in training time and up to 85% in\nprediction time for input images. The results point out the promise of the\nmodel to improve precision agriculture through an AI-based, scalable, and\nefficient disease monitoring system.\n","authors":["Miit Daga","Dhriti Parikh","Swarna Priya Ramu"],"pdf_url":"https://arxiv.org/pdf/2505.10030v1.pdf","comment":"This paper is accepted for publication in IEEE Access journal and is\n  currently pending revisions before publication"},{"id":"http://arxiv.org/abs/2505.09427v2","updated":"2025-05-15T07:22:20Z","published":"2025-05-14T14:28:24Z","title":"SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation","summary":"  Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer.\n","authors":["Achref Doula","Max MÃ¼hlhÃ¤user","Alejandro Sanchez Guinea"],"pdf_url":"https://arxiv.org/pdf/2505.09427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05145v2","updated":"2025-05-15T07:19:33Z","published":"2025-05-08T11:32:46Z","title":"Understanding In-context Learning of Addition via Activation Subspaces","summary":"  To perform in-context learning, language models must extract signals from\nindividual few-shot examples, aggregate these into a learned prediction rule,\nand then apply this rule to new examples. How is this implemented in the\nforward pass of modern transformer models? To study this, we consider a\nstructured family of few-shot learning tasks for which the true prediction rule\nis to add an integer $k$ to the input. We find that Llama-3-8B attains high\naccuracy on this task for a range of $k$, and localize its few-shot ability to\njust three attention heads via a novel optimization approach. We further show\nthe extracted signals lie in a six-dimensional subspace, where four of the\ndimensions track the unit digit and the other two dimensions track overall\nmagnitude. We finally examine how these heads extract information from\nindividual few-shot examples, identifying a self-correction mechanism in which\nmistakes from earlier examples are suppressed by later examples. Our results\ndemonstrate how tracking low-dimensional subspaces across a forward pass can\nprovide insight into fine-grained computational structures.\n","authors":["Xinyan Hu","Kayo Yin","Michael I. Jordan","Jacob Steinhardt","Lijie Chen"],"pdf_url":"https://arxiv.org/pdf/2505.05145v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2505.10010v1","updated":"2025-05-15T06:45:37Z","published":"2025-05-15T06:45:37Z","title":"ImagineBench: Evaluating Reinforcement Learning with Large Language\n  Model Rollouts","summary":"  A central challenge in reinforcement learning (RL) is its dependence on\nextensive real-world interaction data to learn task-specific policies. While\nrecent work demonstrates that large language models (LLMs) can mitigate this\nlimitation by generating synthetic experience (noted as imaginary rollouts) for\nmastering novel tasks, progress in this emerging field is hindered due to the\nlack of a standard benchmark. To bridge this gap, we introduce ImagineBench,\nthe first comprehensive benchmark for evaluating offline RL algorithms that\nleverage both real rollouts and LLM-imaginary rollouts. The key features of\nImagineBench include: (1) datasets comprising environment-collected and\nLLM-imaginary rollouts; (2) diverse domains of environments covering\nlocomotion, robotic manipulation, and navigation tasks; and (3) natural\nlanguage task instructions with varying complexity levels to facilitate\nlanguage-conditioned policy learning. Through systematic evaluation of\nstate-of-the-art offline RL algorithms, we observe that simply applying\nexisting offline RL algorithms leads to suboptimal performance on unseen tasks,\nachieving 35.44% success rate in hard tasks in contrast to 64.37% of method\ntraining on real rollouts for hard tasks. This result highlights the need for\nalgorithm advancements to better leverage LLM-imaginary rollouts. Additionally,\nwe identify key opportunities for future research: including better utilization\nof imaginary rollouts, fast online adaptation and continual learning, and\nextension to multi-modal tasks. Our code is publicly available at\nhttps://github.com/LAMDA-RL/ImagineBench.\n","authors":["Jing-Cheng Pang","Kaiyuan Li","Yidi Wang","Si-Hang Yang","Shengyi Jiang","Yang Yu"],"pdf_url":"https://arxiv.org/pdf/2505.10010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10007v1","updated":"2025-05-15T06:42:25Z","published":"2025-05-15T06:42:25Z","title":"Sample Complexity of Distributionally Robust Average-Reward\n  Reinforcement Learning","summary":"  Motivated by practical applications where stable long-term performance is\ncritical-such as robotics, operations research, and healthcare-we study the\nproblem of distributionally robust (DR) average-reward reinforcement learning.\nWe propose two algorithms that achieve near-optimal sample complexity. The\nfirst reduces the problem to a DR discounted Markov decision process (MDP),\nwhile the second, Anchored DR Average-Reward MDP, introduces an anchoring state\nto stabilize the controlled transition kernels within the uncertainty set.\nAssuming the nominal MDP is uniformly ergodic, we prove that both algorithms\nattain a sample complexity of $\\widetilde{O}\\left(|\\mathbf{S}||\\mathbf{A}|\nt_{\\mathrm{mix}}^2\\varepsilon^{-2}\\right)$ for estimating the optimal policy as\nwell as the robust average reward under KL and $f_k$-divergence-based\nuncertainty sets, provided the uncertainty radius is sufficiently small. Here,\n$\\varepsilon$ is the target accuracy, $|\\mathbf{S}|$ and $|\\mathbf{A}|$ denote\nthe sizes of the state and action spaces, and $t_{\\mathrm{mix}}$ is the mixing\ntime of the nominal MDP. This represents the first finite-sample convergence\nguarantee for DR average-reward reinforcement learning. We further validate the\nconvergence rates of our algorithms through numerical experiments.\n","authors":["Zijun Chen","Shengbo Wang","Nian Si"],"pdf_url":"https://arxiv.org/pdf/2505.10007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10003v1","updated":"2025-05-15T06:32:59Z","published":"2025-05-15T06:32:59Z","title":"AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom\n  Domain Large Model","summary":"  Designing a 6G-oriented universal model capable of processing multi-modal\ndata and executing diverse air interface tasks has emerged as a common goal in\nfuture wireless systems. Building on our prior work in communication\nmulti-modal alignment and telecom large language model (LLM), we propose a\nscalable, task-aware artificial intelligence-air interface multi-modal\nuniversal model (AI2MMUM), which flexibility and effectively perform various\nphysical layer tasks according to subtle task instructions. The LLM backbone\nprovides robust contextual comprehension and generalization capabilities, while\na fine-tuning approach is adopted to incorporate domain-specific knowledge. To\nenhance task adaptability, task instructions consist of fixed task keywords and\nlearnable, implicit prefix prompts. Frozen radio modality encoders extract\nuniversal representations and adapter layers subsequently bridge radio and\nlanguage modalities. Moreover, lightweight task-specific heads are designed to\ndirectly output task objectives. Comprehensive evaluations demonstrate that\nAI2MMUM achieves SOTA performance across five representative physical\nenvironment/wireless channel-based downstream tasks using the WAIR-D and\nDeepMIMO datasets.\n","authors":["Tianyu Jiao","Zhuoran Xiao","Yihang Huang","Chenhui Ye","Yijia Feng","Liyu Cai","Jiang Chang","Fangkun Liu","Yin Xu","Dazhi He","Yunfeng Guan","Wenjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.10003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11641v3","updated":"2025-05-15T06:30:38Z","published":"2024-11-18T15:19:54Z","title":"TSINR: Capturing Temporal Continuity via Implicit Neural Representations\n  for Time Series Anomaly Detection","summary":"  Time series anomaly detection aims to identify unusual patterns in data or\ndeviations from systems' expected behavior. The reconstruction-based methods\nare the mainstream in this task, which learn point-wise representation via\nunsupervised learning. However, the unlabeled anomaly points in training data\nmay cause these reconstruction-based methods to learn and reconstruct anomalous\ndata, resulting in the challenge of capturing normal patterns. In this paper,\nwe propose a time series anomaly detection method based on implicit neural\nrepresentation (INR) reconstruction, named TSINR, to address this challenge.\nDue to the property of spectral bias, TSINR enables prioritizing low-frequency\nsignals and exhibiting poorer performance on high-frequency abnormal data.\nSpecifically, we adopt INR to parameterize time series data as a continuous\nfunction and employ a transformer-based architecture to predict the INR of\ngiven data. As a result, the proposed TSINR method achieves the advantage of\ncapturing the temporal continuity and thus is more sensitive to discontinuous\nanomaly data. In addition, we further design a novel form of INR continuous\nfunction to learn inter- and intra-channel information, and leverage a\npre-trained large language model to amplify the intense fluctuations in\nanomalies. Extensive experiments demonstrate that TSINR achieves superior\noverall performance on both univariate and multivariate time series anomaly\ndetection benchmarks compared to other state-of-the-art reconstruction-based\nmethods. Our codes are available.\n","authors":["Mengxuan Li","Ke Liu","Hongyang Chen","Jiajun Bu","Hongwei Wang","Haishuai Wang"],"pdf_url":"https://arxiv.org/pdf/2411.11641v3.pdf","comment":"Accepted by SIGKDD 2025"},{"id":"http://arxiv.org/abs/2504.09192v4","updated":"2025-05-15T06:21:11Z","published":"2025-04-12T12:17:20Z","title":"Towards More Efficient, Robust, Instance-adaptive, and Generalizable\n  Sequential Decision making","summary":"  The primary goal of my Ph.D. study is to develop provably efficient and\npractical algorithms for data-driven sequential decision-making under\nuncertainty. My work focuses on reinforcement learning (RL), multi-armed\nbandits, and their applications, including recommendation systems, computer\nnetworks, video analytics, and large language models (LLMs). Sequential\ndecision-making methods, such as bandits and RL, have demonstrated remarkable\nsuccess - ranging from outperforming human players in complex games like Atari\nand Go to advancing robotics, recommendation systems, and fine-tuning LLMs.\nDespite these successes, many established algorithms rely on idealized models\nthat can fail under model misspecifications or adversarial perturbations,\nparticularly in settings where accurate prior knowledge of the underlying model\nclass is unavailable or where malicious users operate within dynamic systems.\nThese challenges are pervasive in real-world applications, where robust and\nadaptive solutions are critical. Furthermore, while worst-case guarantees\nprovide theoretical reliability, they often fail to capture instance-dependent\nperformance, which can lead to more efficient and practical solutions. Another\nkey challenge lies in generalizing to new, unseen environments, a crucial\nrequirement for deploying these methods in dynamic and unpredictable settings.\nTo address these limitations, my research aims to develop more efficient,\nrobust, instance-adaptive, and generalizable sequential decision-making\nalgorithms for both reinforcement learning and bandits. Towards this end, I\nfocus on developing more efficient, robust, instance-adaptive, and\ngeneralizable for both general reinforcement learning (RL) and bandits.\n","authors":["Zhiyong Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09192v4.pdf","comment":"Ph.D. Thesis"},{"id":"http://arxiv.org/abs/2505.09983v1","updated":"2025-05-15T05:46:59Z","published":"2025-05-15T05:46:59Z","title":"Sybil-based Virtual Data Poisoning Attacks in Federated Learning","summary":"  Federated learning is vulnerable to poisoning attacks by malicious\nadversaries. Existing methods often involve high costs to achieve effective\nattacks. To address this challenge, we propose a sybil-based virtual data\npoisoning attack, where a malicious client generates sybil nodes to amplify the\npoisoning model's impact. To reduce neural network computational complexity, we\ndevelop a virtual data generation method based on gradient matching. We also\ndesign three schemes for target model acquisition, applicable to online local,\nonline global, and offline scenarios. In simulation, our method outperforms\nother attack algorithms since our method can obtain a global target model under\nnon-independent uniformly distributed data.\n","authors":["Changxun Zhu","Qilong Wu","Lingjuan Lyu","Shibei Xue"],"pdf_url":"https://arxiv.org/pdf/2505.09983v1.pdf","comment":"7 pages, 6 figures, accepted by IEEE Codit 2025"},{"id":"http://arxiv.org/abs/2502.11505v2","updated":"2025-05-15T05:27:59Z","published":"2025-02-17T07:12:39Z","title":"Graph Neural Network-based Spectral Filtering Mechanism for Imbalance\n  Classification in Network Digital Twin","summary":"  Graph neural networks are gaining attention in fifth-generation (5G) core\nnetwork digital twins, which are data-driven complex systems with numerous\ncomponents. Analyzing these data can be challenging due to rare failure types,\nleading to imbalanced classification in multiclass settings. Digital twins of\n5G networks increasingly employ graph classification as the main method for\nidentifying failure types. However, the skewed distribution of failure\noccurrences is a significant class-imbalance problem that prevents practical\ngraph data mining. Previous studies have not sufficiently addressed this\ncomplex problem. This paper, proposes class-Fourier GNN (CF-GNN) that\nintroduces a class-oriented spectral filtering mechanism to ensure precise\nclassification by estimating a unique spectral filter for each class. This work\nemploys eigenvalue and eigenvector spectral filtering to capture and adapt to\nvariations in minority classes, ensuring accurate class-specific feature\ndiscrimination, and adept at graph representation learning for complex local\nstructures among neighbors in an end-to-end setting. The extensive experiments\ndemonstrate that the proposed CF-GNN could help create new techniques for\nenhancing classifiers and investigate the characteristics of the multiclass\nimbalanced data in a network digital twin system.\n","authors":["Abubakar Isah","Ibrahim Aliyu","Sulaiman Muhammad Rashid","Jaehyung Park","Minsoo Hahn","Jinsul Kim"],"pdf_url":"https://arxiv.org/pdf/2502.11505v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.06595"},{"id":"http://arxiv.org/abs/2505.09972v1","updated":"2025-05-15T05:21:34Z","published":"2025-05-15T05:21:34Z","title":"Who Said What WSW 2.0? Enhanced Automated Analysis of Preschool\n  Classroom Speech","summary":"  This paper introduces an automated framework WSW2.0 for analyzing vocal\ninteractions in preschool classrooms, enhancing both accuracy and scalability\nthrough the integration of wav2vec2-based speaker classification and Whisper\n(large-v2 and large-v3) speech transcription. A total of 235 minutes of audio\nrecordings (160 minutes from 12 children and 75 minutes from 5 teachers), were\nused to compare system outputs to expert human annotations. WSW2.0 achieves a\nweighted F1 score of .845, accuracy of .846, and an error-corrected kappa of\n.672 for speaker classification (child vs. teacher). Transcription quality is\nmoderate to high with word error rates of .119 for teachers and .238 for\nchildren. WSW2.0 exhibits relatively high absolute agreement intraclass\ncorrelations (ICC) with expert transcriptions for a range of classroom language\nfeatures. These include teacher and child mean utterance length, lexical\ndiversity, question asking, and responses to questions and other utterances,\nwhich show absolute agreement intraclass correlations between .64 and .98. To\nestablish scalability, we apply the framework to an extensive dataset spanning\ntwo years and over 1,592 hours of classroom audio recordings, demonstrating the\nframework's robustness for broad real-world applications. These findings\nhighlight the potential of deep learning and natural language processing\ntechniques to revolutionize educational research by providing accurate measures\nof key features of preschool classroom speech, ultimately guiding more\neffective intervention strategies and supporting early childhood language\ndevelopment.\n","authors":["Anchen Sun","Tiantian Feng","Gabriela Gutierrez","Juan J Londono","Anfeng Xu","Batya Elbaum","Shrikanth Narayanan","Lynn K Perry","Daniel S Messinger"],"pdf_url":"https://arxiv.org/pdf/2505.09972v1.pdf","comment":"8 pages, 2 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.12665v3","updated":"2025-05-15T05:15:13Z","published":"2024-07-17T15:48:39Z","title":"Beyond Next Token Prediction: Patch-Level Training for Large Language\n  Models","summary":"  The prohibitive training costs of Large Language Models (LLMs) have emerged\nas a significant bottleneck in the development of next-generation LLMs. In this\npaper, we show that it is possible to significantly reduce the training costs\nof LLMs without sacrificing their performance. Specifically, we introduce\npatch-level training for LLMs, in which multiple tokens are aggregated into a\nunit of higher information density, referred to as a `patch', to serve as the\nfundamental text unit for training LLMs. During patch-level training, we feed\nthe language model shorter sequences of patches and train it to predict the\nnext patch, thereby processing the majority of the training data at a\nsignificantly reduced cost. Following this, the model continues token-level\ntraining on the remaining training data to align with the inference mode.\nExperiments on a diverse range of models (370M-2.7B parameters) demonstrate\nthat patch-level training can reduce the overall training costs to 0.5$\\times$,\nwithout compromising the model performance compared to token-level training.\nSource code: https://github.com/shaochenze/PatchTrain.\n","authors":["Chenze Shao","Fandong Meng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.12665v3.pdf","comment":"ICLR 2025 Spotlight"},{"id":"http://arxiv.org/abs/2505.09969v1","updated":"2025-05-15T05:13:38Z","published":"2025-05-15T05:13:38Z","title":"A Comprehensive Machine Learning Framework for Heart Disease Prediction:\n  Performance Evaluation and Future Perspectives","summary":"  This study presents a machine learning-based framework for heart disease\nprediction using the heart-disease dataset, comprising 303 samples with 14\nfeatures. The methodology involves data preprocessing, model training, and\nevaluation using three classifiers: Logistic Regression, K-Nearest Neighbors\n(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and\nRandomizedSearchCV was employed to enhance model performance. The Random Forest\nclassifier outperformed other models, achieving an accuracy of 91% and an\nF1-score of 0.89. Evaluation metrics, including precision, recall, and\nconfusion matrix, revealed balanced performance across classes. The proposed\nmodel demonstrates strong potential for aiding clinical decision-making by\neffectively predicting heart disease. Limitations such as dataset size and\ngeneralizability underscore the need for future studies using larger and more\ndiverse datasets. This work highlights the utility of machine learning in\nhealthcare, offering insights for further advancements in predictive\ndiagnostics.\n","authors":["Ali Azimi Lamir","Shiva Razzagzadeh","Zeynab Rezaei"],"pdf_url":"https://arxiv.org/pdf/2505.09969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01635v7","updated":"2025-05-15T05:02:59Z","published":"2024-06-30T10:53:40Z","title":"Commute Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) have shown remarkable success in learning from\ngraph-structured data. However, their application to directed graphs (digraphs)\npresents unique challenges, primarily due to the inherent asymmetry in node\nrelationships. Traditional GNNs are adept at capturing unidirectional relations\nbut fall short in encoding the mutual path dependencies between nodes, such as\nasymmetrical shortest paths typically found in digraphs. Recognizing this gap,\nwe introduce Commute Graph Neural Networks (CGNN), an approach that seamlessly\nintegrates node-wise commute time into the message passing scheme. The\ncornerstone of CGNN is an efficient method for computing commute time using a\nnewly formulated digraph Laplacian. Commute time is then integrated into the\nneighborhood aggregation process, with neighbor contributions weighted\naccording to their respective commute time to the central node in each layer.\nIt enables CGNN to directly capture the mutual, asymmetric relationships in\ndigraphs. Extensive experiments on 8 benchmarking datasets confirm the\nsuperiority of CGNN against 13 state-of-the-art methods.\n","authors":["Wei Zhuo","Han Yu","Guang Tan","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2407.01635v7.pdf","comment":"Published in International Conference on Machine Learning (ICML),\n  2025"},{"id":"http://arxiv.org/abs/2504.05904v2","updated":"2025-05-15T05:01:49Z","published":"2025-04-08T11:02:14Z","title":"Saliency-Motion Guided Trunk-Collateral Network for Unsupervised Video\n  Object Segmentation","summary":"  Recent mainstream unsupervised video object segmentation (UVOS)\nmotion-appearance approaches use either the bi-encoder structure to separately\nencode motion and appearance features, or the uni-encoder structure for joint\nencoding. However, these methods fail to properly balance the motion-appearance\nrelationship. Consequently, even with complex fusion modules for\nmotion-appearance integration, the extracted suboptimal features degrade the\nmodels' overall performance. Moreover, the quality of optical flow varies\nacross scenarios, making it insufficient to rely solely on optical flow to\nachieve high-quality segmentation results. To address these challenges, we\npropose the Saliency-Motion guided Trunk-Collateral Network (SMTC-Net), which\nbetter balances the motion-appearance relationship and incorporates model's\nintrinsic saliency information to enhance segmentation performance.\nSpecifically, considering that optical flow maps are derived from RGB images,\nthey share both commonalities and differences. Accordingly, we propose a novel\nTrunk-Collateral structure for motion-appearance UVOS. The shared trunk\nbackbone captures the motion-appearance commonality, while the collateral\nbranch learns the uniqueness of motion features. Furthermore, an Intrinsic\nSaliency guided Refinement Module (ISRM) is devised to efficiently leverage the\nmodel's intrinsic saliency information to refine high-level features, and\nprovide pixel-level guidance for motion-appearance fusion, thereby enhancing\nperformance without additional input. Experimental results show that SMTC-Net\nachieved state-of-the-art performance on three UVOS datasets ( 89.2% J&F on\nDAVIS-16, 76% J on YouTube-Objects, 86.4% J on FBMS ) and four standard video\nsalient object detection (VSOD) benchmarks with the notable increase,\ndemonstrating its effectiveness and superiority over previous methods.\n","authors":["Xiangyu Zheng","Wanyun Li","Songcheng He","Jianping Fan","Xiaoqiang Li","We Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.05904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09251v3","updated":"2025-05-15T04:46:39Z","published":"2022-11-16T22:50:40Z","title":"On the Power of Learning-Augmented Search Trees","summary":"  We study learning-augmented binary search trees (BSTs) via Treaps with\ncarefully designed priorities. The result is a simple search tree in which the\ndepth of each item $x$ is determined by its predicted weight $w_x$.\nSpecifically, each item $x$ is assigned a composite priority of\n$-\\lfloor\\log\\log(1/w_x)\\rfloor + U(0, 1)$ where $U(0, 1)$ is the uniform\nrandom variable. By choosing $w_x$ as the relative frequency of $x$, the\nresulting search trees achieve static optimality. This approach generalizes the\nrecent learning-augmented BSTs [Lin-Luo-Woodruff ICML '22], which only work for\nZipfian distributions, by extending them to arbitrary input distributions.\nFurthermore, we demonstrate that our method can be generalized to a B-Tree data\nstructure using the B-Treap approach [Golovin ICALP '09]. Our search trees are\nalso capable of leveraging localities in the access sequence through online\nself-reorganization, thereby achieving the working-set property. Additionally,\nthey are robust to prediction errors and support dynamic operations, such as\ninsertions, deletions, and prediction updates. We complement our analysis with\nan empirical study, demonstrating that our method outperforms prior work and\nclassic data structures.\n","authors":["Jingbang Chen","Xinyuan Cao","Alicia Stepin","Li Chen"],"pdf_url":"https://arxiv.org/pdf/2211.09251v3.pdf","comment":"Accepted by ICML25"}],"Multimedia":[{"id":"http://arxiv.org/abs/2504.19458v3","updated":"2025-05-15T09:07:58Z","published":"2025-04-28T03:48:23Z","title":"Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal\n  Perspective","summary":"  Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from\ndifferent Multi-Modal Knowledge Graphs (MMKGs), a critical information\nretrieval task. Existing studies have explored various fusion paradigms and\nconsistency constraints to improve the alignment of equivalent entities, while\noverlooking that the visual modality may not always contribute positively.\nEmpirically, entities with low-similarity images usually generate\nunsatisfactory performance, highlighting the limitation of overly relying on\nvisual features. We believe the model can be biased toward the visual modality,\nleading to a shortcut image-matching task. To address this, we propose a\ncounterfactual debiasing framework for MMEA, termed CDMEA, which investigates\nvisual modality bias from a causal perspective. Our approach aims to leverage\nboth visual and graph modalities to enhance MMEA while suppressing the direct\ncausal effect of the visual modality on model predictions. By estimating the\nTotal Effect (TE) of both modalities and excluding the Natural Direct Effect\n(NDE) of the visual modality, we ensure that the model predicts based on the\nTotal Indirect Effect (TIE), effectively utilizing both modalities and reducing\nvisual modality bias. Extensive experiments on 9 benchmark datasets show that\nCDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,\nhigh-noise, and low-resource data scenarios.\n","authors":["Taoyu Su","Jiawei Sheng","Duohe Ma","Xiaodong Li","Juwei Yue","Mengxiao Song","Yingkai Tang","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2504.19458v3.pdf","comment":"Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,"},{"id":"http://arxiv.org/abs/2505.10101v1","updated":"2025-05-15T09:04:12Z","published":"2025-05-15T09:04:12Z","title":"LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and\n  StyleGAN2","summary":"  This paper introduces LAV (Latent Audio-Visual), a system that integrates\nEnCodec's neural audio compression with StyleGAN2's generative capabilities to\nproduce visually dynamic outputs driven by pre-recorded audio. Unlike previous\nworks that rely on explicit feature mappings, LAV uses EnCodec embeddings as\nlatent representations, directly transformed into StyleGAN2's style latent\nspace via randomly initialized linear mapping. This approach preserves semantic\nrichness in the transformation, enabling nuanced and semantically coherent\naudio-visual translations. The framework demonstrates the potential of using\npretrained audio compression models for artistic and computational\napplications.\n","authors":["Jongmin Jung","Dasaem Jeong"],"pdf_url":"https://arxiv.org/pdf/2505.10101v1.pdf","comment":"Paper accepted at ISEA 2025, The 30th International Symposium on\n  Electronic/Emerging Art, Seoul, Republic of Korea, 23 - 29 May 2025"},{"id":"http://arxiv.org/abs/2505.09936v1","updated":"2025-05-15T03:45:10Z","published":"2025-05-15T03:45:10Z","title":"CartoAgent: a multimodal large language model-powered multi-agent\n  cartographic framework for map style transfer and evaluation","summary":"  The rapid development of generative artificial intelligence (GenAI) presents\nnew opportunities to advance the cartographic process. Previous studies have\neither overlooked the artistic aspects of maps or faced challenges in creating\nboth accurate and informative maps. In this study, we propose CartoAgent, a\nnovel multi-agent cartographic framework powered by multimodal large language\nmodels (MLLMs). This framework simulates three key stages in cartographic\npractice: preparation, map design, and evaluation. At each stage, different\nMLLMs act as agents with distinct roles to collaborate, discuss, and utilize\ntools for specific purposes. In particular, CartoAgent leverages MLLMs' visual\naesthetic capability and world knowledge to generate maps that are both\nvisually appealing and informative. By separating style from geographic data,\nit can focus on designing stylesheets without modifying the vector-based data,\nthereby ensuring geographic accuracy. We applied CartoAgent to a specific task\ncentered on map restyling-namely, map style transfer and evaluation. The\neffectiveness of this framework was validated through extensive experiments and\na human evaluation study. CartoAgent can be extended to support a variety of\ncartographic design decisions and inform future integrations of GenAI in\ncartography.\n","authors":["Chenglong Wang","Yuhao Kang","Zhaoya Gong","Pengjun Zhao","Yu Feng","Wenjia Zhang","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2505.09936v1.pdf","comment":"57 pages, 17 figures"},{"id":"http://arxiv.org/abs/2503.16112v2","updated":"2025-05-15T03:27:28Z","published":"2025-03-20T13:00:36Z","title":"PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming","summary":"  Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60% of severely distorted\nframes (compared to VQGAN).\n","authors":["Liming Liu","Jiangkai Wu","Haoyang Wang","Peiheng Wang","Zongming Guo","Xinggong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16112v2.pdf","comment":"6 pages (excluding references), 10 figures, to appear in APNET 2025"}]},"2025-05-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2505.09855v1","updated":"2025-05-14T23:31:17Z","published":"2025-05-14T23:31:17Z","title":"Predictability Shapes Adaptation: An Evolutionary Perspective on Modes\n  of Learning in Transformers","summary":"  Transformer models learn in two distinct modes: in-weights learning (IWL),\nencoding knowledge into model weights, and in-context learning (ICL), adapting\nflexibly to context without weight modification. To better understand the\ninterplay between these learning modes, we draw inspiration from evolutionary\nbiology's analogous adaptive strategies: genetic encoding (akin to IWL,\nadapting over generations and fixed within an individual's lifetime) and\nphenotypic plasticity (akin to ICL, enabling flexible behavioral responses to\nenvironmental cues). In evolutionary biology, environmental predictability\ndictates the balance between these strategies: stability favors genetic\nencoding, while reliable predictive cues promote phenotypic plasticity. We\nexperimentally operationalize these dimensions of predictability and\nsystematically investigate their influence on the ICL/IWL balance in\nTransformers. Using regression and classification tasks, we show that high\nenvironmental stability decisively favors IWL, as predicted, with a sharp\ntransition at maximal stability. Conversely, high cue reliability enhances ICL\nefficacy, particularly when stability is low. Furthermore, learning dynamics\nreveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift\noccurs in some settings (e.g., classification with many classes), we\ndemonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL\nacquisition (e.g., regression) can exhibit an initial IWL phase later yielding\nto ICL dominance. These findings support a relative-cost hypothesis for\nexplaining these learning mode transitions, establishing predictability as a\ncritical factor governing adaptive strategies in Transformers, and offering\nnovel insights for understanding ICL and guiding training methodologies.\n","authors":["Alexander Y. Ku","Thomas L. Griffiths","Stephanie C. Y. Chan"],"pdf_url":"https://arxiv.org/pdf/2505.09855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09852v1","updated":"2025-05-14T23:24:22Z","published":"2025-05-14T23:24:22Z","title":"Do Large Language Models Know Conflict? Investigating Parametric vs.\n  Non-Parametric Knowledge of LLMs for Conflict Forecasting","summary":"  Large Language Models (LLMs) have shown impressive performance across natural\nlanguage tasks, but their ability to forecast violent conflict remains\nunderexplored. We investigate whether LLMs possess meaningful parametric\nknowledge-encoded in their pretrained weights-to predict conflict escalation\nand fatalities without external data. This is critical for early warning\nsystems, humanitarian planning, and policy-making. We compare this parametric\nknowledge with non-parametric capabilities, where LLMs access structured and\nunstructured context from conflict datasets (e.g., ACLED, GDELT) and recent\nnews reports via Retrieval-Augmented Generation (RAG). Incorporating external\ninformation could enhance model performance by providing up-to-date context\notherwise missing from pretrained weights. Our two-part evaluation framework\nspans 2020-2024 across conflict-prone regions in the Horn of Africa and the\nMiddle East. In the parametric setting, LLMs predict conflict trends and\nfatalities relying only on pretrained knowledge. In the non-parametric setting,\nmodels receive summaries of recent conflict events, indicators, and\ngeopolitical developments. We compare predicted conflict trend labels (e.g.,\nEscalate, Stable Conflict, De-escalate, Peace) and fatalities against\nhistorical data. Our findings highlight the strengths and limitations of LLMs\nfor conflict forecasting and the benefits of augmenting them with structured\nexternal knowledge.\n","authors":["Apollinaire Poli Nemkova","Sarath Chandra Lingareddy","Sagnik Ray Choudhury","Mark V. Albert"],"pdf_url":"https://arxiv.org/pdf/2505.09852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09825v1","updated":"2025-05-14T22:04:46Z","published":"2025-05-14T22:04:46Z","title":"KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive\n  Reasoning","summary":"  Each year, tens of millions of essays are written and graded in college-level\nEnglish courses. Students are asked to analyze literary and cultural texts\nthrough a process known as close reading, in which they gather textual details\nto formulate evidence-based arguments. Despite being viewed as a basis for\ncritical thinking and widely adopted as a required element of university\ncoursework, close reading has never been evaluated on large language models\n(LLMs), and multi-discipline benchmarks like MMLU do not include literature as\na subject. To fill this gap, we present KRISTEVA, the first close reading\nbenchmark for evaluating interpretive reasoning, consisting of 1331\nmultiple-choice questions adapted from classroom data. With KRISTEVA, we\npropose three progressively more difficult sets of tasks to approximate\ndifferent elements of the close reading process, which we use to test how well\nLLMs may seem to understand and reason about literary works: 1) extracting\nstylistic features, 2) retrieving relevant contextual information from\nparametric knowledge, and 3) multi-hop reasoning between style and external\ncontexts. Our baseline results find that, while state-of-the-art LLMs possess\nsome college-level close reading competency (accuracy 49.7% - 69.7%), their\nperformances still trail those of experienced human evaluators on 10 out of our\n11 tasks.\n","authors":["Peiqi Sui","Juan Diego Rodriguez","Philippe Laban","Dean Murphy","Joseph P. Dexter","Richard Jean So","Samuel Baker","Pramit Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2505.09825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09807v1","updated":"2025-05-14T21:21:08Z","published":"2025-05-14T21:21:08Z","title":"Exploring the generalization of LLM truth directions on conversational\n  formats","summary":"  Several recent works argue that LLMs have a universal truth direction where\ntrue and false statements are linearly separable in the activation space of the\nmodel. It has been demonstrated that linear probes trained on a single hidden\nstate of the model already generalize across a range of topics and might even\nbe used for lie detection in LLM conversations. In this work we explore how\nthis truth direction generalizes between various conversational formats. We\nfind good generalization between short conversations that end on a lie, but\npoor generalization to longer formats where the lie appears earlier in the\ninput prompt. We propose a solution that significantly improves this type of\ngeneralization by adding a fixed key phrase at the end of each conversation.\nOur results highlight the challenges towards reliable LLM lie detectors that\ngeneralize to new settings.\n","authors":["Timour Ichmoukhamedov","David Martens"],"pdf_url":"https://arxiv.org/pdf/2505.09807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10823v2","updated":"2025-05-14T21:15:58Z","published":"2025-04-15T02:54:16Z","title":"CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from\n  Multiple Perspectives","summary":"  Navigating high-stakes dilemmas involving conflicting values is challenging\neven for humans, let alone for AI. Yet prior work in evaluating the reasoning\ncapabilities of large language models (LLMs) in such situations has been\nlimited to everyday scenarios. To close this gap, this work first introduces\nCLASH (Character perspective-based LLM Assessments in Situations with\nHigh-stakes), a meticulously curated dataset consisting of 345 high-impact\ndilemmas along with 3,795 individual perspectives of diverse values. In\nparticular, we design CLASH in a way to support the study of critical aspects\nof value-based decision-making processes which are missing from prior work,\nincluding understanding decision ambivalence and psychological discomfort as\nwell as capturing the temporal shifts of values in characters' perspectives. By\nbenchmarking 10 open and closed frontier models, we uncover several key\nfindings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet,\nachieve less than 50% accuracy in identifying situations where the decision\nshould be ambivalent, while they perform significantly better in clear-cut\nscenarios. (2) While LLMs reasonably predict psychological discomfort as marked\nby human, they inadequately comprehend perspectives involving value shifts,\nindicating a need for LLMs to reason over complex values. (3) Our experiments\nalso reveal a significant correlation between LLMs' value preferences and their\nsteerability towards a given value. (4) Finally, LLMs exhibit greater\nsteerability when engaged in value reasoning from a third-party perspective,\ncompared to a first-person setup, though certain value pairs benefit uniquely\nfrom the first-person framing.\n","authors":["Ayoung Lee","Ryan Sungmo Kwon","Peter Railton","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10823v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08058v2","updated":"2025-05-14T20:57:31Z","published":"2025-05-12T20:49:50Z","title":"Hypernym Mercury: Token Optimization Through Semantic Field Constriction\n  And Reconstruction From Hypernyms. A New Text Compression Method","summary":"  Compute optimization using token reduction of LLM prompts is an emerging task\nin the fields of NLP and next generation, agentic AI. In this white paper, we\nintroduce a novel (patent pending) text representation scheme and a\nfirst-of-its-kind word-level semantic compression of paragraphs that can lead\nto over 90% token reduction, while retaining high semantic similarity to the\nsource text. We explain how this novel compression technique can be lossless\nand how the detail granularity is controllable. We discuss benchmark results\nover open source data (i.e. Bram Stoker's Dracula available through Project\nGutenberg) and show how our results hold at the paragraph level, across\nmultiple genres and models.\n","authors":["Chris Forrester","Octavia Sulea"],"pdf_url":"https://arxiv.org/pdf/2505.08058v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09794v1","updated":"2025-05-14T20:44:29Z","published":"2025-05-14T20:44:29Z","title":"Automated Detection of Clinical Entities in Lung and Breast Cancer\n  Reports Using NLP Techniques","summary":"  Research projects, including those focused on cancer, rely on the manual\nextraction of information from clinical reports. This process is time-consuming\nand prone to errors, limiting the efficiency of data-driven approaches in\nhealthcare. To address these challenges, Natural Language Processing (NLP)\noffers an alternative for automating the extraction of relevant data from\nelectronic health records (EHRs). In this study, we focus on lung and breast\ncancer due to their high incidence and the significant impact they have on\npublic health. Early detection and effective data management in both types of\ncancer are crucial for improving patient outcomes. To enhance the accuracy and\nefficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels\nat identifying relevant entities in clinical texts and converting them into\nstandardized formats such as SNOMED and OMOP. uQuery not only detects and\nclassifies entities but also associates them with contextual information,\nincluding negated entities, temporal aspects, and patient-related details. In\nthis work, we explore the use of NLP techniques, specifically Named Entity\nRecognition (NER), to automatically identify and extract key clinical\ninformation from EHRs related to these two cancers. A dataset from Health\nResearch Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast\ncancer and 400 lung cancer reports, was used, with eight clinical entities\nmanually labeled using the Doccano platform. To perform NER, we fine-tuned the\nbsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained\nin Spanish. Fine-tuning was performed using the Transformers architecture,\nenabling accurate recognition of clinical entities in these cancer types. Our\nresults demonstrate strong overall performance, particularly in identifying\nentities like MET and PAT, although challenges remain with less frequent\nentities like EVOL.\n","authors":["J. Moreno-Casanova","J. M. AuÃ±Ã³n","A. MÃ¡rtinez-PÃ©rez","M. E. PÃ©rez-MartÃ­nez","M. E. Gas-LÃ³pez"],"pdf_url":"https://arxiv.org/pdf/2505.09794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09777v1","updated":"2025-05-14T20:15:52Z","published":"2025-05-14T20:15:52Z","title":"A Survey on Large Language Models in Multimodal Recommender Systems","summary":"  Multimodal recommender systems (MRS) integrate heterogeneous user and item\ndata, such as text, images, and structured information, to enhance\nrecommendation performance. The emergence of large language models (LLMs)\nintroduces new opportunities for MRS by enabling semantic reasoning, in-context\nlearning, and dynamic input handling. Compared to earlier pre-trained language\nmodels (PLMs), LLMs offer greater flexibility and generalisation capabilities\nbut also introduce challenges related to scalability and model accessibility.\nThis survey presents a comprehensive review of recent work at the intersection\nof LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and\ndata adaptation techniques. We propose a novel taxonomy to characterise\nintegration patterns, identify transferable techniques from related\nrecommendation domains, provide an overview of evaluation metrics and datasets,\nand point to possible future directions. We aim to clarify the emerging role of\nLLMs in multimodal recommendation and support future research in this rapidly\nevolving field.\n","authors":["Alejo Lopez-Avila","Jinhua Du"],"pdf_url":"https://arxiv.org/pdf/2505.09777v1.pdf","comment":"30 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.09738v1","updated":"2025-05-14T19:00:27Z","published":"2025-05-14T19:00:27Z","title":"Achieving Tokenizer Flexibility in Language Models through Heuristic\n  Adaptation and Supertoken Learning","summary":"  Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.\n","authors":["Shaurya Sharthak","Vinayak Pahalwan","Adithya Kamath","Adarsh Shirawalmath"],"pdf_url":"https://arxiv.org/pdf/2505.09738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09724v1","updated":"2025-05-14T18:32:18Z","published":"2025-05-14T18:32:18Z","title":"An AI-Powered Research Assistant in the Lab: A Practical Guide for Text\n  Analysis Through Iterative Collaboration with LLMs","summary":"  Analyzing texts such as open-ended responses, headlines, or social media\nposts is a time- and labor-intensive process highly susceptible to bias. LLMs\nare promising tools for text analysis, using either a predefined (top-down) or\na data-driven (bottom-up) taxonomy, without sacrificing quality. Here we\npresent a step-by-step tutorial to efficiently develop, test, and apply\ntaxonomies for analyzing unstructured data through an iterative and\ncollaborative process between researchers and LLMs. Using personal goals\nprovided by participants as an example, we demonstrate how to write prompts to\nreview datasets and generate a taxonomy of life domains, evaluate and refine\nthe taxonomy through prompt and direct modifications, test the taxonomy and\nassess intercoder agreements, and apply the taxonomy to categorize an entire\ndataset with high intercoder reliability. We discuss the possibilities and\nlimitations of using LLMs for text analysis.\n","authors":["Gino Carmona-DÃ­az","William JimÃ©nez-Leal","MarÃ­a Alejandra Grisales","Chandra Sripada","Santiago Amaya","Michael Inzlicht","Juan Pablo BermÃºdez"],"pdf_url":"https://arxiv.org/pdf/2505.09724v1.pdf","comment":"31 pages, 1 figure"},{"id":"http://arxiv.org/abs/2505.09701v1","updated":"2025-05-14T18:02:37Z","published":"2025-05-14T18:02:37Z","title":"VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact\n  Extraction and Reference Facts","summary":"  Large language models (LLMs) excel at generating long-form responses, but\nevaluating their factuality remains challenging due to complex inter-sentence\ndependencies within the generated facts. Prior solutions predominantly follow a\ndecompose-decontextualize-verify pipeline but often fail to capture essential\ncontext and miss key relational facts. In this paper, we introduce VeriFact, a\nfactuality evaluation framework designed to enhance fact extraction by\nidentifying and resolving incomplete and missing facts to support more accurate\nverification results. Moreover, we introduce FactRBench , a benchmark that\nevaluates both precision and recall in long-form model responses, whereas prior\nwork primarily focuses on precision. FactRBench provides reference fact sets\nfrom advanced LLMs and human-written answers, enabling recall assessment.\nEmpirical evaluations show that VeriFact significantly enhances fact\ncompleteness and preserves complex facts with critical relational information,\nresulting in more accurate factuality evaluation. Benchmarking various open-\nand close-weight LLMs on FactRBench indicate that larger models within same\nmodel family improve precision and recall, but high precision does not always\ncorrelate with high recall, underscoring the importance of comprehensive\nfactuality assessment.\n","authors":["Xin Liu","Lechen Zhang","Sheza Munir","Yiyang Gu","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2505.09701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09614v1","updated":"2025-05-14T17:59:35Z","published":"2025-05-14T17:59:35Z","title":"Language Agents Mirror Human Causal Reasoning Biases. How Can We Help\n  Them Think Like Scientists?","summary":"  Language model (LM) agents are increasingly used as autonomous\ndecision-makers who need to actively gather information to guide their\ndecisions. A crucial cognitive skill for such agents is the efficient\nexploration and understanding of the causal structure of the world -- key to\nrobust, scientifically grounded reasoning. Yet, it remains unclear whether LMs\npossess this capability or exhibit systematic biases leading to erroneous\nconclusions. In this work, we examine LMs' ability to explore and infer causal\nrelationships, using the well-established \"Blicket Test\" paradigm from\ndevelopmental psychology. We find that LMs reliably infer the common, intuitive\ndisjunctive causal relationships but systematically struggle with the unusual,\nyet equally (or sometimes even more) evidenced conjunctive ones. This\n\"disjunctive bias\" persists across model families, sizes, and prompting\nstrategies, and performance further declines as task complexity increases.\nInterestingly, an analogous bias appears in human adults, suggesting that LMs\nmay have inherited deep-seated reasoning heuristics from their training data.\nTo this end, we quantify similarities between LMs and humans, finding that LMs\nexhibit adult-like inference profiles (but not children-like). Finally, we\npropose a test-time sampling method which explicitly samples and eliminates\nhypotheses about causal relationships from the LM. This scalable approach\nsignificantly reduces the disjunctive bias and moves LMs closer to the goal of\nscientific, causally rigorous reasoning.\n","authors":["Anthony GX-Chen","Dongyan Lin","Mandana Samiei","Doina Precup","Blake A. Richards","Rob Fergus","Kenneth Marino"],"pdf_url":"https://arxiv.org/pdf/2505.09614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09610v1","updated":"2025-05-14T17:58:40Z","published":"2025-05-14T17:58:40Z","title":"Customizing a Large Language Model for VHDL Design of High-Performance\n  Microprocessors","summary":"  The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world.\n","authors":["Nicolas Dupuis","Ravi Nair","Shyam Ramji","Sean McClintock","Nishant Chauhan","Priyanka Nagpal","Bart Blaner","Ken Valk","Leon Stok","Ruchir Puri"],"pdf_url":"https://arxiv.org/pdf/2505.09610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21696v2","updated":"2025-05-14T17:48:02Z","published":"2025-03-27T17:00:51Z","title":"Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks","summary":"  Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.\n","authors":["Wenqi Zhang","Mengna Wang","Gangao Liu","Xu Huixin","Yiwei Jiang","Yongliang Shen","Guiyang Hou","Zhe Zheng","Hang Zhang","Xin Li","Weiming Lu","Peng Li","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2503.21696v2.pdf","comment":"Code: https://github.com/zwq2018/embodied_reasoner Dataset:\n  https://huggingface.co/datasets/zwq2018/embodied_reasoner"},{"id":"http://arxiv.org/abs/2505.09595v1","updated":"2025-05-14T17:43:40Z","published":"2025-05-14T17:43:40Z","title":"WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives\n  in Large Language Models","summary":"  Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems.\n","authors":["Abdullah Mushtaq","Imran Taj","Rafay Naeem","Ibrahim Ghaznavi","Junaid Qadir"],"pdf_url":"https://arxiv.org/pdf/2505.09595v1.pdf","comment":"Preprint. Submitted to the Journal of Artificial Intelligence\n  Research (JAIR) on April 29, 2025"},{"id":"http://arxiv.org/abs/2502.15507v3","updated":"2025-05-14T17:25:36Z","published":"2025-02-21T15:04:48Z","title":"Activation Steering in Neural Theorem Provers","summary":"  Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.\n","authors":["Shashank Kirtania"],"pdf_url":"https://arxiv.org/pdf/2502.15507v3.pdf","comment":"incorrect explanation for a concept, need to revise and update!"},{"id":"http://arxiv.org/abs/2503.21813v3","updated":"2025-05-14T16:57:13Z","published":"2025-03-25T18:20:04Z","title":"OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language\n  Model Hallucinations in Ontology Matching","summary":"  Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the\nOntology Alignment Evaluation Initiative (OAEI), capturing hallucinations of\nten different LLMs performing OM tasks. These OM-specific hallucinations are\norganised into two primary categories and six sub-categories. We showcase the\nusefulness of the dataset in constructing an LLM leaderboard for OM tasks and\nfor fine-tuning LLMs used in OM tasks.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang","Jing Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.21813v3.pdf","comment":"14 pages, 4 figures, 4 tables, 2 prompt templates"},{"id":"http://arxiv.org/abs/2505.00949v3","updated":"2025-05-14T16:47:23Z","published":"2025-05-02T01:35:35Z","title":"Llama-Nemotron: Efficient Reasoning Models","summary":"  We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.\n","authors":["Akhiad Bercovich","Itay Levy","Izik Golan","Mohammad Dabbah","Ran El-Yaniv","Omri Puny","Ido Galil","Zach Moshe","Tomer Ronen","Najeeb Nabwani","Ido Shahaf","Oren Tropp","Ehud Karpas","Ran Zilberstein","Jiaqi Zeng","Soumye Singhal","Alexander Bukharin","Yian Zhang","Tugrul Konuk","Gerald Shen","Ameya Sunil Mahabaleshwarkar","Bilal Kartal","Yoshi Suhara","Olivier Delalleau","Zijia Chen","Zhilin Wang","David Mosallanezhad","Adi Renduchintala","Haifeng Qian","Dima Rekesh","Fei Jia","Somshubra Majumdar","Vahid Noroozi","Wasi Uddin Ahmad","Sean Narenthiran","Aleksander Ficek","Mehrzad Samadi","Jocelyn Huang","Siddhartha Jain","Igor Gitman","Ivan Moshkov","Wei Du","Shubham Toshniwal","George Armstrong","Branislav Kisacanin","Matvei Novikov","Daria Gitman","Evelina Bakhturina","Jane Polak Scowcroft","John Kamalu","Dan Su","Kezhi Kong","Markus Kliegl","Rabeeh Karimi","Ying Lin","Sanjeev Satheesh","Jupinder Parmar","Pritam Gundecha","Brandon Norick","Joseph Jennings","Shrimai Prabhumoye","Syeda Nahida Akter","Mostofa Patwary","Abhinav Khattar","Deepak Narayanan","Roger Waleffe","Jimmy Zhang","Bor-Yiing Su","Guyue Huang","Terry Kong","Parth Chadha","Sahil Jain","Christine Harvey","Elad Segal","Jining Huang","Sergey Kashirsky","Robert McQueen","Izzy Putterman","George Lam","Arun Venkatesan","Sherry Wu","Vinh Nguyen","Manoj Kilaru","Andrew Wang","Anna Warno","Abhilash Somasamudramath","Sandip Bhaskar","Maka Dong","Nave Assaf","Shahar Mor","Omer Ullman Argov","Scot Junkin","Oleksandr Romanenko","Pedro Larroy","Monika Katariya","Marco Rovinelli","Viji Balas","Nicholas Edelman","Anahita Bhiwandiwalla","Muthu Subramaniam","Smita Ithape","Karthik Ramamoorthy","Yuting Wu","Suguna Varshini Velury","Omri Almog","Joyjit Daw","Denys Fridman","Erick Galinkin","Michael Evans","Shaona Ghosh","Katherine Luna","Leon Derczynski","Nikki Pope","Eileen Long","Seth Schneider","Guillermo Siman","Tomasz Grzegorzek","Pablo Ribalta","Monika Katariya","Chris Alexiuk","Joey Conway","Trisha Saar","Ann Guan","Krzysztof Pawelec","Shyamala Prayaga","Oleksii Kuchaiev","Boris Ginsburg","Oluwatobi Olabiyi","Kari Briski","Jonathan Cohen","Bryan Catanzaro","Jonah Alben","Yonatan Geifman","Eric Chung"],"pdf_url":"https://arxiv.org/pdf/2505.00949v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09666v1","updated":"2025-05-14T16:46:15Z","published":"2025-05-14T16:46:15Z","title":"System Prompt Optimization with Meta-Learning","summary":"  Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.\n","authors":["Yumin Choi","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2505.09666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07890v2","updated":"2025-05-14T16:43:25Z","published":"2025-05-11T14:30:56Z","title":"TSLFormer: A Lightweight Transformer Model for Turkish Sign Language\n  Recognition Using Skeletal Landmarks","summary":"  This study presents TSLFormer, a light and robust word-level Turkish Sign\nLanguage (TSL) recognition model that treats sign gestures as ordered,\nstring-like language. Instead of using raw RGB or depth videos, our method only\nworks with 3D joint positions - articulation points - extracted using Google's\nMediapipe library, which focuses on the hand and torso skeletal locations. This\ncreates efficient input dimensionality reduction while preserving important\nsemantic gesture information.\n  Our approach revisits sign language recognition as sequence-to-sequence\ntranslation, inspired by the linguistic nature of sign languages and the\nsuccess of transformers in natural language processing. Since TSLFormer uses\nthe self-attention mechanism, it effectively captures temporal co-occurrence\nwithin gesture sequences and highlights meaningful motion patterns as words\nunfold.\n  Evaluated on the AUTSL dataset with over 36,000 samples and 227 different\nwords, TSLFormer achieves competitive performance with minimal computational\ncost. These results show that joint-based input is sufficient for enabling\nreal-time, mobile, and assistive communication systems for hearing-impaired\nindividuals.\n","authors":["Kutay ErtÃ¼rk","Furkan AltÄ±nÄ±ÅÄ±k","Ä°rem SarÄ±altÄ±n","Ãmer Nezih Gerek"],"pdf_url":"https://arxiv.org/pdf/2505.07890v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09665v1","updated":"2025-05-14T16:31:08Z","published":"2025-05-14T16:31:08Z","title":"Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns\n  in Reddit via LLM-Enhanced Topic Modeling","summary":"  Wildfires have become increasingly frequent, irregular, and severe in recent\nyears. Understanding how affected populations perceive and respond during\nwildfire crises is critical for timely and empathetic disaster response. Social\nmedia platforms offer a crowd-sourced channel to capture evolving public\ndiscourse, providing hyperlocal information and insight into public sentiment.\nThis study analyzes Reddit discourse during the 2025 Los Angeles wildfires,\nspanning from the onset of the disaster to full containment. We collect 385\nposts and 114,879 comments related to the Palisades and Eaton fires. We adopt\ntopic modeling methods to identify the latent topics, enhanced by large\nlanguage models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we\ndevelop a hierarchical framework to categorize latent topics, consisting of two\nmain categories, Situational Awareness (SA) and Crisis Narratives (CN). The\nvolume of SA category closely aligns with real-world fire progressions, peaking\nwithin the first 2-5 days as the fires reach the maximum extent. The most\nfrequent co-occurring category set of public health and safety, loss and\ndamage, and emergency resources expands on a wide range of health-related\nlatent topics, including environmental health, occupational health, and one\nhealth. Grief signals and mental health risks consistently accounted for 60\npercentage and 40 percentage of CN instances, respectively, with the highest\ntotal volume occurring at night. This study contributes the first annotated\nsocial media dataset on the 2025 LA fires, and introduces a scalable\nmulti-layer framework that leverages topic modeling for crisis discourse\nanalysis. By identifying persistent public health concerns, our results can\ninform more empathetic and adaptive strategies for disaster response, public\nhealth communication, and future research in comparable climate-related\ndisaster events.\n","authors":["Sulong Zhou","Qunying Huang","Shaoheng Zhou","Yun Hang","Xinyue Ye","Aodong Mei","Kathryn Phung","Yuning Ye","Uma Govindswamy","Zehan Li"],"pdf_url":"https://arxiv.org/pdf/2505.09665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09519v1","updated":"2025-05-14T16:16:36Z","published":"2025-05-14T16:16:36Z","title":"PT-MoE: An Efficient Finetuning Framework for Integrating\n  Mixture-of-Experts into Prompt Tuning","summary":"  Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting\nlarge language models, yet existing approaches exhibit counter-intuitive\nphenomena: integrating router into prompt tuning (PT) increases training\nefficiency yet does not improve performance universally; parameter reduction\nthrough matrix decomposition can improve performance in specific domains.\nMotivated by these observations and the modular nature of PT, we propose\nPT-MoE, a novel framework that integrates matrix decomposition with\nmixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets\ndemonstrate that PT-MoE achieves state-of-the-art performance in both question\nanswering (QA) and mathematical problem solving tasks, improving F1 score by\n1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing\nmathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all\nwhile using 25% fewer parameters than LoRA. Our analysis reveals that while PT\nmethods generally excel in QA tasks and LoRA-based methods in math datasets,\nthe integration of matrix decomposition and MoE in PT-MoE yields complementary\nbenefits: decomposition enables efficient parameter sharing across experts\nwhile MoE provides dynamic adaptation, collectively enabling PT-MoE to\ndemonstrate cross-task consistency and generalization abilities. These\nfindings, along with ablation studies on routing mechanisms and architectural\ncomponents, provide insights for future PEFT methods.\n","authors":["Zongqian Li","Yixuan Su","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2505.09519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09436v1","updated":"2025-05-14T14:44:30Z","published":"2025-05-14T14:44:30Z","title":"CXMArena: Unified Dataset to benchmark performance in realistic CXM\n  Scenarios","summary":"  Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.\n","authors":["Raghav Garg","Kapil Sharma","Karan Gupta"],"pdf_url":"https://arxiv.org/pdf/2505.09436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09662v1","updated":"2025-05-14T14:31:33Z","published":"2025-05-14T14:31:33Z","title":"Large Language Models Are More Persuasive Than Incentivized Human\n  Persuaders","summary":"  We directly compare the persuasion capabilities of a frontier large language\nmodel (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an\ninteractive, real-time conversational quiz setting. In this preregistered,\nlarge-scale incentivized experiment, participants (quiz takers) completed an\nonline quiz where persuaders (either humans or LLMs) attempted to persuade quiz\ntakers toward correct or incorrect answers. We find that LLM persuaders\nachieved significantly higher compliance with their directional persuasion\nattempts than incentivized human persuaders, demonstrating superior persuasive\ncapabilities in both truthful (toward correct answers) and deceptive (toward\nincorrect answers) contexts. We also find that LLM persuaders significantly\nincreased quiz takers' accuracy, leading to higher earnings, when steering quiz\ntakers toward correct answers, and significantly decreased their accuracy,\nleading to lower earnings, when steering them toward incorrect answers.\nOverall, our findings suggest that AI's persuasion capabilities already exceed\nthose of humans that have real-money bonuses tied to performance. Our findings\nof increasingly capable AI persuaders thus underscore the urgency of emerging\nalignment and governance frameworks.\n","authors":["Philipp Schoenegger","Francesco Salvi","Jiacheng Liu","Xiaoli Nan","Ramit Debnath","Barbara Fasolo","Evelina Leivada","Gabriel Recchia","Fritz GÃ¼nther","Ali Zarifhonarvar","Joe Kwon","Zahoor Ul Islam","Marco Dehnert","Daryl Y. H. Lee","Madeline G. Reinecke","David G. Kamper","Mert KobaÅ","Adam Sandford","Jonas Kgomo","Luke Hewitt","Shreya Kapoor","Kerem Oktar","Eyup Engin Kucuk","Bo Feng","Cameron R. Jones","Izzy Gainsburg","Sebastian Olschewski","Nora Heinzelmann","Francisco Cruz","Ben M. Tappin","Tao Ma","Peter S. Park","Rayan Onyonka","Arthur Hjorth","Peter Slattery","Qingcheng Zeng","Lennart Finke","Igor Grossmann","Alessandro Salatiello","Ezra Karger"],"pdf_url":"https://arxiv.org/pdf/2505.09662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09407v1","updated":"2025-05-14T14:04:44Z","published":"2025-05-14T14:04:44Z","title":"Multilingual Machine Translation with Quantum Encoder Decoder\n  Attention-based Convolutional Variational Circuits","summary":"  Cloud-based multilingual translation services like Google Translate and\nMicrosoft Translator achieve state-of-the-art translation capabilities. These\nservices inherently use large multilingual language models such as GRU, LSTM,\nBERT, GPT, T5, or similar encoder-decoder architectures with attention\nmechanisms as the backbone. Also, new age natural language systems, for\ninstance ChatGPT and DeepSeek, have established huge potential in multiple\ntasks in natural language processing. At the same time, they also possess\noutstanding multilingual translation capabilities. However, these models use\nthe classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder\nAttention-based Convolutional Variational Circuits) is an alternate solution\nthat explores the quantum computing realm instead of the classical computing\nrealm to study and demonstrate multilingual machine translation. QEDACVC\nintroduces the quantum encoder-decoder architecture that simulates and runs on\nquantum computing hardware via quantum convolution, quantum pooling, quantum\nvariational circuit, and quantum attention as software alterations. QEDACVC\nachieves an Accuracy of 82% when trained on the OPUS dataset for English,\nFrench, German, and Hindi corpora for multilingual translations.\n","authors":["Subrit Dikshit","Ritu Tiwari","Priyank Jain"],"pdf_url":"https://arxiv.org/pdf/2505.09407v1.pdf","comment":"12 pages, 12 figures"},{"id":"http://arxiv.org/abs/2503.24293v2","updated":"2025-05-14T13:59:02Z","published":"2025-03-31T16:41:16Z","title":"Is analogy enough to draw novel adjective-noun inferences?","summary":"  Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition.\n","authors":["Hayley Ross","Kathryn Davidson","Najoung Kim"],"pdf_url":"https://arxiv.org/pdf/2503.24293v2.pdf","comment":"9 pages (17 pages with appendix). Accepted to SCiL 2025"},{"id":"http://arxiv.org/abs/2505.08435v2","updated":"2025-05-14T13:47:12Z","published":"2025-05-13T10:57:32Z","title":"Hakim: Farsi Text Embedding Model","summary":"  Recent advancements in text embedding have significantly improved natural\nlanguage understanding across many languages, yet Persian remains notably\nunderrepresented in large-scale embedding research. In this paper, we present\nHakim, a novel state-of-the-art Persian text embedding model that achieves a\n8.5% performance improvement over existing approaches on the FaMTEB benchmark,\noutperforming all previously developed Persian language models. As part of this\nwork, we introduce three new datasets - Corpesia, Pairsia-sup, and\nPairsia-unsup - to support supervised and unsupervised training scenarios.\nAdditionally, Hakim is designed for applications in chatbots and\nretrieval-augmented generation (RAG) systems, particularly addressing retrieval\ntasks that require incorporating message history within these systems. We also\npropose a new baseline model built on the BERT architecture. Our language model\nconsistently achieves higher accuracy across various Persian NLP tasks, while\nthe RetroMAE-based model proves particularly effective for textual information\nretrieval applications. Together, these contributions establish a new\nfoundation for advancing Persian language understanding.\n","authors":["Mehran Sarmadi","Morteza Alikhani","Erfan Zinvandi","Zahra Pourbahman"],"pdf_url":"https://arxiv.org/pdf/2505.08435v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09388v1","updated":"2025-05-14T13:41:34Z","published":"2025-05-14T13:41:34Z","title":"Qwen3 Technical Report","summary":"  In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.\n","authors":["An Yang","Anfeng Li","Baosong Yang","Beichen Zhang","Binyuan Hui","Bo Zheng","Bowen Yu","Chang Gao","Chengen Huang","Chenxu Lv","Chujie Zheng","Dayiheng Liu","Fan Zhou","Fei Huang","Feng Hu","Hao Ge","Haoran Wei","Huan Lin","Jialong Tang","Jian Yang","Jianhong Tu","Jianwei Zhang","Jianxin Yang","Jiaxi Yang","Jing Zhou","Jingren Zhou","Junyang Lin","Kai Dang","Keqin Bao","Kexin Yang","Le Yu","Lianghao Deng","Mei Li","Mingfeng Xue","Mingze Li","Pei Zhang","Peng Wang","Qin Zhu","Rui Men","Ruize Gao","Shixuan Liu","Shuang Luo","Tianhao Li","Tianyi Tang","Wenbiao Yin","Xingzhang Ren","Xinyu Wang","Xinyu Zhang","Xuancheng Ren","Yang Fan","Yang Su","Yichang Zhang","Yinger Zhang","Yu Wan","Yuqiong Liu","Zekun Wang","Zeyu Cui","Zhenru Zhang","Zhipeng Zhou","Zihan Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.09388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09338v1","updated":"2025-05-14T12:33:05Z","published":"2025-05-14T12:33:05Z","title":"Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment\n  and Distraction in LLMs","summary":"  We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem.\n","authors":["Jingcheng Niu","Xingdi Yuan","Tong Wang","Hamidreza Saghir","Amir H. Abdi"],"pdf_url":"https://arxiv.org/pdf/2505.09338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03343v2","updated":"2025-05-14T12:32:17Z","published":"2024-11-02T17:29:47Z","title":"What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms\n  Behind Attacks","summary":"  Jailbreaks have been a central focus of research regarding the safety and\nreliability of large language models (LLMs), yet the mechanisms underlying\nthese attacks remain poorly understood. While previous studies have\npredominantly relied on linear methods to detect jailbreak attempts and model\nrefusals, we take a different approach by examining both linear and non-linear\nfeatures in prompts that lead to successful jailbreaks. First, we introduce a\nnovel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack\nmethods. Leveraging this dataset, we train probes to classify successful from\nunsuccessful jailbreaks using the latent representations corresponding to\nprompt tokens. Notably, we find that even when probes achieve high accuracy in\npredicting the success of jailbreaks, their performance often fails to\ngeneralize to unseen attack methods. This reveals that different jailbreaking\nstrategies exploit different non-linear, non-universal features. Next, we\ndemonstrate that non-linear probes provide a powerful tool for steering model\nbehavior. Specifically, we use these probes to guide targeted latent space\nperturbations, enabling us to effectively modulate the model's robustness\nagainst jailbreaks. Overall, our findings challenge the assumption that\njailbreaks can be fully understood through linear or simple universal prompt\nfeatures alone, highlighting the importance of a nuanced understanding of the\nmechanisms behind LLM vulnerabilities.\n","authors":["Nathalie Kirch","Constantin Weisser","Severin Field","Helen Yannakoudakis","Stephen Casper"],"pdf_url":"https://arxiv.org/pdf/2411.03343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09316v1","updated":"2025-05-14T12:13:38Z","published":"2025-05-14T12:13:38Z","title":"Scent of Knowledge: Optimizing Search-Enhanced Reasoning with\n  Information Foraging","summary":"  Augmenting large language models (LLMs) with external retrieval has become a\nstandard method to address their inherent knowledge cutoff limitations.\nHowever, traditional retrieval-augmented generation methods employ static,\npre-inference retrieval strategies, making them inadequate for complex tasks\ninvolving ambiguous, multi-step, or evolving information needs. Recent advances\nin test-time scaling techniques have demonstrated significant potential in\nenabling LLMs to dynamically interact with external tools, motivating the shift\ntoward adaptive inference-time retrieval. Inspired by Information Foraging\nTheory (IFT), we propose InForage, a reinforcement learning framework that\nformalizes retrieval-augmented reasoning as a dynamic information-seeking\nprocess. Unlike existing approaches, InForage explicitly rewards intermediate\nretrieval quality, encouraging LLMs to iteratively gather and integrate\ninformation through adaptive search behaviors. To facilitate training, we\nconstruct a human-guided dataset capturing iterative search and reasoning\ntrajectories for complex, real-world web tasks. Extensive evaluations across\ngeneral question answering, multi-hop reasoning tasks, and a newly developed\nreal-time web QA dataset demonstrate InForage's superior performance over\nbaseline methods. These results highlight InForage's effectiveness in building\nrobust, adaptive, and efficient reasoning agents.\n","authors":["Hongjin Qian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2505.09316v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2505.09286v1","updated":"2025-05-14T11:11:17Z","published":"2025-05-14T11:11:17Z","title":"A Scalable Unsupervised Framework for multi-aspect labeling of\n  Multilingual and Multi-Domain Review Data","summary":"  Effectively analyzing online review data is essential across industries.\nHowever, many existing studies are limited to specific domains and languages or\ndepend on supervised learning approaches that require large-scale labeled\ndatasets. To address these limitations, we propose a multilingual, scalable,\nand unsupervised framework for cross-domain aspect detection. This framework is\ndesigned for multi-aspect labeling of multilingual and multi-domain review\ndata. In this study, we apply automatic labeling to Korean and English review\ndatasets spanning various domains and assess the quality of the generated\nlabels through extensive experiments. Aspect category candidates are first\nextracted through clustering, and each review is then represented as an\naspect-aware embedding vector using negative sampling. To evaluate the\nframework, we conduct multi-aspect labeling and fine-tune several pretrained\nlanguage models to measure the effectiveness of the automatically generated\nlabels. Results show that these models achieve high performance, demonstrating\nthat the labels are suitable for training. Furthermore, comparisons with\npublicly available large language models highlight the framework's superior\nconsistency and scalability when processing large-scale data. A human\nevaluation also confirms that the quality of the automatic labels is comparable\nto those created manually. This study demonstrates the potential of a robust\nmulti-aspect labeling approach that overcomes limitations of supervised methods\nand is adaptable to multilingual, multi-domain environments. Future research\nwill explore automatic review summarization and the integration of artificial\nintelligence agents to further improve the efficiency and depth of review\nanalysis.\n","authors":["Jiin Park","Misuk Kim"],"pdf_url":"https://arxiv.org/pdf/2505.09286v1.pdf","comment":"36 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.09269v1","updated":"2025-05-14T10:38:37Z","published":"2025-05-14T10:38:37Z","title":"How an unintended Side Effect of a Research Project led to Boosting the\n  Power of UML","summary":"  This paper describes the design, implementation and use of a new UML modeling\ntool that represents a significant advance over conventional tools. Among other\nthings, it allows the integration of class diagrams and object diagrams as well\nas the execution of objects. This not only enables new software architectures\ncharacterized by the integration of software with corresponding object models,\nbut is also ideal for use in teaching, as it provides students with a\nparticularly stimulating learning experience. A special feature of the project\nis that it has emerged from a long-standing international research project,\nwhich is aimed at a comprehensive multi-level architecture. The project is\ntherefore an example of how research can lead to valuable results that arise as\na side effect of other work.\n","authors":["Ulrich Frank","Pierre Maier"],"pdf_url":"https://arxiv.org/pdf/2505.09269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17599v2","updated":"2025-05-14T10:25:11Z","published":"2025-03-22T01:02:44Z","title":"Evaluating Clinical Competencies of Large Language Models with a General\n  Practice Benchmark","summary":"  Large Language Models (LLMs) have demonstrated considerable potential in\ngeneral practice. However, existing benchmarks and evaluation frameworks\nprimarily depend on exam-style or simplified question-answer formats, lacking a\ncompetency-based structure aligned with the real-world clinical\nresponsibilities encountered in general practice. Consequently, the extent to\nwhich LLMs can reliably fulfill the duties of general practitioners (GPs)\nremains uncertain. In this work, we propose a novel evaluation framework to\nassess the capability of LLMs to function as GPs. Based on this framework, we\nintroduce a general practice benchmark (GPBench), whose data are meticulously\nannotated by domain experts in accordance with routine clinical practice\nstandards. We evaluate ten state-of-the-art LLMs and analyze their\ncompetencies. Our findings indicate that current LLMs are not yet ready for\ndeployment in such settings without human oversight, and further optimization\nspecifically tailored to the daily responsibilities of GPs is essential.\n","authors":["Zheqing Li","Yiying Yang","Jiping Lang","Wenhao Jiang","Yuhang Zhao","Shuang Li","Dingqian Wang","Zhu Lin","Xuanna Li","Yuze Tang","Jiexian Qiu","Xiaolin Lu","Hongji Yu","Shuang Chen","Yuhua Bi","Xiaofei Zeng","Yixian Chen","Junrong Chen","Lin Yao"],"pdf_url":"https://arxiv.org/pdf/2503.17599v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09246v1","updated":"2025-05-14T09:35:56Z","published":"2025-05-14T09:35:56Z","title":"Focus, Merge, Rank: Improved Question Answering Based on Semi-structured\n  Knowledge Bases","summary":"  In many real-world settings, machine learning models and interactive systems\nhave access to both structured knowledge, e.g., knowledge graphs or tables, and\nunstructured content, e.g., natural language documents. However, most rely on\neither. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking\nunstructured content to nodes within structured data, thereby enabling new\nstrategies for knowledge access and use. In this work, we present\nFocusedRetriever, a modular SKB-based framework for multi-hop question\nanswering. It integrates components (VSS-based entity search, LLM-based\ngeneration of Cypher queries and pairwise re-ranking) in a way that enables it\nto outperform state-of-the-art methods across all three STaRK benchmark test\nsets, covering diverse domains and multiple performance metrics. The average\nfirst-hit rate exceeds that of the second-best method by 25.7%.\nFocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to\nextract relational facts and entity attributes from unstructured text, (2) node\nset joins to filter answer candidates based on these extracted triplets and\nconstraints, (3) vector similarity search to retrieve and rank relevant\nunstructured content, and (4) the contextual capabilities of LLMs to finally\nrank the top-k answers. For generality, we only incorporate base LLMs in\nFocusedRetriever in our evaluation. However, our analysis of intermediate\nresults highlights several opportunities for further upgrades including\nfinetuning. The source code is publicly available at\nhttps://github.com/kramerlab/FocusedRetriever .\n","authors":["Derian Boer","Stephen Roth","Stefan Kramer"],"pdf_url":"https://arxiv.org/pdf/2505.09246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10725v3","updated":"2025-05-14T08:07:08Z","published":"2025-02-15T08:28:58Z","title":"PropNet: a White-Box and Human-Like Network for Sentence Representation","summary":"  Transformer-based embedding methods have dominated the field of sentence\nrepresentation in recent years. Although they have achieved remarkable\nperformance on NLP missions, such as semantic textual similarity (STS) tasks,\ntheir black-box nature and large-data-driven training style have raised\nconcerns, including issues related to bias, trust, and safety. Many efforts\nhave been made to improve the interpretability of embedding models, but these\nproblems have not been fundamentally resolved. To achieve inherent\ninterpretability, we propose a purely white-box and human-like sentence\nrepresentation network, PropNet. Inspired by findings from cognitive science,\nPropNet constructs a hierarchical network based on the propositions contained\nin a sentence. While experiments indicate that PropNet has a significant gap\ncompared to state-of-the-art (SOTA) embedding models in STS tasks, case studies\nreveal substantial room for improvement. Additionally, PropNet enables us to\nanalyze and understand the human cognitive processes underlying STS benchmarks.\n","authors":["Fei Yang"],"pdf_url":"https://arxiv.org/pdf/2502.10725v3.pdf","comment":"Clarified some ambiguities in the previous version"},{"id":"http://arxiv.org/abs/2402.01383v3","updated":"2025-05-14T06:05:53Z","published":"2024-02-02T13:06:35Z","title":"LLM-based NLG Evaluation: Current Status and Challenges","summary":"  Evaluating natural language generation (NLG) is a vital but challenging\nproblem in natural language processing. Traditional evaluation metrics mainly\ncapturing content (e.g. n-gram) overlap between system outputs and references\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\nhave demonstrated great potential in NLG evaluation in recent years. Various\nautomatic evaluation methods based on LLMs have been proposed, including\nmetrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM\ncollaborative evaluation. In this survey, we first give a taxonomy of LLM-based\nNLG evaluation methods, and discuss their pros and cons, respectively. Lastly,\nwe discuss several open problems in this area and point out future research\ndirections.\n","authors":["Mingqi Gao","Xinyu Hu","Jie Ruan","Xiao Pu","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2402.01383v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04405v2","updated":"2025-05-14T05:23:45Z","published":"2025-02-06T09:08:12Z","title":"FAS: Fast ANN-SNN Conversion for Spiking Large Language Models","summary":"  Spiking Large Language Models have been shown as a good alternative to LLMs\nin various scenarios. Existing methods for creating Spiking LLMs, i.e., direct\ntraining and ANN-SNN conversion, often suffer from performance degradation and\nrelatively high computational costs. To address these issues, we propose a\nnovel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking\nLLMs in two stages. The first stage employs a full-parameter fine-tuning of\npre-trained models, so it does not need any direct training from scratch. The\nsecond stage introduces a coarse-to-fine calibration method to reduce\nconversion errors and improve accuracy. Experiments on both language and\nvision-language tasks across four different scales of LLMs demonstrate that FAS\ncan achieve state-of-the-art performance yet with significantly reduced\ninference latency and computational costs. Notably, FAS only takes eight\ntimesteps to achieve an accuracy of 3\\% higher than that of the OPT-7B model,\nwhile reducing energy consumption by 96.63\\%. The source code is available at\nhttps://github.com/lc783/FAS\n","authors":["Long Chen","Xiaotian Song","Andy Song","BaDong Chen","Jiancheng Lv","Yanan Sun"],"pdf_url":"https://arxiv.org/pdf/2502.04405v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05084v2","updated":"2025-05-14T04:38:15Z","published":"2025-05-08T09:32:38Z","title":"Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text\n  Detection Framework via Multiscaled Conformal Prediction","summary":"  The rapid advancement of large language models has raised significant\nconcerns regarding their potential misuse by malicious actors. As a result,\ndeveloping effective detectors to mitigate these risks has become a critical\npriority. However, most existing detection methods focus excessively on\ndetection accuracy, often neglecting the societal risks posed by high false\npositive rates (FPRs). This paper addresses this issue by leveraging Conformal\nPrediction (CP), which effectively constrains the upper bound of FPRs. While\ndirectly applying CP constrains FPRs, it also leads to a significant reduction\nin detection performance. To overcome this trade-off, this paper proposes a\nZero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal\nPrediction (MCP), which both enforces the FPR constraint and improves detection\nperformance. This paper also introduces RealDet, a high-quality dataset that\nspans a wide range of domains, ensuring realistic calibration and enabling\nsuperior detection performance when combined with MCP. Empirical evaluations\ndemonstrate that MCP effectively constrains FPRs, significantly enhances\ndetection performance, and increases robustness against adversarial attacks\nacross multiple detectors and datasets.\n","authors":["Xiaowei Zhu","Yubing Ren","Yanan Cao","Xixun Lin","Fang Fang","Yangxi Li"],"pdf_url":"https://arxiv.org/pdf/2505.05084v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08037v2","updated":"2025-05-14T04:04:08Z","published":"2025-05-12T20:08:05Z","title":"TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction\n  covering Multi-Level Error with Data Augmentation","summary":"  Multi-level Tibetan spelling correction addresses errors at both the\ncharacter and syllable levels within a unified model. Existing methods focus\nmainly on single-level correction and lack effective integration of both\nlevels. Moreover, there are no open-source datasets or augmentation methods\ntailored for this task in Tibetan. To tackle this, we propose a data\naugmentation approach using unlabeled text to generate multi-level corruptions,\nand introduce TiSpell, a semi-masked model capable of correcting both\ncharacter- and syllable-level errors. Although syllable-level correction is\nmore challenging due to its reliance on global context, our semi-masked\nstrategy simplifies this process. We synthesize nine types of corruptions on\nclean sentences to create a robust training set. Experiments on both simulated\nand real-world data demonstrate that TiSpell, trained on our dataset,\noutperforms baseline models and matches the performance of state-of-the-art\napproaches, confirming its effectiveness.\n","authors":["Yutong Liu","Feng Xiao","Ziyue Zhang","Yongbin Yu","Cheng Huang","Fan Gao","Xiangxiang Wang","Ma-bao Ban","Manping Fan","Thupten Tsering","Cheng Huang","Gadeng Luosang","Renzeng Duojie","Nyima Tashi"],"pdf_url":"https://arxiv.org/pdf/2505.08037v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.09083v1","updated":"2025-05-14T02:36:26Z","published":"2025-05-14T02:36:26Z","title":"Ornithologist: Towards Trustworthy \"Reasoning\" about Central Bank\n  Communications","summary":"  I develop Ornithologist, a weakly-supervised textual classification system\nand measure the hawkishness and dovishness of central bank text. Ornithologist\nuses ``taxonomy-guided reasoning'', guiding a large language model with\nhuman-authored decision trees. This increases the transparency and\nexplainability of the system and makes it accessible to non-experts. It also\nreduces hallucination risk. Since it requires less supervision than traditional\nclassification systems, it can more easily be applied to other problems or\nsources of text (e.g. news) without much modification. Ornithologist\nmeasurements of hawkishness and dovishness of RBA communication carry\ninformation about the future of the cash rate path and of market expectations.\n","authors":["Dominic Zaun Eu Jones"],"pdf_url":"https://arxiv.org/pdf/2505.09083v1.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.09082v1","updated":"2025-05-14T02:35:47Z","published":"2025-05-14T02:35:47Z","title":"CEC-Zero: Chinese Error Correction Solution Based on LLM","summary":"  Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models.\n","authors":["Sophie Zhang","Zhiming Lin"],"pdf_url":"https://arxiv.org/pdf/2505.09082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09116v2","updated":"2025-05-14T02:29:41Z","published":"2024-11-14T01:29:36Z","title":"P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent\n  Evaluation of LLMs","summary":"  Recent advancements in large language models (LLMs) showcase varied\nmultilingual capabilities across tasks like translation, code generation, and\nreasoning. Previous assessments often limited their scope to fundamental\nnatural language processing (NLP) or isolated capability-specific tasks. To\nalleviate this drawback, we aim to present a comprehensive multilingual\nmultitask benchmark. First, we introduce P-MMEval, a large-scale benchmark\ncovering effective fundamental and capability-specialized datasets.\nFurthermore, P-MMEval delivers consistent language coverage across various\ndatasets and provides parallel samples. Finally, we conduct extensive\nexperiments on representative multilingual model series to compare performances\nacross models and tasks, explore the relationship between multilingual\nperformances and factors such as tasks, model sizes, languages, and prompts,\nand examine the effectiveness of knowledge transfer from English to other\nlanguages. The resulting insights are intended to offer valuable guidance for\nfuture research. The dataset is available at\nhttps://huggingface.co/datasets/Qwen/P-MMEval.\n","authors":["Yidan Zhang","Yu Wan","Boyi Deng","Baosong Yang","Haoran Wei","Fei Huang","Bowen Yu","Junyang Lin","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.09116v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11197v4","updated":"2025-05-14T02:12:43Z","published":"2025-03-14T08:43:53Z","title":"Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering","summary":"  Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa.\n","authors":["Gang Li","Jizhong Liu","Heinrich Dinkel","Yadong Niu","Junbo Zhang","Jian Luan"],"pdf_url":"https://arxiv.org/pdf/2503.11197v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09068v1","updated":"2025-05-14T02:08:40Z","published":"2025-05-14T02:08:40Z","title":"S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent\n  Thinking Assessment","summary":"  This paper introduces S-DAT (Synthetic-Divergent Association Task), a\nscalable, multilingual framework for automated assessment of divergent thinking\n(DT) -a core component of human creativity. Traditional creativity assessments\nare often labor-intensive, language-specific, and reliant on subjective human\nratings, limiting their scalability and cross-cultural applicability. In\ncontrast, S-DAT leverages large language models and advanced multilingual\nembeddings to compute semantic distance -- a language-agnostic proxy for DT. We\nevaluate S-DAT across eleven diverse languages, including English, Spanish,\nGerman, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating\nrobust and consistent scoring across linguistic contexts. Unlike prior DAT\napproaches, the S-DAT shows convergent validity with other DT measures and\ncorrect discriminant validity with convergent thinking. This cross-linguistic\nflexibility allows for more inclusive, global-scale creativity research,\naddressing key limitations of earlier approaches. S-DAT provides a powerful\ntool for fairer, more comprehensive evaluation of cognitive flexibility in\ndiverse populations and can be freely assessed online:\nhttps://sdat.iol.zib.de/.\n","authors":["Jennifer Haase","Paul H. P. Hanel","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2505.09068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09655v1","updated":"2025-05-14T02:02:32Z","published":"2025-05-14T02:02:32Z","title":"DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like\n  Training of Large Language Models","summary":"  Recent advances in reinforcement learning for language model post-training,\nsuch as Group Relative Policy Optimization (GRPO), have shown promise in\nlow-resource settings. However, GRPO typically relies on solution-level and\nscalar reward signals that fail to capture the semantic diversity among sampled\ncompletions. This leads to what we identify as a diversity-quality\ninconsistency, where distinct reasoning paths may receive indistinguishable\nrewards. To address this limitation, we propose $\\textit{Diversity-aware Reward\nAdjustment}$ (DRA), a method that explicitly incorporates semantic diversity\ninto the reward computation. DRA uses Submodular Mutual Information (SMI) to\ndownweight redundant completions and amplify rewards for diverse ones. This\nencourages better exploration during learning, while maintaining stable\nexploitation of high-quality samples. Our method integrates seamlessly with\nboth GRPO and its variant DR.~GRPO, resulting in $\\textit{DRA-GRPO}$ and\n$\\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning\nbenchmarks and find that it outperforms recent strong baselines. It achieves\nstate-of-the-art performance with an average accuracy of 58.2%, using only\n7,000 fine-tuning samples and a total training cost of approximately $55. The\ncode is available at https://github.com/xiwenc1/DRA-GRPO.\n","authors":["Xiwen Chen","Wenhui Zhu","Peijie Qiu","Xuanzhao Dong","Hao Wang","Haiyu Wu","Huayu Li","Aristeidis Sotiras","Yalin Wang","Abolfazl Razi"],"pdf_url":"https://arxiv.org/pdf/2505.09655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04717v4","updated":"2025-05-14T01:48:30Z","published":"2025-04-07T04:00:08Z","title":"Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large\n  Language Models","summary":"  Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.\n","authors":["Yubo Li","Xiaobin Shen","Xinyu Yao","Xueying Ding","Yidi Miao","Ramayya Krishnan","Rema Padman"],"pdf_url":"https://arxiv.org/pdf/2504.04717v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08167v2","updated":"2025-05-14T01:35:33Z","published":"2025-05-13T02:05:25Z","title":"Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method\n  for Enhancing Question-Answering Capabilities of Large Language Models for\n  Chinese Intangible Cultural Heritage","summary":"  The rapid development of large language models (LLMs) has provided\nsignificant support and opportunities for the advancement of domain-specific\nLLMs. However, fine-tuning these large models using Intangible Cultural\nHeritage (ICH) data inevitably faces challenges such as bias, incorrect\nknowledge inheritance, and catastrophic forgetting. To address these issues, we\npropose a novel training method that integrates a bidirectional chains of\nthought and a reward mechanism. This method is built upon ICH-Qwen, a large\nlanguage model specifically designed for the field of intangible cultural\nheritage. The proposed method enables the model to not only perform forward\nreasoning but also enhances the accuracy of the generated answers by utilizing\nreverse questioning and reverse reasoning to activate the model's latent\nknowledge. Additionally, a reward mechanism is introduced during training to\noptimize the decision-making process. This mechanism improves the quality of\nthe model's outputs through structural and content evaluations with different\nweighting schemes. We conduct comparative experiments on ICH-Qwen, with results\ndemonstrating that our method outperforms 0-shot, step-by-step reasoning,\nknowledge distillation, and question augmentation methods in terms of accuracy,\nBleu-4, and Rouge-L scores on the question-answering task. Furthermore, the\npaper highlights the effectiveness of combining the bidirectional chains of\nthought and reward mechanism through ablation experiments. In addition, a\nseries of generalizability experiments are conducted, with results showing that\nthe proposed method yields improvements on various domain-specific datasets and\nadvanced models in areas such as Finance, Wikidata, and StrategyQA. This\ndemonstrates that the method is adaptable to multiple domains and provides a\nvaluable approach for model training in future applications across diverse\nfields.\n","authors":["Ruilin Liu","Zhixiao Zhao","Jieqiong Li","Chang Liu","Dongbo Wang"],"pdf_url":"https://arxiv.org/pdf/2505.08167v2.pdf","comment":"22 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.09056v1","updated":"2025-05-14T01:21:46Z","published":"2025-05-14T01:21:46Z","title":"A Comprehensive Analysis of Large Language Model Outputs: Similarity,\n  Diversity, and Bias","summary":"  Large Language Models (LLMs) represent a major step toward artificial general\nintelligence, significantly advancing our ability to interact with technology.\nWhile LLMs perform well on Natural Language Processing tasks -- such as\ntranslation, generation, code writing, and summarization -- questions remain\nabout their output similarity, variability, and ethical implications. For\ninstance, how similar are texts generated by the same model? How does this\ncompare across different models? And which models best uphold ethical\nstandards? To investigate, we used 5{,}000 prompts spanning diverse tasks like\ngeneration, explanation, and rewriting. This resulted in approximately 3\nmillion texts from 12 LLMs, including proprietary and open-source systems from\nOpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs\nfrom the same LLM are more similar to each other than to human-written texts;\n(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4\nproduces more varied responses; (3) LLM writing styles differ significantly,\nwith Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for\ndistinctiveness; (4) differences in vocabulary and tone underscore the\nlinguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate\ngreater gender balance and reduced bias. These results offer new insights into\nthe behavior and diversity of LLM outputs, helping guide future development and\nethical evaluation.\n","authors":["Brandon Smith","Mohamed Reda Bouadjenek","Tahsin Alamgir Kheya","Phillip Dawson","Sunil Aryal"],"pdf_url":"https://arxiv.org/pdf/2505.09056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09039v1","updated":"2025-05-14T00:39:47Z","published":"2025-05-14T00:39:47Z","title":"Atomic Consistency Preference Optimization for Long-Form Question\n  Answering","summary":"  Large Language Models (LLMs) frequently produce factoid hallucinations -\nplausible yet incorrect answers. A common mitigation strategy is model\nalignment, which improves factual accuracy by training on curated factual and\nnon-factual pairs. However, this approach often relies on a stronger model\n(e.g., GPT-4) or an external knowledge base to assess factual correctness,\nwhich may not always be accessible. To address this, we propose Atomic\nConsistency Preference Optimization (ACPO), a self-supervised preference-tuning\nmethod that enhances factual accuracy without external supervision. ACPO\nleverages atomic consistency signals, i.e., the agreement of individual facts\nacross multiple stochastic responses, to identify high- and low-quality data\npairs for model alignment. By eliminating the need for costly GPT calls, ACPO\nprovides a scalable and efficient approach to improving factoid\nquestion-answering. Despite being self-supervised, empirical results\ndemonstrate that ACPO outperforms FactAlign, a strong supervised alignment\nbaseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its\neffectiveness in enhancing factual reliability without relying on external\nmodels or knowledge bases.\n","authors":["Jingfeng Chen","Raghuveer Thirukovalluru","Junlin Wang","Kaiwei Luo","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2505.09039v1.pdf","comment":"16 pages, 2 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.13933v2","updated":"2025-05-14T23:53:45Z","published":"2024-07-18T23:09:14Z","title":"Unsupervised Video Highlight Detection by Learning from Audio and Visual\n  Recurrence","summary":"  With the exponential growth of video content, the need for automated video\nhighlight detection to extract key moments or highlights from lengthy videos\nhas become increasingly pressing. This technology has the potential to enhance\nuser experiences by allowing quick access to relevant content across diverse\ndomains. Existing methods typically rely either on expensive manually labeled\nframe-level annotations, or on a large external dataset of videos for weak\nsupervision through category information. To overcome this, we focus on\nunsupervised video highlight detection, eliminating the need for manual\nannotations. We propose a novel unsupervised approach which capitalizes on the\npremise that significant moments tend to recur across multiple videos of the\nsimilar category in both audio and visual modalities. Surprisingly, audio\nremains under-explored, especially in unsupervised algorithms, despite its\npotential to detect key moments. Through a clustering technique, we identify\npseudo-categories of videos and compute audio pseudo-highlight scores for each\nvideo by measuring the similarities of audio features among audio clips of all\nthe videos within each pseudo-category. Similarly, we also compute visual\npseudo-highlight scores for each video using visual features. Then, we combine\naudio and visual pseudo-highlights to create the audio-visual pseudo\nground-truth highlight of each video for training an audio-visual highlight\ndetection network. Extensive experiments and ablation studies on three\nbenchmarks showcase the superior performance of our method over prior work.\n","authors":["Zahidul Islam","Sujoy Paul","Mrigank Rochan"],"pdf_url":"https://arxiv.org/pdf/2407.13933v2.pdf","comment":"Accepted to the 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV)"},{"id":"http://arxiv.org/abs/2505.09859v1","updated":"2025-05-14T23:43:57Z","published":"2025-05-14T23:43:57Z","title":"Few-Shot Learning of Visual Compositional Concepts through Probabilistic\n  Schema Induction","summary":"  The ability to learn new visual concepts from limited examples is a hallmark\nof human cognition. While traditional category learning models represent each\nexample as an unstructured feature vector, compositional concept learning is\nthought to depend on (1) structured representations of examples (e.g., directed\ngraphs consisting of objects and their relations) and (2) the identification of\nshared relational structure across examples through analogical mapping. Here,\nwe introduce Probabilistic Schema Induction (PSI), a prototype model that\nemploys deep learning to perform analogical mapping over structured\nrepresentations of only a handful of examples, forming a compositional concept\ncalled a schema. In doing so, PSI relies on a novel conception of similarity\nthat weighs object-level similarity and relational similarity, as well as a\nmechanism for amplifying relations relevant to classification, analogous to\nselective attention parameters in traditional models. We show that PSI produces\nhuman-like learning performance and outperforms two controls: a prototype model\nthat uses unstructured feature vectors extracted from a deep learning model,\nand a variant of PSI with weaker structured representations. Notably, we find\nthat PSI's human-like performance is driven by an adaptive strategy that\nincreases relational similarity over object-level similarity and upweights the\ncontribution of relations that distinguish classes. These findings suggest that\nstructured representations and analogical mapping are critical to modeling\nrapid human-like learning of compositional visual concepts, and demonstrate how\ndeep learning can be leveraged to create psychological models.\n","authors":["Andrew Jun Lee","Taylor Webb","Trevor Bihl","Keith Holyoak","Hongjing Lu"],"pdf_url":"https://arxiv.org/pdf/2505.09859v1.pdf","comment":"Lee, A. J., Webb, T., Bihl, T., Holyoak, K. J., & Lu, H. (2025).\n  Few-shot learning of visual compositional concepts through probabilistic\n  schema induction. In A. Ruggeri, D. Barner, C. Walker, & N. Bramley (Eds.),\n  Proceedings of the 47th Annual Conference of the Cognitive Science Society.\n  Cognitive Science Society"},{"id":"http://arxiv.org/abs/2505.09858v1","updated":"2025-05-14T23:43:29Z","published":"2025-05-14T23:43:29Z","title":"Mission Balance: Generating Under-represented Class Samples using Video\n  Diffusion Models","summary":"  Computer-assisted interventions can improve intra-operative guidance,\nparticularly through deep learning methods that harness the spatiotemporal\ninformation in surgical videos. However, the severe data imbalance often found\nin surgical video datasets hinders the development of high-performing models.\nIn this work, we aim to overcome the data imbalance by synthesizing surgical\nvideos. We propose a unique two-stage, text-conditioned diffusion-based method\nto generate high-fidelity surgical videos for under-represented classes. Our\napproach conditions the generation process on text prompts and decouples\nspatial and temporal modeling by utilizing a 2D latent diffusion model to\ncapture spatial content and then integrating temporal attention layers to\nensure temporal consistency. Furthermore, we introduce a rejection sampling\nstrategy to select the most suitable synthetic samples, effectively augmenting\nexisting datasets to address class imbalance. We evaluate our method on two\ndownstream tasks-surgical action recognition and intra-operative event\nprediction-demonstrating that incorporating synthetic videos from our approach\nsubstantially enhances model performance. We open-source our implementation at\nhttps://gitlab.com/nct_tso_public/surgvgen.\n","authors":["Danush Kumar Venkatesh","Isabel Funke","Micha Pfeiffer","Fiona Kolbinger","Hanna Maria Schmeiser","Juergen Weitz","Marius Distler","Stefanie Speidel"],"pdf_url":"https://arxiv.org/pdf/2505.09858v1.pdf","comment":"Early accept at MICCAI 2025"},{"id":"http://arxiv.org/abs/2505.09831v1","updated":"2025-05-14T22:22:52Z","published":"2025-05-14T22:22:52Z","title":"ImplicitStainer: Data-Efficient Medical Image Translation for Virtual\n  Antibody-based Tissue Staining Using Local Implicit Functions","summary":"  Hematoxylin and eosin (H&E) staining is a gold standard for microscopic\ndiagnosis in pathology. However, H&E staining does not capture all the\ndiagnostic information that may be needed. To obtain additional molecular\ninformation, immunohistochemical (IHC) stains highlight proteins that mark\nspecific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells.\nWhile IHC stains are vital for prognosis and treatment guidance, they are\ntypically only available at specialized centers and time consuming to acquire,\nleading to treatment delays for patients. Virtual staining, enabled by deep\nlearning-based image translation models, provides a promising alternative by\ncomputationally generating IHC stains from H&E stained images. Although many\nGAN and diffusion based image to image (I2I) translation methods have been used\nfor virtual staining, these models treat image patches as independent data\npoints, which results in increased and more diverse data requirements for\neffective generation. We present ImplicitStainer, a novel approach that\nleverages local implicit functions to improve image translation, specifically\nvirtual staining performance, by focusing on pixel-level predictions. This\nmethod enhances robustness to variations in dataset sizes, delivering\nhigh-quality results even with limited data. We validate our approach on two\ndatasets using a comprehensive set of metrics and benchmark it against over\nfifteen state-of-the-art GAN- and diffusion based models. Full Code and models\ntrained will be released publicly via Github upon acceptance.\n","authors":["Tushar Kataria","Beatrice Knudsen","Shireen Y. Elhabian"],"pdf_url":"https://arxiv.org/pdf/2505.09831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09829v1","updated":"2025-05-14T22:15:41Z","published":"2025-05-14T22:15:41Z","title":"BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image\n  Segmentation Performance for Low Data Regimes","summary":"  Obtaining large-scale medical data, annotated or unannotated, is challenging\ndue to stringent privacy regulations and data protection policies. In addition,\nannotating medical images requires that domain experts manually delineate\nanatomical structures, making the process both time-consuming and costly. As a\nresult, semi-supervised methods have gained popularity for reducing annotation\ncosts. However, the performance of semi-supervised methods is heavily dependent\non the availability of unannotated data, and their effectiveness declines when\nsuch data are scarce or absent. To overcome this limitation, we propose a\nsimple, yet effective and computationally efficient approach for medical image\nsegmentation that leverages only existing annotations. We propose BoundarySeg ,\na multi-task framework that incorporates organ boundary prediction as an\nauxiliary task to full organ segmentation, leveraging consistency between the\ntwo task predictions to provide additional supervision. This strategy improves\nsegmentation accuracy, especially in low data regimes, allowing our method to\nachieve performance comparable to or exceeding state-of-the-art semi supervised\napproaches all without relying on unannotated data or increasing computational\ndemands. Code will be released upon acceptance.\n","authors":["Tushar Kataria","Shireen Y. Elhabian"],"pdf_url":"https://arxiv.org/pdf/2505.09829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09827v1","updated":"2025-05-14T22:12:34Z","published":"2025-05-14T22:12:34Z","title":"Dyadic Mamba: Long-term Dyadic Human Motion Synthesis","summary":"  Generating realistic dyadic human motion from text descriptions presents\nsignificant challenges, particularly for extended interactions that exceed\ntypical training sequence lengths. While recent transformer-based approaches\nhave shown promising results for short-term dyadic motion synthesis, they\nstruggle with longer sequences due to inherent limitations in positional\nencoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach\nthat leverages State-Space Models (SSMs) to generate high-quality dyadic human\nmotion of arbitrary length. Our method employs a simple yet effective\narchitecture that facilitates information flow between individual motion\nsequences through concatenation, eliminating the need for complex\ncross-attention mechanisms. We demonstrate that Dyadic Mamba achieves\ncompetitive performance on standard short-term benchmarks while significantly\noutperforming transformer-based approaches on longer sequences. Additionally,\nwe propose a new benchmark for evaluating long-term motion synthesis quality,\nproviding a standardized framework for future research. Our results demonstrate\nthat SSM-based architectures offer a promising direction for addressing the\nchallenging task of long-term dyadic human motion synthesis from text\ndescriptions.\n","authors":["Julian Tanke","Takashi Shibuya","Kengo Uchida","Koichi Saito","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2505.09827v1.pdf","comment":"CVPR 2025 HuMoGen Workshop"},{"id":"http://arxiv.org/abs/2505.09819v1","updated":"2025-05-14T21:47:28Z","published":"2025-05-14T21:47:28Z","title":"Visual Feedback of Pattern Separability Improves Myoelectric Decoding\n  Performance of Upper Limb Prostheses","summary":"  State-of-the-art upper limb myoelectric prostheses often use pattern\nrecognition (PR) control systems that translate electromyography (EMG) signals\ninto desired movements. As prosthesis movement complexity increases, users\noften struggle to produce sufficiently distinct EMG patterns for reliable\nclassification. Existing training typically involves heuristic, trial-and-error\nuser adjustments to static decoder boundaries. Goal: We introduce the Reviewer,\na 3D visual interface projecting EMG signals directly into the decoder's\nclassification space, providing intuitive, real-time insight into PR algorithm\nbehavior. This structured feedback reduces cognitive load and fosters mutual,\ndata-driven adaptation between user-generated EMG patterns and decoder\nboundaries. Methods: A 10-session study with 12 able-bodied participants\ncompared PR performance after motor-based training and updating using the\nReviewer versus conventional virtual arm visualization. Performance was\nassessed using a Fitts law task that involved the aperture of the cursor and\nthe control of orientation. Results: Participants trained with the Reviewer\nachieved higher completion rates, reduced overshoot, and improved path\nefficiency and throughput compared to the standard visualization group.\nSignificance: The Reviewer introduces decoder-informed motor training,\nfacilitating immediate and consistent PR-based myoelectric control\nimprovements. By iteratively refining control through real-time feedback, this\napproach reduces reliance on trial-and-error recalibration, enabling a more\nadaptive, self-correcting training framework. Conclusion: The 3D visual\nfeedback significantly improves PR control in novice operators through\nstructured training, enabling feedback-driven adaptation and reducing reliance\non extensive heuristic adjustments.\n","authors":["Ruichen Yang","GyÃ¶rgy M. LÃ©vay","Christopher L. Hunt","DÃ¡niel Czeiner","Megan C. Hodgson","Damini Agarwal","Rahul R. Kaliki","Nitish V. Thakor"],"pdf_url":"https://arxiv.org/pdf/2505.09819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11792v2","updated":"2025-05-14T21:29:57Z","published":"2025-03-14T18:32:02Z","title":"StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model","summary":"  For 3D face modeling, the recently developed 3D-aware neural rendering\nmethods are able to render photorealistic face images with arbitrary viewing\ndirections. The training of the parametric controllable 3D-aware face models,\nhowever, still relies on a large-scale dataset that is lab-collected. To\naddress this issue, this paper introduces \"StyleMorpheus\", the first\nstyle-based neural 3D Morphable Face Model (3DMM) that is trained on\nin-the-wild images. It inherits 3DMM's disentangled controllability (over face\nidentity, expression, and appearance) but without the need for accurately\nreconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder\nstructure. The encoder aims at learning a representative disentangled\nparametric code space and the decoder improves the disentanglement using shape\nand appearance-related style codes in the different sub-modules of the network.\nFurthermore, we fine-tune the decoder through style-based generative\nadversarial learning to achieve photorealistic 3D rendering quality. The\nproposed style-based design enables StyleMorpheus to achieve state-of-the-art\n3D-aware face reconstruction results, while also allowing disentangled control\nof the reconstructed face. Our model achieves real-time rendering speed,\nallowing its use in virtual reality applications. We also demonstrate the\ncapability of the proposed style-based design in face editing applications such\nas style mixing and color editing. Project homepage:\nhttps://github.com/ubc-3d-vision-lab/StyleMorpheus.\n","authors":["Peizhi Yan","Rabab K. Ward","Dan Wang","Qiang Tang","Shan Du"],"pdf_url":"https://arxiv.org/pdf/2503.11792v2.pdf","comment":"13 pages, work was completed in 2023"},{"id":"http://arxiv.org/abs/2410.16430v2","updated":"2025-05-14T19:34:10Z","published":"2024-10-21T18:50:16Z","title":"HaHeAE: Learning Generalisable Joint Representations of Human Hand and\n  Head Movements in Extended Reality","summary":"  Human hand and head movements are the most pervasive input modalities in\nextended reality (XR) and are significant for a wide range of applications.\nHowever, prior works on hand and head modelling in XR only explored a single\nmodality or focused on specific applications. We present HaHeAE - a novel\nself-supervised method for learning generalisable joint representations of hand\nand head movements in XR. At the core of our method is an autoencoder (AE) that\nuses a graph convolutional network-based semantic encoder and a diffusion-based\nstochastic encoder to learn the joint semantic and stochastic representations\nof hand-head movements. It also features a diffusion-based decoder to\nreconstruct the original signals. Through extensive evaluations on three public\nXR datasets, we show that our method 1) significantly outperforms commonly used\nself-supervised methods by up to 74.0% in terms of reconstruction quality and\nis generalisable across users, activities, and XR environments, 2) enables new\napplications, including interpretable hand-head cluster identification and\nvariable hand-head movement generation, and 3) can serve as an effective\nfeature extractor for downstream tasks. Together, these results demonstrate the\neffectiveness of our method and underline the potential of self-supervised\nmethods for jointly modelling hand-head behaviours in extended reality.\n","authors":["Zhiming Hu","Guanhua Zhang","Zheming Yin","Daniel Haeufle","Syn Schmitt","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2410.16430v2.pdf","comment":"Link: https://zhiminghu.net/hu25_haheae"},{"id":"http://arxiv.org/abs/2505.09746v1","updated":"2025-05-14T19:09:17Z","published":"2025-05-14T19:09:17Z","title":"A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the\n  Left Atrium","summary":"  The left atrium (LA) plays a pivotal role in modulating left ventricular\nfilling, but our comprehension of its hemodynamics is significantly limited by\nthe constraints of conventional ultrasound analysis. 4D flow magnetic resonance\nimaging (4D Flow MRI) holds promise for enhancing our understanding of atrial\nhemodynamics. However, the low velocities within the LA and the limited spatial\nresolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,\nthe absence of dedicated computational frameworks, combined with diverse\nacquisition protocols and vendors, complicates gathering large cohorts for\nstudying the prognostic value of hemodynamic parameters provided by 4D Flow\nMRI. In this study, we introduce the first open-source computational framework\ntailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive\nqualitative and quantitative analysis of advanced hemodynamic parameters. Our\nframework proves robust to data from different centers of varying quality,\nproducing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95\n$<$ 3 mm), even with limited training data. Additionally, we conducted the\nfirst comprehensive assessment of energy, vorticity, and pressure parameters in\nthe LA across a spectrum of disorders to investigate their potential as\nprognostic biomarkers.\n","authors":["Xabier Morales","Ayah Elsayed","Debbie Zhao","Filip Loncaric","Ainhoa Aguado","Mireia Masias","Gina Quill","Marc Ramos","Ada Doltra","Ana Garcia","Marta Sitges","David Marlevi","Alistair Young","Martyn Nash","Bart Bijnens","Oscar Camara"],"pdf_url":"https://arxiv.org/pdf/2505.09746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18418v3","updated":"2025-05-14T18:27:05Z","published":"2024-05-28T17:57:23Z","title":"Hierarchical World Models as Visual Whole-Body Humanoid Controllers","summary":"  Whole-body control for humanoids is challenging due to the high-dimensional\nnature of the problem, coupled with the inherent instability of a bipedal\nmorphology. Learning from visual observations further exacerbates this\ndifficulty. In this work, we explore highly data-driven approaches to visual\nwhole-body humanoid control based on reinforcement learning, without any\nsimplifying assumptions, reward design, or skill primitives. Specifically, we\npropose a hierarchical world model in which a high-level agent generates\ncommands based on visual observations for a low-level agent to execute, both of\nwhich are trained with rewards. Our approach produces highly performant control\npolicies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing\nmotions that are broadly preferred by humans.\n","authors":["Nicklas Hansen","Jyothir S V","Vlad Sobal","Yann LeCun","Xiaolong Wang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2405.18418v3.pdf","comment":"Code and videos at https://nicklashansen.com/rlpuppeteer"},{"id":"http://arxiv.org/abs/2505.09615v1","updated":"2025-05-14T17:59:55Z","published":"2025-05-14T17:59:55Z","title":"UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing","summary":"  Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing\nboth uni-modal events (i.e., those occurring exclusively in either the visual\nor acoustic modality of a video) and multi-modal events (i.e., those occurring\nin both modalities concurrently). Moreover, the prohibitive cost of annotating\ntraining data with the class labels of all these events, along with their start\nand end times, imposes constraints on the scalability of AVVP techniques unless\nthey can be trained in a weakly-supervised setting, where only\nmodality-agnostic, video-level labels are available in the training data. To\nthis end, recently proposed approaches seek to generate segment-level\npseudo-labels to better guide model training. However, the absence of\ninter-segment dependencies when generating these pseudo-labels and the general\nbias towards predicting labels that are absent in a segment limit their\nperformance. This work proposes a novel approach towards overcoming these\nweaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video\nParsing (UWAV). Additionally, our innovative approach factors in the\nuncertainty associated with these estimated pseudo-labels and incorporates a\nfeature mixup based training regularization for improved training. Empirical\nresults show that UWAV outperforms state-of-the-art methods for the AVVP task\non multiple metrics, across two different datasets, attesting to its\neffectiveness and generalizability.\n","authors":["Yung-Hsuan Lai","Janek Ebbers","Yu-Chiang Frank Wang","FranÃ§ois Germain","Michael Jeffrey Jones","Moitreya Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2505.09615v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2505.09608v1","updated":"2025-05-14T17:57:27Z","published":"2025-05-14T17:57:27Z","title":"LightLab: Controlling Light Sources in Images with Diffusion Models","summary":"  We present a simple, yet effective diffusion-based method for fine-grained,\nparametric control over light sources in an image. Existing relighting methods\neither rely on multiple input views to perform inverse rendering at inference\ntime, or fail to provide explicit control over light changes. Our method\nfine-tunes a diffusion model on a small set of real raw photograph pairs,\nsupplemented by synthetically rendered images at scale, to elicit its\nphotorealistic prior for relighting. We leverage the linearity of light to\nsynthesize image pairs depicting controlled light changes of either a target\nlight source or ambient illumination. Using this data and an appropriate\nfine-tuning scheme, we train a model for precise illumination changes with\nexplicit control over light intensity and color. Lastly, we show how our method\ncan achieve compelling light editing results, and outperforms existing methods\nbased on user preference.\n","authors":["Nadav Magar","Amir Hertz","Eric Tabellion","Yael Pritch","Alex Rav-Acha","Ariel Shamir","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2505.09608v1.pdf","comment":"Project Page: https://nadmag.github.io/LightLab/"},{"id":"http://arxiv.org/abs/2503.21696v2","updated":"2025-05-14T17:48:02Z","published":"2025-03-27T17:00:51Z","title":"Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks","summary":"  Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.\n","authors":["Wenqi Zhang","Mengna Wang","Gangao Liu","Xu Huixin","Yiwei Jiang","Yongliang Shen","Guiyang Hou","Zhe Zheng","Hang Zhang","Xin Li","Weiming Lu","Peng Li","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2503.21696v2.pdf","comment":"Code: https://github.com/zwq2018/embodied_reasoner Dataset:\n  https://huggingface.co/datasets/zwq2018/embodied_reasoner"},{"id":"http://arxiv.org/abs/2505.09591v1","updated":"2025-05-14T17:40:22Z","published":"2025-05-14T17:40:22Z","title":"Variational Visual Question Answering","summary":"  Despite remarkable progress in multimodal models for Visual Question\nAnswering (VQA), there remain major reliability concerns because the models can\noften be overconfident and miscalibrated, especially in out-of-distribution\n(OOD) settings. Plenty has been done to address such issues for unimodal\nmodels, but little work exists for multimodal cases. Here, we address\nunreliability in multimodal models by proposing a Variational VQA approach.\nSpecifically, instead of fine-tuning vision-language models by using AdamW, we\nemploy a recently proposed variational algorithm called IVON, which yields a\nposterior distribution over model parameters. Through extensive experiments, we\nshow that our approach improves calibration and abstentions without sacrificing\nthe accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce\nExpected Calibration Error by more than 50% compared to the AdamW baseline and\nraise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of\ndistribution shifts, the performance gain is even higher, achieving 8% Coverage\n(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we\npresent variational learning as a viable option to enhance the reliability of\nmultimodal models.\n","authors":["Tobias Jan Wieczorek","Nathalie Daun","Mohammad Emtiyaz Khan","Marcus Rohrbach"],"pdf_url":"https://arxiv.org/pdf/2505.09591v1.pdf","comment":"19 pages, 16 figures, under review at ICCV 2025"},{"id":"http://arxiv.org/abs/2410.07795v4","updated":"2025-05-14T17:22:25Z","published":"2024-10-10T10:24:59Z","title":"Optimal-state Dynamics Estimation for Physics-based Human Motion Capture\n  from Videos","summary":"  Human motion capture from monocular videos has made significant progress in\nrecent years. However, modern approaches often produce temporal artifacts, e.g.\nin form of jittery motion and struggle to achieve smooth and physically\nplausible motions. Explicitly integrating physics, in form of internal forces\nand exterior torques, helps alleviating these artifacts. Current\nstate-of-the-art approaches make use of an automatic PD controller to predict\ntorques and reaction forces in order to re-simulate the input kinematics, i.e.\nthe joint angles of a predefined skeleton. However, due to imperfect physical\nmodels, these methods often require simplifying assumptions and extensive\npreprocessing of the input kinematics to achieve good performance. To this end,\nwe propose a novel method to selectively incorporate the physics models with\nthe kinematics observations in an online setting, inspired by a neural\nKalman-filtering approach. We develop a control loop as a meta-PD controller to\npredict internal joint torques and external reaction forces, followed by a\nphysics-based motion simulation. A recurrent neural network is introduced to\nrealize a Kalman filter that attentively balances the kinematics input and\nsimulated motion, resulting in an optimal-state dynamics prediction. We show\nthat this filtering step is crucial to provide an online supervision that helps\nbalancing the shortcoming of the respective input motions, thus being important\nfor not only capturing accurate global motion trajectories but also producing\nphysically plausible human poses. The proposed approach excels in the\nphysics-based human pose estimation task and demonstrates the physical\nplausibility of the predictive dynamics, compared to state of the art. The code\nis available on https://github.com/cuongle1206/OSDCap\n","authors":["Cuong Le","Viktor Johansson","Manon Kok","Bastian Wandt"],"pdf_url":"https://arxiv.org/pdf/2410.07795v4.pdf","comment":"17 pages, 7 figure, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2505.09571v1","updated":"2025-05-14T17:15:03Z","published":"2025-05-14T17:15:03Z","title":"Don't Forget your Inverse DDIM for Image Editing","summary":"  The field of text-to-image generation has undergone significant advancements\nwith the introduction of diffusion models. Nevertheless, the challenge of\nediting real images persists, as most methods are either computationally\nintensive or produce poor reconstructions. This paper introduces SAGE\n(Self-Attention Guidance for image Editing) - a novel technique leveraging\npre-trained diffusion models for image editing. SAGE builds upon the DDIM\nalgorithm and incorporates a novel guidance mechanism utilizing the\nself-attention layers of the diffusion U-Net. This mechanism computes a\nreconstruction objective based on attention maps generated during the inverse\nDDIM process, enabling efficient reconstruction of unedited regions without the\nneed to precisely reconstruct the entire input image. Thus, SAGE directly\naddresses the key challenges in image editing. The superiority of SAGE over\nother methods is demonstrated through quantitative and qualitative evaluations\nand confirmed by a statistically validated comprehensive user study, in which\nall 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE\nranks as the top-performing method in seven out of 10 quantitative analyses and\nsecures second and third places in the remaining three.\n","authors":["Guillermo Gomez-Trenado","Pablo Mesejo","Oscar CordÃ³n","StÃ©phane LathuiliÃ¨re"],"pdf_url":"https://arxiv.org/pdf/2505.09571v1.pdf","comment":"12 pages, 12 figures, code available at\n  https://guillermogotre.github.io/sage/"},{"id":"http://arxiv.org/abs/2505.09568v1","updated":"2025-05-14T17:11:07Z","published":"2025-05-14T17:11:07Z","title":"BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset","summary":"  Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.\n","authors":["Jiuhai Chen","Zhiyang Xu","Xichen Pan","Yushi Hu","Can Qin","Tom Goldstein","Lifu Huang","Tianyi Zhou","Saining Xie","Silvio Savarese","Le Xue","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2505.09568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09565v1","updated":"2025-05-14T17:07:37Z","published":"2025-05-14T17:07:37Z","title":"Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using\n  Implicit Neural Representations","summary":"  High-resolution slice-to-volume reconstruction (SVR) from multiple\nmotion-corrupted low-resolution 2D slices constitutes a critical step in\nimage-based diagnostics of moving subjects, such as fetal brain Magnetic\nResonance Imaging (MRI). Existing solutions struggle with image artifacts and\nsevere subject motion or require slice pre-alignment to achieve satisfying\nreconstruction performance. We propose a novel SVR method to enable fast and\naccurate MRI reconstruction even in cases of severe image and motion\ncorruption. Our approach performs motion correction, outlier handling, and\nsuper-resolution reconstruction with all operations being entirely based on\nimplicit neural representations. The model can be initialized with\ntask-specific priors through fully self-supervised meta-learning on either\nsimulated or real-world data. In extensive experiments including over 480\nreconstructions of simulated and clinical MRI brain data from different\ncenters, we prove the utility of our method in cases of severe subject motion\nand image artifacts. Our results demonstrate improvements in reconstruction\nquality, especially in the presence of severe motion, compared to\nstate-of-the-art methods, and up to 50% reduction in reconstruction time.\n","authors":["Maik Dannecker","Thomas Sanchez","Meritxell Bach Cuadra","ÃzgÃ¼n Turgut","Anthony N. Price","Lucilio Cordero-Grande","Vanessa Kyriakopoulou","Joseph V. Hajnal","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2505.09565v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.09564v1","updated":"2025-05-14T17:07:30Z","published":"2025-05-14T17:07:30Z","title":"Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D\n  Cardiac CT Segmentation","summary":"  Cardiac image segmentation is an important step in many cardiac image\nanalysis and modeling tasks such as motion tracking or simulations of cardiac\nmechanics. While deep learning has greatly advanced segmentation in clinical\nsettings, there is limited work on pre-clinical imaging, notably in porcine\nmodels, which are often used due to their anatomical and physiological\nsimilarity to humans. However, differences between species create a domain\nshift that complicates direct model transfer from human to pig data.\n  Recently, foundation models trained on large human datasets have shown\npromise for robust medical image segmentation; yet their applicability to\nporcine data remains largely unexplored. In this work, we investigate whether\nfoundation models can generate sufficiently accurate pseudo-labels for pig\ncardiac CT and propose a simple self-training approach to iteratively refine\nthese labels. Our method requires no manually annotated pig data, relying\ninstead on iterative updates to improve segmentation quality. We demonstrate\nthat this self-training process not only enhances segmentation accuracy but\nalso smooths out temporal inconsistencies across consecutive frames. Although\nour results are encouraging, there remains room for improvement, for example by\nincorporating more sophisticated self-training strategies and by exploring\nadditional foundation models and other cardiac imaging technologies.\n","authors":["Anne-Marie Rickmann","Stephanie L. Thorn","Shawn S. Ahn","Supum Lee","Selen Uman","Taras Lysyy","Rachel Burns","Nicole Guerrera","Francis G. Spinale","Jason A. Burdick","Albert J. Sinusas","James S. Duncan"],"pdf_url":"https://arxiv.org/pdf/2505.09564v1.pdf","comment":"accepted at FIMH 2025"},{"id":"http://arxiv.org/abs/2505.09562v1","updated":"2025-05-14T17:05:12Z","published":"2025-05-14T17:05:12Z","title":"Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through\n  Differentiable Object Shapes","summary":"  Autonomous vehicles need a complete map of their surroundings to plan and\nact. This has sparked research into the tasks of 3D occupancy prediction, 3D\nscene completion, and 3D panoptic scene completion, which predict a dense map\nof the ego vehicle's surroundings as a voxel grid. Scene completion extends\noccupancy prediction by predicting occluded regions of the voxel grid, and\npanoptic scene completion further extends this task by also distinguishing\nobject instances within the same class; both aspects are crucial for path\nplanning and decision-making. However, 3D panoptic scene completion is\ncurrently underexplored. This work introduces a novel framework for 3D panoptic\nscene completion that extends existing 3D semantic scene completion models. We\npropose an Object Module and Panoptic Module that can easily be integrated with\n3D occupancy and scene completion methods presented in the literature. Our\napproach leverages the available annotations in occupancy benchmarks, allowing\nindividual object shapes to be learned as a differentiable problem. The code is\navailable at https://github.com/nicolamarinello/OffsetOcc .\n","authors":["Nicola Marinello","Simen Cassiman","Jonas Heylen","Marc Proesmans","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2505.09562v1.pdf","comment":"Accepted to CVPR 2025 Workshop on Autonomous Driving"},{"id":"http://arxiv.org/abs/2505.09529v1","updated":"2025-05-14T16:24:22Z","published":"2025-05-14T16:24:22Z","title":"Contactless Cardiac Pulse Monitoring Using Event Cameras","summary":"  Time event cameras are a novel technology for recording scene information at\nextremely low latency and with low power consumption. Event cameras output a\nstream of events that encapsulate pixel-level light intensity changes within\nthe scene, capturing information with a higher dynamic range and temporal\nresolution than traditional cameras. This study investigates the contact-free\nreconstruction of an individual's cardiac pulse signal from time event\nrecording of their face using a supervised convolutional neural network (CNN)\nmodel. An end-to-end model is trained to extract the cardiac signal from a\ntwo-dimensional representation of the event stream, with model performance\nevaluated based on the accuracy of the calculated heart rate. The experimental\nresults confirm that physiological cardiac information in the facial region is\neffectively preserved within the event stream, showcasing the potential of this\nnovel sensor for remote heart rate monitoring. The model trained on event\nframes achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)\ncompared to the RMSE of 2.92 bpm achieved by the baseline model trained on\nstandard camera frames. Furthermore, models trained on event frames generated\nat 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an\nRMSE of 2.54 and 2.13 bpm, respectively.\n","authors":["Mohamed Moustafa","Joseph Lemley","Peter Corcoran"],"pdf_url":"https://arxiv.org/pdf/2505.09529v1.pdf","comment":"This paper is a preprint of a paper submitted to IEEE Access and is\n  currently under review"},{"id":"http://arxiv.org/abs/2505.09528v1","updated":"2025-05-14T16:23:26Z","published":"2025-05-14T16:23:26Z","title":"Conformal Bounds on Full-Reference Image Quality for Imaging Inverse\n  Problems","summary":"  In imaging inverse problems, we would like to know how close the recovered\nimage is to the true image in terms of full-reference image quality (FRIQ)\nmetrics like PSNR, SSIM, LPIPS, etc. This is especially important in\nsafety-critical applications like medical imaging, where knowing that, say, the\nSSIM was poor could potentially avoid a costly misdiagnosis. But since we don't\nknow the true image, computing FRIQ is non-trivial. In this work, we combine\nconformal prediction with approximate posterior sampling to construct bounds on\nFRIQ that are guaranteed to hold up to a user-specified error probability. We\ndemonstrate our approach on image denoising and accelerated magnetic resonance\nimaging (MRI) problems. Code is available at\nhttps://github.com/jwen307/quality_uq.\n","authors":["Jeffrey Wen","Rizwan Ahmad","Philip Schniter"],"pdf_url":"https://arxiv.org/pdf/2505.09528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09521v1","updated":"2025-05-14T16:18:21Z","published":"2025-05-14T16:18:21Z","title":"Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI\n  Reconstruction based on Multi-directional Time-Frequency Convolutional\n  Attention Encoder and Vision-Mamba U-Net","summary":"  High-resolution functional magnetic resonance imaging (fMRI) is essential for\nmapping human brain activity; however, it remains costly and logistically\nchallenging. If comparable volumes could be generated directly from widely\navailable scalp electroencephalography (EEG), advanced neuroimaging would\nbecome significantly more accessible. Existing EEG-to-fMRI generators rely on\nplain CNNs that fail to capture cross-channel time-frequency cues or on heavy\ntransformer/GAN decoders that strain memory and stability. We propose\nSpec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts\nthese issues via a Multi-directional Time-Frequency Convolutional Attention\nEncoder, stacking temporal, spectral and joint convolutions with\nself-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space\nblocks enable efficient long-range spatial modelling. Trained end-to-end with a\nhybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on\nthree public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball\nand 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%\nrespectively over previous best SSIM scores. Furthermore, it achieves\ncompetitive PSNR scores, particularly excelling on the CN-EPFL dataset with a\n4.6% improvement over the previous best PSNR, thus striking a better balance in\nreconstruction quality. The proposed model is lightweight and efficient, making\nit suitable for real-time applications in clinical and research settings. The\ncode is available at https://github.com/hdy6438/Spec2VolCAMU-Net.\n","authors":["Dongyi He","Shiyang Li","Bin Jiang","He Yan"],"pdf_url":"https://arxiv.org/pdf/2505.09521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00810v3","updated":"2025-05-14T16:07:36Z","published":"2023-11-01T19:49:32Z","title":"A Call to Arms: AI Should be Critical for Social Media Analysis of\n  Conflict Zones","summary":"  The massive proliferation of social media data represents a transformative\nopportunity for conflict studies and for tracking the proliferation and use of\nweaponry, as conflicts are increasingly documented in these online spaces. At\nthe same time, the scale and types of data available are problematic for\ntraditional open-source intelligence. This paper focuses on identifying\nspecific weapon systems and the insignias of the armed groups using them as\ndocumented in the Ukraine war, as these tasks are critical to operational\nintelligence and tracking weapon proliferation, especially given the scale of\ninternational military aid given to Ukraine. The large scale of social media\nmakes manual assessment difficult, however, so this paper presents early work\nthat uses computer vision models to support this task. We demonstrate that\nthese models can both identify weapons embedded in images shared in social\nmedia and how the resulting collection of military-relevant images and their\npost times interact with the offline, real-world conflict. Not only can we then\ntrack changes in the prevalence of images of tanks, land mines, military\ntrucks, etc., we find correlations among time series data associated with these\nimages and the daily fatalities in this conflict. This work shows substantial\nopportunity for examining similar online documentation of conflict contexts,\nand we also point to future avenues where computer vision can be further\nimproved for these open-source intelligence tasks.\n","authors":["Afia Abedin","Abdul Bais","Cody Buntain","Laura Courchesne","Brian McQuinn","Matthew E. Taylor","Muhib Ullah"],"pdf_url":"https://arxiv.org/pdf/2311.00810v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18769v5","updated":"2025-05-14T16:01:10Z","published":"2024-09-27T14:14:16Z","title":"State-of-the-Art Periorbital Distance Prediction and Disease\n  Classification Using Periorbital Features","summary":"  Periorbital distances are critical markers for diagnosing and monitoring a\nrange of oculoplastic and craniofacial conditions. Manual measurement, however,\nis subjective and prone to intergrader variability. Automated methods have been\ndeveloped but remain limited by standardized imaging requirements, small\ndatasets, and a narrow focus on individual measurements. We developed a\nsegmentation pipeline trained on a domain-specific dataset of healthy eyes and\ncompared its performance against the Segment Anything Model (SAM) and the prior\nbenchmark, PeriorbitAI. Segmentation accuracy was evaluated across multiple\ndisease classes and imaging conditions. We further investigated the use of\npredicted periorbital distances as features for disease classification under\nin-distribution (ID) and out-of-distribution (OOD) settings, comparing shallow\nclassifiers, CNNs, and fusion models. Our segmentation model achieved\nstate-of-the-art accuracy across all datasets, with error rates within\nintergrader variability and superior performance relative to SAM and\nPeriorbitAI. In classification tasks, models trained on periorbital distances\nmatched CNN performance on ID data (77--78\\% accuracy) and substantially\noutperformed CNNs under OOD conditions (63--68\\% accuracy vs. 14\\%). Fusion\nmodels achieved the highest ID accuracy (80\\%) but were sensitive to degraded\nCNN features under OOD shifts. Segmentation-derived periorbital distances\nprovide robust, explainable features for disease classification and generalize\nbetter under domain shift than CNN image classifiers. These results establish a\nnew benchmark for periorbital distance prediction and highlight the potential\nof anatomy-based AI pipelines for real-world deployment in oculoplastic and\ncraniofacial care.\n","authors":["George R. Nahass","Sasha Hubschman","Jeffrey C. Peterson","Ghasem Yazdanpanah","Nicholas Tomaras","Madison Cheung","Alex Palacios","Kevin Heinze","Chad A. Purnell","Pete Setabutr","Ann Q. Tran","Darvin Yi"],"pdf_url":"https://arxiv.org/pdf/2409.18769v5.pdf","comment":"25 pages, 12 figures, 16 tables"},{"id":"http://arxiv.org/abs/2505.09498v1","updated":"2025-05-14T15:45:17Z","published":"2025-05-14T15:45:17Z","title":"Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low\n  Latency and High Throughput","summary":"  In this paper, we introduce Flash-VL 2B, a novel approach to optimizing\nVision-Language Models (VLMs) for real-time applications, targeting ultra-low\nlatency and high throughput without sacrificing accuracy. Leveraging advanced\narchitectural enhancements and efficient computational strategies, Flash-VL 2B\nis designed to maximize throughput by reducing processing time while\nmaintaining competitive performance across multiple vision-language benchmarks.\nOur approach includes tailored architectural choices, token compression\nmechanisms, data curation, training schemes, and a novel image processing\ntechnique called implicit semantic stitching that effectively balances\ncomputational load and model performance. Through extensive evaluations on 11\nstandard VLM benchmarks, we demonstrate that Flash-VL 2B achieves\nstate-of-the-art results in both speed and accuracy, making it a promising\nsolution for deployment in resource-constrained environments and large-scale\nreal-time applications.\n","authors":["Bo Zhang","Shuo Li","Runhe Tian","Yang Yang","Jixin Tang","Jinhao Zhou","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2505.09498v1.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.09484v1","updated":"2025-05-14T15:36:44Z","published":"2025-05-14T15:36:44Z","title":"Denoising and Alignment: Rethinking Domain Generalization for Multimodal\n  Face Anti-Spoofing","summary":"  Face Anti-Spoofing (FAS) is essential for the security of facial recognition\nsystems in diverse scenarios such as payment processing and surveillance.\nCurrent multimodal FAS methods often struggle with effective generalization,\nmainly due to modality-specific biases and domain shifts. To address these\nchallenges, we introduce the \\textbf{M}ulti\\textbf{m}odal \\textbf{D}enoising\nand \\textbf{A}lignment (\\textbf{MMDA}) framework. By leveraging the zero-shot\ngeneralization capability of CLIP, the MMDA framework effectively suppresses\nnoise in multimodal data through denoising and alignment mechanisms, thereby\nsignificantly enhancing the generalization performance of cross-modal\nalignment. The \\textbf{M}odality-\\textbf{D}omain Joint \\textbf{D}ifferential\n\\textbf{A}ttention (\\textbf{MD2A}) module in MMDA concurrently mitigates the\nimpacts of domain and modality noise by refining the attention mechanism based\non extracted common noise features. Furthermore, the \\textbf{R}epresentation\n\\textbf{S}pace \\textbf{S}oft (\\textbf{RS2}) Alignment strategy utilizes the\npre-trained CLIP model to align multi-domain multimodal data into a generalized\nrepresentation space in a flexible manner, preserving intricate representations\nand enhancing the model's adaptability to various unseen conditions. We also\ndesign a \\textbf{U}-shaped \\textbf{D}ual \\textbf{S}pace \\textbf{A}daptation\n(\\textbf{U-DSA}) module to enhance the adaptability of representations while\nmaintaining generalization performance. These improvements not only enhance the\nframework's generalization capabilities but also boost its ability to represent\ncomplex representations. Our experimental results on four benchmark datasets\nunder different evaluation protocols demonstrate that the MMDA framework\noutperforms existing state-of-the-art methods in terms of cross-domain\ngeneralization and multimodal detection accuracy. The code will be released\nsoon.\n","authors":["Yingjie Ma","Xun Lin","Zitong Yu","Xin Liu","Xiaochen Yuan","Weicheng Xie","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2505.09484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09466v1","updated":"2025-05-14T15:17:34Z","published":"2025-05-14T15:17:34Z","title":"A 2D Semantic-Aware Position Encoding for Vision Transformers","summary":"  Vision transformers have demonstrated significant advantages in computer\nvision tasks due to their ability to capture long-range dependencies and\ncontextual relationships through self-attention. However, existing position\nencoding techniques, which are largely borrowed from natural language\nprocessing, fail to effectively capture semantic-aware positional relationships\nbetween image patches. Traditional approaches like absolute position encoding\nand relative position encoding primarily focus on 1D linear position\nrelationship, often neglecting the semantic similarity between distant yet\ncontextually related patches. These limitations hinder model generalization,\ntranslation equivariance, and the ability to effectively handle repetitive or\nstructured patterns in images. In this paper, we propose 2-Dimensional\nSemantic-Aware Position Encoding ($\\text{SaPE}^2$), a novel position encoding\nmethod with semantic awareness that dynamically adapts position representations\nby leveraging local content instead of fixed linear position relationship or\nspatial coordinates. Our method enhances the model's ability to generalize\nacross varying image resolutions and scales, improves translation equivariance,\nand better aggregates features for visually similar but spatially distant\npatches. By integrating $\\text{SaPE}^2$ into vision transformers, we bridge the\ngap between position encoding and perceptual similarity, thereby improving\nperformance on computer vision tasks.\n","authors":["Xi Chen","Shiyang Zhou","Muqi Huang","Jiaxu Feng","Yun Xiong","Kun Zhou","Biao Yang","Yuhui Zhang","Huishuai Bao","Sijia Peng","Chuan Li","Feng Shi"],"pdf_url":"https://arxiv.org/pdf/2505.09466v1.pdf","comment":"14 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2505.09455v1","updated":"2025-05-14T15:05:36Z","published":"2025-05-14T15:05:36Z","title":"Beyond Pixels: Leveraging the Language of Soccer to Improve\n  Spatio-Temporal Action Detection in Broadcast Videos","summary":"  State-of-the-art spatio-temporal action detection (STAD) methods show\npromising results for extracting soccer events from broadcast videos. However,\nwhen operated in the high-recall, low-precision regime required for exhaustive\nevent coverage in soccer analytics, their lack of contextual understanding\nbecomes apparent: many false positives could be resolved by considering a\nbroader sequence of actions and game-state information. In this work, we\naddress this limitation by reasoning at the game level and improving STAD\nthrough the addition of a denoising sequence transduction task. Sequences of\nnoisy, context-free player-centric predictions are processed alongside clean\ngame state information using a Transformer-based encoder-decoder model. By\nmodeling extended temporal context and reasoning jointly over team-level\ndynamics, our method leverages the \"language of soccer\" - its tactical\nregularities and inter-player dependencies - to generate \"denoised\" sequences\nof actions. This approach improves both precision and recall in low-confidence\nregimes, enabling more reliable event extraction from broadcast video and\ncomplementing existing pixel-based methods.\n","authors":["Jeremie Ochin","Raphael Chekroun","Bogdan Stanciulescu","Sotiris Manitsaris"],"pdf_url":"https://arxiv.org/pdf/2505.09455v1.pdf","comment":"12 pages, submitted to Advanced Concepts for Intelligent Vision\n  Systems 2025"},{"id":"http://arxiv.org/abs/2505.09450v1","updated":"2025-05-14T15:01:59Z","published":"2025-05-14T15:01:59Z","title":"MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating\n  Motion during Ultrasound-Guided Aspiration Biopsy","summary":"  Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally\ninvasive diagnostic procedure. However, an aspiration needle tracker addressing\nrapid reciprocating motion is still missing. MrTrack, an aspiration needle\ntracker with a mamba-based register mechanism, is proposed. MrTrack leverages a\nMamba-based register extractor to sequentially distill global context from each\nhistorical search map, storing these temporal cues in a register bank. The\nMamba-based register retriever then retrieves temporal prompts from the\nregister bank to provide external cues when current vision features are\ntemporarily unusable due to rapid reciprocating motion and imaging degradation.\nA self-supervised register diversify loss is proposed to encourage feature\ndiversity and dimension independence within the learned register, mitigating\nfeature collapse. Comprehensive experiments conducted on both motorized and\nmanual aspiration datasets demonstrate that MrTrack not only outperforms\nstate-of-the-art trackers in accuracy and robustness but also achieves superior\ninference efficiency.\n","authors":["Yuelin Zhang","Qingpeng Ding","Long Lei","Yongxuan Feng","Raymond Shing-Yan Tang","Shing Shin Cheng"],"pdf_url":"https://arxiv.org/pdf/2505.09450v1.pdf","comment":"Early Accepted by MICCAI 2025"},{"id":"http://arxiv.org/abs/2412.01986v2","updated":"2025-05-14T14:57:42Z","published":"2024-12-02T21:35:33Z","title":"HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh\n  Quality Assessment","summary":"  Mesh quality assessment (MQA) models play a critical role in the design,\noptimization, and evaluation of mesh operation systems in a wide variety of\napplications. Current MQA models, whether model-based methods using\ntopology-aware features or projection-based approaches working on rendered 2D\nprojections, often fail to capture the intricate interactions between texture\nand 3D geometry. We introduce HybridMQA, a first-of-its-kind hybrid\nfull-reference colored MQA framework that integrates model-based and\nprojection-based approaches, capturing complex interactions between textural\ninformation and 3D structures for enriched quality representations. Our method\nemploys graph learning to extract detailed 3D representations, which are then\nprojected to 2D using a novel feature rendering process that precisely aligns\nthem with colored projections. This enables the exploration of geometry-texture\ninteractions via cross-attention, producing comprehensive mesh quality\nrepresentations. Extensive experiments demonstrate HybridMQA's superior\nperformance across diverse datasets, highlighting its ability to effectively\nleverage geometry-texture interactions for a thorough understanding of mesh\nquality. Our implementation will be made publicly available.\n","authors":["Armin Shafiee Sarvestani","Sheyang Tang","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2412.01986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07409v3","updated":"2025-05-14T14:57:00Z","published":"2025-02-11T09:42:13Z","title":"MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for\n  Few-Shot WSI Classification","summary":"  Whole slide pathology image classification presents challenges due to\ngigapixel image sizes and limited annotation labels, hindering model\ngeneralization. This paper introduces a prompt learning method to adapt large\nvision-language models for few-shot pathology classification. We first extend\nthe Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology\nimage tiles, into a vision-language model by adding adaptors and aligning it\nwith medical text encoders via contrastive learning on 923K image-text pairs.\nThe model is then used to extract visual features and text embeddings from\nfew-shot annotations and fine-tunes with learnable prompt embeddings. Unlike\nprior methods that combine prompts with frozen features using prefix embeddings\nor self-attention, we propose multi-granular attention that compares\ninteractions between learnable prompts with individual image patches and groups\nof them. This approach improves the model's ability to capture both\nfine-grained details and broader context, enhancing its recognition of complex\npatterns across sub-regions. To further improve accuracy, we leverage\n(unbalanced) optimal transport-based visual-text distance to secure model\nrobustness by mitigating perturbations that might occur during the data\naugmentation process. Empirical experiments on lung, kidney, and breast\npathology modalities validate the effectiveness of our approach; thereby, we\nsurpass several of the latest competitors and consistently improve performance\nacross diverse architectures, including CLIP, PLIP, and Prov-GigaPath\nintegrated PLIP. We release our implementations and pre-trained models at this\nMGPATH.\n","authors":["Anh-Tien Nguyen","Duy Minh Ho Nguyen","Nghiem Tuong Diep","Trung Quoc Nguyen","Nhat Ho","Jacqueline Michelle Metsch","Miriam Cindy Maurer","Daniel Sonntag","Hanibal Bohnenberger","Anne-Christin Hauschild"],"pdf_url":"https://arxiv.org/pdf/2502.07409v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09435v1","updated":"2025-05-14T14:43:31Z","published":"2025-05-14T14:43:31Z","title":"Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy\n  Records","summary":"  Pre-training on image-text colonoscopy records offers substantial potential\nfor improving endoscopic image analysis, but faces challenges including\nnon-informative background images, complex medical terminology, and ambiguous\nmulti-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised\nframework that enhances Contrastive Language-Image Pre-training (CLIP) for this\ndomain. Endo-CLIP's three-stage framework--cleansing, attunement, and\nunification--addresses these challenges by (1) removing background frames, (2)\nleveraging large language models to extract clinical attributes for\nfine-grained contrastive learning, and (3) employing patient-level\ncross-attention to resolve multi-polyp ambiguities. Extensive experiments\ndemonstrate that Endo-CLIP significantly outperforms state-of-the-art\npre-training methods in zero-shot and few-shot polyp detection and\nclassification, paving the way for more accurate and clinically relevant\nendoscopic analysis.\n","authors":["Yili He","Yan Zhu","Peiyao Fu","Ruijie Yang","Tianyi Chen","Zhihua Wang","Quanlin Li","Pinghong Zhou","Xian Yang","Shuo Wang"],"pdf_url":"https://arxiv.org/pdf/2505.09435v1.pdf","comment":"Early accepted to MICCAI 2025"},{"id":"http://arxiv.org/abs/2505.09433v1","updated":"2025-05-14T14:38:40Z","published":"2025-05-14T14:38:40Z","title":"Efficient LiDAR Reflectance Compression via Scanning Serialization","summary":"  Reflectance attributes in LiDAR point clouds provide essential information\nfor downstream tasks but remain underexplored in neural compression methods. To\naddress this, we introduce SerLiC, a serialization-based neural compression\nframework to fully exploit the intrinsic characteristics of LiDAR reflectance.\nSerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order\nserialization, offering a device-centric perspective for reflectance analysis.\nEach point is then tokenized into a contextual representation comprising its\nsensor scanning index, radial distance, and prior reflectance, for effective\ndependencies exploration. For efficient sequential modeling, Mamba is\nincorporated with a dual parallelization scheme, enabling simultaneous\nautoregressive dependency capture and fast processing. Extensive experiments\ndemonstrate that SerLiC attains over 2x volume reduction against the original\nreflectance data, outperforming the state-of-the-art method by up to 22%\nreduction of compressed bits while using only 2% of its parameters. Moreover, a\nlightweight version of SerLiC achieves > 10 fps (frames per second) with just\n111K parameters, which is attractive for real-world applications.\n","authors":["Jiahao Zhu","Kang You","Dandan Ding","Zhan Ma"],"pdf_url":"https://arxiv.org/pdf/2505.09433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03370v2","updated":"2025-05-14T14:34:47Z","published":"2024-03-05T23:32:26Z","title":"F$^3$Loc: Fusion and Filtering for Floorplan Localization","summary":"  In this paper we propose an efficient data-driven solution to\nself-localization within a floorplan. Floorplan data is readily available,\nlong-term persistent and inherently robust to changes in the visual appearance.\nOur method does not require retraining per map and location or demand a large\ndatabase of images of the area of interest. We propose a novel probabilistic\nmodel consisting of an observation and a novel temporal filtering module.\nOperating internally with an efficient ray-based representation, the\nobservation module consists of a single and a multiview module to predict\nhorizontal depth from images and fuses their results to benefit from advantages\noffered by either methodology. Our method operates on conventional consumer\nhardware and overcomes a common limitation of competing methods that often\ndemand upright images. Our full system meets real-time requirements, while\noutperforming the state-of-the-art by a significant margin.\n","authors":["Changan Chen","Rui Wang","Christoph Vogel","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2403.03370v2.pdf","comment":"10 pages, 11 figure, accepted to CVPR 2024 (fixed typo eq.8: s_x,s_y,\n  s_phi -> x, y, phi)"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.09861v1","updated":"2025-05-14T23:54:57Z","published":"2025-05-14T23:54:57Z","title":"LiDDA: Data Driven Attribution at LinkedIn","summary":"  Data Driven Attribution, which assigns conversion credits to marketing\ninteractions based on causal patterns learned from data, is the foundation of\nmodern marketing intelligence and vital to any marketing businesses and\nadvertising platform. In this paper, we introduce a unified transformer-based\nattribution approach that can handle member-level data, aggregate-level data,\nand integration of external macro factors. We detail the large scale\nimplementation of the approach at LinkedIn, showcasing significant impact. We\nalso share learning and insights that are broadly applicable to the marketing\nand ad tech fields.\n","authors":["John Bencina","Erkut Aykutlug","Yue Chen","Zerui Zhang","Stephanie Sorenson","Shao Tang","Changshuai Wei"],"pdf_url":"https://arxiv.org/pdf/2505.09861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09847v1","updated":"2025-05-14T23:12:20Z","published":"2025-05-14T23:12:20Z","title":"Causal Predictive Optimization and Generation for Business AI","summary":"  The sales process involves sales functions converting leads or opportunities\nto customers and selling more products to existing customers. The optimization\nof the sales process thus is key to success of any B2B business. In this work,\nwe introduce a principled approach to sales optimization and business AI,\nnamely the Causal Predictive Optimization and Generation, which includes three\nlayers: 1) prediction layer with causal ML 2) optimization layer with\nconstraint optimization and contextual bandit 3) serving layer with Generative\nAI and feedback-loop for system enhancement. We detail the implementation and\ndeployment of the system in LinkedIn, showcasing significant wins over legacy\nsystems and sharing learning and insight broadly applicable to this field.\n","authors":["Liyang Zhao","Olurotimi Seton","Himadeep Reddy Reddivari","Suvendu Jena","Shadow Zhao","Rachit Kumar","Changshuai Wei"],"pdf_url":"https://arxiv.org/pdf/2505.09847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08911v3","updated":"2025-05-14T22:08:01Z","published":"2024-12-12T03:47:40Z","title":"Goal-Conditioned Supervised Learning for Multi-Objective Recommendation","summary":"  Multi-objective learning endeavors to concurrently optimize multiple\nobjectives using a single model, aiming to achieve high and balanced\nperformance across diverse objectives. However, this often entails a more\ncomplex optimization problem, particularly when navigating potential conflicts\nbetween objectives, leading to solutions with higher memory requirements and\ncomputational complexity. This paper introduces a Multi-Objective\nGoal-Conditioned Supervised Learning (MOGCSL) framework for automatically\nlearning to achieve multiple objectives from offline sequential data. MOGCSL\nextends the conventional GCSL method to multi-objective scenarios by redefining\ngoals from one-dimensional scalars to multi-dimensional vectors. It benefits\nfrom naturally eliminating the need for complex architectures and optimization\nconstraints. Moreover, MOGCSL effectively filters out uninformative or noisy\ninstances that fail to achieve desirable long-term rewards across multiple\nobjectives. We also introduces a novel goal-selection algorithm for MOGCSL to\nmodel and identify \"high\" achievable goals for inference.\n  While MOGCSL is quite general, we focus on its application to the next action\nprediction problem in commercial-grade recommender systems. In this context,\nany viable solution needs to be reasonably scalable and also be robust to large\namounts of noisy data that is characteristic of this application space. We show\nthat MOGCSL performs admirably on both counts by extensive experiments on\nreal-world recommendation datasets. Also, analysis and experiments are included\nto explain its strength in discounting the noisier portions of training data in\nrecommender systems with multiple objectives.\n","authors":["Shijun Li","Hilaf Hasson","Jing Hu","Joydeep Ghosh"],"pdf_url":"https://arxiv.org/pdf/2412.08911v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09795v1","updated":"2025-05-14T20:45:29Z","published":"2025-05-14T20:45:29Z","title":"Beyond Pairwise Learning-To-Rank At Airbnb","summary":"  There are three fundamental asks from a ranking algorithm: it should scale to\nhandle a large number of items, sort items accurately by their utility, and\nimpose a total order on the items for logical consistency. But here's the\ncatch-no algorithm can achieve all three at the same time. We call this\nlimitation the SAT theorem for ranking algorithms. Given the dilemma, how can\nwe design a practical system that meets user needs? Our current work at Airbnb\nprovides an answer, with a working solution deployed at scale. We start with\npairwise learning-to-rank (LTR) models-the bedrock of search ranking tech\nstacks today. They scale linearly with the number of items ranked and perform\nstrongly on metrics like NDCG by learning from pairwise comparisons. They are\nat a sweet spot of performance vs. cost, making them an ideal choice for\nseveral industrial applications. However, they have a drawback-by ignoring\ninteractions between items, they compromise on accuracy. To improve accuracy,\nwe create a \"true\" pairwise LTR model-one that captures interactions between\nitems during pairwise comparisons. But accuracy comes at the expense of\nscalability and total order, and we discuss strategies to counter these\nchallenges. For greater accuracy, we take each item in the search result, and\ncompare it against the rest of the items along two dimensions: (1) Superiority:\nHow strongly do searchers prefer the given item over the remaining ones? (2)\nSimilarity: How similar is the given item to all the other items? This forms\nthe basis of our \"all-pairwise\" LTR framework, which factors in interactions\nacross all items at once. Looking at items on the search result page all\ntogether-superiority and similarity combined-gives us a deeper understanding of\nwhat searchers truly want. We quantify the resulting improvements in searcher\nexperience through offline and online experiments at Airbnb.\n","authors":["Malay Haldar","Daochen Zha","Huiji Gao","Liwei He","Sanjeev Katariya"],"pdf_url":"https://arxiv.org/pdf/2505.09795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09777v1","updated":"2025-05-14T20:15:52Z","published":"2025-05-14T20:15:52Z","title":"A Survey on Large Language Models in Multimodal Recommender Systems","summary":"  Multimodal recommender systems (MRS) integrate heterogeneous user and item\ndata, such as text, images, and structured information, to enhance\nrecommendation performance. The emergence of large language models (LLMs)\nintroduces new opportunities for MRS by enabling semantic reasoning, in-context\nlearning, and dynamic input handling. Compared to earlier pre-trained language\nmodels (PLMs), LLMs offer greater flexibility and generalisation capabilities\nbut also introduce challenges related to scalability and model accessibility.\nThis survey presents a comprehensive review of recent work at the intersection\nof LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and\ndata adaptation techniques. We propose a novel taxonomy to characterise\nintegration patterns, identify transferable techniques from related\nrecommendation domains, provide an overview of evaluation metrics and datasets,\nand point to possible future directions. We aim to clarify the emerging role of\nLLMs in multimodal recommendation and support future research in this rapidly\nevolving field.\n","authors":["Alejo Lopez-Avila","Jinhua Du"],"pdf_url":"https://arxiv.org/pdf/2505.09777v1.pdf","comment":"30 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.09776v1","updated":"2025-05-14T20:15:45Z","published":"2025-05-14T20:15:45Z","title":"The Impact of International Collaborations with Highly Publishing\n  Countries in Computer Science","summary":"  This paper analyzes international collaborations in Computer Science,\nfocusing on three major players: China, the European Union, and the United\nStates. Drawing from a comprehensive literature review, we examine\ncollaboration patterns, research impact, retraction rates, and the role of the\nDevelopment Index in shaping research outcomes. Our findings show that while\nChina, the EU, and the US lead global research efforts, other regions are\nnarrowing the gap in publication volume. Collaborations involving these key\nregions tend to have lower retraction rates, reflecting stronger adherence to\nscientific standards. We also find that countries with a Very High Development\nIndex contribute to research with higher citation rates and fewer retractions.\nOverall, this study highlights the value of international collaboration and the\nimportance of inclusive, ethical practices in advancing global research in\nComputer Science.\n","authors":["Alberto Gomez Espes","Michael Faerber","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2505.09776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09590v1","updated":"2025-05-14T17:39:34Z","published":"2025-05-14T17:39:34Z","title":"Distance-aware Self-adaptive Graph Convolution for Fine-grained\n  Hierarchical Recommendation","summary":"  Graph Convolutional Networks (GCNs) are widely used to improve recommendation\naccuracy and performance by effectively learning the representations of user\nand item nodes. However, two major challenges remain: (1) the lack of further\noptimization in the graph representation structure and (2) insufficient\nattention given to the varying contributions of different convolutional\nlayers.This paper proposes SAGCN, a distance-based adaptive hierarchical\naggregation method that refines the aggregation process through differentiated\nrepresentation metrics. SAGCN introduces a detailed approach to multilayer\ninformation aggregation and representation space optimization, enabling the\nmodel to learn hierarchical embedding weights based on the distance between\nhierarchical representations. This innovation allows for more precise\ncross-layer information aggregation, improves the model's ability to capture\nhierarchical embeddings, and optimizes the representation space structure.\nAdditionally, the objective loss function is refined to better align with\nrecommendation tasks.Extensive experiments conducted on four real-world\ndatasets demonstrate significant improvements, including over a 5% increase on\nYelp and a 5.58% increase in Recall@10 on the ML_1M dataset.\n","authors":["Tao Huang","Yihong Chen","Wei Fan","Wei Zhou","Junhao Wen"],"pdf_url":"https://arxiv.org/pdf/2505.09590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21813v3","updated":"2025-05-14T16:57:13Z","published":"2025-03-25T18:20:04Z","title":"OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language\n  Model Hallucinations in Ontology Matching","summary":"  Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the\nOntology Alignment Evaluation Initiative (OAEI), capturing hallucinations of\nten different LLMs performing OM tasks. These OM-specific hallucinations are\norganised into two primary categories and six sub-categories. We showcase the\nusefulness of the dataset in constructing an LLM leaderboard for OM tasks and\nfor fine-tuning LLMs used in OM tasks.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang","Jing Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.21813v3.pdf","comment":"14 pages, 4 figures, 4 tables, 2 prompt templates"},{"id":"http://arxiv.org/abs/2505.09539v1","updated":"2025-05-14T16:32:45Z","published":"2025-05-14T16:32:45Z","title":"GlobalMood: A cross-cultural benchmark for music emotion recognition","summary":"  Human annotations of mood in music are essential for music generation and\nrecommender systems. However, existing datasets predominantly focus on Western\nsongs with mood terms derived from English, which may limit generalizability\nacross diverse linguistic and cultural backgrounds. To address this, we\nintroduce `GlobalMood', a novel cross-cultural benchmark dataset comprising\n1,180 songs sampled from 59 countries, with large-scale annotations collected\nfrom 2,519 individuals across five culturally and linguistically distinct\nlocations: U.S., France, Mexico, S. Korea, and Egypt. Rather than imposing\npredefined mood categories, we implement a bottom-up, participant-driven\napproach to organically elicit culturally specific music-related mood terms. We\nthen recruit another pool of human participants to collect 988,925 ratings for\nthese culture-specific descriptors. Our analysis confirms the presence of a\nvalence-arousal structure shared across cultures, yet also reveals significant\ndivergences in how certain mood terms, despite being dictionary equivalents,\nare perceived cross-culturally. State-of-the-art multimodal models benefit\nsubstantially from fine-tuning on our cross-culturally balanced dataset, as\nevidenced by improved alignment with human evaluations - particularly in\nnon-English contexts. More broadly, our findings inform the ongoing debate on\nthe universality versus cultural specificity of emotional descriptors, and our\nmethodology can contribute to other multimodal and cross-lingual research.\n","authors":["Harin Lee","Elif Ãelen","Peter Harrison","Manuel Anglada-Tort","Pol van Rijn","Minsu Park","Marc SchÃ¶nwiesner","Nori Jacoby"],"pdf_url":"https://arxiv.org/pdf/2505.09539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00762v2","updated":"2025-05-14T16:10:58Z","published":"2025-02-02T11:53:55Z","title":"On Overlap Ratio in Defocused Electron Ptychography","summary":"  Four-dimensional Scanning Transmission Electron Microscopy (4D STEM) with\ndata acquired using a defocused electron probe is a promising tool for\ncharacterising complex biological specimens and materials through a phase\nretrieval process known as Electron Ptychography (EP). The efficacy of 4D STEM\nacquisition and the resulting quality of EP reconstruction depends on the\noverlap ratio of adjacent illuminated areas. This paper demonstrates how the\noverlap ratio impacts the data redundancy and the quality of the EP\nreconstruction. We define two quantities as a function of the overlap ratio\nthat are independent of both the object and the EP algorithm. Subsequently, we\nevaluate an EP algorithm for varying overlap ratios using simulated 4D STEM\ndatasets. Notably, a 40% or greater overlap ratio yields stable, high-quality\nreconstructions.\n","authors":["Amirafshar Moshtaghpour","Angus I. Kirkland"],"pdf_url":"https://arxiv.org/pdf/2502.00762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05009v2","updated":"2025-05-14T15:56:36Z","published":"2025-04-07T12:37:39Z","title":"Deconstructing Jazz Piano Style Using Machine Learning","summary":"  Artistic style has been studied for centuries, and recent advances in machine\nlearning create new possibilities for understanding it computationally.\nHowever, ensuring that machine-learning models produce insights aligned with\nthe interests of practitioners and critics remains a significant challenge.\nHere, we focus on musical style, which benefits from a rich theoretical and\nmathematical analysis tradition. We train a variety of supervised-learning\nmodels to identify 20 iconic jazz musicians across a carefully curated dataset\nof 84 hours of recordings, and interpret their decision-making processes. Our\nmodels include a novel multi-input architecture that enables four musical\ndomains (melody, harmony, rhythm, and dynamics) to be analysed separately.\nThese models enable us to address fundamental questions in music theory and\nalso advance the state-of-the-art in music performer identification (94%\naccuracy across 20 classes). We release open-source implementations of our\nmodels and an accompanying web application for exploring musical styles.\n","authors":["Huw Cheston","Reuben Bance","Peter M. C. Harrison"],"pdf_url":"https://arxiv.org/pdf/2504.05009v2.pdf","comment":"Paper: 40 pages, 11 figures, 1 table; added information on training\n  time + computation cost, corrections to Table 1. Supplementary material: 33\n  pages, 48 figures, 6 tables; corrections to Table S.5"},{"id":"http://arxiv.org/abs/2505.09436v1","updated":"2025-05-14T14:44:30Z","published":"2025-05-14T14:44:30Z","title":"CXMArena: Unified Dataset to benchmark performance in realistic CXM\n  Scenarios","summary":"  Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.\n","authors":["Raghav Garg","Kapil Sharma","Karan Gupta"],"pdf_url":"https://arxiv.org/pdf/2505.09436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09414v1","updated":"2025-05-14T14:10:22Z","published":"2025-05-14T14:10:22Z","title":"FACTors: A New Dataset for Studying the Fact-checking Ecosystem","summary":"  Our fight against false information is spearheaded by fact-checkers. They\ninvestigate the veracity of claims and document their findings as fact-checking\nreports. With the rapid increase in the amount of false information circulating\nonline, the use of automation in fact-checking processes aims to strengthen\nthis ecosystem by enhancing scalability. Datasets containing fact-checked\nclaims play a key role in developing such automated solutions. However, to the\nbest of our knowledge, there is no fact-checking dataset at the ecosystem\nlevel, covering claims from a sufficiently long period of time and sourced from\na wide range of actors reflecting the entire ecosystem that admittedly follows\nwidely-accepted codes and principles of fact-checking. We present a new dataset\nFACTors, the first to fill this gap by presenting ecosystem-level data on\nfact-checking. It contains 118,112 claims from 117,993 fact-checking reports in\nEnglish (co-)authored by 1,953 individuals and published during the period of\n1995-2025 by 39 fact-checking organisations that are active signatories of the\nIFCN (International Fact-Checking Network) and/or EFCSN (European Fact-Checking\nStandards Network). It contains 7,327 overlapping claims investigated by\nmultiple fact-checking organisations, corresponding to 2,977 unique claims. It\nallows to conduct new ecosystem-level studies of the fact-checkers\n(organisations and individuals). To demonstrate the usefulness of FACTors, we\npresent three example applications, including a first-of-its-kind statistical\nanalysis of the fact-checking ecosystem, examining the political inclinations\nof the fact-checking organisations, and attempting to assign a credibility\nscore to each organisation based on the findings of the statistical analysis\nand political leanings. Our methods for constructing FACTors are generic and\ncan be used to maintain a live dataset that can be updated dynamically.\n","authors":["Enes Altuncu","Can BaÅkent","Sanjay Bhattacherjee","Shujun Li","Dwaipayan Roy"],"pdf_url":"https://arxiv.org/pdf/2505.09414v1.pdf","comment":"Accepted for the 48th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval (SIGIR '25)"},{"id":"http://arxiv.org/abs/2505.09364v1","updated":"2025-05-14T13:13:53Z","published":"2025-05-14T13:13:53Z","title":"Diffusion Recommender Models and the Illusion of Progress: A Concerning\n  Study of Reproducibility and a Conceptual Mismatch","summary":"  Countless new machine learning models are published every year and are\nreported to significantly advance the state-of-the-art in \\emph{top-n}\nrecommendation. However, earlier reproducibility studies indicate that progress\nin this area may be quite limited. Specifically, various widespread\nmethodological issues, e.g., comparisons with untuned baseline models, have led\nto an \\emph{illusion of progress}. In this work, our goal is to examine whether\nthese problems persist in today's research. To this end, we aim to reproduce\nthe latest advancements reported from applying modern Denoising Diffusion\nProbabilistic Models to recommender systems, focusing on four models published\nat the top-ranked SIGIR conference in 2023 and 2024. Our findings are\nconcerning, revealing persistent methodological problems. Alarmingly, through\nexperiments, we find that the latest recommendation techniques based on\ndiffusion models, despite their computational complexity and substantial carbon\nfootprint, are consistently outperformed by simpler existing models.\nFurthermore, we identify key mismatches between the characteristics of\ndiffusion models and those of the traditional \\emph{top-n} recommendation task,\nraising doubts about their suitability for recommendation. We also note that,\nin the papers we analyze, the generative capabilities of these models are\nconstrained to a minimum. Overall, our results and continued methodological\nissues call for greater scientific rigor and a disruptive change in the\nresearch and publication culture in this area.\n","authors":["Michael Benigni","Maurizio Ferrari Dacrema","Dietmar Jannach"],"pdf_url":"https://arxiv.org/pdf/2505.09364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09316v1","updated":"2025-05-14T12:13:38Z","published":"2025-05-14T12:13:38Z","title":"Scent of Knowledge: Optimizing Search-Enhanced Reasoning with\n  Information Foraging","summary":"  Augmenting large language models (LLMs) with external retrieval has become a\nstandard method to address their inherent knowledge cutoff limitations.\nHowever, traditional retrieval-augmented generation methods employ static,\npre-inference retrieval strategies, making them inadequate for complex tasks\ninvolving ambiguous, multi-step, or evolving information needs. Recent advances\nin test-time scaling techniques have demonstrated significant potential in\nenabling LLMs to dynamically interact with external tools, motivating the shift\ntoward adaptive inference-time retrieval. Inspired by Information Foraging\nTheory (IFT), we propose InForage, a reinforcement learning framework that\nformalizes retrieval-augmented reasoning as a dynamic information-seeking\nprocess. Unlike existing approaches, InForage explicitly rewards intermediate\nretrieval quality, encouraging LLMs to iteratively gather and integrate\ninformation through adaptive search behaviors. To facilitate training, we\nconstruct a human-guided dataset capturing iterative search and reasoning\ntrajectories for complex, real-world web tasks. Extensive evaluations across\ngeneral question answering, multi-hop reasoning tasks, and a newly developed\nreal-time web QA dataset demonstrate InForage's superior performance over\nbaseline methods. These results highlight InForage's effectiveness in building\nrobust, adaptive, and efficient reasoning agents.\n","authors":["Hongjin Qian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2505.09316v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2505.09246v1","updated":"2025-05-14T09:35:56Z","published":"2025-05-14T09:35:56Z","title":"Focus, Merge, Rank: Improved Question Answering Based on Semi-structured\n  Knowledge Bases","summary":"  In many real-world settings, machine learning models and interactive systems\nhave access to both structured knowledge, e.g., knowledge graphs or tables, and\nunstructured content, e.g., natural language documents. However, most rely on\neither. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking\nunstructured content to nodes within structured data, thereby enabling new\nstrategies for knowledge access and use. In this work, we present\nFocusedRetriever, a modular SKB-based framework for multi-hop question\nanswering. It integrates components (VSS-based entity search, LLM-based\ngeneration of Cypher queries and pairwise re-ranking) in a way that enables it\nto outperform state-of-the-art methods across all three STaRK benchmark test\nsets, covering diverse domains and multiple performance metrics. The average\nfirst-hit rate exceeds that of the second-best method by 25.7%.\nFocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to\nextract relational facts and entity attributes from unstructured text, (2) node\nset joins to filter answer candidates based on these extracted triplets and\nconstraints, (3) vector similarity search to retrieve and rank relevant\nunstructured content, and (4) the contextual capabilities of LLMs to finally\nrank the top-k answers. For generality, we only incorporate base LLMs in\nFocusedRetriever in our evaluation. However, our analysis of intermediate\nresults highlights several opportunities for further upgrades including\nfinetuning. The source code is publicly available at\nhttps://github.com/kramerlab/FocusedRetriever .\n","authors":["Derian Boer","Stephen Roth","Stefan Kramer"],"pdf_url":"https://arxiv.org/pdf/2505.09246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09205v1","updated":"2025-05-14T07:34:36Z","published":"2025-05-14T07:34:36Z","title":"HMamba: Hyperbolic Mamba for Sequential Recommendation","summary":"  Sequential recommendation systems have become a cornerstone of personalized\nservices, adept at modeling the temporal evolution of user preferences by\ncapturing dynamic interaction sequences. Existing approaches predominantly rely\non traditional models, including RNNs and Transformers. Despite their success\nin local pattern recognition, Transformer-based methods suffer from quadratic\ncomputational complexity and a tendency toward superficial attention patterns,\nlimiting their ability to infer enduring preference hierarchies in sequential\nrecommendation data. Recent advances in Mamba-based sequential models introduce\nlinear-time efficiency but remain constrained by Euclidean geometry, failing to\nleverage the intrinsic hyperbolic structure of recommendation data. To bridge\nthis gap, we propose Hyperbolic Mamba, a novel architecture that unifies the\nefficiency of Mamba's selective state space mechanism with hyperbolic\ngeometry's hierarchical representational power. Our framework introduces (1) a\nhyperbolic selective state space that maintains curvature-aware sequence\nmodeling and (2) stabilized Riemannian operations to enable scalable training.\nExperiments across four benchmarks demonstrate that Hyperbolic Mamba achieves\n3-11% improvement while retaining Mamba's linear-time efficiency, enabling\nreal-world deployment. This work establishes a new paradigm for efficient,\nhierarchy-aware sequential modeling.\n","authors":["Qianru Zhang","Honggang Wen","Wei Yuan","Crystal Chen","Menglin Yang","Siu-Ming Yiu","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2505.09205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09065v1","updated":"2025-05-14T01:48:59Z","published":"2025-05-14T01:48:59Z","title":"Display Content, Display Methods and Evaluation Methods of the HCI in\n  Explainable Recommender Systems: A Survey","summary":"  Explainable Recommender Systems (XRS) aim to provide users with\nunderstandable reasons for the recommendations generated by these systems,\nrepresenting a crucial research direction in artificial intelligence (AI).\nRecent research has increasingly focused on the algorithms, display, and\nevaluation methodologies of XRS. While current research and reviews primarily\nemphasize the algorithmic aspects, with fewer studies addressing the\nHuman-Computer Interaction (HCI) layer of XRS. Additionally, existing reviews\nlack a unified taxonomy for XRS and there is insufficient attention given to\nthe emerging area of short video recommendations. In this study, we synthesize\nexisting literature and surveys on XRS, presenting a unified framework for its\nresearch and development. The main contributions are as follows: 1) We adopt a\nlifecycle perspective to systematically summarize the technologies and methods\nused in XRS, addressing challenges posed by the diversity and complexity of\nalgorithmic models and explanation techniques. 2) For the first time, we\nhighlight the application of multimedia, particularly video-based explanations,\nalong with its potential, technical pathways, and challenges in XRS. 3) We\nprovide a structured overview of evaluation methods from both qualitative and\nquantitative dimensions. These findings provide valuable insights for the\nsystematic design, progress, and testing of XRS.\n","authors":["Weiqing Li","Yue Xu","Yuefeng Li","Yinghui Huang"],"pdf_url":"https://arxiv.org/pdf/2505.09065v1.pdf","comment":"2 Tables, 29 figures"},{"id":"http://arxiv.org/abs/2209.13814v2","updated":"2025-05-14T01:21:55Z","published":"2022-09-28T03:39:34Z","title":"Signed Latent Factors for Spamming Activity Detection","summary":"  Due to the increasing trend of performing spamming activities (e.g., Web\nspam, deceptive reviews, fake followers, etc.) on various online platforms to\ngain undeserved benefits, spam detection has emerged as a hot research issue.\nPrevious attempts to combat spam mainly employ features related to metadata,\nuser behaviors, or relational ties. These studies have made considerable\nprogress in understanding and filtering spamming campaigns. However, this\nproblem remains far from fully solved. Almost all the proposed features focus\non a limited number of observed attributes or explainable phenomena, making it\ndifficult for existing methods to achieve further improvement. To broaden the\nvision about solving the spam problem and address long-standing challenges\n(class imbalance and graph incompleteness) in the spam detection area, we\npropose a new attempt of utilizing signed latent factors to filter fraudulent\nactivities. The spam-contaminated relational datasets of multiple online\napplications in this scenario are interpreted by the unified signed network.\nTwo competitive and highly dissimilar algorithms of latent factors mining (LFM)\nmodels are designed based on multi-relational likelihoods estimation (LFM-MRLE)\nand signed pairwise ranking (LFM-SPR), respectively. We then explore how to\napply the mined latent factors to spam detection tasks. Experiments on\nreal-world datasets of different kinds of Web applications (social media and\nWeb forum) indicate that LFM models outperform state-of-the-art baselines in\ndetecting spamming activities. By specifically manipulating experimental data,\nthe effectiveness of our methods in dealing with incomplete and imbalanced\nchallenges is validated.\n","authors":["Yuli Liu"],"pdf_url":"https://arxiv.org/pdf/2209.13814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09033v1","updated":"2025-05-14T00:05:04Z","published":"2025-05-14T00:05:04Z","title":"Item Level Exploration Traffic Allocation in Large-scale Recommendation\n  Systems","summary":"  This paper contributes to addressing the item cold start problem in\nlarge-scale recommender systems, focusing on how to efficiently gain initial\nvisibility for newly ingested content. We propose an exploration system\ndesigned to efficiently allocate impressions to these fresh items. Our approach\nleverages a learned probabilistic model to predict an item's discoverability,\nwhich then informs a scalable and adaptive traffic allocation strategy. This\nsystem intelligently distributes exploration budgets, optimizing for the\nlong-term benefit of the recommendation platform. The impact is a demonstrably\nmore efficient cold-start process, leading to a significant increase in the\ndiscoverability of new content and ultimately enriching the item corpus\navailable for exploitation, as evidenced by its successful deployment in a\nlarge-scale production environment.\n","authors":["Dong Wang","Junyi Jiao","Arnab Bhadury","Yaping Zhang","Mingyan Gao"],"pdf_url":"https://arxiv.org/pdf/2505.09033v1.pdf","comment":"accepted by the 18th ACM Recsys Large Recsys Workshop"}],"Multimedia":[{"id":"http://arxiv.org/abs/2504.17938v3","updated":"2025-05-14T16:55:50Z","published":"2025-04-24T21:00:43Z","title":"Machine Learning-Based Prediction of Quality Shifts on Video Streaming\n  Over 5G","summary":"  The Quality of Experience (QoE) is the users satisfaction while streaming a\nvideo session over an over-the-top (OTT) platform like YouTube. QoE of YouTube\nreflects the smooth streaming session without any buffering and quality shift\nevents. One of the most important factors nowadays affecting QoE of YouTube is\nfrequent shifts from higher to lower resolutions and vice versa. These shifts\nensure a smooth streaming session; however, it might get a lower mean opinion\nscore. For instance, dropping from 1080p to 480p during a video can preserve\ncontinuity but might reduce the viewers enjoyment. Over time, OTT platforms are\nlooking for alternative ways to boost user experience instead of relying on\ntraditional Quality of Service (QoS) metrics such as bandwidth, latency, and\nthroughput. As a result, we look into the relationship between quality shifting\nin YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our\nfindings state that these channel metrics positively correlate with shifts.\nThus, in real-time, OTT can only rely on them to predict video streaming\nsessions into lower- and higher-resolution categories, thus providing more\nresources to improve user experience. Using traditional Machine Learning (ML)\nclassifiers, we achieved an accuracy of 77-percent, while using only RSRP,\nRSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency\nnetworks promise enhanced streaming capabilities, the proposed methodology can\nbe used to improve OTT services.\n","authors":["Raza Ul Mustafa","Sesha Dassanayake","Noman Ashraf"],"pdf_url":"https://arxiv.org/pdf/2504.17938v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09558v1","updated":"2025-05-14T16:54:15Z","published":"2025-05-14T16:54:15Z","title":"WavReward: Spoken Dialogue Models With Generalist Reward Evaluators","summary":"  End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered\nsignificant attention in the speech domain. However, the evaluation of spoken\ndialogue models' conversational performance has largely been overlooked. This\nis primarily due to the intelligent chatbots convey a wealth of non-textual\ninformation which cannot be easily measured using text-based language models\nlike ChatGPT. To address this gap, we propose WavReward, a reward feedback\nmodel based on audio language models that can evaluate both the IQ and EQ of\nspoken dialogue systems with speech input. Specifically, 1) based on audio\nlanguage models, WavReward incorporates the deep reasoning process and the\nnonlinear reward mechanism for post-training. By utilizing multi-sample\nfeedback via the reinforcement learning algorithm, we construct a specialized\nevaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a\npreference dataset used to train WavReward. ChatReward-30K includes both\ncomprehension and generation aspects of spoken dialogue models. These scenarios\nspan various tasks, such as text-based chats, nine acoustic attributes of\ninstruction chats, and implicit chats. WavReward outperforms previous\nstate-of-the-art evaluation models across multiple spoken dialogue scenarios,\nachieving a substantial improvement about Qwen2.5-Omni in objective accuracy\nfrom 55.1$\\%$ to 91.5$\\%$. In subjective A/B testing, WavReward also leads by a\nmargin of 83$\\%$. Comprehensive ablation studies confirm the necessity of each\ncomponent of WavReward. All data and code will be publicly at\nhttps://github.com/jishengpeng/WavReward after the paper is accepted.\n","authors":["Shengpeng Ji","Tianle Liang","Yangzhuo Li","Jialong Zuo","Minghui Fang","Jinzheng He","Yifu Chen","Zhengqing Liu","Ziyue Jiang","Xize Cheng","Siqi Zheng","Jin Xu","Junyang Lin","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.09558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00045v5","updated":"2025-05-14T16:37:28Z","published":"2024-01-22T15:08:19Z","title":"Detecting Multimedia Generated by Large AI Models: A Survey","summary":"  The rapid advancement of Large AI Models (LAIMs), particularly diffusion\nmodels and large language models, has marked a new era where AI-generated\nmultimedia is increasingly integrated into various aspects of daily life.\nAlthough beneficial in numerous fields, this content presents significant\nrisks, including potential misuse, societal disruptions, and ethical concerns.\nConsequently, detecting multimedia generated by LAIMs has become crucial, with\na marked rise in related research. Despite this, there remains a notable gap in\nsystematic surveys that focus specifically on detecting LAIM-generated\nmultimedia. Addressing this, we provide the first survey to comprehensively\ncover existing research on detecting multimedia (such as text, images, videos,\naudio, and multimodal content) created by LAIMs. Specifically, we introduce a\nnovel taxonomy for detection methods, categorized by media modality, and\naligned with two perspectives: pure detection (aiming to enhance detection\nperformance) and beyond detection (adding attributes like generalizability,\nrobustness, and interpretability to detectors). Additionally, we have presented\na brief overview of generation mechanisms, public datasets, online detection\ntools, and evaluation metrics to provide a valuable resource for researchers\nand practitioners in this field. Most importantly, we offer a focused analysis\nfrom a social media perspective to highlight their broader societal impact.\nFurthermore, we identify current challenges in detection and propose directions\nfor future research that address unexplored, ongoing, and emerging issues in\ndetecting multimedia generated by LAIMs. Our aim for this survey is to fill an\nacademic gap and contribute to global AI security efforts, helping to ensure\nthe integrity of information in the digital realm. The project link is\nhttps://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.\n","authors":["Li Lin","Neeraj Gupta","Yue Zhang","Hainan Ren","Chun-Hao Liu","Feng Ding","Xin Wang","Xin Li","Luisa Verdoliva","Shu Hu"],"pdf_url":"https://arxiv.org/pdf/2402.00045v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01986v2","updated":"2025-05-14T14:57:42Z","published":"2024-12-02T21:35:33Z","title":"HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh\n  Quality Assessment","summary":"  Mesh quality assessment (MQA) models play a critical role in the design,\noptimization, and evaluation of mesh operation systems in a wide variety of\napplications. Current MQA models, whether model-based methods using\ntopology-aware features or projection-based approaches working on rendered 2D\nprojections, often fail to capture the intricate interactions between texture\nand 3D geometry. We introduce HybridMQA, a first-of-its-kind hybrid\nfull-reference colored MQA framework that integrates model-based and\nprojection-based approaches, capturing complex interactions between textural\ninformation and 3D structures for enriched quality representations. Our method\nemploys graph learning to extract detailed 3D representations, which are then\nprojected to 2D using a novel feature rendering process that precisely aligns\nthem with colored projections. This enables the exploration of geometry-texture\ninteractions via cross-attention, producing comprehensive mesh quality\nrepresentations. Extensive experiments demonstrate HybridMQA's superior\nperformance across diverse datasets, highlighting its ability to effectively\nleverage geometry-texture interactions for a thorough understanding of mesh\nquality. Our implementation will be made publicly available.\n","authors":["Armin Shafiee Sarvestani","Sheyang Tang","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2412.01986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08175v2","updated":"2025-05-14T06:07:26Z","published":"2025-05-13T02:25:47Z","title":"Fast Text-to-Audio Generation with Adversarial Post-Training","summary":"  Text-to-audio systems, while increasingly performant, are slow at inference\ntime, thus making their latency unpractical for many creative applications. We\npresent Adversarial Relativistic-Contrastive (ARC) post-training, the first\nadversarial acceleration algorithm for diffusion/flow models not based on\ndistillation. While past adversarial post-training methods have struggled to\ncompare against their expensive distillation counterparts, ARC post-training is\na simple procedure that (1) extends a recent relativistic adversarial\nformulation to diffusion/flow post-training and (2) combines it with a novel\ncontrastive discriminator objective to encourage better prompt adherence. We\npair ARC post-training with a number optimizations to Stable Audio Open and\nbuild a model capable of generating $\\approx$12s of 44.1kHz stereo audio in\n$\\approx$75ms on an H100, and $\\approx$7s on a mobile edge-device, the fastest\ntext-to-audio model to our knowledge.\n","authors":["Zachary Novack","Zach Evans","Zack Zukowski","Josiah Taylor","CJ Carr","Julian Parker","Adnan Al-Sinan","Gian Marco Iodice","Julian McAuley","Taylor Berg-Kirkpatrick","Jordi Pons"],"pdf_url":"https://arxiv.org/pdf/2505.08175v2.pdf","comment":null}]},"2025-05-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2505.09031v1","updated":"2025-05-13T23:57:02Z","published":"2025-05-13T23:57:02Z","title":"Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency,\n  and Self-Verification","summary":"  Hallucination, where large language models (LLMs) generate confident but\nincorrect or irrelevant information, remains a key limitation in their\napplication to complex, open-ended tasks. Chain-of-thought (CoT) prompting has\nemerged as a promising method for improving multistep reasoning by guiding\nmodels through intermediate steps. However, CoT alone does not fully address\nthe hallucination problem. In this work, we investigate how combining CoT with\nretrieval-augmented generation (RAG), as well as applying self-consistency and\nself-verification strategies, can reduce hallucinations and improve factual\naccuracy. By incorporating external knowledge sources during reasoning and\nenabling models to verify or revise their own outputs, we aim to generate more\naccurate and coherent responses. We present a comparative evaluation of\nbaseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification\ntechniques. Our results highlight the effectiveness of each method and identify\nthe most robust approach for minimizing hallucinations while preserving fluency\nand reasoning depth.\n","authors":["Adarsh Kumar","Hwiyoon Kim","Jawahar Sai Nathani","Neil Roy"],"pdf_url":"https://arxiv.org/pdf/2505.09031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09024v1","updated":"2025-05-13T23:42:36Z","published":"2025-05-13T23:42:36Z","title":"Automated Meta Prompt Engineering for Alignment with the Theory of Mind","summary":"  We introduce a method of meta-prompting that jointly produces fluent text for\ncomplex tasks while optimizing the similarity of neural states between a\nhuman's mental expectation and a Large Language Model's (LLM) neural\nprocessing. A technique of agentic reinforcement learning is applied, in which\nan LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,\nhow to produce content by interpreting the intended and unintended generated\ntext traits. To measure human mental beliefs around content production, users\nmodify long form AI-generated text articles before publication at the US Open\n2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)\nalignment problem by anticipating and including human edits within the creation\nof text from an LLM. Throughout experimentation and by interpreting the results\nof a live production system, the expectations of human content reviewers had\n100% of alignment with AI 53.8% of the time with an average iteration count of\n4.38. The geometric interpretation of content traits such as factualness,\nnovelty, repetitiveness, and relevancy over a Hilbert vector space combines\nspatial volume (all trait importance) with vertices alignment (individual trait\nrelevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an\nincrease in content quality by extending the coverage of tennis action. Our\nwork that was deployed at the US Open 2024 has been used across other live\nevents within sports and entertainment.\n","authors":["Aaron Baughman","Rahul Agarwal","Eduardo Morales","Gozde Akay"],"pdf_url":"https://arxiv.org/pdf/2505.09024v1.pdf","comment":"9 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2505.09005v1","updated":"2025-05-13T22:41:13Z","published":"2025-05-13T22:41:13Z","title":"For GPT-4 as with Humans: Information Structure Predicts Acceptability\n  of Long-Distance Dependencies","summary":"  It remains debated how well any LM understands natural language or generates\nreliable metalinguistic judgments. Moreover, relatively little work has\ndemonstrated that LMs can represent and respect subtle relationships between\nform and function proposed by linguists. We here focus on a particular such\nrelationship established in recent work: English speakers' judgments about the\ninformation structure of canonical sentences predicts independently collected\nacceptability ratings on corresponding 'long distance dependency' [LDD]\nconstructions, across a wide array of base constructions and multiple types of\nLDDs. To determine whether any LM captures this relationship, we probe GPT-4 on\nthe same tasks used with humans and new extensions.Results reveal reliable\nmetalinguistic skill on the information structure and acceptability tasks,\nreplicating a striking interaction between the two, despite the zero-shot,\nexplicit nature of the tasks, and little to no chance of contamination [Studies\n1a, 1b]. Study 2 manipulates the information structure of base sentences and\nconfirms a causal relationship: increasing the prominence of a constituent in a\ncontext sentence increases the subsequent acceptability ratings on an LDD\nconstruction. The findings suggest a tight relationship between natural and\nGPT-4 generated English, and between information structure and syntax, which\nbegs for further exploration.\n","authors":["Nicole Cuneo","Eleanor Graves","Supantho Rakshit","Adele E. Goldberg"],"pdf_url":"https://arxiv.org/pdf/2505.09005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16560v2","updated":"2025-05-13T22:37:48Z","published":"2025-02-23T12:57:40Z","title":"An Analytical Emotion Framework of Rumour Threads on Social Media","summary":"  Rumours in online social media pose significant risks to modern society,\nmotivating the need for better understanding of how they develop. We focus\nspecifically on the interface between emotion and rumours in threaded\ndiscourses, building on the surprisingly sparse literature on the topic which\nhas largely focused on single aspect of emotions within the original rumour\nposts themselves, and largely overlooked the comparative differences between\nrumours and non-rumours. In this work, we take one step further to provide a\ncomprehensive analytical emotion framework with multi-aspect emotion detection,\ncontrasting rumour and non-rumour threads and provide both correlation and\ncausal analysis of emotions. We applied our framework on existing widely-used\nrumour datasets to further understand the emotion dynamics in online social\nmedia threads. Our framework reveals that rumours trigger more negative\nemotions (e.g., anger, fear, pessimism), while non-rumours evoke more positive\nones. Emotions are contagious, rumours spread negativity, non-rumours spread\npositivity. Causal analysis shows surprise bridges rumours and other emotions;\npessimism comes from sadness and fear, while optimism arises from joy and love.\n","authors":["Rui Xing","Boyang Sun","Kun Zhang","Preslav Nakov","Timothy Baldwin","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2502.16560v2.pdf","comment":"Accepted to ICWSM 2025 MisD Workshop"},{"id":"http://arxiv.org/abs/2505.08996v1","updated":"2025-05-13T22:18:51Z","published":"2025-05-13T22:18:51Z","title":"A suite of LMs comprehend puzzle statements as well as humans","summary":"  Recent claims suggest that large language models (LMs) underperform humans in\ncomprehending minimally complex English statements (Dentella et al., 2024).\nHere, we revisit those findings and argue that human performance was\noverestimated, while LLM abilities were underestimated. Using the same stimuli,\nwe report a preregistered study comparing human responses in two conditions:\none allowed rereading (replicating the original study), and one that restricted\nrereading (a more naturalistic comprehension test). Human accuracy dropped\nsignificantly when rereading was restricted (73%), falling below that of\nFalcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect\naccuracy. Results further show that both humans and models are\ndisproportionately challenged by queries involving potentially reciprocal\nactions (e.g., kissing), suggesting shared pragmatic sensitivities rather than\nmodel-specific deficits. Additional analyses using Llama-2-70B log\nprobabilities, a recoding of open-ended model responses, and grammaticality\nratings of other sentences reveal systematic underestimation of model\nperformance. We find that GPT-4o can align with either naive or expert\ngrammaticality judgments, depending on prompt framing. These findings\nunderscore the need for more careful experimental design and coding practices\nin LLM evaluation, and they challenge the assumption that current models are\ninherently weaker than humans at language comprehension.\n","authors":["Adele E Goldberg","Supantho Rakshit","Jennifer Hu","Kyle Mahowald"],"pdf_url":"https://arxiv.org/pdf/2505.08996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08971v1","updated":"2025-05-13T21:27:52Z","published":"2025-05-13T21:27:52Z","title":"Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training","summary":"  In standard large vision-language models (LVLMs) pre-training, the model\ntypically maximizes the joint probability of the caption conditioned on the\nimage via next-token prediction (NTP); however, since only a small subset of\ncaption tokens directly relates to the visual content, this naive NTP\nunintentionally fits the model to noise and increases the risk of\nhallucination. We present PRIOR, a simple vision-language pre-training approach\nthat addresses this issue by prioritizing image-related tokens through\ndifferential weighting in the NTP loss, drawing from the importance sampling\nframework. PRIOR introduces a reference model-a text-only large language model\n(LLM) trained on the captions without image inputs, to weight each token based\non its probability for LVLMs training. Intuitively, tokens that are directly\nrelated to the visual inputs are harder to predict without the image and thus\nreceive lower probabilities from the text-only reference LLM. During training,\nwe implement a token-specific re-weighting term based on the importance scores\nto adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs\nwith visual encoders and LVLMs without visual encoders. We observe 19% and 8%\naverage relative improvement, respectively, on several vision-language\nbenchmarks compared to NTP. In addition, PRIOR exhibits superior scaling\nproperties, as demonstrated by significantly higher scaling coefficients,\nindicating greater potential for performance gains compared to NTP given\nincreasing compute and data.\n","authors":["Yangyi Chen","Hao Peng","Tong Zhang","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2505.08971v1.pdf","comment":"The code will be available at https://github.com/Yangyi-Chen/PRIOR"},{"id":"http://arxiv.org/abs/2505.08941v1","updated":"2025-05-13T20:10:00Z","published":"2025-05-13T20:10:00Z","title":"ForeCite: Adapting Pre-Trained Language Models to Predict Future\n  Citation Rates of Academic Papers","summary":"  Predicting the future citation rates of academic papers is an important step\ntoward the automation of research evaluation and the acceleration of scientific\nprogress. We present $\\textbf{ForeCite}$, a simple but powerful framework to\nappend pre-trained causal language models with a linear head for average\nmonthly citation rate prediction. Adapting transformers for regression tasks,\nForeCite achieves a test correlation of $\\rho = 0.826$ on a curated dataset of\n900K+ biomedical papers published between 2000 and 2024, a 27-point improvement\nover the previous state-of-the-art. Comprehensive scaling-law analysis reveals\nconsistent gains across model sizes and data volumes, while temporal holdout\nexperiments confirm practical robustness. Gradient-based saliency heatmaps\nsuggest a potentially undue reliance on titles and abstract texts. These\nresults establish a new state-of-the-art in forecasting the long-term influence\nof academic research and lay the groundwork for the automated, high-fidelity\nevaluation of scientific contributions.\n","authors":["Gavin Hull","Alex Bihlo"],"pdf_url":"https://arxiv.org/pdf/2505.08941v1.pdf","comment":"16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2503.10652v2","updated":"2025-05-13T19:38:19Z","published":"2025-03-07T10:37:31Z","title":"Simulating and Analysing Human Survey Responses with Large Language\n  Models: A Case Study in Energy Stated Preference","summary":"  Survey research plays a crucial role in studies by capturing consumer\npreferences and informing policy decisions. Stated preference (SP) surveys help\nresearchers understand how individuals make trade-offs in hypothetical,\npotentially futuristic, scenarios. However, traditional methods are costly,\ntime-consuming, and affected by respondent fatigue and ethical constraints.\nLarge language models (LLMs) have shown remarkable capabilities in generating\nhuman-like responses, prompting interest in their use in survey research. This\nstudy investigates LLMs for simulating consumer choices in energy-related SP\nsurveys and explores their integration into data collection and analysis\nworkflows. Test scenarios were designed to assess the simulation performance of\nseveral LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and\naggregated levels, considering prompt design, in-context learning (ICL),\nchain-of-thought (CoT) reasoning, model types, integration with traditional\nchoice models, and potential biases. While LLMs achieve accuracy above random\nguessing, performance remains insufficient for practical simulation use.\nCloud-based LLMs do not consistently outperform smaller local models.\nDeepSeek-R1 achieves the highest average accuracy (77%) and outperforms\nnon-reasoning LLMs in accuracy, factor identification, and choice distribution\nalignment. Previous SP choices are the most effective input; longer prompts\nwith more factors reduce accuracy. Mixed logit models can support LLM prompt\nrefinement. Reasoning LLMs show potential in data analysis by indicating factor\nsignificance, offering a qualitative complement to statistical models. Despite\nlimitations, pre-trained LLMs offer scalability and require minimal historical\ndata. Future work should refine prompts, further explore CoT reasoning, and\ninvestigate fine-tuning techniques.\n","authors":["Han Wang","Jacek Pawlak","Aruna Sivakumar"],"pdf_url":"https://arxiv.org/pdf/2503.10652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09650v2","updated":"2025-05-13T18:54:09Z","published":"2025-02-11T17:01:11Z","title":"Principled Data Selection for Alignment: The Hidden Risks of Difficult\n  Examples","summary":"  The alignment of large language models (LLMs) often assumes that using more\nclean data yields better outcomes, overlooking the match between model capacity\nand example difficulty. Challenging this, we propose a new principle:\nPreference data vary in difficulty, and overly difficult examples hinder\nalignment, by exceeding the model's capacity. Through systematic\nexperimentation, we validate this principle with three key findings: (1)\npreference examples vary in difficulty, as evidenced by consistent learning\norders across alignment runs; (2) overly difficult examples significantly\ndegrade performance across four LLMs and two datasets; and (3) the capacity of\na model dictates its threshold for handling difficult examples, underscoring a\ncritical relationship between data selection and model capacity. Building on\nthis principle, we introduce Selective DPO, which filters out overly difficult\nexamples. This simple adjustment improves alignment performance by 9-16% in win\nrates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a\nseries of DPO variants with different algorithmic adjustments. Together, these\nresults illuminate the importance of aligning data difficulty with model\ncapacity, offering a transformative perspective for improving alignment\nstrategies in LLMs. Code is available at\nhttps://github.com/glorgao/SelectiveDPO.\n","authors":["Chengqian Gao","Haonan Li","Liu Liu","Zeke Xie","Peilin Zhao","Zhiqiang Xu"],"pdf_url":"https://arxiv.org/pdf/2502.09650v2.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2505.08905v1","updated":"2025-05-13T18:50:03Z","published":"2025-05-13T18:50:03Z","title":"Grounding Synthetic Data Evaluations of Language Models in Unsupervised\n  Document Corpora","summary":"  Language Models (LMs) continue to advance, improving response quality and\ncoherence. Given Internet-scale training datasets, LMs have likely encountered\nmuch of what users might ask them to generate in some form during their\ntraining. A plethora of evaluation benchmarks have been constructed to assess\nmodel quality, response appropriateness, and reasoning capabilities. However,\nthe human effort required for benchmark construction is limited and being\nrapidly outpaced by the size and scope of the models under evaluation.\nAdditionally, having humans build a benchmark for every possible domain of\ninterest is impractical. Therefore, we propose a methodology for automating the\nconstruction of fact-based synthetic data model evaluations grounded in\ndocument populations. This work leverages those very same LMs to evaluate\ndomain-specific knowledge automatically, using only grounding documents (e.g.,\na textbook) as input. This synthetic data benchmarking approach corresponds\nwell with human curated questions with a Spearman ranking correlation of 0.96\nand a benchmark evaluation Pearson accuracy correlation of 0.79. This novel\ntool supports generating both multiple choice and open-ended synthetic data\nquestions to gain diagnostic insight of LM capability. We apply this\nmethodology to evaluate model performance on a recent relevant arXiv preprint,\ndiscovering a surprisingly strong performance from Gemma3 models.\n","authors":["Michael Majurski","Cynthia Matuszek"],"pdf_url":"https://arxiv.org/pdf/2505.08905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08902v1","updated":"2025-05-13T18:44:22Z","published":"2025-05-13T18:44:22Z","title":"Performance Gains of LLMs With Humans in a World of LLMs Versus Humans","summary":"  Currently, a considerable research effort is devoted to comparing LLMs to a\ngroup of human experts, where the term \"expert\" is often ill-defined or\nvariable, at best, in a state of constantly updating LLM releases. Without\nproper safeguards in place, LLMs will threaten to cause harm to the established\nstructure of safe delivery of patient care which has been carefully developed\nthroughout history to keep the safety of the patient at the forefront. A key\ndriver of LLM innovation is founded on community research efforts which, if\ncontinuing to operate under \"humans versus LLMs\" principles, will expedite this\ntrend. Therefore, research efforts moving forward must focus on effectively\ncharacterizing the safe use of LLMs in clinical settings that persist across\nthe rapid development of novel LLM models. In this communication, we\ndemonstrate that rather than comparing LLMs to humans, there is a need to\ndevelop strategies enabling efficient work of humans with LLMs in an almost\nsymbiotic manner.\n","authors":["Lucas McCullum","Pelagie Ami Agassi","Leo Anthony Celi","Daniel K. Ebner","Chrystinne Oliveira Fernandes","Rachel S. Hicklen","Mkliwa Koumbia","Lisa Soleymani Lehmann","David Restrepo"],"pdf_url":"https://arxiv.org/pdf/2505.08902v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.00039v2","updated":"2025-05-13T17:19:55Z","published":"2025-04-29T18:36:57Z","title":"Graph RAG for Legal Norms: A Hierarchical and Temporal Approach","summary":"  This article proposes an adaptation of Graph Retrieval Augmented Generation\n(Graph RAG) specifically designed for the analysis and comprehension of legal\nnorms, which are characterized by their predefined hierarchical structure,\nextensive network of internal and external references and multiple temporal\nversions. By combining structured knowledge graphs with contextually enriched\ntext segments, Graph RAG offers a promising solution to address the inherent\ncomplexity and vast volume of legal data. The integration of hierarchical\nstructure and temporal evolution into knowledge graphs - along with the concept\nof comprehensive Text Units - facilitates the construction of richer,\ninterconnected representations of legal knowledge. Through a detailed analysis\nof Graph RAG and its application to legal norm datasets, this article aims to\nadvance the field of Artificial Intelligence applied to Law, creating\nopportunities for more effective systems in legal research, legislative\nanalysis, and decision support.\n","authors":["Hudson de Martim"],"pdf_url":"https://arxiv.org/pdf/2505.00039v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08728v1","updated":"2025-05-13T16:39:00Z","published":"2025-05-13T16:39:00Z","title":"Securing RAG: A Risk Assessment and Mitigation Framework","summary":"  Retrieval Augmented Generation (RAG) has emerged as the de facto industry\nstandard for user-facing NLP applications, offering the ability to integrate\ndata without re-training or fine-tuning Large Language Models (LLMs). This\ncapability enhances the quality and accuracy of responses but also introduces\nnovel security and privacy challenges, particularly when sensitive data is\nintegrated. With the rapid adoption of RAG, securing data and services has\nbecome a critical priority. This paper first reviews the vulnerabilities of RAG\npipelines, and outlines the attack surface from data pre-processing and data\nstorage management to integration with LLMs. The identified risks are then\npaired with corresponding mitigations in a structured overview. In a second\nstep, the paper develops a framework that combines RAG-specific security\nconsiderations, with existing general security guidelines, industry standards,\nand best practices. The proposed framework aims to guide the implementation of\nrobust, compliant, secure, and trustworthy RAG systems.\n","authors":["Lukas Ammann","Sara Ott","Christoph R. Landolt","Marco P. Lehmann"],"pdf_url":"https://arxiv.org/pdf/2505.08728v1.pdf","comment":"8 pages, 3 figures, Sara Ott and Lukas Ammann contributed equally"},{"id":"http://arxiv.org/abs/2503.21098v3","updated":"2025-05-13T11:54:26Z","published":"2025-03-27T02:36:48Z","title":"Alleviating LLM-based Generative Retrieval Hallucination in Alipay\n  Search","summary":"  Generative retrieval (GR) has revolutionized document retrieval with the\nadvent of large language models (LLMs), and LLM-based GR is gradually being\nadopted by the industry. Despite its remarkable advantages and potential,\nLLM-based GR suffers from hallucination and generates documents that are\nirrelevant to the query in some instances, severely challenging its credibility\nin practical applications. We thereby propose an optimized GR framework\ndesigned to alleviate retrieval hallucination, which integrates knowledge\ndistillation reasoning in model training and incorporate decision agent to\nfurther improve retrieval precision. Specifically, we employ LLMs to assess and\nreason GR retrieved query-document (q-d) pairs, and then distill the reasoning\ndata as transferred knowledge to the GR model. Moreover, we utilize a decision\nagent as post-processing to extend the GR retrieved documents through retrieval\nmodel and select the most relevant ones from multi perspectives as the final\ngenerative retrieval result. Extensive offline experiments on real-world\ndatasets and online A/B tests on Fund Search and Insurance Search in Alipay\ndemonstrate our framework's superiority and effectiveness in improving search\nquality and conversion gains.\n","authors":["Yedan Shen","Kaixin Wu","Yuechen Ding","Jingyuan Wen","Hong Liu","Mingjie Zhong","Zhouhan Lin","Jia Xu","Linjian Mo"],"pdf_url":"https://arxiv.org/pdf/2503.21098v3.pdf","comment":"Accepted by SIGIR 2025"},{"id":"http://arxiv.org/abs/2505.08471v1","updated":"2025-05-13T11:53:26Z","published":"2025-05-13T11:53:26Z","title":"Interest Changes: Considering User Interest Life Cycle in Recommendation\n  System","summary":"  In recommendation systems, user interests are always in a state of constant\nflux. Typically, a user interest experiences a emergent phase, a stable phase,\nand a declining phase, which are referred to as the \"user interest life-cycle\".\nRecent papers on user interest modeling have primarily focused on how to\ncompute the correlation between the target item and user's historical\nbehaviors, without thoroughly considering the life-cycle features of user\ninterest. In this paper, we propose an effective method called Deep Interest\nLife-cycle Network (DILN), which not only captures the interest life-cycle\nfeatures efficiently, but can also be easily integrated to existing ranking\nmodels. DILN contains two key components: Interest Life-cycle Encoder Module\nconstructs historical activity histograms of the user interest and then encodes\nthem into dense representation. Interest Life-cycle Fusion Module injects the\nencoded dense representation into multiple expert networks, with the aim of\nenabling the specific phase of interest life-cycle to activate distinct\nexperts. Online A/B testing reveals that DILN achieves significant improvements\nof +0.38% in CTR, +1.04% in CVR and +0.25% in duration per user, which\ndemonstrates its effectiveness. In addition, DILN inherently increase the\nexposure of users' emergent and stable interests while decreasing the exposure\nof declining interests. DILN has been deployed on the Lofter App.\n","authors":["Yinjiang Cai","Jiangpan Hou","Yangping Zhu","Yuan Nie"],"pdf_url":"https://arxiv.org/pdf/2505.08471v1.pdf","comment":"Accepted by SIGIR 2025"},{"id":"http://arxiv.org/abs/2504.04752v3","updated":"2025-05-13T11:02:51Z","published":"2025-04-07T05:58:01Z","title":"Investigating Popularity Bias Amplification in Recommender Systems\n  Employed in the Entertainment Domain","summary":"  Recommender systems have become an integral part of our daily online\nexperience by analyzing past user behavior to suggest relevant content in\nentertainment domains such as music, movies, and books. Today, they are among\nthe most widely used applications of AI and machine learning. Consequently,\nregulations and guidelines for trustworthy AI, such as the European AI Act,\nwhich addresses issues like bias and fairness, are highly relevant to the\ndesign, development, and evaluation of recommender systems. One particularly\nimportant type of bias in this context is popularity bias, which results in the\nunfair underrepresentation of less popular content in recommendation lists.\nThis work summarizes our research on investigating the amplification of\npopularity bias in recommender systems within the entertainment sector.\nAnalyzing datasets from three entertainment domains, music, movies, and anime,\nwe demonstrate that an item's recommendation frequency is positively correlated\nwith its popularity. As a result, user groups with little interest in popular\ncontent receive less accurate recommendations compared to those who prefer\nwidely popular items. Furthermore, this work contributes to a better\nunderstanding of the connection between recommendation accuracy, calibration\nquality of algorithms, and popularity bias amplification.\n","authors":["Dominik Kowald"],"pdf_url":"https://arxiv.org/pdf/2504.04752v3.pdf","comment":"Accepted at EWAF'25, summarizes fairness and popularity bias research\n  presented in Dr. Kowald's habilitation:\n  https://domkowald.github.io/documents/others/2024habilitation_recsys.pdf"},{"id":"http://arxiv.org/abs/2505.08411v1","updated":"2025-05-13T10:09:51Z","published":"2025-05-13T10:09:51Z","title":"Lost in Transliteration: Bridging the Script Gap in Neural IR","summary":"  Most human languages use scripts other than the Latin alphabet. Search users\nin these languages often formulate their information needs in a transliterated\n-- usually Latinized -- form for ease of typing. For example, Greek speakers\nmight use Greeklish, and Arabic speakers might use Arabizi. This paper shows\nthat current search systems, including those that use multilingual dense\nembeddings such as BGE-M3, do not generalise to this setting, and their\nperformance rapidly deteriorates when exposed to transliterated queries. This\ncreates a ``script gap\" between the performance of the same queries when\nwritten in their native or transliterated form. We explore whether adapting the\npopular ``translate-train\" paradigm to transliterations can enhance the\nrobustness of multilingual Information Retrieval (IR) methods and bridge the\ngap between native and transliterated scripts. By exploring various\ncombinations of non-Latin and Latinized query text for training, we investigate\nwhether we can enhance the capacity of existing neural retrieval techniques and\nenable them to apply to this important setting. We show that by further\nfine-tuning IR models on an even mixture of native and Latinized text, they can\nperform this cross-script matching at nearly the same performance as when the\nquery was formulated in the native script. Out-of-domain evaluation and further\nqualitative analysis show that transliterations can also cause queries to lose\nsome of their nuances, motivating further research in this direction.\n","authors":["Andreas Chari","Iadh Ounis","Sean MacAvaney"],"pdf_url":"https://arxiv.org/pdf/2505.08411v1.pdf","comment":"6 pages, 2 tables. paper accepted at the Short Paper track of The\n  48th International ACM SIGIR Conference on Research and Development in\n  Information Retrieval"},{"id":"http://arxiv.org/abs/2505.08385v1","updated":"2025-05-13T09:32:09Z","published":"2025-05-13T09:32:09Z","title":"TikTok Search Recommendations: Governance and Research Challenges","summary":"  Like other social media, TikTok is embracing its use as a search engine,\ndeveloping search products to steer users to produce searchable content and\nengage in content discovery. Their recently developed product search\nrecommendations are preformulated search queries recommended to users on\nvideos. However, TikTok provides limited transparency about how search\nrecommendations are generated and moderated, despite requirements under\nregulatory frameworks like the European Union's Digital Services Act. By\nsuggesting that the platform simply aggregates comments and common searches\nlinked to videos, it sidesteps responsibility and issues that arise from\ncontextually problematic recommendations, reigniting long-standing concerns\nabout platform liability and moderation. This position paper addresses the\nnovelty of search recommendations on TikTok by highlighting the challenges that\nthis feature poses for platform governance and offering a computational\nresearch agenda, drawing on preliminary qualitative analysis. It sets out the\nneed for transparency in platform documentation, data access and research to\nstudy search recommendations.\n","authors":["Taylor Annabell","Robert Gorwa","Rebecca Scharlach","Jacob van de Kerkhof","Thales Bertaglia"],"pdf_url":"https://arxiv.org/pdf/2505.08385v1.pdf","comment":"to appear in The 1st international Workshop on Computational\n  Approaches to Content Moderation and Platform Governance (COMPASS), held at\n  ICWSM 2025"},{"id":"http://arxiv.org/abs/2407.00104v2","updated":"2025-05-13T09:29:47Z","published":"2024-06-27T07:33:34Z","title":"Clinically inspired enhance Explainability and Interpretability of an\n  AI-Tool for BCC diagnosis based on expert annotation","summary":"  An AI tool has been developed to provide interpretable support for the\ndiagnosis of BCC via teledermatology, thus speeding up referrals and optimizing\nresource utilization. The interpretability is provided in two ways: on the one\nhand, the main BCC dermoscopic patterns are found in the image to justify the\nBCC/Non BCC classification. Secondly, based on the common visual XAI Grad-CAM,\na clinically inspired visual explanation is developed where the relevant\nfeatures for diagnosis are located. Since there is no established ground truth\nfor BCC dermoscopic features, a standard reference is inferred from the\ndiagnosis of four dermatologists using an Expectation Maximization (EM) based\nalgorithm. The results demonstrate significant improvements in classification\naccuracy and interpretability, positioning this approach as a valuable tool for\nearly BCC detection and referral to dermatologists. The BCC/non-BCC\nclassification achieved an accuracy rate of 90%. For Clinically-inspired XAI\nresults, the detection of BCC patterns useful to clinicians reaches 99%\naccuracy. As for the Clinically-inspired Visual XAI results, the mean of the\nGrad-CAM normalized value within the manually segmented clinical features is\n0.57, while outside this region it is 0.16. This indicates that the model\nstruggles to accurately identify the regions of the BCC patterns. These results\nprove the ability of the AI tool to provide a useful explanation.\n","authors":["IvÃ¡n Matas","Carmen Serrano","Francisca Silva","Amalia Serrano","TomÃ¡s Toledo-Pastrana","BegoÃ±a Acha"],"pdf_url":"https://arxiv.org/pdf/2407.00104v2.pdf","comment":"8 pages, 4 figures, 4 tables, under review"},{"id":"http://arxiv.org/abs/2505.07577v2","updated":"2025-05-13T09:27:19Z","published":"2025-05-12T13:57:47Z","title":"From raw affiliations to organization identifiers","summary":"  Accurate affiliation matching, which links affiliation strings to\nstandardized organization identifiers, is critical for improving research\nmetadata quality, facilitating comprehensive bibliometric analyses, and\nsupporting data interoperability across scholarly knowledge bases. Existing\napproaches fail to handle the complexity of affiliation strings that often\ninclude mentions of multiple organizations or extraneous information. In this\npaper, we present AffRo, a novel approach designed to address these challenges,\nleveraging advanced parsing and disambiguation techniques. We also introduce\nAffRoDB, an expert-curated dataset to systematically evaluate affiliation\nmatching algorithms, ensuring robust benchmarking. Results demonstrate the\neffectiveness of AffRp in accurately identifying organizations from complex\naffiliation strings.\n","authors":["Myrto Kallipoliti","Serafeim Chatzopoulos","Miriam Baglioni","Eleni Adamidi","Paris Koloveas","Thanasis Vergoulis"],"pdf_url":"https://arxiv.org/pdf/2505.07577v2.pdf","comment":"16 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.10613v3","updated":"2025-05-13T05:08:02Z","published":"2024-08-20T07:48:19Z","title":"Task-level Distributionally Robust Optimization for Large Language\n  Model-based Dense Retrieval","summary":"  Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous\nheterogeneous fine-tuning collections from different domains. However, the\ndiscussion about its training data distribution is still minimal. Previous\nstudies rely on empirically assigned dataset choices or sampling ratios, which\ninevitably lead to sub-optimal retrieval performances. In this paper, we\npropose a new task-level Distributionally Robust Optimization (tDRO) algorithm\nfor LLM-DR fine-tuning, targeted at improving the universal domain\ngeneralization ability by end-to-end reweighting the data distribution of each\ntask. The tDRO parameterizes the domain weights and updates them with scaled\ndomain gradients. The optimized weights are then transferred to the LLM-DR\nfine-tuning to train more robust retrievers. Experiments show optimal\nimprovements in large-scale retrieval benchmarks and reduce up to 30% dataset\nusage after applying our optimization algorithm with a series of\ndifferent-sized LLM-DR models.\n","authors":["Guangyuan Ma","Yongliang Ma","Xing Wu","Zhenpeng Su","Ming Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.10613v3.pdf","comment":"Accepted by AAAI25. Source code is available at\n  https://github.com/ma787639046/tdro"},{"id":"http://arxiv.org/abs/2501.02968v3","updated":"2025-05-13T02:16:51Z","published":"2025-01-06T12:24:57Z","title":"FlippedRAG: Black-Box Opinion Manipulation Adversarial Attacks to\n  Retrieval-Augmented Generation Models","summary":"  Retrieval-Augmented Generation (RAG) enriches LLMs by dynamically retrieving\nexternal knowledge, reducing hallucinations and satisfying real-time\ninformation needs. While existing research mainly targets RAG's performance and\nefficiency, emerging studies highlight critical security concerns. Yet, current\nadversarial approaches remain limited, mostly addressing white-box scenarios or\nheuristic black-box attacks without fully investigating vulnerabilities in the\nretrieval phase. Additionally, prior works mainly focus on factoid QA tasks,\ntheir attacks lack complexity and can be easily corrected by advanced LLMs. In\nthis paper, we investigate a more realistic and critical threat scenario:\nadversarial attacks intended for opinion manipulation against black-box RAG\nmodels, particularly on controversial topics. Specifically, we propose\nFlippedRAG, a transfer-based adversarial attack against black-box RAG systems.\nWe first demonstrate that the underlying retriever of a black-box RAG system\ncan be reverse-engineered, enabling us to train a surrogate retriever.\nLeveraging the surrogate retriever, we further craft target poisoning triggers,\naltering vary few documents to effectively manipulate both retrieval and\nsubsequent generation. Extensive empirical results show that FlippedRAG\nsubstantially outperforms baseline methods, improving the average attack\nsuccess rate by 16.7%. FlippedRAG achieves on average a 50% directional shift\nin the opinion polarity of RAG-generated responses, ultimately causing a\nnotable 20% shift in user cognition. Furthermore, we evaluate the performance\nof several potential defensive measures, concluding that existing mitigation\nstrategies remain insufficient against such sophisticated manipulation attacks.\nThese results highlight an urgent need for developing innovative defensive\nsolutions to ensure the security and trustworthiness of RAG systems.\n","authors":["Zhuo Chen","Jiawei Liu","Yuyang Gong","Miaokun Chen","Haotan Liu","Qikai Cheng","Fan Zhang","Wei Lu","Xiaozhong Liu","Xiaofeng Wang"],"pdf_url":"https://arxiv.org/pdf/2501.02968v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.13757"},{"id":"http://arxiv.org/abs/2405.13238v6","updated":"2025-05-13T02:16:29Z","published":"2024-05-21T22:53:00Z","title":"Enhancing User Interest based on Stream Clustering and Memory Networks\n  in Large-Scale Recommender Systems","summary":"  Recommender Systems (RSs) provide personalized recommendation service based\non user interest, which are widely used in various platforms. However, there\nare lots of users with sparse interest due to lacking consumption behaviors,\nwhich leads to poor recommendation results for them. This problem is widespread\nin large-scale RSs and is particularly difficult to address. To solve this\nchallenging problem, we propose an innovative solution called User Interest\nEnhancement (UIE). UIE enhances user interest including user profile and user\nhistory behavior sequences by leveraging the enhancement vectors and\npersonalized enhancement vectors generated based on dynamic streaming\nclustering of similar users and items from multiple perspectives, which are\nstored and updated in memory networks. UIE not only remarkably improves model\nperformance for users with sparse interest, but also delivers notable gains for\nother users. As an end-to-end solution, UIE is easy to implement on top of\nexisting ranking models. Furthermore, we extend our approach to long-tail items\nusing similar methods, which also yields excellent improvements. We conduct\nextensive offline and online experiments in an industrial RS. The results\ndemonstrate that UIE substantially outperforms other existing approaches,\nespecially for users with sparse interest. UIE has been deployed in several\nlarge-scale RSs at Tencent since 2022, which was made public on 21 May 2024. In\naddition, UIE-based methods have also been successfully applied in candidate\ngeneration, pre-ranking, and context-DNN stages. Multiple teams have developed\nsolutions based on UIE, focusing on updating clustering algorithms and\nattention mechanisms. As far as we know, UIE has been applied in multiple RSs,\nadvertising systems and search engines. The thoughts of UIE, dynamic streaming\nclustering and similarity enhancement, have inspired subsequent relevant works.\n","authors":["Peng Liu","Nian Wang","Cong Xu","Ming Zhao","Bin Wang","Yi Ren"],"pdf_url":"https://arxiv.org/pdf/2405.13238v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08157v1","updated":"2025-05-13T01:30:27Z","published":"2025-05-13T01:30:27Z","title":"Hyperbolic Contrastive Learning with Model-augmentation for\n  Knowledge-aware Recommendation","summary":"  Benefiting from the effectiveness of graph neural networks (GNNs) and\ncontrastive learning, GNN-based contrastive learning has become mainstream for\nknowledge-aware recommendation. However, most existing contrastive\nlearning-based methods have difficulties in effectively capturing the\nunderlying hierarchical structure within user-item bipartite graphs and\nknowledge graphs. Moreover, they commonly generate positive samples for\ncontrastive learning by perturbing the graph structure, which may lead to a\nshift in user preference learning. To overcome these limitations, we propose\nhyperbolic contrastive learning with model-augmentation for knowledge-aware\nrecommendation. To capture the intrinsic hierarchical graph structures, we\nfirst design a novel Lorentzian knowledge aggregation mechanism, which enables\nmore effective representations of users and items. Then, we propose three\nmodel-level augmentation techniques to assist Hyperbolic contrastive learning.\nDifferent from the classical structure-level augmentation (e.g., edge\ndropping), the proposed model-augmentations can avoid preference shifts between\nthe augmented positive pair. Finally, we conduct extensive experiments to\ndemonstrate the superiority (maximum improvement of $11.03\\%$) of proposed\nmethods over existing baselines.\n","authors":["Shengyin Sun","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2505.08157v1.pdf","comment":"18 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.08990v1","updated":"2025-05-13T22:00:22Z","published":"2025-05-13T22:00:22Z","title":"Toward Accessible and Safe Live Streaming Using Distributed Content\n  Filtering with MoQ","summary":"  Live video streaming is increasingly popular on social media platforms. With\nthe growth of live streaming comes an increased need for robust content\nmoderation to remove dangerous, illegal, or otherwise objectionable content.\nWhereas video on demand distribution enables offline content analysis, live\nstreaming imposes restrictions on latency for both analysis and distribution.\nIn this paper, we present extensions to the in-progress Media Over QUIC\nTransport protocol that enable real-time content moderation in one-to-many\nvideo live streams. Importantly, our solution removes only the video segments\nthat contain objectionable content, allowing playback resumption as soon as the\nstream conforms to content policies again. Content analysis tasks may be\ntransparently distributed to arbitrary client devices. We implement and\nevaluate our system in the context of light strobe removal for photosensitive\nviewers, finding that streaming clients experience an increased latency of only\none group-of-pictures duration.\n","authors":["Andrew C. Freeman"],"pdf_url":"https://arxiv.org/pdf/2505.08990v1.pdf","comment":"Accepted to the ICME 2025 LIVES workshop"},{"id":"http://arxiv.org/abs/2504.07687v3","updated":"2025-05-13T14:09:20Z","published":"2025-04-10T12:16:32Z","title":"FMNV: A Dataset of Media-Published News Videos for Fake News Detection","summary":"  News media, particularly video-based platforms, have become deeply embed-ded\nin daily life, concurrently amplifying the risks of misinformation\ndissem-ination. Consequently, multimodal fake news detection has garnered\nsignifi-cant research attention. However, existing datasets predominantly\ncomprise user-generated videos characterized by crude editing and limited\npublic en-gagement, whereas professionally crafted fake news videos\ndisseminated by media outlets-often politically or virally motivated-pose\nsubstantially greater societal harm. To address this gap, we construct FMNV, a\nnovel da-taset exclusively composed of news videos published by media\norganizations. Through empirical analysis of existing datasets and our curated\ncollection, we categorize fake news videos into four distinct types. Building\nupon this taxonomy, we employ Large Language Models (LLMs) to automatically\ngenerate deceptive content by manipulating authentic media-published news\nvideos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream\narchitecture that integrates spatio-temporal motion features from a 3D\nResNeXt-101 backbone and static visual semantics from CLIP. The two streams are\nfused via an attention-based mechanism, while co-attention modules refine the\nvisual, textual, and audio features for effective multi-modal aggregation.\nComparative experiments demonstrate both the generali-zation capability of FMNV\nacross multiple baselines and the superior detec-tion efficacy of FMNVD. This\nwork establishes critical benchmarks for de-tecting high-impact fake news in\nmedia ecosystems while advancing meth-odologies for cross-modal inconsistency\nanalysis. Our dataset is available in https://github.com/DennisIW/FMNV.\n","authors":["Yihao Wang","Zhong Qian","Peifeng Li"],"pdf_url":"https://arxiv.org/pdf/2504.07687v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08137v1","updated":"2025-05-13T00:19:04Z","published":"2025-05-13T00:19:04Z","title":"Large Language Models for Computer-Aided Design: A Survey","summary":"  Large Language Models (LLMs) have seen rapid advancements in recent years,\nwith models like ChatGPT and DeepSeek, showcasing their remarkable capabilities\nacross diverse domains. While substantial research has been conducted on LLMs\nin various fields, a comprehensive review focusing on their integration with\nComputer-Aided Design (CAD) remains notably absent. CAD is the industry\nstandard for 3D modeling and plays a vital role in the design and development\nof products across different industries. As the complexity of modern designs\nincreases, the potential for LLMs to enhance and streamline CAD workflows\npresents an exciting frontier. This article presents the first systematic\nsurvey exploring the intersection of LLMs and CAD. We begin by outlining the\nindustrial significance of CAD, highlighting the need for AI-driven innovation.\nNext, we provide a detailed overview of the foundation of LLMs. We also examine\nboth closed-source LLMs as well as publicly available models. The core of this\nreview focuses on the various applications of LLMs in CAD, providing a taxonomy\nof six key areas where these models are making considerable impact. Finally, we\npropose several promising future directions for further advancements, which\noffer vast opportunities for innovation and are poised to shape the future of\nCAD technology. Github:\nhttps://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy\n","authors":["Licheng Zhang","Bach Le","Naveed Akhtar","Siew-Kei Lam","Tuan Ngo"],"pdf_url":"https://arxiv.org/pdf/2505.08137v1.pdf","comment":null}]},"2025-05-12T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2503.22742v2","updated":"2025-05-12T21:58:10Z","published":"2025-03-26T19:32:31Z","title":"Adaptive Integrated Layered Attention (AILA)","summary":"  We propose Adaptive Integrated Layered Attention (AILA), a neural network\narchitecture that combines dense skip connections with different mechanisms for\nadaptive feature reuse across network layers. We evaluate AILA on three\nchallenging tasks: price forecasting for various commodities and indices (S&P\n500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the\nCIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In\nall cases, AILA matches strong deep learning baselines (LSTMs, Transformers,\nand ResNets), achieving it at a fraction of the training and inference time.\nNotably, we implement and test two versions of the model - AILA-Architecture 1,\nwhich uses simple linear layers as the connection mechanism between layers, and\nAILA-Architecture 2, which implements an attention mechanism to selectively\nfocus on outputs from previous layers. Both architectures are applied in a\nsingle-task learning setting, with each model trained separately for individual\ntasks. Results confirm that AILA's adaptive inter-layer connections yield\nrobust gains by flexibly reusing pertinent features at multiple network depths.\nThe AILA approach thus presents an extension to existing architectures,\nimproving long-range sequence modeling, image recognition with optimised\ncomputational speed, and SOTA classification performance in practice.\n","authors":["William Claster","Suhas KM","Dhairya Gundechia"],"pdf_url":"https://arxiv.org/pdf/2503.22742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10497v2","updated":"2025-05-12T21:51:32Z","published":"2025-04-06T06:52:38Z","title":"Exploring Generative AI Techniques in Government: A Case Study","summary":"  The swift progress of Generative Artificial intelligence (GenAI), notably\nLarge Language Models (LLMs), is reshaping the digital landscape. Recognizing\nthis transformative potential, the National Research Council of Canada (NRC)\nlaunched a pilot initiative to explore the integration of GenAI techniques into\nits daily operation for performance excellence, where 22 projects were launched\nin May 2024. Within these projects, this paper presents the development of the\nintelligent agent Pubbie as a case study, targeting the automation of\nperformance measurement, data management and insight reporting at the NRC.\nCutting-edge techniques are explored, including LLM orchestration and semantic\nembedding via RoBERTa, while strategic fine-tuning and few-shot learning\napproaches are incorporated to infuse domain knowledge at an affordable cost.\nThe user-friendly interface of Pubbie allows general government users to input\nqueries in natural language and easily upload or download files with a simple\nbutton click, greatly reducing manual efforts and accessibility barriers.\n","authors":["Sunyi Liu","Mengzhe Geng","Rebecca Hart"],"pdf_url":"https://arxiv.org/pdf/2504.10497v2.pdf","comment":"In submission to IEEE Intelligent Systems"},{"id":"http://arxiv.org/abs/2412.06695v3","updated":"2025-05-12T19:57:03Z","published":"2024-12-09T17:41:25Z","title":"Towards Brain Passage Retrieval -- An Investigation of EEG Query\n  Representations","summary":"  Information Retrieval (IR) systems primarily rely on users' ability to\ntranslate their internal information needs into (text) queries. However, this\ntranslation process is often uncertain and cognitively demanding, leading to\nqueries that incompletely or inaccurately represent users' true needs. This\nchallenge is particularly acute for users with ill-defined information needs or\nphysical impairments that limit traditional text input, where the gap between\ncognitive intent and query expression becomes even more pronounced. Recent\nneuroscientific studies have explored Brain-Machine Interfaces (BMIs) as a\npotential solution, aiming to bridge the gap between users' cognitive semantics\nand their search intentions. However, current approaches attempting to decode\nexplicit text queries from brain signals have shown limited effectiveness in\nlearning robust brain-to-text representations, often failing to capture the\nnuanced semantic information present in brain patterns. To address these\nlimitations, we propose BPR (Brain Passage Retrieval), a novel framework that\neliminates the need for intermediate query translation by enabling direct\nretrieval of relevant passages from users' brain signals. Our approach\nleverages dense retrieval architectures to map EEG signals and text passages\ninto a shared semantic space. Through comprehensive experiments on the ZuCo\ndataset, we demonstrate that BPR achieves up to 8.81% improvement in\nprecision@5 over existing EEG-to-text baselines, while maintaining\neffectiveness across 30 participants. Our ablation studies reveal the critical\nrole of hard negative sampling and specialised brain encoders in achieving\nrobust cross-modal alignment. These results establish the viability of direct\nbrain-to-passage retrieval and provide a foundation for developing more natural\ninterfaces between users' cognitive states and IR systems.\n","authors":["Niall McGuire","Yashar Moshfeghi"],"pdf_url":"https://arxiv.org/pdf/2412.06695v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14614v2","updated":"2025-05-12T18:06:39Z","published":"2024-05-23T14:26:04Z","title":"Push and Pull: A Framework for Measuring Attentional Agency on Digital\n  Platforms","summary":"  We propose a framework for measuring attentional agency, which we define as a\nuser's ability to allocate attention according to their own desires, goals, and\nintentions on digital platforms that use statistical learning to prioritize\ninformational content. Such platforms extend people's limited powers of\nattention by extrapolating their preferences to large collections of previously\nunconsidered informational objects. However, platforms typically also allow\nusers to influence the attention of other users in various ways. We introduce a\nformal framework for measuring how much a given platform empowers each user to\nboth pull information into their own attention and push information into the\nattention of others. We also use these definitions to clarify the implications\nof generative foundation models and other recent advances in AI for the\nstructure and efficiency of digital platforms. We conclude with a set of\npossible strategies for better understanding and reshaping attentional agency\nonline.\n","authors":["Zachary Wojtowicz","Shrey Jain","Nicholas Vincent"],"pdf_url":"https://arxiv.org/pdf/2405.14614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07730v1","updated":"2025-05-12T16:37:47Z","published":"2025-05-12T16:37:47Z","title":"Reproducibility, Replicability, and Insights into Visual Document\n  Retrieval with Late Interaction","summary":"  Visual Document Retrieval (VDR) is an emerging research area that focuses on\nencoding and retrieving document images directly, bypassing the dependence on\nOptical Character Recognition (OCR) for document search. A recent advance in\nVDR was introduced by ColPali, which significantly improved retrieval\neffectiveness through a late interaction mechanism. ColPali's approach\ndemonstrated substantial performance gains over existing baselines that do not\nuse late interaction on an established benchmark. In this study, we investigate\nthe reproducibility and replicability of VDR methods with and without late\ninteraction mechanisms by systematically evaluating their performance across\nmultiple pre-trained vision-language models. Our findings confirm that late\ninteraction yields considerable improvements in retrieval effectiveness;\nhowever, it also introduces computational inefficiencies during inference.\nAdditionally, we examine the adaptability of VDR models to textual inputs and\nassess their robustness across text-intensive datasets within the proposed\nbenchmark, particularly when scaling the indexing mechanism. Furthermore, our\nresearch investigates the specific contributions of late interaction by looking\ninto query-patch matching in the context of visual document retrieval. We find\nthat although query tokens cannot explicitly match image patches as in the text\nretrieval scenario, they tend to match the patch contains visually similar\ntokens or their surrounding patches.\n","authors":["Jingfen Qiao","Jia-Huei Ju","Xinyu Ma","Evangelos Kanoulas","Andrew Yates"],"pdf_url":"https://arxiv.org/pdf/2505.07730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07671v1","updated":"2025-05-12T15:34:45Z","published":"2025-05-12T15:34:45Z","title":"Benchmarking Retrieval-Augmented Generation for Chemistry","summary":"  Retrieval-augmented generation (RAG) has emerged as a powerful framework for\nenhancing large language models (LLMs) with external knowledge, particularly in\nscientific domains that demand specialized and dynamic information. Despite its\npromise, the application of RAG in the chemistry domain remains underexplored,\nprimarily due to the lack of high-quality, domain-specific corpora and\nwell-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a\ncomprehensive benchmark designed to systematically assess the effectiveness of\nRAG across a diverse set of chemistry-related tasks. The accompanying chemistry\ncorpus integrates heterogeneous knowledge sources, including scientific\nliterature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia\nentries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG\ntoolkit that supports five retrieval algorithms and eight LLMs. Using\nChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain\n-- achieving an average relative improvement of 17.4% over direct inference\nmethods. We further conduct in-depth analyses on retriever architectures,\ncorpus selection, and the number of retrieved passages, culminating in\npractical recommendations to guide future research and deployment of RAG\nsystems in the chemistry domain. The code and data is available at\nhttps://chemrag.github.io.\n","authors":["Xianrui Zhong","Bowen Jin","Siru Ouyang","Yanzhen Shen","Qiao Jin","Yin Fang","Zhiyong Lu","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2505.07671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07917v1","updated":"2025-05-12T14:51:47Z","published":"2025-05-12T14:51:47Z","title":"Efficient and Reproducible Biomedical Question Answering using Retrieval\n  Augmented Generation","summary":"  Biomedical question-answering (QA) systems require effective retrieval and\ngeneration components to ensure accuracy, efficiency, and scalability. This\nstudy systematically examines a Retrieval-Augmented Generation (RAG) system for\nbiomedical QA, evaluating retrieval strategies and response time trade-offs. We\nfirst assess state-of-the-art retrieval methods, including BM25, BioBERT,\nMedCPT, and a hybrid approach, alongside common data stores such as\nElasticsearch, MongoDB, and FAISS, on a ~10% subset of PubMed (2.4M documents)\nto measure indexing efficiency, retrieval latency, and retriever performance in\nthe end-to-end RAG system. Based on these insights, we deploy the final RAG\nsystem on the full 24M PubMed corpus, comparing different retrievers' impact on\noverall performance. Evaluations of the retrieval depth show that retrieving 50\ndocuments with BM25 before reranking with MedCPT optimally balances accuracy\n(0.90), recall (0.90), and response time (1.91s). BM25 retrieval time remains\nstable (82ms), while MedCPT incurs the main computational cost. These results\nhighlight previously not well-known trade-offs in retrieval depth, efficiency,\nand scalability for biomedical QA. With open-source code, the system is fully\nreproducible and extensible.\n","authors":["Linus Stuhlmann","Michael Alexander Saxer","Jonathan FÃ¼rst"],"pdf_url":"https://arxiv.org/pdf/2505.07917v1.pdf","comment":"Accepted at SDS25"},{"id":"http://arxiv.org/abs/2505.07618v1","updated":"2025-05-12T14:42:19Z","published":"2025-05-12T14:42:19Z","title":"KAQG: A Knowledge-Graph-Enhanced RAG for Difficulty-Controlled Question\n  Generation","summary":"  KAQG introduces a decisive breakthrough for Retrieval-Augmented Generation\n(RAG) by explicitly tackling the two chronic weaknesses of current pipelines:\ntransparent multi-step reasoning and fine-grained cognitive difficulty control.\nThis transforms RAG from a passive retriever into an accountable generator of\ncalibrated exam items. Technically, the framework fuses knowledge graphs, RAG\nretrieval, and educational assessment theory into a single pipeline. Domain\npassages are parsed into a structured graph; graph-aware retrieval feeds fact\nchains to an LLM; and an assessment layer governed by Bloom's Taxonomy levels\nand Item Response Theory (IRT) transforms those chains into psychometrically\nsound questions. This cross-disciplinary marriage yields two scholarly\ncontributions: it shows how semantic graph contexts guide LLM reasoning paths,\nand it operationalizes difficulty metrics within the generation process,\nproducing items whose IRT parameters match expert benchmarks. Every module,\nfrom KG construction scripts to the multi-agent reasoning scheduler and the\nautomatic IRT validator, is openly released on GitHub. This enables peer\nlaboratories to replicate experiments, benchmark against baselines, and extend\nindividual components without licensing barriers. Its reproducible design paves\nthe way for rigorous ablation studies, cross-domain transfer experiments, and\nshared leaderboards on multi-step reasoning benchmarks.\n","authors":["Ching Han Chen","Ming Fang Shiu"],"pdf_url":"https://arxiv.org/pdf/2505.07618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07554v1","updated":"2025-05-12T13:31:26Z","published":"2025-05-12T13:31:26Z","title":"Injecting Knowledge Graphs into Large Language Models","summary":"  Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) remains a key challenge for symbolic reasoning. Existing\nmethods mainly rely on prompt engineering or fine-tuning, which lose structural\nfidelity or incur high computational costs. Building on recent encoding\ntechniques which integrate graph embeddings within the LLM input as tokens, we\nextend this paradigm to the KG domain by leveraging Knowledge Graph Embedding\n(KGE) models, thus enabling graph-aware reasoning. Our approach is\nmodel-agnostic, resource-efficient, and compatible with any LLMs. Extensive\nexperimentation on synthetic and real-world datasets shows that our method\nimproves reasoning performance over established baselines, further achieving\nthe best trade-off in terms of accuracy and efficiency against state-of-the-art\nLLMs.\n","authors":["Erica Coppolillo"],"pdf_url":"https://arxiv.org/pdf/2505.07554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07546v1","updated":"2025-05-12T13:27:35Z","published":"2025-05-12T13:27:35Z","title":"GRADA: Graph-based Reranker against Adversarial Documents Attack","summary":"  Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large\nlanguage models (LLMs) by integrating external knowledge from retrieved\ndocuments, thereby overcoming the limitations of models' static intrinsic\nknowledge. However, these systems are susceptible to adversarial attacks that\nmanipulate the retrieval process by introducing documents that are adversarial\nyet semantically similar to the query. Notably, while these adversarial\ndocuments resemble the query, they exhibit weak similarity to benign documents\nin the retrieval set. Thus, we propose a simple yet effective Graph-based\nReranking against Adversarial Document Attacks (GRADA) framework aiming at\npreserving retrieval quality while significantly reducing the success of\nadversaries. Our study evaluates the effectiveness of our approach through\nexperiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b,\nLlama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with\nresults from the Natural Questions dataset demonstrating up to an 80% reduction\nin attack success rates while maintaining minimal loss in accuracy.\n","authors":["Jingjie Zheng","Aryo Pradipta Gema","Giwon Hong","Xuanli He","Pasquale Minervini","Youcheng Sun","Qiongkai Xu"],"pdf_url":"https://arxiv.org/pdf/2505.07546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07459v1","updated":"2025-05-12T11:47:42Z","published":"2025-05-12T11:47:42Z","title":"Why Uncertainty Estimation Methods Fall Short in RAG: An Axiomatic\n  Analysis","summary":"  Large Language Models (LLMs) are valued for their strong performance across\nvarious tasks, but they also produce inaccurate or misleading outputs.\nUncertainty Estimation (UE) quantifies the model's confidence and helps users\nassess response reliability. However, existing UE methods have not been\nthoroughly examined in scenarios like Retrieval-Augmented Generation (RAG),\nwhere the input prompt includes non-parametric knowledge. This paper shows that\ncurrent UE methods cannot reliably assess correctness in the RAG setting. We\nfurther propose an axiomatic framework to identify deficiencies in existing\nmethods and guide the development of improved approaches. Our framework\nintroduces five constraints that an effective UE method should meet after\nincorporating retrieved documents into the LLM's prompt. Experimental results\nreveal that no existing UE method fully satisfies all the axioms, explaining\ntheir suboptimal performance in RAG. We further introduce a simple yet\neffective calibration function based on our framework, which not only satisfies\nmore axioms than baseline methods but also improves the correlation between\nuncertainty estimates and correctness.\n","authors":["Heydar Soudani","Evangelos Kanoulas","Faegheh Hasibi"],"pdf_url":"https://arxiv.org/pdf/2505.07459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07431v1","updated":"2025-05-12T10:47:59Z","published":"2025-05-12T10:47:59Z","title":"Diffusion-driven SpatioTemporal Graph KANsformer for Medical Examination\n  Recommendation","summary":"  Recommendation systems in AI-based medical diagnostics and treatment\nconstitute a critical component of AI in healthcare. Although some studies have\nexplored this area and made notable progress, healthcare recommendation systems\nremain in their nascent stage. And these researches mainly target the treatment\nprocess such as drug or disease recommendations. In addition to the treatment\nprocess, the diagnostic process, particularly determining which medical\nexaminations are necessary to evaluate the condition, also urgently requires\nintelligent decision support. To bridge this gap, we first formalize the task\nof medical examination recommendations. Compared to traditional\nrecommendations, the medical examination recommendation involves more complex\ninteractions. This complexity arises from two folds: 1) The historical medical\nrecords for examination recommendations are heterogeneous and redundant, which\nmakes the recommendation results susceptible to noise. 2) The correlation\nbetween the medical history of patients is often irregular, making it\nchallenging to model spatiotemporal dependencies. Motivated by the above\nobservation, we propose a novel Diffusion-driven SpatioTemporal Graph\nKANsformer for Medical Examination Recommendation (DST-GKAN) with a two-stage\nlearning paradigm to solve the above challenges. In the first stage, we exploit\na task-adaptive diffusion model to distill recommendation-oriented information\nby reducing the noises in heterogeneous medical data. In the second stage, a\nspatiotemporal graph KANsformer is proposed to simultaneously model the complex\nspatial and temporal relationships. Moreover, to facilitate the medical\nexamination recommendation research, we introduce a comprehensive dataset. The\nexperimental results demonstrate the state-of-the-art performance of the\nproposed method compared to various competitive baselines.\n","authors":["Jianan Li","Yangtao Zhou","Zhifu Zhao","Qinglan Huang","Jian Qi","Xiao He","Hua Chu","Fu Li"],"pdf_url":"https://arxiv.org/pdf/2505.07431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07377v3","updated":"2025-05-12T09:24:03Z","published":"2025-03-10T14:31:00Z","title":"Process-Supervised LLM Recommenders via Flow-guided Tuning","summary":"  While large language models (LLMs) are increasingly adapted for\nrecommendation systems via supervised fine-tuning (SFT), this approach\namplifies popularity bias due to its likelihood maximization objective,\ncompromising recommendation diversity and fairness. To address this, we present\nFlow-guided fine-tuning recommender (Flower), which replaces SFT with a\nGenerative Flow Network (GFlowNet) framework that enacts process supervision\nthrough token-level reward propagation. Flower's key innovation lies in\ndecomposing item-level rewards into constituent token rewards, enabling direct\nalignment between token generation probabilities and their reward signals. This\nmechanism achieves three critical advancements: (1) popularity bias mitigation\nand fairness enhancement through empirical distribution matching, (2)\npreservation of diversity through GFlowNet's proportional sampling, and (3)\nflexible integration of personalized preferences via adaptable token rewards.\nExperiments demonstrate Flower's superior distribution-fitting capability and\nits significant advantages over traditional SFT in terms of accuracy, fairness,\nand diversity, highlighting its potential to improve LLM-based recommendation\nsystems. The implementation is available via\nhttps://github.com/MrPeach0301/Flower\n","authors":["Chongming Gao","Mengyao Gao","Chenxiao Fan","Shuai Yuan","Wentao Shi","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2503.07377v3.pdf","comment":"Accepted by SIGIR 2025"},{"id":"http://arxiv.org/abs/2505.07345v1","updated":"2025-05-12T08:35:09Z","published":"2025-05-12T08:35:09Z","title":"QUPID: Quantified Understanding for Enhanced Performance, Insights, and\n  Decisions in Korean Search Engines","summary":"  Large language models (LLMs) have been widely used for relevance assessment\nin information retrieval. However, our study demonstrates that combining two\ndistinct small language models (SLMs) with different architectures can\noutperform LLMs in this task. Our approach -- QUPID -- integrates a generative\nSLM with an embedding-based SLM, achieving higher relevance judgment accuracy\nwhile reducing computational costs compared to state-of-the-art LLM solutions.\nThis computational efficiency makes QUPID highly scalable for real-world search\nsystems processing millions of queries daily. In experiments across diverse\ndocument types, our method demonstrated consistent performance improvements\n(Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x\nfaster inference times. Furthermore, when integrated into production search\npipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how\narchitectural diversity in model combinations can significantly enhance both\nsearch relevance and operational efficiency in information retrieval systems.\n","authors":["Ohjoon Kwon","Changsu Lee","Jihye Back","Lim Sun Suk","Inho Kang","Donghyeon Jeon"],"pdf_url":"https://arxiv.org/pdf/2505.07345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10208v2","updated":"2025-05-12T07:58:20Z","published":"2025-04-14T13:21:29Z","title":"From Prompting to Alignment: A Generative Framework for Query\n  Recommendation","summary":"  In modern search systems, search engines often suggest relevant queries to\nusers through various panels or components, helping refine their information\nneeds. Traditionally, these recommendations heavily rely on historical search\nlogs to build models, which suffer from cold-start or long-tail issues.\nFurthermore, tasks such as query suggestion, completion or clarification are\nstudied separately by specific design, which lacks generalizability and hinders\nadaptation to novel applications. Despite recent attempts to explore the use of\nLLMs for query recommendation, these methods mainly rely on the inherent\nknowledge of LLMs or external sources like few-shot examples, retrieved\ndocuments, or knowledge bases, neglecting the importance of the calibration and\nalignment with user feedback, thus limiting their practical utility. To address\nthese challenges, we first propose a general Generative Query Recommendation\n(GQR) framework that aligns LLM-based query generation with user preference.\nSpecifically, we unify diverse query recommendation tasks by a universal prompt\nframework, leveraging the instruct-following capability of LLMs for effective\ngeneration. Secondly, we align LLMs with user feedback via presenting a\nCTR-alignment framework, which involves training a query-wise CTR predictor as\na process reward model and employing list-wise preference alignment to maximize\nthe click probability of the generated query list. Furthermore, recognizing the\ninconsistency between LLM knowledge and proactive search intents arising from\nthe separation of user-initiated queries from models, we align LLMs with user\ninitiative via retrieving co-occurrence queries as side information when\nhistorical logs are available.\n","authors":["Erxue Min","Hsiu-Yuan Huang","Min Yang","Xihong Yang","Xin Jia","Yunfang Wu","Hengyi Cai","Junfeng Wang","Shuaiqiang Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2504.10208v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07257v1","updated":"2025-05-12T06:18:31Z","published":"2025-05-12T06:18:31Z","title":"DARLR: Dual-Agent Offline Reinforcement Learning for Recommender Systems\n  with Dynamic Reward","summary":"  Model-based offline reinforcement learning (RL) has emerged as a promising\napproach for recommender systems, enabling effective policy learning by\ninteracting with frozen world models. However, the reward functions in these\nworld models, trained on sparse offline logs, often suffer from inaccuracies.\nSpecifically, existing methods face two major limitations in addressing this\nchallenge: (1) deterministic use of reward functions as static look-up tables,\nwhich propagates inaccuracies during policy learning, and (2) static\nuncertainty designs that fail to effectively capture decision risks and\nmitigate the impact of these inaccuracies. In this work, a dual-agent\nframework, DARLR, is proposed to dynamically update world models to enhance\nrecommendation policies. To achieve this, a \\textbf{\\textit{selector}} is\nintroduced to identify reference users by balancing similarity and diversity so\nthat the \\textbf{\\textit{recommender}} can aggregate information from these\nusers and iteratively refine reward estimations for dynamic reward shaping.\nFurther, the statistical features of the selected users guide the dynamic\nadaptation of an uncertainty penalty to better align with evolving\nrecommendation requirements. Extensive experiments on four benchmark datasets\ndemonstrate the superior performance of DARLR, validating its effectiveness.\nThe code is available at https://github.com/ArronDZhang/DARLR.\n","authors":["Yi Zhang","Ruihong Qiu","Xuwei Xu","Jiajun Liu","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2505.07257v1.pdf","comment":"SIGIR 2025"},{"id":"http://arxiv.org/abs/2407.13163v2","updated":"2025-05-12T05:47:47Z","published":"2024-07-18T05:07:11Z","title":"ROLeR: Effective Reward Shaping in Offline Reinforcement Learning for\n  Recommender Systems","summary":"  Offline reinforcement learning (RL) is an effective tool for real-world\nrecommender systems with its capacity to model the dynamic interest of users\nand its interactive nature. Most existing offline RL recommender systems focus\non model-based RL through learning a world model from offline data and building\nthe recommendation policy by interacting with this model. Although these\nmethods have made progress in the recommendation performance, the effectiveness\nof model-based offline RL methods is often constrained by the accuracy of the\nestimation of the reward model and the model uncertainties, primarily due to\nthe extreme discrepancy between offline logged data and real-world data in user\ninteractions with online platforms. To fill this gap, a more accurate reward\nmodel and uncertainty estimation are needed for the model-based RL methods. In\nthis paper, a novel model-based Reward Shaping in Offline Reinforcement\nLearning for Recommender Systems, ROLeR, is proposed for reward and uncertainty\nestimation in recommendation systems. Specifically, a non-parametric reward\nshaping method is designed to refine the reward model. In addition, a flexible\nand more representative uncertainty penalty is designed to fit the needs of\nrecommendation systems. Extensive experiments conducted on four benchmark\ndatasets showcase that ROLeR achieves state-of-the-art performance compared\nwith existing baselines. The source code can be downloaded at\nhttps://github.com/ArronDZhang/ROLeR.\n","authors":["Yi Zhang","Ruihong Qiu","Jiajun Liu","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13163v2.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2505.07197v1","updated":"2025-05-12T03:01:14Z","published":"2025-05-12T03:01:14Z","title":"A Generative Re-ranking Model for List-level Multi-objective\n  Optimization at Taobao","summary":"  E-commerce recommendation systems aim to generate ordered lists of items for\ncustomers, optimizing multiple business objectives, such as clicks, conversions\nand Gross Merchandise Volume (GMV). Traditional multi-objective optimization\nmethods like formulas or Learning-to-rank (LTR) models take effect at\nitem-level, neglecting dynamic user intent and contextual item interactions.\nList-level multi-objective optimization in the re-ranking stage can overcome\nthis limitation, but most current re-ranking models focus more on accuracy\nimprovement with context. In addition, re-ranking is faced with the challenges\nof time complexity and diversity. In light of this, we propose a novel\nend-to-end generative re-ranking model named Sequential Ordered Regression\nTransformer-Generator (SORT-Gen) for the less-studied list-level\nmulti-objective optimization problem. Specifically, SORT-Gen is divided into\ntwo parts: 1)Sequential Ordered Regression Transformer innovatively uses\nTransformer and ordered regression to accurately estimate multi-objective\nvalues for variable-length sub-lists. 2)Mask-Driven Fast Generation Algorithm\ncombines multi-objective candidate queues, efficient item selection and\ndiversity mechanism into model inference, providing a fast online list\ngeneration method. Comprehensive online experiments demonstrate that SORT-Gen\nbrings +4.13% CLCK and +8.10% GMV for Baiyibutie, a notable Mini-app of Taobao.\nCurrently, SORT-Gen has been successfully deployed in multiple scenarios of\nTaobao App, serving for a vast number of users.\n","authors":["Yue Meng","Cheng Guo","Yi Cao","Tong Liu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.07197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07171v1","updated":"2025-05-12T01:49:52Z","published":"2025-05-12T01:49:52Z","title":"ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for\n  Few-Shot Knowledge Graph Completion","summary":"  Knowledge Graphs (KGs), composed of triples in the form of (head, relation,\ntail) and consisting of entities and relations, play a key role in information\nretrieval systems such as question answering, entity search, and\nrecommendation. In real-world KGs, although many entities exist, the relations\nexhibit a long-tail distribution, which can hinder information retrieval\nperformance. Previous few-shot knowledge graph completion studies focused\nexclusively on the positive triple information that exists in the graph or,\nwhen negative triples were incorporated, used them merely as a signal to\nindicate incorrect triples. To overcome this limitation, we propose\nRelation-Based Conditional Diffusion with Attention Pooling (ReCDAP). First,\nnegative triples are generated by randomly replacing the tail entity in the\nsupport set. By conditionally incorporating positive information in the KG and\nnon-existent negative information into the diffusion process, the model\nseparately estimates the latent distributions for positive and negative\nrelations. Moreover, including an attention pooler enables the model to\nleverage the differences between positive and negative cases explicitly.\nExperiments on two widely used datasets demonstrate that our method outperforms\nexisting approaches, achieving state-of-the-art performance. The code is\navailable at https://github.com/hou27/ReCDAP-FKGC.\n","authors":["Jeongho Kim","Chanyeong Heo","Jaehee Jung"],"pdf_url":"https://arxiv.org/pdf/2505.07171v1.pdf","comment":"Accepted by SIGIR 2025, 5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2505.07166v1","updated":"2025-05-12T01:24:00Z","published":"2025-05-12T01:24:00Z","title":"Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval\n  Knowledge Acquisition","summary":"  Dense retrievers utilize pre-trained backbone language models (e.g., BERT,\nLLaMA) that are fine-tuned via contrastive learning to perform the task of\nencoding text into sense representations that can be then compared via a\nshallow similarity operation, e.g. inner product. Recent research has\nquestioned the role of fine-tuning vs. that of pre-training within dense\nretrievers, specifically arguing that retrieval knowledge is primarily gained\nduring pre-training, meaning knowledge not acquired during pre-training cannot\nbe sub-sequentially acquired via fine-tuning. We revisit this idea here as the\nclaim was only studied in the context of a BERT-based encoder using DPR as\nrepresentative dense retriever. We extend the previous analysis by testing\nother representation approaches (comparing the use of CLS tokens with that of\nmean pooling), backbone architectures (encoder-only BERT vs. decoder-only\nLLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Our\nstudy confirms that in DPR tuning, pre-trained knowledge underpins retrieval\nperformance, with fine-tuning primarily adjusting neuron activation rather than\nreorganizing knowledge. However, this pattern does not hold universally, such\nas in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure full\nreproducibility and make our implementation publicly available at\nhttps://github.com/ielab/DenseRetriever-Knowledge-Acquisition.\n","authors":["Zheng Yao","Shuai Wang","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2505.07166v1.pdf","comment":"Accepted in SIGIR-2025"},{"id":"http://arxiv.org/abs/2505.07155v1","updated":"2025-05-12T00:15:02Z","published":"2025-05-12T00:15:02Z","title":"Reassessing Large Language Model Boolean Query Generation for Systematic\n  Reviews","summary":"  Systematic reviews are comprehensive literature reviews that address highly\nfocused research questions and represent the highest form of evidence in\nmedicine. A critical step in this process is the development of complex Boolean\nqueries to retrieve relevant literature. Given the difficulty of manually\nconstructing these queries, recent efforts have explored Large Language Models\n(LLMs) to assist in their formulation. One of the first studies,Wang et al.,\ninvestigated ChatGPT for this task, followed by Staudinger et al., which\nevaluated multiple LLMs in a reproducibility study. However, the latter\noverlooked several key aspects of the original work, including (i) validation\nof generated queries, (ii) output formatting constraints, and (iii) selection\nof examples for chain-of-thought (Guided) prompting. As a result, its findings\ndiverged significantly from the original study. In this work, we systematically\nreproduce both studies while addressing these overlooked factors. Our results\nshow that query effectiveness varies significantly across models and prompt\ndesigns, with guided query formulation benefiting from well-chosen seed\nstudies. Overall, prompt design and model selection are key drivers of\nsuccessful query formulation. Our findings provide a clearer understanding of\nLLMs' potential in Boolean query generation and highlight the importance of\nmodel- and prompt-specific optimisations. The complex nature of systematic\nreviews adds to challenges in both developing and reproducing methods but also\nhighlights the importance of reproducibility studies in this domain.\n","authors":["Shuai Wang","Harrisen Scells","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2505.07155v1.pdf","comment":"Accepted in SIGIR-2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.07912v1","updated":"2025-05-12T13:38:20Z","published":"2025-05-12T13:38:20Z","title":"SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for\n  Scientific Videos and Podcasts","summary":"  Democratic societies need accessible, reliable information. Videos and\nPodcasts have established themselves as the medium of choice for civic\ndissemination, but also as carriers of misinformation. The emerging Science\nCommunication Knowledge Infrastructure (SciCom KI) curating non-textual media\nis still fragmented and not adequately equipped to scale against the content\nflood. Our work sets out to support the SciCom KI with a central, collaborative\nplatform, the SciCom Wiki, to facilitate FAIR (findable, accessible,\ninteroperable, reusable) media representation and the fact-checking of their\ncontent, particularly for videos and podcasts. Building an open-source service\nsystem centered around Wikibase, we survey requirements from 53 stakeholders,\nrefine these in 11 interviews, and evaluate our prototype based on these\nrequirements with another 14 participants. To address the most requested\nfeature, fact-checking, we developed a neurosymbolic computational\nfact-checking approach, converting heterogenous media into knowledge graphs.\nThis increases machine-readability and allows comparing statements against\nequally represented ground-truth. Our computational fact-checking tool was\niteratively evaluated through 10 expert interviews, a public user survey with\n43 participants verified the necessity and usability of our tool. Overall, our\nfindings identified several needs to systematically support the SciCom KI. The\nSciCom Wiki, as a FAIR digital library complementing our neurosymbolic\ncomputational fact-checking framework, was found suitable to address the raised\nrequirements. Further, we identified that the SciCom KI is severely\nunderdeveloped regarding FAIR knowledge and related systems facilitating its\ncollaborative creation and curation. Our system can provide a central knowledge\nnode, yet a collaborative effort is required to scale against the imminent\n(mis-)information flood.\n","authors":["Tim Wittenborg","Constantin Sebastian Tremel","Niklas Stehr","Oliver Karras","Markus Stocker","SÃ¶ren Auer"],"pdf_url":"https://arxiv.org/pdf/2505.07912v1.pdf","comment":"18 pages, 10 figures, submitted to TPDL 2025"},{"id":"http://arxiv.org/abs/2505.07365v1","updated":"2025-05-12T09:04:16Z","published":"2025-05-12T09:04:16Z","title":"Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning\n  in The DCASE 2025 Challenge","summary":"  We present Task 5 of the DCASE 2025 Challenge: an Audio Question Answering\n(AQA) benchmark spanning multiple domains of sound understanding. This task\ndefines three QA subsets (Bioacoustics, Temporal Soundscapes, and Complex QA)\nto test audio-language models on interactive question-answering over diverse\nacoustic scenes. We describe the dataset composition (from marine mammal calls\nto soundscapes and complex real-world clips), the evaluation protocol (top-1\naccuracy with answer-shuffling robustness), and baseline systems\n(Qwen2-Audio-7B, AudioFlamingo 2, Gemini-2-Flash). Preliminary results on the\ndevelopment set are compared, showing strong variation across models and\nsubsets. This challenge aims to advance the audio understanding and reasoning\ncapabilities of audio-language models toward human-level acuity, which are\ncrucial for enabling AI agents to perceive and interact about the world\neffectively.\n","authors":["Chao-Han Huck Yang","Sreyan Ghosh","Qing Wang","Jaeyeon Kim","Hengyi Hong","Sonal Kumar","Guirui Zhong","Zhifeng Kong","S Sakshi","Vaibhavi Lokegaonkar","Oriol Nieto","Ramani Duraiswami","Dinesh Manocha","Gunhee Kim","Jun Du","Rafael Valle","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2505.07365v1.pdf","comment":"Preprint. DCASE 2025 Audio QA Challenge:\n  https://dcase.community/challenge2025/task-audio-question-answering"},{"id":"http://arxiv.org/abs/2505.07164v1","updated":"2025-05-12T01:15:50Z","published":"2025-05-12T01:15:50Z","title":"EmoVLM-KD: Fusing Distilled Expertise with Vision-Language Models for\n  Visual Emotion Analysis","summary":"  Visual emotion analysis, which has gained considerable attention in the field\nof affective computing, aims to predict the dominant emotions conveyed by an\nimage. Despite advancements in visual emotion analysis with the emergence of\nvision-language models, we observed that instruction-tuned vision-language\nmodels and conventional vision models exhibit complementary strengths in visual\nemotion analysis, as vision-language models excel in certain cases, whereas\nvision models perform better in others. This finding highlights the need to\nintegrate these capabilities to enhance the performance of visual emotion\nanalysis. To bridge this gap, we propose EmoVLM-KD, an instruction-tuned\nvision-language model augmented with a lightweight module distilled from\nconventional vision models. Instead of deploying both models simultaneously,\nwhich incurs high computational costs, we transfer the predictive patterns of a\nconventional vision model into the vision-language model using a knowledge\ndistillation framework. Our approach first fine-tunes a vision-language model\non emotion-specific instruction data and then attaches a distilled module to\nits visual encoder while keeping the vision-language model frozen. Predictions\nfrom the vision language model and the distillation module are effectively\nbalanced by a gate module, which subsequently generates the final outcome.\nExtensive experiments show that EmoVLM-KD achieves state-of-the-art performance\non multiple visual emotion analysis benchmark datasets, outperforming the\nexisting methods while maintaining computational efficiency. The code is\navailable in https://github.com/sange1104/EmoVLM-KD.\n","authors":["SangEun Lee","Yubeen Lee","Eunil Park"],"pdf_url":"https://arxiv.org/pdf/2505.07164v1.pdf","comment":"Accepted at Workshop and Competition on Affective & Behavior Analysis\n  in-the-wild (ABAW), CVPR 2025, 10 pages, 4 figures, 4 tables"}]},"2025-05-11T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.07105v1","updated":"2025-05-11T20:00:00Z","published":"2025-05-11T20:00:00Z","title":"Knowledge Distillation for Enhancing Walmart E-commerce Search Relevance\n  Using Large Language Models","summary":"  Ensuring the products displayed in e-commerce search results are relevant to\nusers queries is crucial for improving the user experience. With their advanced\nsemantic understanding, deep learning models have been widely used for\nrelevance matching in search tasks. While large language models (LLMs) offer\nsuperior ranking capabilities, it is challenging to deploy LLMs in real-time\nsystems due to the high-latency requirements. To leverage the ranking power of\nLLMs while meeting the low-latency demands of production systems, we propose a\nnovel framework that distills a high performing LLM into a more efficient,\nlow-latency student model. To help the student model learn more effectively\nfrom the teacher model, we first train the teacher LLM as a classification\nmodel with soft targets. Then, we train the student model to capture the\nrelevance margin between pairs of products for a given query using mean squared\nerror loss. Instead of using the same training data as the teacher model, we\nsignificantly expand the student model dataset by generating unlabeled data and\nlabeling it with the teacher model predictions. Experimental results show that\nthe student model performance continues to improve as the size of the augmented\ntraining data increases. In fact, with enough augmented data, the student model\ncan outperform the teacher model. The student model has been successfully\ndeployed in production at Walmart.com with significantly positive metrics.\n","authors":["Hongwei Shang","Nguyen Vo","Nitin Yadav","Tian Zhang","Ajit Puthenputhussery","Xunfan Cai","Shuyi Chen","Prijith Chandran","Changsung Kang"],"pdf_url":"https://arxiv.org/pdf/2505.07105v1.pdf","comment":"9 pages, published at WWWW'25"},{"id":"http://arxiv.org/abs/2505.07042v1","updated":"2025-05-11T16:27:19Z","published":"2025-05-11T16:27:19Z","title":"A Reinforcement Learning Framework for Application-Specific TCP\n  Congestion-Control","summary":"  The Congestion Control (CC) module plays a critical role in the Transmission\nControl Protocol (TCP), ensuring the stability and efficiency of network data\ntransmission. The CC approaches that are commonly used these days employ\nheuristics-based rules to adjust the sending rate. Due to their\nheuristics-based nature, these approaches are not only unable to adapt to\nchanging network conditions but are also agnostic to the diverse requirements\nthat different applications often have. Recently, several learning-based CC\napproaches have been proposed to adapt to changing network conditions.\nUnfortunately, they are not designed to take application requirements into\naccount. Prior heuristics-based as well as learning-based CC approaches focus\non achieving a singular objective, which is often to maximize throughput, even\nthough a lot of applications care more about latency, packet losses, jitter,\nand different combinations of various network metrics. Motivated by this, we\npropose a Deep Reinforcement Learning (DRL) based CC framework, namely ASC,\nwhich allows any application to specify any arbitrary objectives that the\nnetwork traffic of that application should achieve and is able to swiftly adapt\nto the changes in the objectives of the applications as well as to the changes\nin the network conditions. Our ASC framework further employs a client-server\narchitecture that serves two purposes: 1) it makes ASC highly scalable in terms\nof the arrival and departure of TCP connections, and 2) it makes ASC very\nlightweight for the nodes maintaining the TCP connections. We implemented and\nextensively evaluated ASC in a variety of settings. Our results show that it\ncan not only achieve various objectives but also outperforms prior approaches\neven in the specific objectives that those approaches were designed to achieve.\n","authors":["Jinming Xing","Muhammad Shahzad"],"pdf_url":"https://arxiv.org/pdf/2505.07042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07034v1","updated":"2025-05-11T16:10:37Z","published":"2025-05-11T16:10:37Z","title":"NetSight: Graph Attention Based Traffic Forecasting in Computer Networks","summary":"  The traffic in today's networks is increasingly influenced by the\ninteractions among network nodes as well as by the temporal fluctuations in the\ndemands of the nodes. Traditional statistical prediction methods are becoming\nobsolete due to their inability to address the non-linear and dynamic\nspatio-temporal dependencies present in today's network traffic. The most\npromising direction of research today is graph neural networks (GNNs) based\nprediction approaches that are naturally suited to handle graph-structured\ndata. Unfortunately, the state-of-the-art GNN approaches separate the modeling\nof spatial and temporal information, resulting in the loss of important\ninformation about joint dependencies. These GNN based approaches further do not\nmodel information at both local and global scales simultaneously, leaving\nsignificant room for improvement. To address these challenges, we propose\nNetSight. NetSight learns joint spatio-temporal dependencies simultaneously at\nboth global and local scales from the time-series of measurements of any given\nnetwork metric collected at various nodes in a network. Using the learned\ninformation, NetSight can then accurately predict the future values of the\ngiven network metric at those nodes in the network. We propose several new\nconcepts and techniques in the design of NetSight, such as spatio-temporal\nadjacency matrix and node normalization. Through extensive evaluations and\ncomparison with prior approaches using data from two large real-world networks,\nwe show that NetSight significantly outperforms all prior state-of-the-art\napproaches. We will release the source code and data used in the evaluation of\nNetSight on the acceptance of this paper.\n","authors":["Jinming Xing","Guoheng Sun","Hui Sun","Linchao Pan","Shakir Mahmood","Xuanhao Luo","Muhammad Shahzad"],"pdf_url":"https://arxiv.org/pdf/2505.07034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06972v1","updated":"2025-05-11T13:07:15Z","published":"2025-05-11T13:07:15Z","title":"Web Page Classification using LLMs for Crawling Support","summary":"  A web crawler is a system designed to collect web pages, and efficient\ncrawling of new pages requires appropriate algorithms. While website features\nsuch as XML sitemaps and the frequency of past page updates provide important\nclues for accessing new pages, their universal application across diverse\nconditions is challenging. In this study, we propose a method to efficiently\ncollect new pages by classifying web pages into two types, \"Index Pages\" and\n\"Content Pages,\" using a large language model (LLM), and leveraging the\nclassification results to select index pages as starting points for accessing\nnew pages. We construct a dataset with automatically annotated web page types\nand evaluate our approach from two perspectives: the page type classification\nperformance and coverage of new pages. Experimental results demonstrate that\nthe LLM-based method outperformed baseline methods in both evaluation metrics.\n","authors":["Yuichi Sasazawa","Yasuhiro Sogawa"],"pdf_url":"https://arxiv.org/pdf/2505.06972v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2505.06914v1","updated":"2025-05-11T09:25:05Z","published":"2025-05-11T09:25:05Z","title":"The Distracting Effect: Understanding Irrelevant Passages in RAG","summary":"  A well-known issue with Retrieval Augmented Generation (RAG) is that\nretrieved passages that are irrelevant to the query sometimes distract the\nanswer-generating LLM, causing it to provide an incorrect response. In this\npaper, we shed light on this core issue and formulate the distracting effect of\na passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the\ndistracting effect of a passage and demonstrate its robustness across LLMs.\n  Our research introduces novel methods for identifying and using hard\ndistracting passages to improve RAG systems. By fine-tuning LLMs with these\ncarefully selected distracting passages, we achieve up to a 7.5% increase in\nanswering accuracy compared to counterparts fine-tuned on conventional RAG\ndatasets. Our contribution is two-fold: first, we move beyond the simple binary\nclassification of irrelevant passages as either completely unrelated vs.\ndistracting, and second, we develop and analyze multiple methods for finding\nhard distracting passages. To our knowledge, no other research has provided\nsuch a comprehensive framework for identifying and utilizing hard distracting\npassages.\n","authors":["Chen Amiraz","Florin Cuconasu","Simone Filice","Zohar Karnin"],"pdf_url":"https://arxiv.org/pdf/2505.06914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06885v1","updated":"2025-05-11T07:33:31Z","published":"2025-05-11T07:33:31Z","title":"Incremental Analysis of Legacy Applications Using Knowledge Graphs for\n  Application Modernization","summary":"  Industries such as banking, telecom, and airlines - o6en have large so6ware\nsystems that are several decades old. Many of these systems are written in old\nprogramming languages such as COBOL, PL/1, Assembler, etc. In many cases, the\ndocumentation is not updated, and those who developed/designed these systems\nare no longer around. Understanding these systems for either modernization or\neven regular maintenance has been a challenge. An extensive application may\nhave natural boundaries based on its code dependencies and architecture. There\nare also other logical boundaries in an enterprise setting driven by business\nfunctions, data domains, etc. Due to these complications, the system architects\ngenerally plan their modernization across these logical boundaries in parts,\nthereby adopting an incremental approach for the modernization journey of the\nentire system. In this work, we present a so6ware system analysis tool that\nallows a subject ma=er expert (SME) or system architect to analyze a large\nso6ware system incrementally. We analyze the source code and other artifacts\n(such as data schema) to create a knowledge graph using a customizable\nontology/schema. Entities and relations in our ontology can be defined for any\ncombination of programming languages and platforms. Using this knowledge graph,\nthe analyst can then define logical boundaries around dependent Entities (e.g.\nPrograms, Transactions, Database Tables etc.). Our tool then presents different\nviews showcasing the dependencies from the newly defined boundary to/from the\nother logical groups of the system. This exercise is repeated interactively to\n1) Identify the Entities and groupings of interest for a modernization task and\n2) Understand how a change in one part of the system may affect the other\nparts. To validate the efficacy of our tool, we provide an initial study of our\nsystem on two client applications.\n","authors":["Saravanan Krishnan","Amith Singhee","Keerthi Narayan Raghunath","Alex Mathai","Atul Kumar","David Wenk"],"pdf_url":"https://arxiv.org/pdf/2505.06885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06862v1","updated":"2025-05-11T06:14:39Z","published":"2025-05-11T06:14:39Z","title":"A Split-then-Join Approach to Abstractive Summarization for Very Long\n  Documents in a Low Resource Setting","summary":"  $\\texttt{BIGBIRD-PEGASUS}$ model achieves $\\textit{state-of-the-art}$ on\nabstractive text summarization for long documents. However it's capacity still\nlimited to maximum of $4,096$ tokens, thus caused performance degradation on\nsummarization for very long documents. Common method to deal with the issue is\nto truncate the documents. In this reasearch, we'll use different approach.\nWe'll use the pretrained $\\texttt{BIGBIRD-PEGASUS}$ model by fine tuned the\nmodel on other domain dataset. First, we filter out all documents which length\nless than $20,000$ tokens to focus on very long documents. To prevent domain\nshifting problem and overfitting on transfer learning due to small dataset, we\naugment the dataset by splitting document-summary training pair into parts, to\nfit the document into $4,096$ tokens. Source code available on\n$\\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$.\n","authors":["Lhuqita Fazry"],"pdf_url":"https://arxiv.org/pdf/2505.06862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06841v1","updated":"2025-05-11T04:53:34Z","published":"2025-05-11T04:53:34Z","title":"Optimizing Recommendations using Fine-Tuned LLMs","summary":"  As digital media platforms strive to meet evolving user expectations,\ndelivering highly personalized and intuitive movies and media recommendations\nhas become essential for attracting and retaining audiences. Traditional\nsystems often rely on keyword-based search and recommendation techniques, which\nlimit users to specific keywords and a combination of keywords. This paper\nproposes an approach that generates synthetic datasets by modeling real-world\nuser interactions, creating complex chat-style data reflective of diverse\npreferences. This allows users to express more information with complex\npreferences, such as mood, plot details, and thematic elements, in addition to\nconventional criteria like genre, title, and actor-based searches. In today's\nsearch space, users cannot write queries like ``Looking for a fantasy movie\nfeaturing dire wolves, ideally set in a harsh frozen world with themes of\nloyalty and survival.''\n  Building on these contributions, we evaluate synthetic datasets for diversity\nand effectiveness in training and benchmarking models, particularly in areas\noften absent from traditional datasets. This approach enhances personalization\nand accuracy by enabling expressive and natural user queries. It establishes a\nfoundation for the next generation of conversational AI-driven search and\nrecommendation systems in digital entertainment.\n","authors":["Prabhdeep Cheema","Erhan Guven"],"pdf_url":"https://arxiv.org/pdf/2505.06841v1.pdf","comment":"Accepted and presented at IEEE CAI 2025. This version includes minor\n  clarifications and formatting updates"}],"Multimedia":[{"id":"http://arxiv.org/abs/2105.00335v2","updated":"2025-05-11T23:57:58Z","published":"2021-05-01T19:38:30Z","title":"Audio Transformers","summary":"  Over the past two decades, CNN architectures have produced compelling models\nof sound perception and cognition, learning hierarchical organizations of\nfeatures. Analogous to successes in computer vision, audio feature\nclassification can be optimized for a particular task of interest, over a wide\nvariety of datasets and labels. In fact similar architectures designed for\nimage understanding have proven effective for acoustic scene analysis. Here we\npropose applying Transformer based architectures without convolutional layers\nto raw audio signals. On a standard dataset of Free Sound 50K,comprising of 200\ncategories, our model outperforms convolutional models to produce state of the\nart results. This is significant as unlike in natural language processing and\ncomputer vision, we do not perform unsupervised pre-training for outperforming\nconvolutional architectures. On the same training set, with respect mean\naver-age precision benchmarks, we show a significant improvement. We further\nimprove the performance of Transformer architectures by using techniques such\nas pooling inspired from convolutional net-work designed in the past few years.\nIn addition, we also show how multi-rate signal processing ideas inspired from\nwavelets, can be applied to the Transformer embeddings to improve the results.\nWe also show how our models learns a non-linear non constant band-width\nfilter-bank, which shows an adaptable time frequency front end representation\nfor the task of audio understanding, different from other tasks e.g. pitch\nestimation.\n","authors":["Prateek Verma","Jonathan Berger"],"pdf_url":"https://arxiv.org/pdf/2105.00335v2.pdf","comment":"5 pages, 4 figures; Under review WASPAA 2021; Typo Fixes"},{"id":"http://arxiv.org/abs/2505.03420v2","updated":"2025-05-11T15:47:48Z","published":"2025-05-06T10:55:21Z","title":"Mitigating Image Captioning Hallucinations in Vision-Language Models","summary":"  Hallucinations in vision-language models (VLMs) hinder reliability and\nreal-world applicability, usually stemming from distribution shifts between\npretraining data and test samples. Existing solutions, such as retraining or\nfine-tuning on additional data, demand significant computational resources and\nlabor-intensive data collection, while ensemble-based methods incur additional\ncosts by introducing auxiliary VLMs. To address these challenges, we propose a\nnovel test-time adaptation framework using reinforcement learning to mitigate\nhallucinations during inference without retraining or any auxiliary VLMs. By\nupdating only the learnable parameters in the layer normalization of the\nlanguage model (approximately 0.003% of the model parameters), our method\nreduces distribution shifts between test samples and pretraining samples. A\nCLIP-based hallucination evaluation model is proposed to provide dual rewards\nto VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in\nhallucination rates on LLaVA and InstructBLIP, respectively. Our approach\noutperforms state-of-the-art baselines with a 68.3% improvement in\nhallucination mitigation, demonstrating its effectiveness.\n","authors":["Fei Zhao","Chengcui Zhang","Runlin Zhang","Tianyang Wang","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2505.03420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03603v3","updated":"2025-05-11T13:44:53Z","published":"2025-05-06T15:03:58Z","title":"PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model","summary":"  Audio-driven human animation technology is widely used in human-computer\ninteraction, and the emergence of diffusion models has further advanced its\ndevelopment. Currently, most methods rely on multi-stage generation and\nintermediate representations, resulting in long inference time and issues with\ngeneration quality in specific foreground regions and audio-motion consistency.\nThese shortcomings are primarily due to the lack of localized fine-grained\nsupervised guidance. To address above challenges, we propose PAHA, an\nend-to-end audio-driven upper-body human animation framework with diffusion\nmodel. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts\nConsistency Enhancement (PCE). PAR dynamically adjusts regional training loss\nweights based on pose confidence scores, effectively improving visual quality.\nPCE constructs and trains diffusion-based regional audio-visual classifiers to\nimprove the consistency of motion and co-speech audio. Afterwards, we design\ntwo novel inference guidance methods for the foregoing classifiers, Sequential\nGuidance (SG) and Differential Guidance (DG), to balance efficiency and quality\nrespectively. Additionally, we build CNAS, the first public Chinese News Anchor\nSpeech dataset, to advance research and validation in this field. Extensive\nexperimental results and user studies demonstrate that PAHA significantly\noutperforms existing methods in audio-motion alignment and video-related\nevaluations. The codes and CNAS dataset will be released upon acceptance.\n","authors":["S. Z. Zhou","Y. B. Wang","J. F. Wu","T. Hu","J. N. Zhang","Z. J. Li","Y. Liu"],"pdf_url":"https://arxiv.org/pdf/2505.03603v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06803v1","updated":"2025-05-11T01:01:44Z","published":"2025-05-11T01:01:44Z","title":"Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models\n  to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via\n  Cross-Modal Distillation","summary":"  Audio large language models (LLMs) are considered experts at recognizing\nsound objects, yet their performance relative to LLMs in other sensory\nmodalities, such as visual or audio-visual LLMs, and to humans using their\nears, eyes, or both remains unexplored. To investigate this, we systematically\nevaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio,\nQwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of\ndifferent classes from audio-only, silent video, or sounded video inputs. We\nuncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the\nsensory discrepancy between human ears and eyes. To reduce this gap, we\nintroduce a cross-modal distillation framework, where an LLM in one modality\nserves as the teacher and another as the student, with knowledge transfer in\nsound classes predicted as more challenging to the student by a heuristic\nmodel. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice\nversa, leads to notable improvements, particularly in challenging classes. This\nwork highlights the sensory gap in LLMs from a human-aligned perspective and\nproposes a principled approach to enhancing modality-specific perception in\nmultimodal LLMs.\n","authors":["Xilin Jiang","Junkai Wu","Vishal Choudhari","Nima Mesgarani"],"pdf_url":"https://arxiv.org/pdf/2505.06803v1.pdf","comment":null}]},"2025-05-10T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.07884v1","updated":"2025-05-10T22:59:24Z","published":"2025-05-10T22:59:24Z","title":"Development of a WAZOBIA-Named Entity Recognition System","summary":"  Named Entity Recognition NER is very crucial for various natural language\nprocessing applications, including information extraction, machine translation,\nand sentiment analysis. Despite the ever-increasing interest in African\nlanguages within computational linguistics, existing NER systems focus mainly\non English, European, and a few other global languages, leaving a significant\ngap for under-resourced languages. This research presents the development of a\nWAZOBIA-NER system tailored for the three most prominent Nigerian languages:\nHausa, Yoruba, and Igbo. This research begins with a comprehensive compilation\nof annotated datasets for each language, addressing data scarcity and\nlinguistic diversity challenges. Exploring the state-of-the-art machine\nlearning technique, Conditional Random Fields (CRF) and deep learning models\nsuch as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder\nRepresentation from Transformers (Bert) and fine-tune with a Recurrent Neural\nNetwork (RNN), the study evaluates the effectiveness of these approaches in\nrecognizing three entities: persons, organizations, and locations. The system\nutilizes optical character recognition (OCR) technology to convert textual\nimages into machine-readable text, thereby enabling the Wazobia system to\naccept both input text and textual images for extraction purposes. The system\nachieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in\nF1-score, and 0.9301 in accuracy. The model's evaluation was conducted across\nthree languages, with precision, recall, F1-score, and accuracy as key\nassessment metrics. The Wazobia-NER system demonstrates that it is feasible to\nbuild robust NER tools for under-resourced African languages using current NLP\nframeworks and transfer learning.\n","authors":["S. E Emedem","I. E Onyenwe","E. G Onyedinma"],"pdf_url":"https://arxiv.org/pdf/2505.07884v1.pdf","comment":"6 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2505.07879v1","updated":"2025-05-10T14:24:41Z","published":"2025-05-10T14:24:41Z","title":"OMGM: Orchestrate Multiple Granularities and Modalities for Efficient\n  Multimodal Retrieval","summary":"  Vision-language retrieval-augmented generation (RAG) has become an effective\napproach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which\nrequires external knowledge beyond the visual content presented in images. The\neffectiveness of Vision-language RAG systems hinges on multimodal retrieval,\nwhich is inherently challenging due to the diverse modalities and knowledge\ngranularities in both queries and knowledge bases. Existing methods have not\nfully tapped into the potential interplay between these elements. We propose a\nmultimodal RAG system featuring a coarse-to-fine, multi-step retrieval that\nharmonizes multiple granularities and modalities to enhance efficacy. Our\nsystem begins with a broad initial search aligning knowledge granularity for\ncross-modal retrieval, followed by a multimodal fusion reranking to capture the\nnuanced multimodal information for top entity selection. A text reranker then\nfilters out the most relevant fine-grained section for augmented generation.\nExtensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our\nmethod achieves state-of-the-art retrieval performance and highly competitive\nanswering results, underscoring its effectiveness in advancing KB-VQA systems.\n","authors":["Wei Yang","Jingjing Fu","Rui Wang","Jinyu Wang","Lei Song","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2505.07879v1.pdf","comment":"19 pages, 6 figures, 17 tables"},{"id":"http://arxiv.org/abs/2505.06612v1","updated":"2025-05-10T11:51:22Z","published":"2025-05-10T11:51:22Z","title":"Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic\n  Modeling in Social Recommendation","summary":"  In the era of rapid development of social media, social recommendation\nsystems as hybrid recommendation systems have been widely applied. Existing\nmethods capture interest similarity between users to filter out\ninterest-irrelevant relations in social networks that inevitably decrease\nrecommendation accuracy, however, limited research has a focus on the mutual\ninfluence of semantic information between the social network and the user-item\ninteraction network for further improving social recommendation. To address\nthese issues, we introduce a social \\underline{r}ecommendation model with\nro\\underline{bu}st g\\underline{r}aph denoisin\\underline{g}-augmentation fusion\nand multi-s\\underline{e}mantic Modeling(Burger). Specifically, we firstly\npropose to construct a social tensor in order to smooth the training process of\nthe model. Then, a graph convolutional network and a tensor convolutional\nnetwork are employed to capture user's item preference and social preference,\nrespectively. Considering the different semantic information in the user-item\ninteraction network and the social network, a bi-semantic coordination loss is\nproposed to model the mutual influence of semantic information. To alleviate\nthe interference of interest-irrelevant relations on multi-semantic modeling,\nwe further use Bayesian posterior probability to mine potential social\nrelations to replace social noise. Finally, the sliding window mechanism is\nutilized to update the social tensor as the input for the next iteration.\nExtensive experiments on three real datasets show Burger has a superior\nperformance compared with the state-of-the-art models.\n","authors":["Yuqin Lan"],"pdf_url":"https://arxiv.org/pdf/2505.06612v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.17844v2","updated":"2025-05-10T09:09:46Z","published":"2024-04-27T09:44:56Z","title":"Towards Robust Recommendation: A Review and an Adversarial Robustness\n  Evaluation Library","summary":"  Recently, recommender system has achieved significant success. However, due\nto the openness of recommender systems, they remain vulnerable to malicious\nattacks. Additionally, natural noise in training data and issues such as data\nsparsity can also degrade the performance of recommender systems. Therefore,\nenhancing the robustness of recommender systems has become an increasingly\nimportant research topic. In this survey, we provide a comprehensive overview\nof the robustness of recommender systems. Based on our investigation, we\ncategorize the robustness of recommender systems into adversarial robustness\nand non-adversarial robustness. In the adversarial robustness, we introduce the\nfundamental principles and classical methods of recommender system adversarial\nattacks and defenses. In the non-adversarial robustness, we analyze\nnon-adversarial robustness from the perspectives of data sparsity, natural\nnoise, and data imbalance. Additionally, we summarize commonly used datasets\nand evaluation metrics for evaluating the robustness of recommender systems.\nFinally, we also discuss the current challenges in the field of recommender\nsystem robustness and potential future research directions. Additionally, to\nfacilitate fair and efficient evaluation of attack and defense methods in\nadversarial robustness, we propose an adversarial robustness evaluation\nlibrary--ShillingREC, and we conduct evaluations of basic attack models and\nrecommendation models. ShillingREC project is released at\nhttps://github.com/chengleileilei/ShillingREC.\n","authors":["Lei Cheng","Xiaowen Huang","Jitao Sang","Jian Yu"],"pdf_url":"https://arxiv.org/pdf/2404.17844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06569v1","updated":"2025-05-10T08:50:44Z","published":"2025-05-10T08:50:44Z","title":"MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context\n  RAG","summary":"  Long-context (LC) Large Language Models (LLMs) combined with\nRetrieval-Augmented Generation (RAG) hold strong potential for complex\nmulti-hop and large-document tasks. However, existing RAG systems often suffer\nfrom imprecise retrieval, incomplete context coverage under constrained context\nwindows, and fragmented information caused by suboptimal context construction.\nWe introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical\nretrieval framework that compresses and partitions documents into\ncoarse-to-fine granularities, then adaptively merges relevant contexts through\nchunk- and document-level expansions in real time. By starting from the\nfinest-level retrieval and progressively incorporating higher-level and broader\ncontext, MacRAG constructs effective query-specific long contexts, optimizing\nboth precision and coverage. Evaluations on the challenging LongBench\nexpansions of HotpotQA, 2WikiMultihopQA, and Musique confirm that MacRAG\nconsistently surpasses baseline RAG pipelines on single- and multi-step\ngeneration with Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish\nMacRAG as an efficient, scalable solution for real-world long-context,\nmulti-hop reasoning. Our code is available at\nhttps://github.com/Leezekun/MacRAG.\n","authors":["Woosang Lim","Zekun Li","Gyuwan Kim","Sungyoung Ji","HyeonJung Kim","Kyuri Choi","Jin Hyuk Lim","Kyungpyo Park","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.06569v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.06685v1","updated":"2025-05-10T16:15:26Z","published":"2025-05-10T16:15:26Z","title":"Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General\n  Vision-Language Understanding","summary":"  Emotion understanding in videos aims to accurately recognize and interpret\nindividuals' emotional states by integrating contextual, visual, textual, and\nauditory cues. While Large Multimodal Models (LMMs) have demonstrated\nsignificant progress in general vision-language (VL) tasks, their performance\nin emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on\nemotion-related tasks often leads to catastrophic forgetting, hindering their\nability to generalize across diverse tasks. To address these challenges, we\npresent Emotion-Qwen, a tailored multimodal framework designed to enhance both\nemotion understanding and general VL reasoning. Emotion-Qwen incorporates a\nsophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm,\nwhich dynamically routes inputs to balance emotion-specific and general-purpose\nprocessing. The model is pre-trained in a three-stage pipeline on large-scale\ngeneral and emotional image datasets to support robust multimodal\nrepresentations. Furthermore, we construct the Video Emotion Reasoning (VER)\ndataset, comprising more than 40K bilingual video clips with fine-grained\ndescriptive annotations, to further enrich Emotion-Qwen's emotional reasoning\ncapability. Experimental results demonstrate that Emotion-Qwen achieves\nstate-of-the-art performance on multiple emotion recognition benchmarks, while\nmaintaining competitive results on general VL tasks. Code and models are\navailable at https://anonymous.4open.science/r/Emotion-Qwen-Anonymous.\n","authors":["Dawei Huang","Qing Li","Chuan Yan","Zebang Cheng","Yurong Huang","Xiang Li","Bin Li","Xiaohui Wang","Zheng Lian","Xiaojiang Peng"],"pdf_url":"https://arxiv.org/pdf/2505.06685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07901v2","updated":"2025-05-10T09:50:11Z","published":"2024-09-12T10:10:22Z","title":"Bridging Discrete and Continuous: A Multimodal Strategy for Complex\n  Emotion Detection","summary":"  In the domain of human-computer interaction, accurately recognizing and\ninterpreting human emotions is crucial yet challenging due to the complexity\nand subtlety of emotional expressions. This study explores the potential for\ndetecting a rich and flexible range of emotions through a multimodal approach\nwhich integrates facial expressions, voice tones, and transcript from video\nclips. We propose a novel framework that maps variety of emotions in a\nthree-dimensional Valence-Arousal-Dominance (VAD) space, which could reflect\nthe fluctuations and positivity/negativity of emotions to enable a more variety\nand comprehensive representation of emotional states. We employed K-means\nclustering to transit emotions from traditional discrete categorization to a\ncontinuous labeling system and built a classifier for emotion recognition upon\nthis system. The effectiveness of the proposed model is evaluated using the\nMER2024 dataset, which contains culturally consistent video clips from Chinese\nmovies and TV series, annotated with both discrete and open-vocabulary emotion\nlabels. Our experiment successfully achieved the transformation between\ndiscrete and continuous models, and the proposed model generated a more diverse\nand comprehensive set of emotion vocabulary while maintaining strong accuracy.\n","authors":["Jiehui Jia","Huan Zhang","Jinhua Liang"],"pdf_url":"https://arxiv.org/pdf/2409.07901v2.pdf","comment":null}]},"2025-05-09T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.06445v1","updated":"2025-05-09T21:27:59Z","published":"2025-05-09T21:27:59Z","title":"Tweedie Regression for Video Recommendation System","summary":"  Modern recommendation systems aim to increase click-through rates (CTR) for\nbetter user experience, through commonly treating ranking as a classification\ntask focused on predicting CTR. However, there is a gap between this method and\nthe actual objectives of businesses across different sectors. In video\nrecommendation services, the objective of video on demand (VOD) extends beyond\nmerely encouraging clicks, but also guiding users to discover their true\ninterests, leading to increased watch time. And longer users watch time will\nleads to more revenue through increased chances of presenting online display\nadvertisements. This research addresses the issue by redefining the problem\nfrom classification to regression, with a focus on maximizing revenue through\nuser viewing time. Due to the lack of positive labels on recommendation, the\nstudy introduces Tweedie Loss Function, which is better suited in this scenario\nthan the traditional mean square error loss. The paper also provides insights\non how Tweedie process capture users diverse interests. Our offline simulation\nand online A/B test revealed that we can substantially enhance our core\nbusiness objectives: user engagement in terms of viewing time and,\nconsequently, revenue. Additionally, we provide a theoretical comparison\nbetween the Tweedie Loss and the commonly employed viewing time weighted\nLogloss, highlighting why Tweedie Regression stands out as an efficient\nsolution. We further outline a framework for designing a loss function that\nfocuses on a singular objective.\n","authors":["Yan Zheng","Qiang Chen","Chenglei Niu"],"pdf_url":"https://arxiv.org/pdf/2505.06445v1.pdf","comment":"ICMI 2025 IEEE 4th International Conference on Computing and Machine\n  Intelligence April 05-06, 2025"},{"id":"http://arxiv.org/abs/2404.18546v3","updated":"2025-05-09T19:55:20Z","published":"2024-04-29T09:37:24Z","title":"ir_explain: a Python Library of Explainable IR Methods","summary":"  While recent advancements in Neural Ranking Models have resulted in\nsignificant improvements over traditional statistical retrieval models, it is\ngenerally acknowledged that the use of large neural architectures and the\napplication of complex language models in Information Retrieval (IR) have\nreduced the transparency of retrieval methods. Consequently, Explainability and\nInterpretability have emerged as important research topics in IR. Several\naxiomatic and post-hoc explanation methods, as well as approaches that attempt\nto be interpretable-by-design, have been proposed. This article presents\n\\irexplain, an open-source Python library that implements a variety of\nwell-known techniques for Explainable IR (ExIR) within a common, extensible\nframework. \\irexplain supports the three standard categories of post-hoc\nexplanations, namely pointwise, pairwise, and listwise explanations. The\nlibrary is designed to make it easy to reproduce state-of-the-art ExIR\nbaselines on standard test collections, as well as to explore new approaches to\nexplaining IR models and methods. To facilitate adoption, \\irexplain is\nwell-integrated with widely-used toolkits such as Pyserini and \\irdatasets.\n","authors":["Sourav Saha","Harsh Agarwal","V Venktesh","Avishek Anand","Swastik Mohanty","Debapriyo Majumdar","Mandar Mitra"],"pdf_url":"https://arxiv.org/pdf/2404.18546v3.pdf","comment":"To appear as a Resources and Reproducibility Track paper in Proc. ACM\n  SIGIR 2025"},{"id":"http://arxiv.org/abs/2505.06184v1","updated":"2025-05-09T16:51:24Z","published":"2025-05-09T16:51:24Z","title":"From Millions of Tweets to Actionable Insights: Leveraging LLMs for User\n  Profiling","summary":"  Social media user profiling through content analysis is crucial for tasks\nlike misinformation detection, engagement prediction, hate speech monitoring,\nand user behavior modeling. However, existing profiling techniques, including\ntweet summarization, attribute-based profiling, and latent representation\nlearning, face significant limitations: they often lack transferability,\nproduce non-interpretable features, require large labeled datasets, or rely on\nrigid predefined categories that limit adaptability. We introduce a novel large\nlanguage model (LLM)-based approach that leverages domain-defining statements,\nwhich serve as key characteristics outlining the important pillars of a domain\nas foundations for profiling. Our two-stage method first employs\nsemi-supervised filtering with a domain-specific knowledge base, then generates\nboth abstractive (synthesized descriptions) and extractive (representative\ntweet selections) user profiles. By harnessing LLMs' inherent knowledge with\nminimal human validation, our approach is adaptable across domains while\nreducing the need for large labeled datasets. Our method generates\ninterpretable natural language user profiles, condensing extensive user data\ninto a scale that unlocks LLMs' reasoning and knowledge capabilities for\ndownstream social network tasks. We contribute a Persian political Twitter (X)\ndataset and an LLM-based evaluation framework with human validation.\nExperimental results show our method significantly outperforms state-of-the-art\nLLM-based and traditional methods by 9.8%, demonstrating its effectiveness in\ncreating flexible, adaptable, and interpretable user profiles.\n","authors":["Vahid Rahimzadeh","Ali Hamzehpour","Azadeh Shakery","Masoud Asadpour"],"pdf_url":"https://arxiv.org/pdf/2505.06184v1.pdf","comment":"Accepted at MisD @ AAAI ICWSM 2025"},{"id":"http://arxiv.org/abs/2503.24289v2","updated":"2025-05-09T15:58:09Z","published":"2025-03-31T16:36:00Z","title":"Rec-R1: Bridging Generative Large Language Models and User-Centric\n  Recommendation Systems via Reinforcement Learning","summary":"  We propose Rec-R1, a general reinforcement learning framework that bridges\nlarge language models (LLMs) with recommendation systems through closed-loop\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\ndirectly optimizes LLM generation using feedback from a fixed black-box\nrecommendation model, without relying on synthetic SFT data from proprietary\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\nrepresentative tasks: product search and sequential recommendation.\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\nprompting- and SFT-based methods, but also achieves significant gains over\nstrong discriminative baselines, even when used with simple retrievers such as\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\nunlike SFT, which often impairs instruction-following and reasoning. These\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\nadaptation without catastrophic forgetting.\n","authors":["Jiacheng Lin","Tian Wang","Kun Qian"],"pdf_url":"https://arxiv.org/pdf/2503.24289v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20698v4","updated":"2025-05-09T15:18:56Z","published":"2025-03-26T16:28:04Z","title":"MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion","summary":"  Videos inherently contain multiple modalities, including visual events, text\noverlays, sounds, and speech, all of which are important for retrieval.\nHowever, state-of-the-art multimodal language models like VAST and LanguageBind\nare built on vision-language models (VLMs), and thus overly prioritize visual\nsignals. Retrieval benchmarks further reinforce this bias by focusing on visual\nqueries and neglecting other modalities. We create a search system MMMORRF that\nextracts text and features from both visual and audio modalities and integrates\nthem with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is\nboth effective and efficient, demonstrating practicality in searching videos\nbased on users' information needs instead of visual descriptive queries. We\nevaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed\nfor more targeted information needs, and find that it improves nDCG@20 by 81%\nover leading multimodal encoders and 37% over single-modality retrieval,\ndemonstrating the value of integrating diverse modalities.\n","authors":["Saron Samuel","Dan DeGenaro","Jimena Guallar-Blasco","Kate Sanders","Oluwaseun Eisape","Tanner Spendlove","Arun Reddy","Alexander Martin","Andrew Yates","Eugene Yang","Cameron Carpenter","David Etter","Efsun Kayi","Matthew Wiesner","Kenton Murray","Reno Kriz"],"pdf_url":"https://arxiv.org/pdf/2503.20698v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05989v1","updated":"2025-05-09T12:18:34Z","published":"2025-05-09T12:18:34Z","title":"Modeling Multi-Hop Semantic Paths for Recommendation in Heterogeneous\n  Information Networks","summary":"  This study focuses on the problem of path modeling in heterogeneous\ninformation networks and proposes a multi-hop path-aware recommendation\nframework. The method centers on multi-hop paths composed of various types of\nentities and relations. It models user preferences through three stages: path\nselection, semantic representation, and attention-based fusion. In the path\nselection stage, a path filtering mechanism is introduced to remove redundant\nand noisy information. In the representation learning stage, a sequential\nmodeling structure is used to jointly encode entities and relations, preserving\nthe semantic dependencies within paths. In the fusion stage, an attention\nmechanism assigns different weights to each path to generate a global user\ninterest representation. Experiments conducted on real-world datasets such as\nAmazon-Book show that the proposed method significantly outperforms existing\nrecommendation models across multiple evaluation metrics, including HR@10,\nRecall@10, and Precision@10. The results confirm the effectiveness of multi-hop\npaths in capturing high-order interaction semantics and demonstrate the\nexpressive modeling capabilities of the framework in heterogeneous\nrecommendation scenarios. This method provides both theoretical and practical\nvalue by integrating structural information modeling in heterogeneous networks\nwith recommendation algorithm design. It offers a more expressive and flexible\nparadigm for learning user preferences in complex data environments.\n","authors":["Hongye Zheng","Yue Xing","Lipeng Zhu","Xu Han","Junliang Du","Wanyu Cui"],"pdf_url":"https://arxiv.org/pdf/2505.05989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04938v2","updated":"2025-05-09T11:41:16Z","published":"2025-05-08T04:27:11Z","title":"FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image\n  Registration","summary":"  In recent years, deformable medical image registration techniques have made\nsignificant progress. However, existing models still lack efficiency in\nparallel extraction of coarse and fine-grained features. To address this, we\nconstruct a new pyramid registration network based on feature and deformation\nfield (FF-PNet). For coarse-grained feature extraction, we design a Residual\nFeature Fusion Module (RFFM), for fine-grained image deformation, we propose a\nResidual Deformation Field Fusion Module (RDFFM). Through the parallel\noperation of these two modules, the model can effectively handle complex image\ndeformations. It is worth emphasizing that the encoding stage of FF-PNet only\nemploys traditional convolutional neural networks without any attention\nmechanisms or multilayer perceptrons, yet it still achieves remarkable\nimprovements in registration accuracy, fully demonstrating the superior feature\ndecoding capabilities of RFFM and RDFFM. We conducted extensive experiments on\nthe LPBA and OASIS datasets. The results show our network consistently\noutperforms popular methods in metrics like the Dice Similarity Coefficient.\n","authors":["Ying Zhang","Shuai Guo","Chenxi Sun","Yuchen Zhu","Jinhai Xiang"],"pdf_url":"https://arxiv.org/pdf/2505.04938v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05885v1","updated":"2025-05-09T08:53:59Z","published":"2025-05-09T08:53:59Z","title":"Cost-Effective, Low Latency Vector Search with Azure Cosmos DB","summary":"  Vector indexing enables semantic search over diverse corpora and has become\nan important interface to databases for both users and AI agents. Efficient\nvector search requires deep optimizations in database systems. This has\nmotivated a new class of specialized vector databases that optimize for vector\nsearch quality and cost. Instead, we argue that a scalable, high-performance,\nand cost-efficient vector search system can be built inside a cloud-native\noperational database like Azure Cosmos DB while leveraging the benefits of a\ndistributed database such as high availability, durability, and scale. We do\nthis by deeply integrating DiskANN, a state-of-the-art vector indexing library,\ninside Azure Cosmos DB NoSQL. This system uses a single vector index per\npartition stored in existing index trees, and kept in sync with underlying\ndata. It supports < 20ms query latency over an index spanning 10 million of\nvectors, has stable recall over updates, and offers nearly 15x and 41x lower\nquery cost compared to Zilliz and Pinecone serverless enterprise products. It\nalso scales out to billions of vectors via automatic partitioning. This\nconvergent design presents a point in favor of integrating vector indices into\noperational databases in the context of recent debates on specialized vector\ndatabases, and offers a template for vector indexing in other databases.\n","authors":["Nitish Upreti","Krishnan Sundaram","Hari Sudan Sundar","Samer Boshra","Balachandar Perumalswamy","Shivam Atri","Martin Chisholm","Revti Raman Singh","Greg Yang","Subramanyam Pattipaka","Tamara Hass","Nitesh Dudhey","James Codella","Mark Hildebrand","Magdalen Manohar","Jack Moffitt","Haiyang Xu","Naren Datha","Suryansh Gupta","Ravishankar Krishnaswamy","Prashant Gupta","Abhishek Sahu","Ritika Mor","Santosh Kulkarni","Hemeswari Varada","Sudhanshu Barthwal","Amar Sagare","Dinesh Billa","Zishan Fu","Neil Deshpande","Shaun Cooper","Kevin Pilch","Simon Moreno","Aayush Kataria","Vipul Vishal","Harsha Vardhan Simhadri"],"pdf_url":"https://arxiv.org/pdf/2505.05885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06324v1","updated":"2025-05-09T04:40:11Z","published":"2025-05-09T04:40:11Z","title":"Document Attribution: Examining Citation Relationships using Large\n  Language Models","summary":"  As Large Language Models (LLMs) are increasingly applied to document-based\ntasks - such as document summarization, question answering, and information\nextraction - where user requirements focus on retrieving information from\nprovided documents rather than relying on the model's parametric knowledge,\nensuring the trustworthiness and interpretability of these systems has become a\ncritical concern. A central approach to addressing this challenge is\nattribution, which involves tracing the generated outputs back to their source\ndocuments. However, since LLMs can produce inaccurate or imprecise responses,\nit is crucial to assess the reliability of these citations.\n  To tackle this, our work proposes two techniques. (1) A zero-shot approach\nthat frames attribution as a straightforward textual entailment task. Our\nmethod using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the\nbest baseline of ID and OOD sets of AttributionBench, respectively. (2) We also\nexplore the role of the attention mechanism in enhancing the attribution\nprocess. Using a smaller LLM, flan-t5-small, the F1 scores outperform the\nbaseline across almost all layers except layer 4 and layers 8 through 11.\n","authors":["Vipula Rawte","Ryan A. Rossi","Franck Dernoncourt","Nedim Lipka"],"pdf_url":"https://arxiv.org/pdf/2505.06324v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.06149v1","updated":"2025-05-09T16:00:01Z","published":"2025-05-09T16:00:01Z","title":"Can Prompting LLMs Unlock Hate Speech Detection across Languages? A\n  Zero-shot and Few-shot Study","summary":"  Despite growing interest in automated hate speech detection, most existing\napproaches overlook the linguistic diversity of online content. Multilingual\ninstruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ\noffer promising capabilities across languages, but their effectiveness in\nidentifying hate speech through zero-shot and few-shot prompting remains\nunderexplored. This work evaluates LLM prompting-based detection across eight\nnon-English languages, utilizing several prompting techniques and comparing\nthem to fine-tuned encoder models. We show that while zero-shot and few-shot\nprompting lag behind fine-tuned encoder models on most of the real-world\nevaluation sets, they achieve better generalization on functional tests for\nhate speech detection. Our study also reveals that prompt design plays a\ncritical role, with each language often requiring customized prompting\ntechniques to maximize performance.\n","authors":["Faeze Ghorbanpour","Daryna Dementieva","Alexander Fraser"],"pdf_url":"https://arxiv.org/pdf/2505.06149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06107v1","updated":"2025-05-09T15:03:39Z","published":"2025-05-09T15:03:39Z","title":"Differentiating Emigration from Return Migration of Scholars Using\n  Name-Based Nationality Detection Models","summary":"  Most web and digital trace data do not include information about an\nindividual's nationality due to privacy concerns. The lack of data on\nnationality can create challenges for migration research. It can lead to a\nleft-censoring issue since we are uncertain about the migrant's country of\norigin. Once we observe an emigration event, if we know the nationality, we can\ndifferentiate it from return migration. We propose methods to detect the\nnationality with the least available data, i.e., full names. We use the\ndetected nationality in comparison with the country of academic origin, which\nis a common approach in studying the migration of researchers. We gathered 2.6\nmillion unique name-nationality pairs from Wikipedia and categorized them into\nfamilies of nationalities with three granularity levels to use as our training\ndata. Using a character-based machine learning model, we achieved a weighted F1\nscore of 84% for the broadest and 67% for the most granular, country-level\ncategorization. In our empirical study, we used the trained and tested model to\nassign nationality to 8+ million scholars' full names in Scopus data. Our\nresults show that using the country of first publication as a proxy for\nnationality underestimates the size of return flows, especially for countries\nwith a more diverse academic workforce, such as the USA, Australia, and Canada.\nWe found that around 48% of emigration from the USA was return migration once\nwe used the country of name origin, in contrast to 33% based on academic\norigin. In the most recent period, 79% of scholars whose affiliation has\nconsistently changed from the USA to China, and are considered emigrants, have\nChinese names in contrast to 41% with a Chinese academic origin. Our proposed\nmethods for addressing left-censoring issues are beneficial for other research\nthat uses digital trace data to study migration.\n","authors":["Faeze Ghorbanpour","Thiago Zordan Malaguth","Aliakbar Akbaritabar"],"pdf_url":"https://arxiv.org/pdf/2505.06107v1.pdf","comment":"Accepted to appear @ ICWSM 2025. The link to the camera-ready paper\n  will be added soon"},{"id":"http://arxiv.org/abs/2502.14439v2","updated":"2025-05-09T08:42:22Z","published":"2025-02-20T10:42:29Z","title":"Visual and Auditory Aesthetic Preferences Across Cultures","summary":"  Research on how humans perceive aesthetics in shapes, colours, and music has\npredominantly focused on Western populations, limiting our understanding of how\ncultural environments shape aesthetic preferences. We present a large-scale\ncross-cultural study examining aesthetic preferences across five distinct\nmodalities extensively explored in the literature: shape, curvature, colour,\nmusical harmony and melody. We gather 401,403 preference judgements from 4,835\nparticipants across 10 countries, systematically sampling two-dimensional\nparameter spaces for each modality. The findings reveal both universal patterns\nand cultural variations. Preferences for shape and curvature cross-culturally\ndemonstrate a consistent preference for symmetrical forms. While colour\npreferences are categorically consistent, ratio-like preferences vary across\ncultures. Musical harmony shows strong agreement in interval relationships\ndespite differing regions of preference within the broad frequency spectrum,\nwhile melody shows the highest cross-cultural variation. These results suggest\nthat aesthetic preferences emerge from an interplay between shared perceptual\nmechanisms and cultural learning.\n","authors":["Harin Lee","Eline Van Geert","Elif Celen","Raja Marjieh","Pol van Rijn","Minsu Park","Nori Jacoby"],"pdf_url":"https://arxiv.org/pdf/2502.14439v2.pdf","comment":"To be presented at CogSci 2025"},{"id":"http://arxiv.org/abs/2305.03568v3","updated":"2025-05-09T08:19:45Z","published":"2023-05-05T14:19:46Z","title":"A vector quantized masked autoencoder for audiovisual speech emotion\n  recognition","summary":"  An important challenge in emotion recognition is to develop methods that can\nleverage unlabeled training data. In this paper, we propose the VQ-MAE-AV\nmodel, a self-supervised multimodal model that leverages masked autoencoders to\nlearn representations of audiovisual speech without labels. The model includes\nvector quantized variational autoencoders that compress raw audio and visual\nspeech data into discrete tokens. The audiovisual speech tokens are used to\ntrain a multimodal masked autoencoder that consists of an encoder-decoder\narchitecture with attention mechanisms. The model is designed to extract both\nlocal (i.e., at the frame level) and global (i.e., at the sequence level)\nrepresentations of audiovisual speech. During self-supervised pre-training, the\nVQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual\nspeech, for the task of reconstructing randomly masked audiovisual speech\ntokens and with a contrastive learning strategy. During this pre-training, the\nencoder learns to extract a representation of audiovisual speech that can be\nsubsequently leveraged for emotion recognition. During the supervised\nfine-tuning stage, a small classification model is trained on top of the\nVQ-MAE-AV encoder for an emotion recognition task. The proposed approach\nachieves state-of-the-art emotion recognition results across several datasets\nin both controlled and in-the-wild conditions.\n","authors":["Samir Sadok","Simon Leglaive","Renaud SÃ©guier"],"pdf_url":"https://arxiv.org/pdf/2305.03568v3.pdf","comment":"13 pages, 6 figures, https://samsad35.github.io/VQ-MAE-AudioVisual/"},{"id":"http://arxiv.org/abs/2501.02704v3","updated":"2025-05-09T06:46:46Z","published":"2025-01-06T01:15:35Z","title":"Persistence of Backdoor-based Watermarks for Neural Networks: A\n  Comprehensive Evaluation","summary":"  Deep Neural Networks (DNNs) have gained considerable traction in recent years\ndue to the unparalleled results they gathered. However, the cost behind\ntraining such sophisticated models is resource intensive, resulting in many to\nconsider DNNs to be intellectual property (IP) to model owners. In this era of\ncloud computing, high-performance DNNs are often deployed all over the internet\nso that people can access them publicly. As such, DNN watermarking schemes,\nespecially backdoor-based watermarks, have been actively developed in recent\nyears to preserve proprietary rights. Nonetheless, there lies much uncertainty\non the robustness of existing backdoor watermark schemes, towards both\nadversarial attacks and unintended means such as fine-tuning neural network\nmodels. One reason for this is that no complete guarantee of robustness can be\nassured in the context of backdoor-based watermark. In this paper, we\nextensively evaluate the persistence of recent backdoor-based watermarks within\nneural networks in the scenario of fine-tuning, we propose/develop a novel\ndata-driven idea to restore watermark after fine-tuning without exposing the\ntrigger set. Our empirical results show that by solely introducing training\ndata after fine-tuning, the watermark can be restored if model parameters do\nnot shift dramatically during fine-tuning. Depending on the types of trigger\nsamples used, trigger accuracy can be reinstated to up to 100%. Our study\nfurther explores how the restoration process works using loss landscape\nvisualization, as well as the idea of introducing training data in fine-tuning\nstage to alleviate watermark vanishing.\n","authors":["Anh Tu Ngo","Chuan Song Heng","Nandish Chattopadhyay","Anupam Chattopadhyay"],"pdf_url":"https://arxiv.org/pdf/2501.02704v3.pdf","comment":"Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS)"}]}}