<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-15T00:00:00Z">2025-05-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">89</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal
  Mathematical Reasoning <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language image-caption datasets, widely used for training Large
Multimodal Models, mainly focus on natural scenarios and overlook the intricate
details of mathematical figures that are critical for problem-solving,
hindering the advancement of current LMMs in multimodal mathematical reasoning.
To this end, we propose leveraging code as supervision for cross-modal
alignment, since code inherently encodes all information needed to generate
corresponding figures, establishing a precise connection between the two
modalities. Specifically, we co-develop our image-to-code model and dataset
with model-in-the-loop approach, resulting in an image-to-code model,
FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.
Furthermore, we utilize FigCodifier to synthesize novel mathematical figures
and then construct MM-MathInstruct-3M, a high-quality multimodal math
instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with
ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on
MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a
new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and
Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista,
achieving improvements of 8.9% and 9.2%. The dataset and models will be
released at https://github.com/mathllm/MathCoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large
  Reasoning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, Junnan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large reasoning models (LRMs) already possess a latent capacity for long
chain-of-thought reasoning. Prior work has shown that outcome-based
reinforcement learning (RL) can incidentally elicit advanced reasoning
behaviors such as self-correction, backtracking, and verification phenomena
often referred to as the model's "aha moment". However, the timing and
consistency of these emergent behaviors remain unpredictable and
uncontrollable, limiting the scalability and reliability of LRMs' reasoning
capabilities. To address these limitations, we move beyond reliance on prompts
and coincidental "aha moments". Instead, we explicitly align models with three
meta-abilities: deduction, induction, and abduction, using automatically
generated, self-verifiable tasks. Our three stage-pipeline individual
alignment, parameter-space merging, and domain-specific reinforcement learning,
boosting performance by over 10\% relative to instruction-tuned baselines.
Furthermore, domain-specific RL from the aligned checkpoint yields an
additional 2\% average gain in the performance ceiling across math, coding, and
science benchmarks, demonstrating that explicit meta-ability alignment offers a
scalable and dependable foundation for reasoning. Code is available at:
https://github.com/zhiyuanhubj/Meta-Ability-Alignment
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Deeper Understanding of Reasoning Capabilities in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annie Wong, Thomas Bäck, Aske Plaat, Niki van Stein, Anna V. Kononova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models demonstrate impressive performance on static
benchmarks, the true potential of large language models as self-learning and
reasoning agents in dynamic environments remains unclear. This study
systematically evaluates the efficacy of self-reflection, heuristic mutation,
and planning as prompting techniques to test the adaptive capabilities of
agents. We conduct experiments with various open-source language models in
dynamic environments and find that larger models generally outperform smaller
ones, but that strategic prompting can close this performance gap. Second, a
too-long prompt can negatively impact smaller models on basic reactive tasks,
while larger models show more robust behaviour. Third, advanced prompting
techniques primarily benefit smaller models on complex games, but offer less
improvement for already high-performing large language models. Yet, we find
that advanced reasoning methods yield highly variable outcomes: while capable
of significantly improving performance when reasoning and decision-making
align, they also introduce instability and can lead to big performance drops.
Compared to human performance, our findings reveal little evidence of true
emergent reasoning. Instead, large language model performance exhibits
persistent limitations in crucial areas such as planning, reasoning, and
spatial coordination, suggesting that current-generation large language models
still suffer fundamental shortcomings that may not be fully overcome through
self-reflective prompting alone. Reasoning is a multi-faceted task, and while
reasoning methods like Chain of thought improves multi-step reasoning on math
word problems, our findings using dynamic benchmarks highlight important
shortcomings in general reasoning capabilities, indicating a need to move
beyond static benchmarks to capture the complexity of reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WorldPM: Scaling Human Preference Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binghai Wang, Runji Lin, Keming Lu, Le Yu, Zhenru Zhang, Fei Huang, Chujie Zheng, Kai Dang, Yang Fan, Xingzhang Ren, An Yang, Binyuan Hui, Dayiheng Liu, Tao Gui, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang, Bowen Yu, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by scaling laws in language modeling that demonstrate how test loss
scales as a power law with model and dataset sizes, we find that similar laws
exist in preference modeling. We propose World Preference Modeling$ (WorldPM)
to emphasize this scaling potential, where World Preference embodies a unified
representation of human preferences. In this paper, we collect preference data
from public forums covering diverse user communities, and conduct extensive
training using 15M-scale data across models ranging from 1.5B to 72B
parameters. We observe distinct patterns across different evaluation metrics:
(1) Adversarial metrics (ability to identify deceptive features) consistently
scale up with increased training data and base model size; (2) Objective
metrics (objective knowledge with well-defined answers) show emergent behavior
in larger language models, highlighting WorldPM's scalability potential; (3)
Subjective metrics (subjective preferences from a limited number of humans or
AI) do not demonstrate scaling trends. Further experiments validate the
effectiveness of WorldPM as a foundation for preference fine-tuning. Through
evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly
improves the generalization performance across human preference datasets of
varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%
on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we
observe significant improvements on both in-house and public evaluation sets,
with notable gains of 4% to 8% in our in-house evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative
  Decoding of Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mugilan Ganesan, Shane Segal, Ankur Aggarwal, Nish Sinnadurai, Sean Lie, Vithursan Thangarasa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Token Prediction Needs Registers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasios Gerontopoulos, Spyros Gidaris, Nikos Komodakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Devil Is in the Word Alignment Details: On Translation-Based
  Cross-Lingual Transfer for Token Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Ebing, Goran Glavaš
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translation-based strategies for cross-lingual transfer XLT such as
translate-train -- training on noisy target language data translated from the
source language -- and translate-test -- evaluating on noisy source language
data translated from the target language -- are competitive XLT baselines. In
XLT for token classification tasks, however, these strategies include label
projection, the challenging step of mapping the labels from each token in the
original sentence to its counterpart(s) in the translation. Although word
aligners (WAs) are commonly used for label projection, the low-level design
decisions for applying them to translation-based XLT have not been
systematically investigated. Moreover, recent marker-based methods, which
project labeled spans by inserting tags around them before (or after)
translation, claim to outperform WAs in label projection for XLT. In this work,
we revisit WAs for label projection, systematically investigating the effects
of low-level design decisions on token-level XLT: (i) the algorithm for
projecting labels between (multi-)token spans, (ii) filtering strategies to
reduce the number of noisily mapped labels, and (iii) the pre-tokenization of
the translated sentences. We find that all of these substantially impact
translation-based XLT performance and show that, with optimized choices, XLT
with WA offers performance at least comparable to that of marker-based methods.
We then introduce a new projection strategy that ensembles translate-train and
translate-test predictions and demonstrate that it substantially outperforms
the marker-based projection. Crucially, we show that our proposed ensembling
also reduces sensitivity to low-level WA design choices, resulting in more
robust XLT for token classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RouteNator: A Router-Based Multi-Modal Architecture for Generating
  Synthetic Training Data for Function Calling LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vibha Belavadi, Tushar Vatsa, Dewang Sultania, Suhas Suresha, Ishita Verma, Cheng Chen, Tracy Holloway King, Michael Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses fine-tuning Large Language Models (LLMs) for function
calling tasks when real user interaction data is unavailable. In digital
content creation tools, where users express their needs through natural
language queries that must be mapped to API calls, the lack of real-world
task-specific data and privacy constraints for training on it necessitate
synthetic data generation. Existing approaches to synthetic data generation
fall short in diversity and complexity, failing to replicate real-world data
distributions and leading to suboptimal performance after LLM fine-tuning. We
present a novel router-based architecture that leverages domain resources like
content metadata and structured knowledge graphs, along with text-to-text and
vision-to-text language models to generate high-quality synthetic training
data. Our architecture's flexible routing mechanism enables synthetic data
generation that matches observed real-world distributions, addressing a
fundamental limitation of traditional approaches. Evaluation on a comprehensive
set of real user queries demonstrates significant improvements in both function
classification accuracy and API parameter selection. Models fine-tuned with our
synthetic data consistently outperform traditional approaches, establishing new
benchmarks for function calling tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 4th International Workshop on Knowledge-Augmented
  Methods for Natural Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can You Really Trust Code Copilots? Evaluating Large Language Models
  from a Code Security Perspective <span class="chip">ACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Mou, Xiao Deng, Yuxiao Luo, Shikun Zhang, Wei Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code security and usability are both essential for various coding assistant
applications driven by large language models (LLMs). Current code security
benchmarks focus solely on single evaluation task and paradigm, such as code
completion and generation, lacking comprehensive assessment across dimensions
like secure code generation, vulnerability repair and discrimination. In this
paper, we first propose CoV-Eval, a multi-task benchmark covering various tasks
such as code completion, vulnerability repair, vulnerability detection and
classification, for comprehensive evaluation of LLM code security. Besides, we
developed VC-Judge, an improved judgment model that aligns closely with human
experts and can review LLM-generated programs for vulnerabilities in a more
efficient and reliable way. We conduct a comprehensive evaluation of 20
proprietary and open-source LLMs. Overall, while most LLMs identify vulnerable
codes well, they still tend to generate insecure codes and struggle with
recognizing specific vulnerability types and performing repairs. Extensive
experiments and qualitative analyses reveal key challenges and optimization
directions, offering insights for future research in LLM code security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with
  Curriculum Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaohan Wang, Licheng Zhang, Zheren Fu, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) is an effective method to enhance the
capabilities of large language models (LLMs). Existing methods focus on
optimizing the retriever or generator in the RAG system by directly utilizing
the top-k retrieved documents. However, the documents effectiveness are various
significantly across user queries, i.e. some documents provide valuable
knowledge while others totally lack critical information. It hinders the
retriever and generator's adaptation during training. Inspired by human
cognitive learning, curriculum learning trains models using samples progressing
from easy to difficult, thus enhancing their generalization ability, and we
integrate this effective paradigm to the training of the RAG system. In this
paper, we propose a multi-stage Curriculum Learning based RAG system training
framework, named CL-RAG. We first construct training data with multiple
difficulty levels for the retriever and generator separately through sample
evolution. Then, we train the model in stages based on the curriculum learning
approach, thereby optimizing the overall performance and generalization of the
RAG system more effectively. Our CL-RAG framework demonstrates consistent
effectiveness across four open-domain QA datasets, achieving performance gains
of 2% to 4% over multiple advanced methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Scaling Law for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouxiang Chen, Binyuan Hui, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Jianling Sun, Junyang Lin, Zhongxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is commonly believed that scaling language models should commit a
significant space or time cost, by increasing the parameters (parameter
scaling) or output tokens (inference-time scaling). We introduce the third and
more inference-efficient scaling paradigm: increasing the model's parallel
computation during both training and inference time. We apply $P$ diverse and
learnable transformations to the input, execute forward passes of the model in
parallel, and dynamically aggregate the $P$ outputs. This method, namely
parallel scaling (ParScale), scales parallel computation by reusing existing
parameters and can be applied to any model structure, optimization procedure,
data, or task. We theoretically propose a new scaling law and validate it
through large-scale pre-training, which shows that a model with $P$ parallel
streams is similar to scaling the parameters by $O(\log P)$ while showing
superior inference efficiency. For example, ParScale can use up to 22$\times$
less memory increase and 6$\times$ less latency increase compared to parameter
scaling that achieves the same performance improvement. It can also recycle an
off-the-shelf pre-trained model into a parallelly scaled one by post-training
on a small amount of tokens, further reducing the training budget. The new
scaling law we discovered potentially facilitates the deployment of more
powerful models in low-resource scenarios, and provides an alternative
perspective for the role of computation in machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superposition Yields Robust Neural Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou liu, Ziming Liu, Jeff Gore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of today's large language models (LLMs) depends on the
observation that larger models perform better. However, the origin of this
neural scaling law -- the finding that loss decreases as a power law with model
size -- remains unclear. Starting from two empirical principles -- that LLMs
represent more things than the model dimensions (widths) they have (i.e.,
representations are superposed), and that words or concepts in language occur
with varying frequencies -- we constructed a toy model to study the loss
scaling with model size. We found that when superposition is weak, meaning only
the most frequent features are represented without interference, the scaling of
loss with model size depends on the underlying feature frequency; if feature
frequencies follow a power law, so does the loss. In contrast, under strong
superposition, where all features are represented but overlap with each other,
the loss becomes inversely proportional to the model dimension across a wide
range of feature frequency distributions. This robust scaling behavior is
explained geometrically: when many more vectors are packed into a lower
dimensional space, the interference (squared overlaps) between vectors scales
inversely with that dimension. We then analyzed four families of open-sourced
LLMs and found that they exhibit strong superposition and quantitatively match
the predictions of our toy model. The Chinchilla scaling law turned out to also
agree with our results. We conclude that representation superposition is an
important mechanism underlying the observed neural scaling laws. We anticipate
that these insights will inspire new training strategies and model
architectures to achieve better performance with less computation and fewer
parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcing the Diffusion Chain of Lateral Thought with Diffusion
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, Guo-Jun Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the \emph{Diffusion Chain of Lateral Thought (DCoLT)}, a
reasoning framework for diffusion language models. DCoLT treats each
intermediate step in the reverse diffusion process as a latent "thinking"
action and optimizes the entire reasoning trajectory to maximize the reward on
the correctness of the final answer with outcome-based Reinforcement Learning
(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,
linear thinking process, DCoLT allows bidirectional, non-linear reasoning with
no strict rule on grammatical correctness amid its intermediate steps of
thought. We implement DCoLT on two representative Diffusion Language Models
(DLMs). First, we choose SEDD as a representative continuous-time discrete
diffusion model, where its concrete score derives a probabilistic policy to
maximize the RL reward over the entire sequence of intermediate diffusion
steps. We further consider the discrete-time masked diffusion language model --
LLaDA, and find that the order to predict and unmask tokens plays an essential
role to optimize its RL action resulting from the ranking-based Unmasking
Policy Module (UPM) defined by the Plackett-Luce model. Experiments on both
math and code generation tasks show that using only public data and 16 H800
GPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even
both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,
+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Document Refinement for Long-context Retrieval-augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yongkang Wu, Zhonghua Li, Qi Ye, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world RAG applications often encounter long-context input scenarios,
where redundant information and noise results in higher inference costs and
reduced performance. To address these challenges, we propose LongRefiner, an
efficient plug-and-play refiner that leverages the inherent structural
characteristics of long documents. LongRefiner employs dual-level query
analysis, hierarchical document structuring, and adaptive refinement through
multi-task learning on a single foundation model. Experiments on seven QA
datasets demonstrate that LongRefiner achieves competitive performance in
various scenarios while using 10x fewer computational costs and latency
compared to the best baseline. Further analysis validates that LongRefiner is
scalable, efficient, and effective, providing practical insights for real-world
long-text RAG applications. Our code is available at
https://github.com/ignorejjj/LongRefiner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are LLM-generated plain language summaries truly understandable? A
  large-scale crowdsourced evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Guo, Jae Ho Sohn, Gondy Leroy, Trevor Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Plain language summaries (PLSs) are essential for facilitating effective
communication between clinicians and patients by making complex medical
information easier for laypeople to understand and act upon. Large language
models (LLMs) have recently shown promise in automating PLS generation, but
their effectiveness in supporting health information comprehension remains
unclear. Prior evaluations have generally relied on automated scores that do
not measure understandability directly, or subjective Likert-scale ratings from
convenience samples with limited generalizability. To address these gaps, we
conducted a large-scale crowdsourced evaluation of LLM-generated PLSs using
Amazon Mechanical Turk with 150 participants. We assessed PLS quality through
subjective Likert-scale ratings focusing on simplicity, informativeness,
coherence, and faithfulness; and objective multiple-choice comprehension and
recall measures of reader understanding. Additionally, we examined the
alignment between 10 automated evaluation metrics and human judgments. Our
findings indicate that while LLMs can generate PLSs that appear
indistinguishable from human-written ones in subjective evaluations,
human-written PLSs lead to significantly better comprehension. Furthermore,
automated evaluation metrics fail to reflect human judgment, calling into
question their suitability for evaluating PLSs. This is the first study to
systematically evaluate LLM-generated PLSs based on both reader preferences and
comprehension outcomes. Our findings highlight the need for evaluation
frameworks that move beyond surface-level quality and for generation methods
that explicitly optimize for layperson comprehension.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Repetition Problems of LLMs in Code Generation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Dong, Yuchen Liu, Xue Jiang, Zhi Jin, Ge Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of neural language models, the performance of code generation
has been significantly boosted. However, the problem of repetitions during the
generation process continues to linger. Previous work has primarily focused on
content repetition, which is merely a fraction of the broader repetition
problem in code generation. A more prevalent and challenging problem is
structural repetition. In structural repetition, the repeated code appears in
various patterns but possesses a fixed structure, which can be inherently
reflected in grammar. In this paper, we formally define structural repetition
and propose an efficient decoding approach called RPG, which stands for
Repetition Penalization based on Grammar, to alleviate the repetition problems
in code generation for LLMs. Specifically, RPG first leverages grammar rules to
identify repetition problems during code generation, and then strategically
decays the likelihood of critical tokens that contribute to repetitions,
thereby mitigating them in code generation. To facilitate this study, we
construct a new dataset CodeRepetEval to comprehensively evaluate approaches
for mitigating the repetition problems in code generation. Extensive
experimental results demonstrate that RPG substantially outperforms the
best-performing baselines on CodeRepetEval dataset as well as HumanEval and
MBPP benchmarks, effectively reducing repetitions and enhancing the quality of
generated code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 (main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-domain Multilingual Sentiment Analysis in Industry: Predicting
  Aspect-based Opinion Quadruples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin White, Anastasia Shimorina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the design of an aspect-based sentiment analysis system
using large language models (LLMs) for real-world use. We focus on quadruple
opinion extraction -- identifying aspect categories, sentiment polarity,
targets, and opinion expressions from text data across different domains and
languages. Using internal datasets, we investigate whether a single fine-tuned
model can effectively handle multiple domain-specific taxonomies
simultaneously. We demonstrate that a combined multi-domain model achieves
performance comparable to specialized single-domain models while reducing
operational complexity. We also share lessons learned for handling
non-extractive predictions and evaluating various failure modes when developing
LLM-based systems for structured prediction tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coherent Language Reconstruction from Brain Recordings with Flexible
  Multi-Modal Input Stimuli 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyu Ye, Shaonan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding thoughts from brain activity offers valuable insights into human
cognition and enables promising applications in brain-computer interaction.
While prior studies have explored language reconstruction from fMRI data, they
are typically limited to single-modality inputs such as images or audio. In
contrast, human thought is inherently multimodal. To bridge this gap, we
propose a unified and flexible framework for reconstructing coherent language
from brain recordings elicited by diverse input modalities-visual, auditory,
and textual. Our approach leverages visual-language models (VLMs), using
modality-specific experts to jointly interpret information across modalities.
Experiments demonstrate that our method achieves performance comparable to
state-of-the-art systems while remaining adaptable and extensible. This work
advances toward more ecologically valid and generalizable mind decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with
  Relative Representations <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yile Wang, Zhanyu Shen, Hui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic text representation is a fundamental task in the field of natural
language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have
demonstrated excellent performance, but the values of each dimension are
difficult to trace and interpret. Bag-of-words, as classic sparse interpretable
embeddings, suffers from poor performance. Recently, Benara et al. (2024)
propose interpretable text embeddings using large language models, which forms
"0/1" embeddings based on responses to a series of questions. These
interpretable text embeddings are typically high-dimensional (larger than
10,000). In this work, we propose Low-dimensional (lower than 500) Dense and
Interpretable text embeddings with Relative representations (LDIR). The
numerical values of its dimensions indicate semantic relatedness to different
anchor texts through farthest point sampling, offering both semantic
representation as well as a certain level of traceability and interpretability.
We validate LDIR on multiple semantic textual similarity, retrieval, and
clustering tasks. Extensive experimental results show that LDIR performs close
to the black-box baseline models and outperforms the interpretable embeddings
baselines with much fewer dimensions. Code is available at
https://github.com/szu-tera/LDIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, Swarnadeep Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 tables, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StoryReasoning <span class="highlight-title">Dataset</span>: Using Chain-of-Thought for Scene Understanding
  and Grounded Story Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel A. P. Oliveira, David Martins de Matos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual storytelling systems struggle to maintain character identity across
frames and link actions to appropriate subjects, frequently leading to
referential hallucinations. These issues can be addressed through grounding of
characters, objects, and other entities on the visual elements. We propose
StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie
images, with both structured scene analyses and grounded stories. Each story
maintains character and object consistency across frames while explicitly
modeling multi-frame relationships through structured tabular representations.
Our approach features cross-frame object re-identification using visual
similarity and face recognition, chain-of-thought reasoning for explicit
narrative modeling, and a grounding scheme that links textual elements to
visual entities across multiple frames. We establish baseline performance by
fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end
object detection, re-identification, and landmark detection while maintaining
consistent object references throughout the story. Evaluation demonstrates a
reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when
compared to a non-fine-tuned model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Questions to Clinical Recommendations: Large Language Models
  Driving Evidence-Based Clinical Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dubai Li, Nan Jiang, Kangping Huang, Ruiqi Tu, Shuyu Ouyang, Huayu Yu, Lin Qiao, Chen Yu, Tianshu Zhou, Danyang Tong, Qian Wang, Mengtao Li, Xiaofeng Zeng, Yu Tian, Xinping Tian, Jingsong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical evidence, derived from rigorous research and data analysis, provides
healthcare professionals with reliable scientific foundations for informed
decision-making. Integrating clinical evidence into real-time practice is
challenging due to the enormous workload, complex professional processes, and
time constraints. This highlights the need for tools that automate evidence
synthesis to support more efficient and accurate decision making in clinical
settings. This study introduces Quicker, an evidence-based clinical decision
support system powered by large language models (LLMs), designed to automate
evidence synthesis and generate clinical recommendations modeled after standard
clinical guideline development processes. Quicker implements a fully automated
chain that covers all phases, from questions to clinical recommendations, and
further enables customized decision-making through integrated tools and
interactive user interfaces. To evaluate Quicker's capabilities, we developed
the Q2CRBench-3 benchmark dataset, based on clinical guideline development
records for three different diseases. Experimental results highlighted
Quicker's strong performance, with fine-grained question decomposition tailored
to user preferences, retrieval sensitivities comparable to human experts, and
literature screening performance approaching comprehensive inclusion of
relevant studies. In addition, Quicker-assisted evidence assessment effectively
supported human reviewers, while Quicker's recommendations were more
comprehensive and logically coherent than those of clinicians. In system-level
testing, collaboration between a single reviewer and Quicker reduced the time
required for recommendation development to 20-40 minutes. In general, our
findings affirm the potential of Quicker to help physicians make quicker and
more reliable evidence-based clinical decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Evolving Landscape of Generative Large Language Models and
  Traditional Natural Language Processing in Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Huitao Li, Matthew Yu Heng Wong, Yuhe Ke, Xin Li, Kunyu Yu, Jingchi Liao, Jonathan Chong Kai Liew, Sabarinath Vinod Nair, Jasmine Chiat Ling Ong, Irene Li, Douglas Teodoro, Chuan Hong, Daniel Shu Wei Ting, Nan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing (NLP) has been traditionally applied to medicine,
and generative large language models (LLMs) have become prominent recently.
However, the differences between them across different medical tasks remain
underexplored. We analyzed 19,123 studies, finding that generative LLMs
demonstrate advantages in open-ended tasks, while traditional NLP dominates in
information extraction and analysis tasks. As these technologies advance,
ethical use of them is essential to ensure their potential in medical
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing LLM Text Annotation Skills: A Study on Human Rights Violations
  in Social Media Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Poli Apollinaire Nemkova, Solomon Ubani, Mark V. Albert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of increasingly sophisticated natural language processing (NLP)
systems, large language models (LLMs) have demonstrated remarkable potential
for diverse applications, including tasks requiring nuanced textual
understanding and contextual reasoning. This study investigates the
capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,
Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex
textual dataset comprising social media posts in Russian and Ukrainian.
Specifically, the focus is on the binary classification task of identifying
references to human rights violations within the dataset.
  To evaluate the effectiveness of these models, their annotations are compared
against a gold standard set of human double-annotated labels across 1000
samples. The analysis includes assessing annotation performance under different
prompting conditions, with prompts provided in both English and Russian.
Additionally, the study explores the unique patterns of errors and
disagreements exhibited by each model, offering insights into their strengths,
limitations, and cross-linguistic adaptability.
  By juxtaposing LLM outputs with human annotations, this research contributes
to understanding the reliability and applicability of LLMs for sensitive,
domain-specific tasks in multilingual contexts. It also sheds light on how
language models handle inherently subjective and context-dependent judgments, a
critical consideration for their deployment in real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Interplay of Human-AI Alignment,Fairness, and Performance
  Trade-offs in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Luo, Ziyu Zhou, Zixin Shu, Aurélie Pahud de Mortanges, Robert Berke, Mauricio Reyes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks excel in medical imaging but remain prone to biases,
leading to fairness gaps across demographic groups. We provide the first
systematic exploration of Human-AI alignment and fairness in this domain. Our
results show that incorporating human insights consistently reduces fairness
gaps and enhances out-of-domain generalization, though excessive alignment can
introduce performance trade-offs, emphasizing the need for calibrated
strategies. These findings highlight Human-AI alignment as a promising approach
for developing fair, robust, and generalizable medical AI systems, striking a
balance between expert guidance and automated efficiency. Our code is available
at https://github.com/Roypic/Aligner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComplexFormer: Disruptively Advancing <span class="highlight-title">Transformer</span> Inference Ability via
  Head-Specific Complex Vector Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintian Shao, Hongyi Huang, Jiayi Wu, Beiwen Zhang, ZhiYu Wu, You Shan, MingKai Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models rely on self-attention to capture token dependencies but
face challenges in effectively integrating positional information while
allowing multi-head attention (MHA) flexibility. Prior methods often model
semantic and positional differences disparately or apply uniform positional
adjustments across heads, potentially limiting representational capacity. This
paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.
CMHA empowers each head to independently model semantic and positional
differences unified within the complex plane, representing interactions as
rotations and scaling. ComplexFormer incorporates two key improvements: (1) a
per-head Euler transformation, converting real-valued query/key projections
into polar-form complex vectors for head-specific complex subspace operation;
and (2) a per-head adaptive differential rotation mechanism,
exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct
strategies for integrating semantic angle differences (ASmn,i) with relative
positional encodings (Delta(Pmn),i). Extensive experiments on language
modeling, text generation, code generation, and mathematical reasoning show
ComplexFormer achieves superior performance, significantly lower generation
perplexity , and improved long-context coherence compared to strong baselines
like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,
offering a more expressive, adaptable attention mechanism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable
  Reward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongsheng Wang, Kaili Sun, Bowen Wu, Qun Yu, Ying Li, Baoxun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Role-playing conversational agents (RPCAs) face persistent challenges in
maintaining role consistency. To address this, we propose RAIDEN-R1, a novel
reinforcement learning framework that integrates Verifiable Role-Awareness
Reward (VRAR). The method introduces both singular and multi-term mining
strategies to generate quantifiable rewards by assessing role-specific keys.
Additionally, we construct a high-quality, role-aware Chain-of-Thought dataset
through multi-LLM collaboration, and implement experiments to enhance reasoning
coherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's
superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on
Script-Based Knowledge and Conversation Memory metrics, respectively,
outperforming baseline models while maintaining robustness. Case analyses
further reveal the model's enhanced ability to resolve conflicting contextual
cues and sustain first-person narrative consistency. This work bridges the
non-quantifiability gap in RPCA training and provides insights into role-aware
reasoning patterns, advancing the development of RPCAs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VQ-Logits: Compressing the Output Bottleneck of Large Language Models
  via Vector Quantized Logits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintian Shao, Hongyi Huang, Jiayi Wu, YiMing Cheng, ZhiYu Wu, You Shan, MingKai Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable success but face
significant computational and memory challenges, particularly due to their
extensive output vocabularies. The final linear projection layer, mapping
hidden states to vocabulary-sized logits, often constitutes a substantial
portion of the model's parameters and computational cost during inference.
Existing methods like adaptive softmax or hierarchical softmax introduce
structural complexities. In this paper, we propose VQ-Logits, a novel approach
that leverages Vector Quantization (VQ) to drastically reduce the parameter
count and computational load of the LLM output layer. VQ-Logits replaces the
large V * dmodel output embedding matrix with a small, shared codebook of K
embedding vectors (K << V ). Each token in the vocabulary is mapped to one of
these K codebook vectors. The LLM predicts logits over this compact codebook,
which are then efficiently "scattered" to the full vocabulary space using the
learned or preassigned mapping. We demonstrate through extensive experiments on
standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits
can achieve up to 99% parameter reduction in the output layer and 6x speedup in
logit computation, with only a marginal 4% increase in perplexity compared to
full softmax baselines. We further provide detailed ablation studies on
codebook size, initialization, and learning strategies, showcasing the
robustness and effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a
  Reasoning Model will Think 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongyun Lee, Seungone Kim, Minju Seo, Yongrae Jo, Dongyoung Go, Hyeonbin Hwang, Jinho Park, Xiang Yue, Sean Welleck, Graham Neubig, Moontae Lee, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long chain-of-thought (CoT) is an essential ingredient in effective usage of
modern large language models, but our understanding of the reasoning strategies
underlying these capabilities remains limited. While some prior works have
attempted to categorize CoTs using predefined strategy types, such approaches
are constrained by human intuition and fail to capture the full diversity of
model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up
framework for analyzing and steering model reasoning. Our method automatically
extracts diverse reasoning criteria from model-generated CoTs, embeds them into
a semantic space, clusters them into representative categories, and derives
contrastive rubrics to interpret reasoning behavior. Human evaluations show
that this framework produces more interpretable and comprehensive analyses than
existing methods. Moreover, we demonstrate that this understanding enables
performance gains: we can predict which strategy a model is likely to use and
guide it toward more effective alternatives. Finally, we provide practical
insights, such as that training data format (e.g., free-form vs.
multiple-choice) has a far greater impact on reasoning behavior than data
domain, underscoring the importance of format-aware model design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining Hidden Thoughts from Texts: Evaluating Continual <span class="highlight-title">Pretrain</span>ing with
  Synthetic Data for LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoichi Ishibashi, Taro Yano, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GE-Chat: A Graph Enhanced RAG Framework for Evidential Response
  Generation of LLMs <span class="chip">IJCAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longchao Da, Parth Mitesh Shah, Kuan-Ru Liou, Jiaxing Zhang, Hua Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models are now key assistants in human decision-making
processes. However, a common note always seems to follow: "LLMs can make
mistakes. Be careful with important info." This points to the reality that not
all outputs from LLMs are dependable, and users must evaluate them manually.
The challenge deepens as hallucinated responses, often presented with seemingly
plausible explanations, create complications and raise trust issues among
users. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph
enhanced retrieval-augmented generation framework to provide Evidence-based
response generation. Specifically, when the user uploads a material document, a
knowledge graph will be created, which helps construct a retrieval-augmented
agent, enhancing the agent's responses with additional knowledge beyond its
training corpus. Then we leverage Chain-of-Thought (CoT) logic generation,
n-hop sub-graph searching, and entailment-based sentence generation to realize
accurate evidence retrieval. We demonstrate that our method improves the
existing models' performance in terms of identifying the exact evidence in a
free-form context, providing a reliable way to examine the resources of LLM's
conclusion and help with the judgment of the trustworthiness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, accepted to IJCAI2025 demo track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via
  Multi-Objective Balanced Covering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangfu Li, Hongjian Zhan, Tianyi Chen, Qi Liu, Yue Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing visual token pruning methods target prompt alignment and visual
preservation with static strategies, overlooking the varying relative
importance of these objectives across tasks, which leads to inconsistent
performance. To address this, we derive the first closed-form error bound for
visual token pruning based on the Hausdorff distance, uniformly characterizing
the contributions of both objectives. Moreover, leveraging $\epsilon$-covering
theory, we reveal an intrinsic trade-off between these objectives and quantify
their optimal attainment levels under a fixed budget. To practically handle
this trade-off, we propose Multi-Objective Balanced Covering (MoB), which
reformulates visual token pruning as a bi-objective covering problem. In this
framework, the attainment trade-off reduces to budget allocation via greedy
radius trading. MoB offers a provable performance bound and linear scalability
with respect to the number of input visual tokens, enabling adaptation to
challenging pruning scenarios. Extensive experiments show that MoB preserves
96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual
tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible
performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm
that MoB integrates seamlessly into advanced MLLMs and diverse vision-language
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages,9 figures,conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Virtual Machine Scheduling in Cloud Computing through Language
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JieHao Wu, Ziwei Wang, Junjie Sheng, Wenhao Li, Xiangfei Wang, Jun Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In cloud services, virtual machine (VM) scheduling is a typical Online
Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by
large-scale complexity and fluctuating demands. Traditional optimization
methods struggle to adapt to real-time changes, domain-expert-designed
heuristic approaches suffer from rigid strategies, and existing learning-based
methods often lack generalizability and interpretability. To address these
limitations, this paper proposes a hierarchical language agent framework named
MiCo, which provides a large language model (LLM)-driven heuristic design
paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov
Decision Process with Options (SMDP-Option), enabling dynamic scheduling
through a two-stage architecture, i.e., Option Miner and Option Composer.
Option Miner utilizes LLMs to discover diverse and useful non-context-aware
strategies by interacting with constructed environments. Option Composer
employs LLMs to discover a composing strategy that integrates the
non-context-aware strategies with the contextual ones. Extensive experiments on
real-world enterprise datasets demonstrate that MiCo achieves a 96.9\%
competitive ratio in large-scale scenarios involving more than 10,000 virtual
machines. It maintains high performance even under nonstationary request flows
and diverse configurations, thus validating its effectiveness in complex and
large-scale cloud environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Does Neuro Mean to Cardio? Investigating the Role of Clinical
  Specialty Data in Medical LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinlan Yan, Di Wu, Yibin Lei, Christof Monz, Iacer Calixto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce S-MedQA, an English medical question-answering
(QA) dataset for benchmarking large language models in fine-grained clinical
specialties. We use S-MedQA to check the applicability of a popular hypothesis
related to knowledge injection in the knowledge-intense scenario of medical QA,
and show that: 1) training on data from a speciality does not necessarily lead
to best performance on that specialty and 2) regardless of the specialty
fine-tuned on, token probabilities of clinically relevant terms for all
specialties increase consistently. Thus, we believe improvement gains come
mostly from domain shifting (e.g., general to medical) rather than knowledge
injection and suggest rethinking the role of fine-tuning data in the medical
domain. We release S-MedQA and all code needed to reproduce all our experiments
to the research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Text to Network: Constructing a Knowledge Graph of Taiwan-Based
  China Studies Using Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsuan-Lei Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Taiwanese China Studies (CS) has developed into a rich, interdisciplinary
research field shaped by the unique geopolitical position and long standing
academic engagement with Mainland China. This study responds to the growing
need to systematically revisit and reorganize decades of Taiwan based CS
scholarship by proposing an AI assisted approach that transforms unstructured
academic texts into structured, interactive knowledge representations. We apply
generative AI (GAI) techniques and large language models (LLMs) to extract and
standardize entity relation triples from 1,367 peer reviewed CS articles
published between 1996 and 2019. These triples are then visualized through a
lightweight D3.js based system, forming the foundation of a domain specific
knowledge graph and vector database for the field. This infrastructure allows
users to explore conceptual nodes and semantic relationships across the corpus,
revealing previously uncharted intellectual trajectories, thematic clusters,
and research gaps. By decomposing textual content into graph structured
knowledge units, our system enables a paradigm shift from linear text
consumption to network based knowledge navigation. In doing so, it enhances
scholarly access to CS literature while offering a scalable, data driven
alternative to traditional ontology construction. This work not only
demonstrates how generative AI can augment area studies and digital humanities
but also highlights its potential to support a reimagined scholarly
infrastructure for regional knowledge systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XRAG: Cross-lingual Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Liu, Sony Trenous, Leonardo F. R. Ribeiro, Bill Byrne, Felix Hieber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose XRAG, a novel benchmark designed to evaluate the generation
abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)
settings where the user language does not match the retrieval results. XRAG is
constructed from recent news articles to ensure that its questions require
external knowledge to be answered. It covers the real-world scenarios of
monolingual and multilingual retrieval, and provides relevancy annotations for
each retrieved document. Our novel dataset construction pipeline results in
questions that require complex reasoning, as evidenced by the significant gap
between human and LLM performance. Consequently, XRAG serves as a valuable
benchmark for studying LLM reasoning abilities, even before considering the
additional cross-lingual complexity. Experimental results on five LLMs uncover
two previously unreported challenges in cross-lingual RAG: 1) in the
monolingual retrieval setting, all evaluated models struggle with response
language correctness; 2) in the multilingual retrieval setting, the main
challenge lies in reasoning over retrieved information across languages rather
than generation of non-English text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing and Contextualising Probes for African Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wisdom Aduah, Francois Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) for African languages are continually
improving, but the reasons behind these advances remain unclear. This paper
presents the first systematic investigation into probing PLMs for linguistic
knowledge about African languages. We train layer-wise probes for six
typologically diverse African languages to analyse how linguistic features are
distributed. We also design control tasks, a way to interpret probe
performance, for the MasakhaPOS dataset. We find PLMs adapted for African
languages to encode more linguistic information about target languages than
massively multilingual PLMs. Our results reaffirm previous findings that
token-level syntactic information concentrates in middle-to-last layers, while
sentence-level semantic information is distributed across all layers. Through
control tasks and probing baselines, we confirm that performance reflects the
internal knowledge of PLMs rather than probe memorisation. Our study applies
established interpretability techniques to African-language PLMs. In doing so,
we highlight the internal mechanisms underlying the success of strategies like
active learning and multilingual adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dark LLMs: The Growing Threat of Unaligned AI Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Fire, Yitzhak Elbazis, Adi Wasenstein, Lior Rokach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) rapidly reshape modern life, advancing fields
from healthcare to education and beyond. However, alongside their remarkable
capabilities lies a significant threat: the susceptibility of these models to
jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems
from the very data they learn from. As long as this training data includes
unfiltered, problematic, or 'dark' content, the models can inherently learn
undesirable patterns or weaknesses that allow users to circumvent their
intended safety controls. Our research identifies the growing threat posed by
dark LLMs models deliberately designed without ethical guardrails or modified
through jailbreak techniques. In our research, we uncovered a universal
jailbreak attack that effectively compromises multiple state-of-the-art models,
enabling them to answer almost any question and produce harmful outputs upon
request. The main idea of our attack was published online over seven months
ago. However, many of the tested LLMs were still vulnerable to this attack.
Despite our responsible disclosure efforts, responses from major LLM providers
were often inadequate, highlighting a concerning gap in industry practices
regarding AI safety. As model training becomes more accessible and cheaper, and
as open-source LLMs proliferate, the risk of widespread misuse escalates.
Without decisive intervention, LLMs may continue democratizing access to
dangerous knowledge, posing greater risks than anticipated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance
  Multi-Document QA Capability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Peng, Jinhao Jiang, Zican Dong, Wayne Xin Zhao, Lei Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in Large Language Models (LLMs) have extended their input
context length, yet they still struggle with retrieval and reasoning in
long-context inputs. Existing methods propose to utilize the prompt strategy
and retrieval head to alleviate this limitation. However, they still face
challenges in balancing retrieval precision and recall, impacting their
efficacy in answering questions. To address this, we introduce $\textbf{CAFE}$,
a two-stage coarse-to-fine method to enhance multi-document question-answering
capacities. By gradually eliminating the negative impacts of background and
distracting documents, CAFE makes the responses more reliant on the evidence
documents. Initially, a coarse-grained filtering method leverages retrieval
heads to identify and rank relevant documents. Then, a fine-grained steering
method guides attention to the most relevant content. Experiments across
benchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%
SubEM improvement over SFT and RAG methods on the Mistral model, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lake Yin, Fan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) have risen in prominence over the past few
years, there has been concern over the potential biases in LLMs inherited from
the training data. Previous studies have examined how LLMs exhibit implicit
bias, such as when response generation changes when different social contexts
are introduced. We argue that this implicit bias is not only an ethical, but
also a technical issue, as it reveals an inability of LLMs to accommodate
extraneous information. However, unlike other measures of LLM intelligence,
there are no standard methods to benchmark this specific subset of LLM bias. To
bridge this gap, we developed a method for calculating an easily interpretable
benchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM
logic and math problem datasets with sociodemographic personas. We demonstrate
that this method can statistically validate the presence of implicit bias in
LLM behavior and find an inverse trend between question answering accuracy and
implicit bias, supporting our argument.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced Crash Causation Analysis for Freeway Safety: A Large Language
  Model Approach to Identifying Key Contributing Factors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Samgyu Yang, Abdulrahman Faden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the factors contributing to traffic crashes and developing
strategies to mitigate their severity is essential. Traditional statistical
methods and machine learning models often struggle to capture the complex
interactions between various factors and the unique characteristics of each
crash. This research leverages large language model (LLM) to analyze freeway
crash data and provide crash causation analysis accordingly. By compiling 226
traffic safety studies related to freeway crashes, a training dataset
encompassing environmental, driver, traffic, and geometric design factors was
created. The Llama3 8B model was fine-tuned using QLoRA to enhance its
understanding of freeway crashes and their contributing factors, as covered in
these studies. The fine-tuned Llama3 8B model was then used to identify crash
causation without pre-labeled data through zero-shot classification, providing
comprehensive explanations to ensure that the identified causes were reasonable
and aligned with existing research. Results demonstrate that LLMs effectively
identify primary crash causes such as alcohol-impaired driving, speeding,
aggressive driving, and driver inattention. Incorporating event data, such as
road maintenance, offers more profound insights. The model's practical
applicability and potential to improve traffic safety measures were validated
by a high level of agreement among researchers in the field of traffic safety,
as reflected in questionnaire results with 88.89%. This research highlights the
complex nature of traffic crashes and how LLMs can be used for comprehensive
analysis of crash causation and other contributing factors. Moreover, it
provides valuable insights and potential countermeasures to aid planners and
policymakers in developing more effective and efficient traffic safety
practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalizing Large Language Models using Retrieval Augmented Generation
  and Knowledge Graph <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deeksha Prahlad, Chanhee Lee, Dongha Kim, Hokeun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) has allowed numerous applications,
including the generation of queried responses, to be leveraged in chatbots and
other conversational assistants. Being trained on a plethora of data, LLMs
often undergo high levels of over-fitting, resulting in the generation of extra
and incorrect data, thus causing hallucinations in output generation. One of
the root causes of such problems is the lack of timely, factual, and
personalized information fed to the LLM. In this paper, we propose an approach
to address these problems by introducing retrieval augmented generation (RAG)
using knowledge graphs (KGs) to assist the LLM in personalized response
generation tailored to the users. KGs have the advantage of storing
continuously updated factual information in a structured way. While our KGs can
be used for a variety of frequently updated personal data, such as calendar,
contact, and location data, we focus on calendar data in this paper. Our
experimental results show that our approach works significantly better in
understanding personal information and generating accurate responses compared
to the baseline LLMs using personal data as text inputs, with a moderate
reduction in response time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Companion Proceedings of the ACM Web Conference 2025
  (WWW Companion '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking <span class="highlight-title">Prompt</span> Optimizers: From <span class="highlight-title">Prompt</span> Merits to Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiao Zhu, Hanzhang Zhou, Zijian Feng, Tianjiao Li, Chua Jia Jim Deryl, Mak Lee Onn, Gee Wah Ng, Kezhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt optimization (PO) offers a practical alternative to fine-tuning large
language models (LLMs), enabling performance improvements without altering
model weights. Existing methods typically rely on advanced, large-scale LLMs
like GPT-4 to generate optimized prompts. However, due to limited downward
compatibility, verbose, instruction-heavy prompts from advanced LLMs can
overwhelm lightweight inference models and degrade response quality. In this
work, we rethink prompt optimization through the lens of interpretable design.
We first identify a set of model-agnostic prompt quality merits and empirically
validate their effectiveness in enhancing prompt and response quality. We then
introduce MePO, a merit-guided, lightweight, and locally deployable prompt
optimizer trained on our preference dataset built from merit-aligned prompts
generated by a lightweight LLM. Unlike prior work, MePO avoids online
optimization reliance, reduces cost and privacy concerns, and, by learning
clear, interpretable merits, generalizes effectively to both large-scale and
lightweight inference models. Experiments demonstrate that MePO achieves better
results across diverse tasks and model types, offering a scalable and robust
solution for real-world deployment. Our model and dataset are available at:
https://github.com/MidiyaZhu/MePO
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework
  for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidan Wang, Yubing Ren, Yanan Cao, Binxing Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Large Language Models (LLMs) has heightened concerns about the
misuse of AI-generated text, making watermarking a promising solution.
Mainstream watermarking schemes for LLMs fall into two categories: logits-based
and sampling-based. However, current schemes entail trade-offs among
robustness, text quality, and security. To mitigate this, we integrate
logits-based and sampling-based schemes, harnessing their respective strengths
to achieve synergy. In this paper, we propose a versatile symbiotic
watermarking framework with three strategies: serial, parallel, and hybrid. The
hybrid framework adaptively embeds watermarks using token entropy and semantic
entropy, optimizing the balance between detectability, robustness, text
quality, and security. Furthermore, we validate our approach through
comprehensive experiments on various datasets and models. Experimental results
indicate that our method outperforms existing baselines and achieves
state-of-the-art (SOTA) performance. We believe this framework provides novel
insights into diverse watermarking paradigms. Our code is available at
\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative
  In-Context Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidan Wang, Yanan Cao, Yubing Ren, Fang Fang, Zheng Lin, Binxing Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in various domains but pose inherent
privacy risks. Existing methods to evaluate privacy leakage in LLMs often use
memorized prefixes or simple instructions to extract data, both of which
well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM
safety mechanisms to generate harmful content, but their role in privacy
scenarios remains underexplored. In this paper, we examine the effectiveness of
jailbreak attacks in extracting sensitive information, bridging privacy leakage
and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework
targeting Personally Identifiable Information (PII) and addressing the
limitations of current jailbreak methods. Specifically, PIG identifies PII
entities and their types in privacy queries, uses in-context learning to build
a privacy context, and iteratively updates it with three gradient-based
strategies to elicit target PII. We evaluate PIG and existing jailbreak methods
using two privacy-related datasets. Experiments on four white-box and two
black-box LLMs show that PIG outperforms baseline methods and achieves
state-of-the-art (SoTA) results. The results underscore significant privacy
risks in LLMs, emphasizing the need for stronger safeguards. Our code is
availble at
\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crossing Borders Without Crossing Boundaries: How Sociolinguistic
  Awareness Can Optimize User Engagement with Localized Spanish AI Models
  Across Hispanophone Countries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Capdevila, Esteban Villa Turek, Ellen Karina Chumbe Fernandez, Luis Felipe Polo Galvez, Luis Cadavid, Andrea Marroquin, Rebeca Vargas Quesada, Johanna Crew, Nicole Vallejo Galarraga, Christopher Rodriguez, Diego Gutierrez, Radhi Datla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are, by definition, based on language. In an effort to
underscore the critical need for regional localized models, this paper examines
primary differences between variants of written Spanish across Latin America
and Spain, with an in-depth sociocultural and linguistic contextualization
therein. We argue that these differences effectively constitute significant
gaps in the quotidian use of Spanish among dialectal groups by creating
sociolinguistic dissonances, to the extent that locale-sensitive AI models
would play a pivotal role in bridging these divides. In doing so, this approach
informs better and more efficient localization strategies that also serve to
more adequately meet inclusivity goals, while securing sustainable active daily
user growth in a major low-risk investment geographic area. Therefore,
implementing at least the proposed five sub variants of Spanish addresses two
lines of action: to foment user trust and reliance on AI language models while
also demonstrating a level of cultural, historical, and sociolinguistic
awareness that reflects positively on any internationalization strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Exploration-Exploitation Strategies of LLMs and Humans:
  Insights from Standard Multi-armed Bandit Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyuan Zhang, Darcy Wang, Ningyuan Chen, Rodrigo Mansur, Vahid Sarhangian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly used to simulate or automate
human behavior in complex sequential decision-making tasks. A natural question
is then whether LLMs exhibit similar decision-making behavior to humans, and
can achieve comparable (or superior) performance. In this work, we focus on the
exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic
decision-making under uncertainty. We employ canonical multi-armed bandit (MAB)
tasks introduced in the cognitive science and psychiatry literature to conduct
a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.
We use interpretable choice models to capture the E&E strategies of the agents
and investigate how explicit reasoning, through both prompting strategies and
reasoning-enhanced models, shapes LLM decision-making. We find that reasoning
shifts LLMs toward more human-like behavior, characterized by a mix of random
and directed exploration. In simple stationary tasks, reasoning-enabled LLMs
exhibit similar levels of random and directed exploration compared to humans.
However, in more complex, non-stationary environments, LLMs struggle to match
human adaptability, particularly in effective directed exploration, despite
achieving similar regret in certain scenarios. Our findings highlight both the
promise and limits of LLMs as simulators of human behavior and tools for
automated decision-making and point to potential areas of improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does Knowledge Selection Help Retrieval Augmented Generation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13258v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13258v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangci Li, Jessica Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is a powerful method for enhancing
natural language generation by integrating external knowledge into a model's
output. While prior work has demonstrated the importance of improving knowledge
retrieval for boosting generation quality, the role of knowledge selection
remains less clear. This paper empirically analyzes how knowledge selection
influences downstream generation performance in RAG systems. By simulating
different retrieval and selection conditions through a controlled mixture of
gold and distractor knowledge, we assess the impact of these factors on
generation outcomes. Our findings indicate that the downstream generator
model's capability, as well as the complexity of the task and dataset,
significantly influence the impact of knowledge selection on the overall RAG
system performance. In typical scenarios, improving the knowledge recall score
is key to enhancing generation outcomes, with the knowledge selector providing
limited benefit when a strong generator model is used on clear, well-defined
tasks. For weaker generator models or more ambiguous tasks and datasets, the
knowledge F1 score becomes a critical factor, and the knowledge selector plays
a more prominent role in improving overall performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARR: Question Answering with Large Language Models via Analyzing,
  Retrieving, and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04689v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04689v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Yin, Giuseppe Carenini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive capabilities on
complex evaluation benchmarks, many of which are formulated as
question-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts
is becoming increasingly vital for advancing their development and
applicability. This paper introduces ARR, an intuitive, effective, and general
QA solving method that explicitly incorporates three key steps: analyzing the
intent of the question, retrieving relevant information, and reasoning step by
step. Notably, this paper is the first to introduce intent analysis in QA,
which plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA
tasks demonstrate that ARR consistently outperforms the baseline methods.
Ablation and case studies further validate the positive contributions of each
ARR component. Furthermore, experiments involving variations in prompt design
indicate that ARR maintains its effectiveness regardless of the specific prompt
formulation. Additionally, extensive evaluations across various model sizes,
LLM series, and generation settings solidify the effectiveness, robustness, and
generalizability of ARR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages. Code: https://github.com/YuweiYin/ARR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information
  Funneling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02069v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02069v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Yucheng Li, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Junjie Hu, Wen Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we investigate whether attention-based information flow inside
large language models (LLMs) is aggregated through noticeable patterns for long
context processing. Our observations reveal that LLMs aggregate information
through Pyramidal Information Funneling where attention is scattering widely in
lower layers, progressively consolidating within specific contexts, and
ultimately focusing on critical tokens (a.k.a massive activation or attention
sink) in higher layers. Motivated by these insights, we developed PyramidKV, a
novel and effective KV cache compression method. This approach dynamically
adjusts the KV cache size across different layers, allocating more cache in
lower layers and less in higher ones, diverging from traditional methods that
maintain a uniform KV cache size. Our experimental evaluations, utilizing the
LongBench benchmark, show that PyramidKV matches the performance of models with
a full KV cache while retaining only 12% of the KV cache, thus significantly
reducing memory usage. In scenarios emphasizing memory efficiency, where only
0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache
compression techniques, achieving up to a 20.5 absolute accuracy improvement on
TREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms
competing methods in maintaining long-context comprehension in LLMs; notably,
retaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve
100.0 Acc. performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Generative AI for Scoring Medical Student Interviews in
  Objective Structured Clinical Examinations (OSCEs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jadon Geathers, Yann Hicke, Colleen Chan, Niroop Rajashekar, Justin Sewell, Susannah Cornes, Rene F. Kizilcec, Dennis Shung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective Structured Clinical Examinations (OSCEs) are widely used to assess
medical students' communication skills, but scoring interview-based assessments
is time-consuming and potentially subject to human bias. This study explored
the potential of large language models (LLMs) to automate OSCE evaluations
using the Master Interview Rating Scale (MIRS). We compared the performance of
four state-of-the-art LLMs (GPT-4o, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro)
in evaluating OSCE transcripts across all 28 items of the MIRS under the
conditions of zero-shot, chain-of-thought (CoT), few-shot, and multi-step
prompting. The models were benchmarked against a dataset of 10 OSCE cases with
174 expert consensus scores available. Model performance was measured using
three accuracy metrics (exact, off-by-one, thresholded). Averaging across all
MIRS items and OSCE cases, LLMs performed with low exact accuracy (0.27 to
0.44), and moderate to high off-by-one accuracy (0.67 to 0.87) and thresholded
accuracy (0.75 to 0.88). A zero temperature parameter ensured high intra-rater
reliability ({\alpha} = 0.98 for GPT-4o). CoT, few-shot, and multi-step
techniques proved valuable when tailored to specific assessment items. The
performance was consistent across MIRS items, independent of encounter phases
and communication domains. We demonstrated the feasibility of AI-assisted OSCE
evaluation and provided benchmarking of multiple LLMs across multiple prompt
techniques. Our work provides a baseline performance assessment for LLMs that
lays a foundation for future research into automated assessment of clinical
communication skills.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages + 3 pages of references, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling Memory and Reasoning Ability in Large Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13504v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13504v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang, Wenyue Hua, Ruixiang Tang, William Yang Wang, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated strong performance in handling
complex tasks requiring both extensive knowledge and reasoning abilities.
However, the existing LLM inference pipeline operates as an opaque process
without explicit separation between knowledge retrieval and reasoning steps,
making the model's decision-making process unclear and disorganized. This
ambiguity can lead to issues such as hallucinations and knowledge forgetting,
which significantly impact the reliability of LLMs in high-stakes domains. In
this paper, we propose a new inference paradigm that decomposes the complex
inference process into two distinct and clear actions: (1) memory recall: which
retrieves relevant knowledge, and (2) reasoning: which performs logical steps
based on the recalled knowledge. To facilitate this decomposition, we introduce
two special tokens memory and reason, guiding the model to distinguish between
steps that require knowledge retrieval and those that involve reasoning. Our
experiment results show that this decomposition not only improves model
performance but also enhances the interpretability of the inference process,
enabling users to identify sources of error and refine model responses
effectively. The code is available at
https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SceneGenAgent: Precise Industrial Scene Generation with Coding Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xia, Dan Zhang, Zibo Liao, Zhenyu Hou, Tianrui Sun, Jing Li, Ling Fu, Yuxiao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The modeling of industrial scenes is essential for simulations in industrial
manufacturing. While large language models (LLMs) have shown significant
progress in generating general 3D scenes from textual descriptions, generating
industrial scenes with LLMs poses a unique challenge due to their demand for
precise measurements and positioning, requiring complex planning over spatial
arrangement. To address this challenge, we introduce SceneGenAgent, an
LLM-based agent for generating industrial scenes through C# code. SceneGenAgent
ensures precise layout planning through a structured and calculable format,
layout verification, and iterative refinement to meet the quantitative
requirements of industrial scenarios. Experiment results demonstrate that LLMs
powered by SceneGenAgent exceed their original performance, reaching up to
81.0% success rate in real-world industrial scene generation tasks and
effectively meeting most scene generation requirements. To further enhance
accessibility, we construct SceneInstruct, a dataset designed for fine-tuning
open-source LLMs to integrate into SceneGenAgent. Experiments show that
fine-tuning open-source LLMs on SceneInstruct yields significant performance
improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our
code and data are available at https://github.com/THUDM/SceneGenAgent .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Driven Calibration of Prediction Sets in Large Vision-Language
  Models Based on Inductive Conformal Prediction <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17671v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17671v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchang Ye, Weiyan Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the critical challenge of hallucination mitigation in
Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks
through a Split Conformal Prediction (SCP) framework. While LVLMs excel in
multi-modal reasoning, their outputs often exhibit hallucinated content with
high confidence, posing risks in safety-critical applications. We propose a
model-agnostic uncertainty quantification method that integrates dynamic
threshold calibration and cross-modal consistency verification. By partitioning
data into calibration and test sets, the framework computes nonconformity
scores to construct prediction sets with statistical guarantees under
user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous
control of \textbf{marginal coverage} to ensure empirical error rates remain
strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes
inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of
prior distribution assumptions and retraining requirements. Evaluations on
benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces
theoretical guarantees across all $\alpha$ values. The framework achieves
stable performance across varying calibration-to-test split ratios,
underscoring its robustness for real-world deployment in healthcare, autonomous
systems, and other safety-sensitive domains. This work bridges the gap between
theoretical reliability and practical applicability in multi-modal AI systems,
offering a scalable solution for hallucination detection and uncertainty-aware
decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICIPCA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Multiple Large Language Models: A <span class="highlight-title">Survey</span> on LLM Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18036v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18036v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Dingqi Yang, Hailong Sun, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM Ensemble -- which involves the comprehensive use of multiple large
language models (LLMs), each aimed at handling user queries during downstream
inference, to benefit from their individual strengths -- has gained substantial
attention recently. The widespread availability of LLMs, coupled with their
varying strengths and out-of-the-box usability, has profoundly advanced the
field of LLM Ensemble. This paper presents the first systematic review of
recent developments in LLM Ensemble. First, we introduce our taxonomy of LLM
Ensemble and discuss several related research problems. Then, we provide a more
in-depth classification of the methods under the broad categories of
"ensemble-before-inference, ensemble-during-inference,
ensemble-after-inference'', and review all relevant methods. Finally, we
introduce related benchmarks and applications, summarize existing studies, and
suggest several future research directions. A curated list of papers on LLM
Ensemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, codebase:
  https://github.com/junchenzhi/Awesome-LLM-Ensemble</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tokenization Matters! Degrading Large Language Models through
  Challenging Their Tokenization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dixuan Wang, Yanda Li, Junyuan Jiang, Zepeng Ding, Ziqin Luo, Guochao Jiang, Jiaqing Liang, Deqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in language
understanding and generation. Nonetheless, it was also witnessed that LLMs tend
to produce inaccurate responses to specific queries. This deficiency can be
traced to the tokenization step LLMs must undergo, which is an inevitable
limitation inherent to all LLMs. In fact, incorrect tokenization is the
critical point that hinders LLMs in understanding the input precisely, thus
leading to unsatisfactory output. This defect is more obvious in Chinese
scenarios. To demonstrate this flaw of LLMs, we construct an adversarial
dataset, named as $\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which
draws upon the vocabularies of various open-source LLMs to challenge LLMs'
tokenization. ADT consists of two subsets: the manually constructed ADT-Human
and the automatically generated ADT-Auto. Our empirical results reveal that our
ADT is highly effective on challenging the tokenization of leading LLMs,
including GPT-4o, Llama-3, Deepseek-R1 and so on, thus degrading these LLMs'
capabilities. Moreover, our method of automatic data generation has been proven
efficient and robust, which can be applied to any open-source LLMs. In this
paper, we substantially investigate LLMs' vulnerability in terms of challenging
their token segmentation, which will shed light on the subsequent research of
improving LLMs' capabilities through optimizing their tokenization process and
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Harris, Fan Grayson, Felix Feldman, Timothy Laurence, Toby Nonnenmacher, Oliver Higgins, Leo Loman, Selina Patel, Thomas Finnie, Samuel Collins, Michael Borowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) become widely accessible, a detailed
understanding of their knowledge within specific domains becomes necessary for
successful real world use. This is particularly critical in public health,
where failure to retrieve relevant, accurate, and current information could
significantly impact UK residents. However, currently little is known about LLM
knowledge of UK Government public health information. To address this issue,
this paper introduces a new benchmark, PubHealthBench, with over 8000 questions
for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form
responses to public health queries. To create PubHealthBench we extract free
text from 687 current UK government guidance documents and implement an
automated pipeline for generating MCQA samples. Assessing 24 LLMs on
PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a
high degree of knowledge, achieving >90% accuracy in the MCQA setup, and
outperform humans with cursory search engine use. However, in the free form
setup we see lower performance with no model scoring >75%. Importantly we find
in both setups LLMs have higher accuracy on guidance intended for the general
public. Therefore, there are promising signs that state of the art (SOTA) LLMs
are an increasingly accurate source of public health information, but
additional safeguards or tools may still be needed when providing free form
responses on public health topics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 pages main text</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient
  Fine-Tuning of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyegang Son, Yonglak Son, Changhoon Kim, Young Geun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large-scale pre-trained models achieve great success.
Fine-tuning is the standard practice for leveraging these models in downstream
tasks. Among the fine-tuning methods, adapter-tuning provides a
parameter-efficient fine-tuning by introducing lightweight trainable modules
while keeping most pre-trained parameters frozen. However, existing
adapter-tuning methods still impose substantial resource usage. Through our
investigation, we show that each adapter unequally contributes to both task
performance and resource usage. Motivated by this insight, we propose Selective
Adapter FrEezing (SAFE), which gradually freezes less important adapters early
to reduce unnecessary resource usage while maintaining performance. In our
experiments, SAFE reduces memory usage, computation amount, and training time
by 42.85\%, 34.59\%, and 11.82\%, respectively, while achieving comparable or
better task performance compared to the baseline. We also demonstrate that SAFE
induces regularization effect, thereby smoothing the loss landscape, which
enables the model to generalize better by avoiding sharp minima.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>URL: https://aclanthology.org/2025.naacl-long.480/ Volume:
  Proceedings of the 2025 Conference of the Nations of the Americas Chapter of
  the Association for Computational Linguistics: Human Language Technologies
  (Volume 1: Long Papers) Year: 2025 Address: Albuquerque, New Mexico</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KwaiChat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue
  Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoming Shi, Zeming Liu, Yiming Lei, Chenkai Zhang, Haitao Leng, Chuan Wang, Qingjie Liu, Wanxiang Che, Shaoguo Liu, Size Li, Yunhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-based dialogue systems, such as education assistants, have compelling
application value, thereby garnering growing interest. However, the current
video-based dialogue systems are limited by their reliance on a single dialogue
type, which hinders their versatility in practical applications across a range
of scenarios, including question-answering, emotional dialog, etc. In this
paper, we identify this challenge as how to generate video-driven multilingual
mixed-type dialogues. To mitigate this challenge, we propose a novel task and
create a human-to-human video-driven multilingual mixed-type dialogue corpus,
termed KwaiChat, containing a total of 93,209 videos and 246,080 dialogues,
across 4 dialogue types, 30 domains, 4 languages, and 13 topics. Additionally,
we establish baseline models on KwaiChat. An extensive analysis of 7 distinct
LLMs on KwaiChat reveals that GPT-4o achieves the best performance but still
cannot perform well in this situation even with the help of in-context learning
and fine-tuning, which indicates that the task is not trivial and needs further
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pose Priors from Language Models <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjay Subramanian, Evonne Ng, Lea Müller, Dan Klein, Shiry Ginosar, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is often used to describe physical interaction, yet most 3D human
pose estimation methods overlook this rich source of information. We bridge
this gap by leveraging large multimodal models (LMMs) as priors for
reconstructing contact poses, offering a scalable alternative to traditional
methods that rely on human annotations or motion capture data. Our approach
extracts contact-relevant descriptors from an LMM and translates them into
tractable losses to constrain 3D human pose optimization. Despite its
simplicity, our method produces compelling reconstructions for both two-person
interactions and self-contact scenarios, accurately capturing the semantics of
physical and social interactions. Our results demonstrate that LMMs can serve
as powerful tools for contact prediction and pose estimation, offering an
alternative to costly manual human annotations or motion capture data. Our code
is publicly available at https://prosepose.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conversational Query Reformulation with the Guidance of Retrieved
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12363v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12363v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeonghyun Park, Hwanhee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational search seeks to retrieve relevant passages for the given
questions in conversational question answering. Conversational Query
Reformulation (CQR) improves conversational search by refining the original
queries into de-contextualized forms to resolve the issues in the original
queries, such as omissions and coreferences. Previous CQR methods focus on
imitating human written queries which may not always yield meaningful search
results for the retriever. In this paper, we introduce GuideCQR, a framework
that refines queries for CQR by leveraging key information from the initially
retrieved documents. Specifically, GuideCQR extracts keywords and generates
expected answers from the retrieved documents, then unifies them with the
queries after filtering to add useful information that enhances the search
process. Experimental results demonstrate that our proposed method achieves
state-of-the-art performance across multiple datasets, outperforming previous
CQR methods. Additionally, we show that GuideCQR can get additional performance
gains in conversational search using various types of queries, even for queries
written by humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FitCF: A Framework for Automatic Feature Importance-guided
  Counterfactual Example Generation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianli Wang, Nils Feldhus, Simon Ostermann, Luis Felipe Villa-Arenas, Sebastian Möller, Vera Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual examples are widely used in natural language processing (NLP)
as valuable data to improve models, and in explainable artificial intelligence
(XAI) to understand model behavior. The automated generation of counterfactual
examples remains a challenging task even for large language models (LLMs),
despite their impressive performance on many tasks. In this paper, we first
introduce ZeroCF, a faithful approach for leveraging important words derived
from feature attribution methods to generate counterfactual examples in a
zero-shot setting. Second, we present a new framework, FitCF, which further
verifies aforementioned counterfactuals by label flip verification and then
inserts them as demonstrations for few-shot prompting, outperforming two
state-of-the-art baselines. Through ablation studies, we identify the
importance of each of FitCF's core components in improving the quality of
counterfactuals, as assessed through flip rate, perplexity, and similarity
measures. Furthermore, we show the effectiveness of LIME and Integrated
Gradients as backbone attribution methods for FitCF and find that the number of
demonstrations has the largest effect on performance. Finally, we reveal a
strong correlation between the faithfulness of feature attribution scores and
the quality of generated counterfactuals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings; camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 100 Days After DeepSeek-R1: A <span class="highlight-title">Survey</span> on Replication Studies and More
  Directions for Reasoning Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00551v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00551v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Zhang, Yue Deng, Xiang Lin, Bin Wang, Dianwen Ng, Hai Ye, Xingxuan Li, Yao Xiao, Zhanfeng Mo, Qi Zhang, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development of reasoning language models (RLMs) represents a novel
evolution in large language models. In particular, the recent release of
DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in
the research community for exploring the explicit reasoning paradigm of
language models. However, the implementation details of the released models
have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero,
DeepSeek-R1, and the distilled small models. As a result, many replication
studies have emerged aiming to reproduce the strong performance achieved by
DeepSeek-R1, reaching comparable performance through similar training
procedures and fully open-source data resources. These works have investigated
feasible strategies for supervised fine-tuning (SFT) and reinforcement learning
from verifiable rewards (RLVR), focusing on data preparation and method design,
yielding various valuable insights. In this report, we provide a summary of
recent replication studies to inspire future research. We primarily focus on
SFT and RLVR as two main directions, introducing the details for data
construction, method design and training procedure of current replication
studies. Moreover, we conclude key findings from the implementation details and
experimental results reported by these studies, anticipating to inspire future
research. We also discuss additional techniques of enhancing RLMs, highlighting
the potential of expanding the application scope of these models, and
discussing the challenges in development. By this survey, we aim to help
researchers and developers of RLMs stay updated with the latest advancements,
and seek to inspire new ideas to further enhance RLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time Awareness in Large Language Models: Benchmarking Fact Recall Across
  Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13338v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13338v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Herel, Vojtech Bartek, Jiri Jirak, Tomas Mikolov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Who is the US President? The answer changes depending on when the question is
asked. While large language models (LLMs) are evaluated on various reasoning
tasks, they often miss a crucial dimension: time. In real-world scenarios, the
correctness of answers is frequently tied to temporal context. To address this
gap, we present a novel framework and dataset spanning over 8,000 events from
2018 to 2024, annotated with day-level granularity and sourced globally across
domains such as politics, science, and business. Our TimeShift evaluation
method systematically probes LLMs for temporal reasoning, revealing that base
models often outperform instruction-tuned and synthetic-trained counterparts on
time-sensitive recall. Additionally, we find that even large-scale models
exhibit brittleness in handling paraphrased facts, highlighting unresolved
challenges in temporal consistency. By identifying these limitations, our work
provides a significant step toward advancing time-aware language models capable
of adapting to the dynamic nature of real-world knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Provable Scaling Laws for the Test-Time Compute of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19477v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19477v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose two simple, principled and practical algorithms that enjoy
provable scaling laws for the test-time compute of large language models
(LLMs). The first one is a two-stage knockout-style algorithm: given an input
problem, it first generates multiple candidate solutions, and then aggregate
them via a knockout tournament for the final output. Assuming that the LLM can
generate a correct solution with non-zero probability and do better than a
random guess in comparing a pair of correct and incorrect solutions, we prove
theoretically that the failure probability of this algorithm decays to zero
exponentially or by a power law (depending on the specific way of scaling) as
its test-time compute grows. The second one is a two-stage league-style
algorithm, where each candidate is evaluated by its average win rate against
multiple opponents, rather than eliminated upon loss to a single opponent.
Under analogous but more robust assumptions, we prove that its failure
probability also decays to zero exponentially with more test-time compute. Both
algorithms require a black-box LLM and nothing else (e.g., no verifier or
reward model) for a minimalistic implementation, which makes them appealing for
practical applications and easy to adapt for different tasks. Through extensive
experiments with diverse models and datasets, we validate the proposed theories
and demonstrate the outstanding scaling properties of both algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TopoLM: brain-like spatio-functional organization in a topographic
  language model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11516v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11516v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil Rathi, Johannes Mehrer, Badr AlKhamissi, Taha Binhuraib, Nicholas M. Blauch, Martin Schrimpf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neurons in the brain are spatially organized such that neighbors on tissue
often exhibit similar response profiles. In the human language system,
experimental studies have observed clusters for syntactic and semantic
categories, but the mechanisms underlying this functional organization remain
unclear. Here, building on work from the vision literature, we develop TopoLM,
a transformer language model with an explicit two-dimensional spatial
representation of model units. By combining a next-token prediction objective
with a spatial smoothness loss, representations in this model assemble into
clusters that correspond to semantically interpretable groupings of text and
closely match the functional organization in the brain's language system.
TopoLM successfully predicts the emergence of the spatio-functional
organization of a cortical language system as well as the organization of
functional clusters selective for fine-grained linguistic features empirically
observed in human cortex. Our results suggest that the functional organization
of the human language system is driven by a unified spatial objective, and
provide a functionally and spatially aligned model of language processing in
the brain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KBAlign: Efficient Self Adaptation on Specific Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14790v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14790v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheni Zeng, Yuxuan Chen, Shi Yu, Ruobing Wang, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although retrieval-augmented generation (RAG) remains essential for
knowledge-based question answering (KBQA), current paradigms face critical
challenges under specific domains. Existing methods struggle with targeted
adaptation on small-scale KBs: vanilla unsupervised training exhibits poor
effectiveness, while fine-tuning incurs prohibitive costs of external signals.
We present KBAlign, a self-supervised framework that enhances RAG systems
through efficient model adaptation. Our key insight is to leverage the model's
intrinsic capabilities for knowledge alignment through two innovative
mechanisms: multi-grained self-annotation that captures global knowledge for
data construction, and iterative tuning that accelerates convergence through
self verification. This framework enables cost-effective model adaptation to
specific textual KBs, without human supervision or external model assistance.
Experiments demonstrate that KBAlign can achieve 90\% of the performance gain
obtained through GPT-4-supervised adaptation, while relying entirely on
self-annotation of much smaller models. KBAlign significantly improves
downstream QA accuracy across multiple domains with tiny costs, particularly
benefiting scenarios requiring deep knowledge integration from specialized
corpora. We release our experimental data, models, and process analyses to the
community for further exploration (https://github.com/thunlp/KBAlign).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and
  Compression in LLMs <span class="chip">IJCNN 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Gu, Wuyang Zhou, Giorgos Iacovides, Danilo Mandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reasoning abilities of Large Language Models (LLMs) can be improved by
structurally denoising their weights, yet existing techniques primarily focus
on denoising the feed-forward network (FFN) of the transformer block, and can
not efficiently utilise the Multi-head Attention (MHA) block, which is the core
of transformer architectures. To address this issue, we propose a novel
intuitive framework that, at its very core, performs MHA compression through a
multi-head tensorisation process and the Tucker decomposition. This enables
both higher-dimensional structured denoising and compression of the MHA
weights, by enforcing a shared higher-dimensional subspace across the weights
of the multiple attention heads. We demonstrate that this approach consistently
enhances the reasoning capabilities of LLMs across multiple benchmark datasets,
and for both encoder-only and decoder-only architectures, while achieving
compression rates of up to $\sim 250$ times in the MHA weights, all without
requiring any additional data, training, or fine-tuning. Furthermore, we show
that the proposed method can be seamlessly combined with existing
FFN-only-based denoising techniques to achieve further improvements in LLM
reasoning performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpeted for IEEE International Joint Conference on Neural Networks
  (IJCNN 2025). The code is available at https://github.com/guyuxuan9/TensorLLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Action <span class="highlight-title">Pretrain</span>ing from Videos <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, Dieter Fox, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Latent Action Pretraining for general Action models (LAPA), an
unsupervised method for pretraining Vision-Language-Action (VLA) models without
ground-truth robot action labels. Existing Vision-Language-Action models
require action labels typically collected by human teleoperators during
pretraining, which significantly limits possible data sources and scale. In
this work, we propose a method to learn from internet-scale videos that do not
have robot action labels. We first train an action quantization model
leveraging VQ-VAE-based objective to learn discrete latent actions between
image frames, then pretrain a latent VLA model to predict these latent actions
from observations and task descriptions, and finally finetune the VLA on
small-scale robot manipulation data to map from latent to robot actions.
Experimental results demonstrate that our method significantly outperforms
existing techniques that train robot manipulation policies from large-scale
videos. Furthermore, it outperforms the state-of-the-art VLA model trained with
robotic action labels on real-world manipulation tasks that require language
conditioning, generalization to unseen objects, and semantic generalization to
unseen instructions. Training only on human manipulation videos also shows
positive transfer, opening up the potential for leveraging web-scale data for
robotics foundation model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Website: https://latentactionpretraining.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Phase Diagram of Vision Large Language Models Inference: A Perspective
  from Interaction across Image and Instruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houjing Wei, Yuting Shi, Naoya Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Large Language Models (VLLMs) usually take input as a concatenation of
image token embeddings and text token embeddings and conduct causal modeling.
However, their internal behaviors remain underexplored, raising the question of
interaction among two types of tokens. To investigate such multimodal
interaction during model inference, in this paper, we measure the
contextualization among the hidden state vectors of tokens from different
modalities. Our experiments uncover a four-phase inference dynamics of VLLMs
against the depth of Transformer-based LMs, including (I) Alignment: In very
early layers, contextualization emerges between modalities, suggesting a
feature space alignment. (II) Intra-modal Encoding: In early layers,
intra-modal contextualization is enhanced while inter-modal interaction is
suppressed, suggesting a local encoding within modalities. (III) Inter-modal
Encoding: In later layers, contextualization across modalities is enhanced,
suggesting a deeper fusion across modalities. (IV) Output Preparation: In very
late layers, contextualization is reduced globally, and hidden states are
aligned towards the unembedding space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peichao Lai, Kexuan Zhang, Yi Lin, Linyihan Zhang, Feiyang Ye, Jinhao Yan, Yanwei Xu, Conghui He, Yilei Wang, Wentao Zhang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subjective Answer Grading (SAG) plays a crucial role in education,
standardized testing, and automated assessment systems, particularly for
evaluating short-form responses in Short Answer Scoring (SAS). However,
existing approaches often produce coarse-grained scores and lack detailed
reasoning. Although large language models (LLMs) have demonstrated potential as
zero-shot evaluators, they remain susceptible to bias, inconsistencies with
human judgment, and limited transparency in scoring decisions. To overcome
these limitations, we introduce SAS-Bench, a benchmark specifically designed
for LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,
expert-annotated error categories, and a diverse range of question types
derived from real-world subject-specific exams. This benchmark facilitates
detailed evaluation of model reasoning processes and explainability. We also
release an open-source dataset containing 1,030 questions and 4,109 student
responses, each annotated by domain experts. Furthermore, we conduct
comprehensive experiments with various LLMs, identifying major challenges in
scoring science-related questions and highlighting the effectiveness of
few-shot prompting in improving scoring accuracy. Our work offers valuable
insights into the development of more robust, fair, and educationally
meaningful LLM-based evaluation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Mosaic Memory of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Shilov, Matthieu Meeus, Yves-Alexandre de Montjoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) become widely adopted, understanding how they
learn from, and memorize, training data becomes crucial. Memorization in LLMs
is widely assumed to only occur as a result of sequences being repeated in the
training data. Instead, we show that LLMs memorize by assembling information
from similar sequences, a phenomena we call mosaic memory. We show major LLMs
to exhibit mosaic memory, with fuzzy duplicates contributing to memorization as
much as 0.8 of an exact duplicate and even heavily modified sequences
contributing substantially to memorization. Despite models display reasoning
capabilities, we somewhat surprisingly show memorization to be predominantly
syntactic rather than semantic. We finally show fuzzy duplicates to be
ubiquitous in real-world data, untouched by deduplication techniques. Taken
together, our results challenge widely held beliefs and show memorization to be
a more complex, mosaic process, with real-world implications for privacy,
confidentiality, model utility and evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal
  Perspective <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19458v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19458v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taoyu Su, Jiawei Sheng, Duohe Ma, Xiaodong Li, Juwei Yue, Mengxiao Song, Yingkai Tang, Tingwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from
different Multi-Modal Knowledge Graphs (MMKGs), a critical information
retrieval task. Existing studies have explored various fusion paradigms and
consistency constraints to improve the alignment of equivalent entities, while
overlooking that the visual modality may not always contribute positively.
Empirically, entities with low-similarity images usually generate
unsatisfactory performance, highlighting the limitation of overly relying on
visual features. We believe the model can be biased toward the visual modality,
leading to a shortcut image-matching task. To address this, we propose a
counterfactual debiasing framework for MMEA, termed CDMEA, which investigates
visual modality bias from a causal perspective. Our approach aims to leverage
both visual and graph modalities to enhance MMEA while suppressing the direct
causal effect of the visual modality on model predictions. By estimating the
Total Effect (TE) of both modalities and excluding the Natural Direct Effect
(NDE) of the visual modality, we ensure that the model predicts based on the
Total Indirect Effect (TIE), effectively utilizing both modalities and reducing
visual modality bias. Extensive experiments on 9 benchmark datasets show that
CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,
high-noise, and low-resource data scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PersLLM: A Personified Training Approach for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12393v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12393v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheni Zeng, Jiayi Chen, Huimin Chen, Yukun Yan, Yuxuan Chen, Zhenghao Liu, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit human-like intelligence, enabling them
to simulate human behavior and support various applications that require both
humanized communication and extensive knowledge reserves. Efforts are made to
personify LLMs with special training data or hand-crafted prompts, while
correspondingly faced with challenges such as insufficient data usage or rigid
behavior patterns. Consequently, personified LLMs fail to capture personified
knowledge or express persistent opinion. To fully unlock the potential of LLM
personification, we propose PersLLM, a framework for better data construction
and model tuning. For insufficient data usage, we incorporate strategies such
as Chain-of-Thought prompting and anti-induction, improving the quality of data
construction and capturing the personality experiences, knowledge, and thoughts
more comprehensively. For rigid behavior patterns, we design the tuning process
and introduce automated DPO to enhance the specificity and dynamism of the
models' personalities, which leads to a more natural opinion communication.
Both automated metrics and expert human evaluations demonstrate the
effectiveness of our approach. Case studies in human-machine interactions and
multi-agent systems further suggest potential application scenarios and future
directions for LLM personification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages for main text, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding In-context Learning of Addition via Activation Subspaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyan Hu, Kayo Yin, Michael I. Jordan, Jacob Steinhardt, Lijie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To perform in-context learning, language models must extract signals from
individual few-shot examples, aggregate these into a learned prediction rule,
and then apply this rule to new examples. How is this implemented in the
forward pass of modern transformer models? To study this, we consider a
structured family of few-shot learning tasks for which the true prediction rule
is to add an integer $k$ to the input. We find that Llama-3-8B attains high
accuracy on this task for a range of $k$, and localize its few-shot ability to
just three attention heads via a novel optimization approach. We further show
the extracted signals lie in a six-dimensional subspace, where four of the
dimensions track the unit digit and the other two dimensions track overall
magnitude. We finally examine how these heads extract information from
individual few-shot examples, identifying a self-correction mechanism in which
mistakes from earlier examples are suppressed by later examples. Our results
demonstrate how tracking low-dimensional subspaces across a forward pass can
provide insight into fine-grained computational structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compensate Quantization Errors+: Quantized Models Are Inquisitive
  Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15508v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15508v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Gao, Jie Ou, Lei Wang, Jun Cheng, Mengchu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quantization of large language models (LLMs) has been a prominent
research area aimed at enabling their lightweight deployment in practice.
Existing research about LLM's quantization has mainly explored the interplay
between weights and activations, or employing auxiliary components while
neglecting the necessity of adjusting weights during quantization.
Consequently, original weight distributions frequently fail to yield desired
results after round-to-nearest (RTN) quantization. Even though incorporating
techniques such as mixed precision and low-rank error approximation in LLM's
quantization can yield improved results, they inevitably introduce additional
computational overhead. On the other hand, traditional techniques for weight
quantization, such as Generative Post-Training Quantization, rely on manually
tweaking weight distributions to minimize local errors, but they fall short of
achieving globally optimal outcomes. Although the recently proposed Learnable
Singular-value Increment improves global weight quantization by modifying
weight distributions, it disrupts the original distribution considerably. This
introduces pronounced bias toward the training data and can degrade downstream
task performance. In this paper, we introduce Singular-value Diagonal
Expansion, a more nuanced approach to refining weight distributions to achieve
better quantization alignment. Furthermore, we introduce Cross-layer Learning
that improves overall quantization outcomes by distributing errors more evenly
across layers. Our plug-and-play weight-quantization methods demonstrate
substantial performance improvements over state-of-the-art approaches,
including OmniQuant, DuQuant, and PrefixQuant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Effecient Quantization Methods for LLMs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Next Token Prediction: Patch-Level Training for Large Language
  Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12665v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12665v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenze Shao, Fandong Meng, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prohibitive training costs of Large Language Models (LLMs) have emerged
as a significant bottleneck in the development of next-generation LLMs. In this
paper, we show that it is possible to significantly reduce the training costs
of LLMs without sacrificing their performance. Specifically, we introduce
patch-level training for LLMs, in which multiple tokens are aggregated into a
unit of higher information density, referred to as a `patch', to serve as the
fundamental text unit for training LLMs. During patch-level training, we feed
the language model shorter sequences of patches and train it to predict the
next patch, thereby processing the majority of the training data at a
significantly reduced cost. Following this, the model continues token-level
training on the remaining training data to align with the inference mode.
Experiments on a diverse range of models (370M-2.7B parameters) demonstrate
that patch-level training can reduce the overall training costs to 0.5$\times$,
without compromising the model performance compared to token-level training.
Source code: https://github.com/shaochenze/PatchTrain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MultiMed: Multilingual Medical Speech Recognition via Attention Encoder
  Decoder <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14074v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14074v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khai Le-Duc, Phuc Phan, Tan-Hanh Pham, Bach Phan Tat, Minh-Huong Ngo, Chris Ngo, Thanh Nguyen-Tang, Truong-Son Hy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual automatic speech recognition (ASR) in the medical domain serves
as a foundational task for various downstream applications such as speech
translation, spoken language understanding, and voice-activated assistants.
This technology improves patient care by enabling efficient communication
across language barriers, alleviating specialized workforce shortages, and
facilitating improved diagnosis and treatment, particularly during pandemics.
In this work, we introduce MultiMed, the first multilingual medical ASR
dataset, along with the first collection of small-to-large end-to-end medical
ASR models, spanning five languages: Vietnamese, English, German, French, and
Mandarin Chinese. To our best knowledge, MultiMed stands as the world's largest
medical ASR dataset across all major benchmarks: total duration, number of
recording conditions, number of accents, and number of speaking roles.
Furthermore, we present the first multilinguality study for medical ASR, which
includes reproducible empirical baselines, a monolinguality-multilinguality
analysis, Attention Encoder Decoder (AED) vs Hybrid comparative study and a
linguistic analysis. We present practical ASR end-to-end training schemes
optimized for a fixed number of trainable parameters that are common in
industry settings. All code, data, and models are available online:
https://github.com/leduckhai/MultiMed/tree/master/MultiMed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025, 38 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Behind Maya: Building a Multilingual Vision Language Model <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nahid Alam, Karthik Reddy Kanjula, Surya Guthikonda, Timothy Chung, Bala Krishna S Vegesna, Abhipsha Das, Anthony Susevski, Ryan Sze-Yin Chan, S M Iftekhar Uddin, Shayekh Bin Islam, Roshan Santhosh, Snegha A, Drishti Sharma, Chen Liu, Isha Chaturvedi, Genta Indra Winata, Ashvanth. S, Snehanshu Mukherjee, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, we have seen a rapid development of large Vision-Language
Models (VLMs). They have shown impressive results on academic benchmarks,
primarily in widely spoken languages but lack performance on low-resource
languages and varied cultural contexts. To address these limitations, we
introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a
multilingual image-text pretraining dataset in eight languages, based on the
LLaVA pretraining dataset; and 2) a multilingual image-text model supporting
these languages, enhancing cultural and linguistic comprehension in
vision-language tasks. Code available at https://github.com/nahidalam/maya.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at VLMs4ALL CVPR 2025 Workshop; corrected workshop name
  spelling</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RM-R1: Reward Modeling as Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.02387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.02387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward modeling is essential for aligning large language models (LLMs) with
human preferences through reinforcement learning (RL). To provide accurate
reward signals, a reward model (RM) should stimulate deep thinking and conduct
interpretable reasoning before assigning a score or a judgment. Inspired by
recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we
hypothesize and validate that integrating reasoning capabilities into reward
modeling significantly enhances RM's interpretability and performance. To this
end, we introduce a new class of generative reward models -- Reasoning Reward
Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We
propose a reasoning-oriented training pipeline and train a family of ReasRMs,
RM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism -- self-generating
sample-level chat rubrics or math/code solutions, and evaluating candidate
responses against them. The training of M-R1 consists of two key stages: (1)
distillation of high-quality reasoning chains and (2) reinforcement learning
with verifiable rewards. Empirically, our models achieve state-of-the-art
performance across three reward model benchmarks on average, outperforming much
larger open-weight models (e.g., INF-ORM-Llama3.1-70B) and proprietary ones
(e.g., GPT-4o) by up to 4.9%. Beyond final performance, we perform thorough
empirical analysis to understand the key ingredients of successful ReasRM
training. To facilitate future research, we release six ReasRM models along
with code and data at https://github.com/RM-R1-UIUC/RM-R1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language Reinforcement Learning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xidong Feng, Bo Liu, Ziyu Wan, Haotian Fu, Girish A. Koushik, Zhiyuan Hu, Mengyue Yang, Ying Wen, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) mathematically formulates decision-making with
Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable
breakthroughs across various domains, including games, robotics, and language
models. This paper seeks a new possibility, Natural Language Reinforcement
Learning (NLRL), by extending traditional MDP to natural language-based
representation space. Specifically, NLRL innovatively redefines RL principles,
including task objectives, policy, value function, Bellman equation, and policy
iteration, into their language counterparts. With recent advancements in large
language models (LLMs), NLRL can be practically implemented to achieve RL-like
policy and value improvement by either pure prompting or gradient-based
training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games
demonstrate the effectiveness, efficiency, and interpretability of the NLRL
framework among diverse use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025 Workshop SSI-FM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concise Reasoning via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Fatemi, Banafsheh Rafiee, Mingjie Tang, Kartik Talamadupula
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in large language models (LLMs), a major
drawback of reasoning models is their enormous token usage, which increases
computational cost, resource requirements, and response time. In this work, we
revisit the core principles of reinforcement learning (RL) and, through
mathematical analysis, demonstrate that the tendency to generate lengthy
responses arises inherently from RL-based optimization during training. This
finding questions the prevailing assumption that longer responses inherently
improve reasoning accuracy. Instead, we uncover a natural correlation between
conciseness and accuracy that has been largely overlooked. We show that
introducing a secondary phase of RL training, using a very small set of
problems, can significantly reduce chains of thought while maintaining or even
enhancing accuracy. Additionally, we demonstrate that, while GRPO shares some
interesting properties of PPO, it suffers from collapse modes, which limit its
reliability for concise reasoning. Finally, we validate our conclusions through
extensive experimental results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Utility Law: Evaluating LLMs beyond Performance through Mechanism
  Interpretable Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.07440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.07440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Cao, Jiahao Ying, Yaoning Wang, Xipeng Qiu, Xuanjing Huang, Yugang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become indispensable across academia,
industry, and daily applications, yet current evaluation methods struggle to
keep pace with their rapid development. One core challenge of evaluation in the
large language model (LLM) era is the generalization issue: how to infer a
model's near-unbounded abilities from inevitably bounded benchmarks. We address
this challenge by proposing Model Utilization Index (MUI), a mechanism
interpretability enhanced metric that complements traditional performance
scores. MUI quantifies the effort a model expends on a task, defined as the
proportion of activated neurons or features during inference. Intuitively, a
truly capable model should achieve higher performance with lower effort.
Extensive experiments across popular LLMs reveal a consistent inverse
logarithmic relationship between MUI and performance, which we formulate as the
Utility Law. From this law we derive four practical corollaries that (i) guide
training diagnostics, (ii) expose data contamination issue, (iii) enable fairer
model comparisons, and (iv) design model-specific dataset diversity. Our code
can be found at https://github.com/ALEX-nlp/MUI-Eva.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Scaling Law for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17785v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17785v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Xiong, Xiansheng Chen, Xin Ye, Hui Chen, Zijia Lin, Haoran Lian, Zhenpeng Su, Wei Huang, Jianwei Niu, Jungong Han, Guiguang Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) have been widely adopted in a wide
range of tasks, leading to increasing attention towards the research on how
scaling LLMs affects their performance. Existing works, termed Scaling Laws,
have discovered that the final test loss of LLMs scales as power-laws with
model size, computational budget, and dataset size. However, the temporal
change of the test loss of an LLM throughout its pre-training process remains
unexplored, though it is valuable in many aspects, such as selecting better
hyperparameters \textit{directly} on the target LLM. In this paper, we propose
the novel concept of Temporal Scaling Law, studying how the test loss of an LLM
evolves as the training steps scale up. In contrast to modeling the test loss
as a whole in a coarse-grained manner, we break it down and dive into the
fine-grained test loss of each token position, and further develop a dynamic
hyperbolic-law. Afterwards, we derive the much more precise temporal scaling
law by studying the temporal patterns of the parameters in the dynamic
hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution
(OOD) validation datasets demonstrate that our temporal scaling law accurately
predicts the test loss of LLMs across training steps. Our temporal scaling law
has broad practical applications. First, it enables direct and efficient
hyperparameter selection on the target LLM, such as data mixture proportions.
Secondly, viewing the LLM pre-training dynamics from the token position
granularity provides some insights to enhance the understanding of LLM
pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, Currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04526v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04526v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqiao Xue, Xiaojing Li, Fan Zhou, Qingyang Dai, Zhixuan Chu, Hongyuan Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce FAMMA, an open-source benchmark for
\underline{f}in\underline{a}ncial \underline{m}ultilingual
\underline{m}ultimodal question \underline{a}nswering (QA). Our benchmark aims
to evaluate the abilities of large language models (LLMs) in answering complex
reasoning questions that require advanced financial knowledge. The benchmark
has two versions: FAMMA-Basic consists of 1,945 questions extracted from
university textbooks and exams, along with human-annotated answers and
rationales; FAMMA-LivePro consists of 103 novel questions created by human
domain experts, with answers and rationales held out from the public for a
contamination-free evaluation. These questions cover advanced knowledge of 8
major subfields in finance (e.g., corporate finance, derivatives, and portfolio
management). Some are in Chinese or French, while a majority of them are in
English. Each question has some non-text data such as charts, diagrams, or
tables. Our experiments reveal that FAMMA poses a significant challenge on
LLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,
we curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,
and fine-tuned a series of open-source Qwen models using this reasoning data.
We found that training a model on these reasoning trajectories can
significantly improve its performance on FAMMA-LivePro. We released our
leaderboard, data, code, and trained models at
https://famma-bench.github.io/famma/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Construction and Application of Materials Knowledge Graph in
  Multidisciplinary Materials Science via Large Language Model <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03080v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03080v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Imran Razzak, Bram Hoex, Haofen Wang, Tong Xie, Wenjie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge in materials science is widely dispersed across extensive
scientific literature, posing significant challenges to the efficient discovery
and integration of new materials. Traditional methods, often reliant on costly
and time-consuming experimental approaches, further complicate rapid
innovation. Addressing these challenges, the integration of artificial
intelligence with materials science has opened avenues for accelerating the
discovery process, though it also demands precise annotation, data extraction,
and traceability of information. To tackle these issues, this article
introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural
language processing techniques integrated with large language models to extract
and systematically organize a decade's worth of high-quality research into
structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes
information into comprehensive labels such as Name, Formula, and Application,
structured around a meticulously designed ontology, thus enhancing data
usability and integration. By implementing network-based algorithms, MKG not
only facilitates efficient link prediction but also significantly reduces
reliance on traditional experimental methods. This structured approach not only
streamlines materials research but also lays the groundwork for more
sophisticated science knowledge graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 38th Conference on Neural Information Processing Systems
  (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChronoFact: Timeline-based Temporal Fact Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anab Maulana Barik, Wynne Hsu, Mong Li Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal claims, often riddled with inaccuracies, are a significant challenge
in the digital misinformation landscape. Fact-checking systems that can
accurately verify such claims are crucial for combating misinformation. Current
systems struggle with the complexities of evaluating the accuracy of these
claims, especially when they include multiple, overlapping, or recurring
events. We introduce a novel timeline-based fact verification framework that
identify events from both claim and evidence and organize them into their
respective chronological timelines. The framework systematically examines the
relationships between the events in both claim and evidence to predict the
veracity of each claim event and their chronological accuracy. This allows us
to accurately determine the overall veracity of the claim. We also introduce a
new dataset of complex temporal claims involving timeline-based reasoning for
the training and evaluation of our proposed framework. Experimental results
demonstrate the effectiveness of our approach in handling the intricacies of
temporal claim verification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ro<span class="highlight-title">BERT</span>a-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Mostafizer Rahman, Ariful Islam Shiplu, Yutaka Watanobe, Md. Ashad Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively analyzing the comments to uncover latent intentions holds immense
value in making strategic decisions across various domains. However, several
challenges hinder the process of sentiment analysis including the lexical
diversity exhibited in comments, the presence of long dependencies within the
text, encountering unknown symbols and words, and dealing with imbalanced
datasets. Moreover, existing sentiment analysis tasks mostly leveraged
sequential models to encode the long dependent texts and it requires longer
execution time as it processes the text sequentially. In contrast, the
Transformer requires less execution time due to its parallel processing nature.
In this work, we introduce a novel hybrid deep learning model, RoBERTa-BiLSTM,
which combines the Robustly Optimized BERT Pretraining Approach (RoBERTa) with
Bidirectional Long Short-Term Memory (BiLSTM) networks. RoBERTa is utilized to
generate meaningful word embedding vectors, while BiLSTM effectively captures
the contextual semantics of long-dependent texts. The RoBERTa-BiLSTM hybrid
model leverages the strengths of both sequential and Transformer models to
enhance performance in sentiment analysis. We conducted experiments using
datasets from IMDb, Twitter US Airline, and Sentiment140 to evaluate the
proposed model against existing state-of-the-art methods. Our experimental
findings demonstrate that the RoBERTa-BiLSTM model surpasses baseline models
(e.g., BERT, RoBERTa-base, RoBERTa-GRU, and RoBERTa-LSTM), achieving accuracies
of 80.74%, 92.36%, and 82.25% on the Twitter US Airline, IMDb, and Sentiment140
datasets, respectively. Additionally, the model achieves F1-scores of 80.73%,
92.35%, and 82.25% on the same datasets, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in
  Low-Data Regimes <span class="chip">NAACL'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01257v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01257v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Waheed, Karima Kadaoui, Bhiksha Raj, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on distilling Whisper's knowledge into small models using
pseudo-labels shows promising performance while reducing the size by up to 50%.
This results in small, efficient, and dedicated models. However, a critical
step of distillation using pseudo-labels involves filtering high-quality
predictions and using only those during training. This step requires ground
truth labels to compare with and filter low-quality examples, making the
process dependent on human labels. Additionally, the distillation process
requires a large amount of data thereby limiting its applicability in
low-resource settings. To address this, we propose a distillation framework
that does not require any labeled data. Through experimentation, we show that
our best-distilled models outperform the teacher model by 5-7 WER points and
are on par with or outperform similar supervised data filtering setups. When
scaling the data, our models significantly outperform all zero-shot and
supervised models. Our models are also 25-50% more compute- and
memory-efficient while maintaining performance equal to or better than that of
the teacher model. For more details about our models, dataset, and other
resources, please visit our GitHub page:
https://github.com/UBC-NLP/uDistilWhisper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL'25 main conference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">114</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-Fixup: Advancing Photo Editing with 3D Priors <span class="chip">SIGGRAPH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alex Schwing, Liangyan Gui, Matheus Gadelha, Paul Guerrero, Nanxuan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advances in modeling image priors via diffusion models,
3D-aware image editing remains challenging, in part because the object is only
specified via a single image. To tackle this challenge, we propose 3D-Fixup, a
new framework for editing 2D images guided by learned 3D priors. The framework
supports difficult editing situations such as object translation and 3D
rotation. To achieve this, we leverage a training-based approach that harnesses
the generative power of diffusion models. As video data naturally encodes
real-world physical dynamics, we turn to video data for generating training
data pairs, i.e., a source and a target frame. Rather than relying solely on a
single trained model to infer transformations between source and target frames,
we incorporate 3D guidance from an Image-to-3D model, which bridges this
challenging task by explicitly projecting 2D information into 3D space. We
design a data generation pipeline to ensure high-quality 3D guidance throughout
training. Results show that by integrating these 3D priors, 3D-Fixup
effectively supports complex, identity coherent 3D-aware edits, achieving
high-quality results and advancing the application of diffusion models in
realistic image manipulation. The code is provided at
https://3dfixup.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH 2025. Project page: https://3dfixup.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth Anything with Any Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Wang, Siyu Chen, Lihe Yang, Jialei Wang, Ziang Zhang, Hengshuang Zhao, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents Prior Depth Anything, a framework that combines incomplete
but precise metric information in depth measurement with relative but complete
geometric structures in depth prediction, generating accurate, dense, and
detailed metric depth maps for any scene. To this end, we design a
coarse-to-fine pipeline to progressively integrate the two complementary depth
sources. First, we introduce pixel-level metric alignment and distance-aware
weighting to pre-fill diverse metric priors by explicitly using depth
prediction. It effectively narrows the domain gap between prior patterns,
enhancing generalization across varying scenarios. Second, we develop a
conditioned monocular depth estimation (MDE) model to refine the inherent noise
of depth priors. By conditioning on the normalized pre-filled prior and
prediction, the model further implicitly merges the two complementary depth
sources. Our model showcases impressive zero-shot generalization across depth
completion, super-resolution, and inpainting over 7 real-world datasets,
matching or even surpassing previous task-specific methods. More importantly,
it performs well on challenging, unseen mixed priors and enables test-time
improvements by switching prediction models, providing a flexible
accuracy-efficiency trade-off while evolving with advancements in MDE models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Home page: https://prior-depth-anything.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Vision Tokenizer Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Fan Zhang, Yufeng Cui, Haiwen Diao, Zhuoyan Luo, Huchuan Lu, Jing Liu, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing vision tokenization isolates the optimization of vision tokenizers
from downstream training, implicitly assuming the visual tokens can generalize
well across various tasks, e.g., image generation and visual question
answering. The vision tokenizer optimized for low-level reconstruction is
agnostic to downstream tasks requiring varied representations and semantics.
This decoupled paradigm introduces a critical misalignment: The loss of the
vision tokenization can be the representation bottleneck for target tasks. For
example, errors in tokenizing text in a given image lead to poor results when
recognizing or generating them. To address this, we propose ETT, an end-to-end
vision tokenizer tuning approach that enables joint optimization between vision
tokenization and target autoregressive tasks. Unlike prior autoregressive
models that use only discrete indices from a frozen vision tokenizer, ETT
leverages the visual embeddings of the tokenizer codebook, and optimizes the
vision tokenizers end-to-end with both reconstruction and caption objectives.
ETT can be seamlessly integrated into existing training pipelines with minimal
architecture modifications. Our ETT is simple to implement and integrate,
without the need to adjust the original codebooks or architectures of the
employed large language models. Extensive experiments demonstrate that our
proposed end-to-end vision tokenizer tuning unlocks significant performance
gains, i.e., 2-6% for multimodal understanding and visual generation tasks
compared to frozen tokenizer baselines, while preserving the original
reconstruction capability. We hope this very simple and strong method can
empower multimodal foundation models besides image generation and
understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal
  Mathematical Reasoning <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language image-caption datasets, widely used for training Large
Multimodal Models, mainly focus on natural scenarios and overlook the intricate
details of mathematical figures that are critical for problem-solving,
hindering the advancement of current LMMs in multimodal mathematical reasoning.
To this end, we propose leveraging code as supervision for cross-modal
alignment, since code inherently encodes all information needed to generate
corresponding figures, establishing a precise connection between the two
modalities. Specifically, we co-develop our image-to-code model and dataset
with model-in-the-loop approach, resulting in an image-to-code model,
FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.
Furthermore, we utilize FigCodifier to synthesize novel mathematical figures
and then construct MM-MathInstruct-3M, a high-quality multimodal math
instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with
ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on
MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a
new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and
Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista,
achieving improvements of 8.9% and 9.2%. The dataset and models will be
released at https://github.com/mathllm/MathCoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Style Customization of Text-to-Vector Generation with Image Diffusion
  Priors <span class="chip">SIGGRAPH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiying Zhang, Nanxuan Zhao, Jing Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scalable Vector Graphics (SVGs) are highly favored by designers due to their
resolution independence and well-organized layer structure. Although existing
text-to-vector (T2V) generation methods can create SVGs from text prompts, they
often overlook an important need in practical applications: style
customization, which is vital for producing a collection of vector graphics
with consistent visual appearance and coherent aesthetics. Extending existing
T2V methods for style customization poses certain challenges.
Optimization-based T2V models can utilize the priors of text-to-image (T2I)
models for customization, but struggle with maintaining structural regularity.
On the other hand, feed-forward T2V models can ensure structural regularity,
yet they encounter difficulties in disentangling content and style due to
limited SVG training data.
  To address these challenges, we propose a novel two-stage style customization
pipeline for SVG generation, making use of the advantages of both feed-forward
T2V models and T2I image priors. In the first stage, we train a T2V diffusion
model with a path-level representation to ensure the structural regularity of
SVGs while preserving diverse expressive capabilities. In the second stage, we
customize the T2V diffusion model to different styles by distilling customized
T2I models. By integrating these techniques, our pipeline can generate
high-quality and diverse SVGs in custom styles based on text prompts in an
efficient feed-forward manner. The effectiveness of our method has been
validated through extensive experiments. The project page is
https://customsvg.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGGRAPH 2025 (Conference Paper). Project page:
  https://customsvg.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Feasibility Matter? Understanding the Impact of Feasibility on
  Synthetic Training Data <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Liu, Jessica Bader, Jae Myung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of photorealistic diffusion models, models trained in
part or fully on synthetic data achieve progressively better results. However,
diffusion models still routinely generate images that would not exist in
reality, such as a dog floating above the ground or with unrealistic texture
artifacts. We define the concept of feasibility as whether attributes in a
synthetic image could realistically exist in the real-world domain; synthetic
images containing attributes that violate this criterion are considered
infeasible. Intuitively, infeasible images are typically considered
out-of-distribution; thus, training on such images is expected to hinder a
model's ability to generalize to real-world data, and they should therefore be
excluded from the training set whenever possible. However, does feasibility
really matter? In this paper, we investigate whether enforcing feasibility is
necessary when generating synthetic training data for CLIP-based classifiers,
focusing on three target attributes: background, color, and texture. We
introduce VariReal, a pipeline that minimally edits a given source image to
include feasible or infeasible attributes given by the textual prompt generated
by a large language model. Our experiments show that feasibility minimally
affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference
in top-1 accuracy across three fine-grained datasets. Also, the attribute
matters on whether the feasible/infeasible images adversarially influence the
classification performance. Finally, mixing feasible and infeasible images in
training datasets does not significantly impact performance compared to using
purely feasible or infeasible datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPRW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Implicit Visual Misunderstandings in Multimodal Large Language
  Models through Attention Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Wang, Guohai Xu, Weinong Wang, Junjie Yang, Jie Lou, Yunhua Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements have enhanced the capability of Multimodal Large Language
Models (MLLMs) to comprehend multi-image information. However, existing
benchmarks primarily evaluate answer correctness, overlooking whether models
genuinely comprehend the visual input. To address this, we define implicit
visual misunderstanding (IVM), where MLLMs provide correct answers without
fully comprehending the visual input. Through our analysis, we decouple the
visual and textual modalities within the causal attention module, revealing
that attention distribution increasingly converges on the image associated with
the correct answer as the network layers deepen. This insight leads to the
introduction of a scale-agnostic metric, \textit{attention accuracy}, and a
novel benchmark for quantifying IVMs. Attention accuracy directly evaluates the
model's visual understanding via internal mechanisms, remaining robust to
positional biases for more reliable assessments. Furthermore, we extend our
approach to finer granularities and demonstrate its effectiveness in unimodal
scenarios, underscoring its versatility and generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Multi-Image Question Answering via Submodular Subset Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaryan Sharma, Shivansh Gupta, Samar Agarwal, Vishak Prasad C., Ganesh Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) have achieved high performance in
vision-language tasks involving single image but they struggle when presented
with a collection of multiple images (Multiple Image Question Answering
scenario). These tasks, which involve reasoning over large number of images,
present issues in scalability (with increasing number of images) and retrieval
performance. In this work, we propose an enhancement for retriever framework
introduced in MIRAGE model using submodular subset selection techniques. Our
method leverages query-aware submodular functions, such as GraphCut, to
pre-select a subset of semantically relevant images before main retrieval
component. We demonstrate that using anchor-based queries and augmenting the
data improves submodular-retriever pipeline effectiveness, particularly in
large haystack sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative
  Decoding of Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mugilan Ganesan, Shane Segal, Ankur Aggarwal, Nish Sinnadurai, Sean Lie, Vithursan Thangarasa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Token Prediction Needs Registers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasios Gerontopoulos, Spyros Gidaris, Nikos Komodakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face
  Morphing Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iurii Medvedev, Nuno Goncalves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition has evolved significantly with the advancement of deep
learning techniques, enabling its widespread adoption in various applications
requiring secure authentication. However, this progress has also increased its
exposure to presentation attacks, including face morphing, which poses a
serious security threat by allowing one identity to impersonate another.
Therefore, modern face recognition systems must be robust against such attacks.
  In this work, we propose a novel approach for training deep networks for face
recognition with enhanced robustness to face morphing attacks. Our method
modifies the classification task by introducing a dual-branch classification
strategy that effectively handles the ambiguity in the labeling of face morphs.
This adaptation allows the model to incorporate morph images into the training
process, improving its ability to distinguish them from bona fide samples.
  Our strategy has been validated on public benchmarks, demonstrating its
effectiveness in enhancing robustness against face morphing attacks.
Furthermore, our approach is universally applicable and can be integrated into
existing face recognition training pipelines to improve classification-based
recognition methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of
  Synthetic Chest Radiographs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raman Dutt, Pedro Sanchez, Yongchen Yao, Steven McDonagh, Sotirios A. Tsaftaris, Timothy Hospedales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CheXGenBench, a rigorous and multifaceted evaluation framework
for synthetic chest radiograph generation that simultaneously assesses
fidelity, privacy risks, and clinical utility across state-of-the-art
text-to-image generative models. Despite rapid advancements in generative AI
for real-world imagery, medical domain evaluations have been hindered by
methodological inconsistencies, outdated architectural comparisons, and
disconnected assessment criteria that rarely address the practical clinical
value of synthetic samples. CheXGenBench overcomes these limitations through
standardised data partitioning and a unified evaluation protocol comprising
over 20 quantitative metrics that systematically analyse generation quality,
potential privacy vulnerabilities, and downstream clinical applicability across
11 leading text-to-image architectures. Our results reveal critical
inefficiencies in the existing evaluation protocols, particularly in assessing
generative fidelity, leading to inconsistent and uninformative comparisons. Our
framework establishes a standardised benchmark for the medical AI community,
enabling objective and reproducible comparisons while facilitating seamless
integration of both existing and future generative models. Additionally, we
release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K
radiographs generated by the top-performing model (Sana 0.6B) in our benchmark
to support further research in this critical domain. Through CheXGenBench, we
establish a new state-of-the-art and release our framework, models, and
SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-contrast laser endoscopy for in vivo gastrointestinal imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taylor L. Bobrow, Mayank Golhar, Suchapa Arayakarnkul, Anthony A. Song, Saowanee Ngamruengphong, Nicholas J. Durr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  White light endoscopy is the clinical gold standard for detecting diseases in
the gastrointestinal tract. Most applications involve identifying visual
abnormalities in tissue color, texture, and shape. Unfortunately, the contrast
of these features is often subtle, causing many clinically relevant cases to go
undetected. To overcome this challenge, we introduce Multi-contrast Laser
Endoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable
spectral, coherent, and directional illumination. We demonstrate three
capabilities of MLE: enhancing tissue chromophore contrast with multispectral
diffuse reflectance, quantifying blood flow using laser speckle contrast
imaging, and characterizing mucosal topography using photometric stereo. We
validate MLE with benchtop models, then demonstrate MLE in vivo during clinical
colonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold
improvement in contrast and a five-fold improvement in color difference
compared to white light and narrow band imaging. With the ability to reveal
multiple complementary types of tissue contrast while seamlessly integrating
into the clinical environment, MLE shows promise as an investigative tool to
improve gastrointestinal imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniEval: Unified Holistic Evaluation for Unified Multimodal
  Understanding and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Li, Haonan Wang, Qixiang Zhang, Boyu Xiao, Chenchang Hu, Hualiang Wang, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of unified multimodal understanding and generation models is
rapidly attracting attention because of their ability to enhance
instruction-following capabilities while minimizing model redundancy. However,
there is a lack of a unified evaluation framework for these models, which would
enable an elegant, simplified, and overall evaluation. Current models conduct
evaluations on multiple task-specific benchmarks, but there are significant
limitations, such as the lack of overall results, errors from extra evaluation
models, reliance on extensive labeled images, benchmarks that lack diversity,
and metrics with limited capacity for instruction-following evaluation. To
tackle these challenges, we introduce UniEval, the first evaluation framework
designed for unified multimodal models without extra models, images, or
annotations. This facilitates a simplified and unified evaluation process. The
UniEval framework contains a holistic benchmark, UniBench (supports both
unified and visual generation models), along with the corresponding UniScore
metric. UniBench includes 81 fine-grained tags contributing to high diversity.
Experimental results indicate that UniBench is more challenging than existing
benchmarks, and UniScore aligns closely with human evaluations, surpassing
current metrics. Moreover, we extensively evaluated SoTA unified and visual
generation models, uncovering new insights into Univeral's unique values.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>UniEval is the first evaluation framework designed for unified
  multimodal models, including a holistic benchmark UniBench and the UniScore
  metric</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logos as a Well-Tempered <span class="highlight-title">Pre-train</span> for Sign Language Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Ovodov, Petr Surovtsev, Karina Kvanchiani, Alexander Kapitanov, Alexander Nagaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper examines two aspects of the isolated sign language recognition
(ISLR) task. First, despite the availability of a number of datasets, the
amount of data for most individual sign languages is limited. It poses the
challenge of cross-language ISLR model training, including transfer learning.
Second, similar signs can have different semantic meanings. It leads to
ambiguity in dataset labeling and raises the question of the best policy for
annotating such signs. To address these issues, this study presents Logos, a
novel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by
the number of signers and one of the largest available datasets while also the
largest RSL dataset in size and vocabulary. It is shown that a model,
pre-trained on the Logos dataset can be used as a universal encoder for other
language SLR tasks, including few-shot learning. We explore cross-language
transfer learning approaches and find that joint training using multiple
classification heads benefits accuracy for the target lowresource datasets the
most. The key feature of the Logos dataset is explicitly annotated visually
similar sign groups. We show that explicitly labeling visually similar signs
improves trained model quality as a visual encoder for downstream tasks. Based
on the proposed contributions, we outperform current state-of-the-art results
for the WLASL dataset and get competitive results for the AUTSL dataset, with a
single stream model processing solely RGB video. The source code, dataset, and
pre-trained models are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistent Quantity-Quality Control across Scenes for Deployment-Aware
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengdi Zhang, Hongkun Cao, Ruqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks
to minimize the number of Gaussians used while preserving high rendering
quality, introducing an inherent trade-off between Gaussian quantity and
rendering quality. Existing methods strive for better quantity-quality
performance, but lack the ability for users to intuitively adjust this
trade-off to suit practical needs such as model deployment under diverse
hardware and communication constraints. Here, we present ControlGS, a 3DGS
optimization method that achieves semantically meaningful and cross-scene
consistent quantity-quality control while maintaining strong quantity-quality
performance. Through a single training run using a fixed setup and a
user-specified hyperparameter reflecting quantity-quality preference, ControlGS
can automatically find desirable quantity-quality trade-off points across
diverse scenes, from compact objects to large outdoor scenes. It also
outperforms baselines by achieving higher rendering quality with fewer
Gaussians, and supports a broad adjustment range with stepless control over the
trade-off.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric
  Lesion Segmentation <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Liang, Lihuan Dai, Xiaoqi Sheng, Xiangguang Chen, Chun Yao, Guihua Tao, Qibin Leng, Honming Cai, Xi Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal medical image segmentation faces significant challenges in the
context of gastric cancer lesion analysis. This clinical context is defined by
the scarcity of independent multimodal datasets and the imperative to
amalgamate inherently misaligned modalities. As a result, algorithms are
constrained to train on approximate data and depend on application migration,
leading to substantial resource expenditure and a potential decline in analysis
accuracy. To address those challenges, we have made two major contributions:
First, we publicly disseminate the GCM 2025 dataset, which serves as the first
large-scale, open-source collection of gastric cancer multimodal MRI scans,
featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500
patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework
that employs an original HWA block with learnable window aggregation layers to
establish dynamic feature correspondences between different modalities'
anatomical structures, and leverages the innovative tri-orientated fusion mamba
mechanism for context modeling and capturing long-range spatial dependencies.
Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021
dataset validate the performance of our framework, demonstrating that the new
approach surpasses existing methods by up to 1.68\% in the Dice score while
maintaining solid robustness. The dataset and code are public via
https://github.com/JeMing-creater/HWA-UNETR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been provisionally accepted for MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEAL: Searching Expandable Architectures for Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Gambella, Vicente Javier Castro Solar, Manuel Roveri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental learning is a machine learning paradigm where a model learns from
a sequential stream of tasks. This setting poses a key challenge: balancing
plasticity (learning new tasks) and stability (preserving past knowledge).
Neural Architecture Search (NAS), a branch of AutoML, automates the design of
the architecture of Deep Neural Networks and has shown success in static
settings. However, existing NAS-based approaches to incremental learning often
rely on expanding the model at every task, making them impractical in
resource-constrained environments. In this work, we introduce SEAL, a NAS-based
framework tailored for data-incremental learning, a scenario where disjoint
data samples arrive sequentially and are not stored for future access. SEAL
adapts the model structure dynamically by expanding it only when necessary,
based on a capacity estimation metric. Stability is preserved through
cross-distillation training after each expansion step. The NAS component
jointly searches for both the architecture and the optimal expansion policy.
Experiments across multiple benchmarks demonstrate that SEAL effectively
reduces forgetting and enhances accuracy while maintaining a lower model size
compared to prior methods. These results highlight the promise of combining NAS
and selective expansion for efficient, adaptive learning in incremental
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision language models have difficulty recognizing virtual objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Tran, Sangeet Khemlani, J. G. Trafton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision language models (VLMs) are AI systems paired with both language and
vision encoders to process multimodal input. They are capable of performing
complex semantic tasks such as automatic captioning, but it remains an open
question about how well they comprehend the visuospatial properties of scenes
depicted in the images they process. We argue that descriptions of virtual
objects -- objects that are not visually represented in an image -- can help
test scene comprehension in these AI systems. For example, an image that
depicts a person standing under a tree can be paired with the following prompt:
imagine that a kite is stuck in the tree. VLMs that comprehend the scene should
update their representations and reason sensibly about the spatial relations
between all three objects. We describe systematic evaluations of
state-of-the-art VLMs and show that their ability to process virtual objects is
inadequate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIF: Anomaly detection via preference embedding <span class="chip">ICPR
  2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Leveni, Luca Magri, Giacomo Boracchi, Cesare Alippi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of detecting anomalies with respect to structured
patterns. To this end, we conceive a novel anomaly detection method called PIF,
that combines the advantages of adaptive isolation methods with the flexibility
of preference embedding. Specifically, we propose to embed the data in a high
dimensional space where an efficient tree-based method, PI-Forest, is employed
to compute an anomaly score. Experiments on synthetic and real datasets
demonstrate that PIF favorably compares with state-of-the-art anomaly detection
techniques, and confirm that PI-Forest is better at measuring arbitrary
distances and isolate points in the preference space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Pattern Recognition (ICPR
  2020)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learned Lightweight Smartphone ISP with Unpaired Data <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Arhire, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Image Signal Processor (ISP) is a fundamental component in modern
smartphone cameras responsible for conversion of RAW sensor image data to RGB
images with a strong focus on perceptual quality. Recent work highlights the
potential of deep learning approaches and their ability to capture details with
a quality increasingly close to that of professional cameras. A difficult and
costly step when developing a learned ISP is the acquisition of pixel-wise
aligned paired data that maps the raw captured by a smartphone camera sensor to
high-quality reference images. In this work, we address this challenge by
proposing a novel training method for a learnable ISP that eliminates the need
for direct correspondences between raw images and ground-truth data with
matching content. Our unpaired approach employs a multi-term loss function
guided by adversarial training with multiple discriminators processing feature
maps from pre-trained networks to maintain content structure while learning
color and texture characteristics from the target RGB dataset. Using
lightweight neural network architectures suitable for mobile devices as
backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm
UltraISP datasets. Compared to paired training methods, our unpaired learning
strategy shows strong potential and achieves high fidelity across multiple
evaluation metrics. The code and pre-trained models are available at
https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPRW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Fidelity Index for Generative Semantic Communications with
  Critical Information Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Huang, Qunsong Zeng, Kaibin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative semantic communication (Gen-SemCom) with large artificial
intelligence (AI) model promises a transformative paradigm for 6G networks,
which reduces communication costs by transmitting low-dimensional prompts
rather than raw data. However, purely prompt-driven generation loses
fine-grained visual details. Additionally, there is a lack of systematic
metrics to evaluate the performance of Gen-SemCom systems. To address these
issues, we develop a hybrid Gen-SemCom system with a critical information
embedding (CIE) framework, where both text prompts and semantically critical
features are extracted for transmissions. First, a novel approach of semantic
filtering is proposed to select and transmit the semantically critical features
of images relevant to semantic label. By integrating the text prompt and
critical features, the receiver reconstructs high-fidelity images using a
diffusion-based generative model. Next, we propose the generative visual
information fidelity (GVIF) metric to evaluate the visual quality of the
generated image. By characterizing the statistical models of image features,
the GVIF metric quantifies the mutual information between the distorted
features and their original counterparts. By maximizing the GVIF metric, we
design a channel-adaptive Gen-SemCom system that adaptively control the volume
of features and compression rate according to the channel state. Experimental
results validate the GVIF metric's sensitivity to visual fidelity, correlating
with both the PSNR and critical information volume. In addition, the optimized
system achieves superior performance over benchmarking schemes in terms of
higher PSNR and lower FID scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikeVideoFormer: An Efficient Spike-Driven Video <span class="highlight-title">Transformer</span> with
  Hamming Attention and $\mathcal{O}(T)$ Complexity <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Zou, Qingfeng Li, Wei Ji, Jingjing Li, Yongkui Yang, Guoqi Li, Chao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) have shown competitive performance to
Artificial Neural Networks (ANNs) in various vision tasks, while offering
superior energy efficiency. However, existing SNN-based Transformers primarily
focus on single-image tasks, emphasizing spatial features while not effectively
leveraging SNNs' efficiency in video-based vision tasks. In this paper, we
introduce SpikeVideoFormer, an efficient spike-driven video Transformer,
featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design
a spike-driven Hamming attention (SDHA) which provides a theoretically guided
adaptation from traditional real-valued attention to spike-driven attention.
Building on SDHA, we further analyze various spike-driven space-time attention
designs and identify an optimal scheme that delivers appealing performance for
video tasks, while maintaining only linear temporal complexity. The
generalization ability and efficiency of our model are demonstrated across
diverse downstream video tasks, including classification, human pose tracking,
and semantic segmentation. Empirical results show our method achieves
state-of-the-art (SOTA) performance compared to existing SNN approaches, with
over 15\% improvement on the latter two tasks. Additionally, it matches the
performance of recent ANN-based methods while offering significant efficiency
gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the
three tasks. https://github.com/JimmyZou/SpikeVideoFormer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified and Scalable Membership Inference Method for Visual
  <span class="highlight-title">Self-supervised</span> Encoder via Part-aware Capability <span class="chip">CCS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhu, Jirong Zha, Ding Li, Leye Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning shows promise in harnessing extensive unlabeled
data, but it also confronts significant privacy concerns, especially in vision.
In this paper, we perform membership inference on visual self-supervised models
in a more realistic setting: self-supervised training method and details are
unknown for an adversary when attacking as he usually faces a black-box system
in practice. In this setting, considering that self-supervised model could be
trained by completely different self-supervised paradigms, e.g., masked image
modeling and contrastive learning, with complex training details, we propose a
unified membership inference method called PartCrop. It is motivated by the
shared part-aware capability among models and stronger part response on the
training data. Specifically, PartCrop crops parts of objects in an image to
query responses within the image in representation space. We conduct extensive
attacks on self-supervised models with different training protocols and
structures using three widely used image datasets. The results verify the
effectiveness and generalization of PartCrop. Moreover, to defend against
PartCrop, we evaluate two common approaches, i.e., early stop and differential
privacy, and propose a tailored method called shrinking crop scale range. The
defense experiments indicate that all of them are effective. Finally, besides
prototype testing on toy visual encoders and small-scale image datasets, we
quantitatively study the impacts of scaling from both data and model aspects in
a realistic scenario and propose a scalable PartCrop-v2 by introducing two
structural improvements to PartCrop. Our code is at
https://github.com/JiePKU/PartCrop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An extension of our ACM CCS2024 conference paper (arXiv:2404.02462).
  We show the impacts of scaling from both data and model aspects on membership
  inference for self-supervised visual encoders</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human
  Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Tuan Ha, Hoang Khang Phan, Thai Minh Tien Ngo, Anh Phan Truong, Nhat Tan Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of Human Activity Recognition (HAR), obtaining high quality and
variance data is still a persistent challenge due to high costs and the
inherent variability of real-world activities. This study introduces a
generation dataset by deep learning approaches (Attention Autoencoder and
conditional Generative Adversarial Networks). Another problem that data
heterogeneity is a critical challenge, one of the solutions is to shuffle the
data to homogenize the distribution. Experimental results demonstrate that the
random sequence strategy significantly improves classification performance,
achieving an accuracy of up to 0.70 $\pm$ 0.03 and a macro F1 score of 0.64
$\pm$ 0.01. For that, disrupting temporal dependencies through random sequence
reordering compels the model to focus on instantaneous recognition, thereby
improving robustness against activity transitions. This approach not only
broadens the effective training dataset but also offers promising avenues for
enhancing HAR systems in complex, real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images
  using ViT Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Balezo, Roger Trullo, Albert Pla Planas, Etienne Decenciere, Thomas Walter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathological analysis is a cornerstone of cancer diagnosis, with
Hematoxylin and Eosin (H&E) staining routinely acquired for every patient to
visualize cell morphology and tissue architecture. On the other hand, multiplex
immunofluorescence (mIF) enables more precise cell type identification via
proteomic markers, but has yet to achieve widespread clinical adoption due to
cost and logistical constraints. To bridge this gap, we introduce MIPHEI
(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired
architecture that integrates state-of-the-art ViT foundation models as encoders
to predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of
markers spanning nuclear content, immune lineages (T cells, B cells, myeloid),
epithelium, stroma, vasculature, and proliferation. We train our model using
the publicly available ORION dataset of restained H&E and mIF images from
colorectal cancer tissue, and validate it on two independent datasets. MIPHEI
achieves accurate cell-type classification from H&E alone, with F1 scores of
0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,
substantially outperforming both a state-of-the-art baseline and a random
classifier for most markers. Our results indicate that our model effectively
captures the complex relationships between nuclear morphologies in their tissue
context, as visible in H&E images and molecular markers defining specific cell
types. MIPHEI offers a promising step toward enabling cell-type-aware analysis
of large-scale H&E datasets, in view of uncovering relationships between
spatial cellular organization and patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StoryReasoning <span class="highlight-title">Dataset</span>: Using Chain-of-Thought for Scene Understanding
  and Grounded Story Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel A. P. Oliveira, David Martins de Matos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual storytelling systems struggle to maintain character identity across
frames and link actions to appropriate subjects, frequently leading to
referential hallucinations. These issues can be addressed through grounding of
characters, objects, and other entities on the visual elements. We propose
StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie
images, with both structured scene analyses and grounded stories. Each story
maintains character and object consistency across frames while explicitly
modeling multi-frame relationships through structured tabular representations.
Our approach features cross-frame object re-identification using visual
similarity and face recognition, chain-of-thought reasoning for explicit
narrative modeling, and a grounding scheme that links textual elements to
visual entities across multiple frames. We establish baseline performance by
fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end
object detection, re-identification, and landmark detection while maintaining
consistent object references throughout the story. Evaluation demonstrates a
reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when
compared to a non-fine-tuned model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Wang, Shuai Xu, Xuelin Zhu, Yicong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object
combinations by leveraging known combinations. Existing studies basically rely
on the cross-modal alignment capabilities of CLIP but tend to overlook its
limitations in capturing fine-grained local features, which arise from its
architectural and training paradigm. To address this issue, we propose a
Multi-Stage Cross-modal Interaction (MSCI) model that effectively explores and
utilizes intermediate-layer information from CLIP's visual encoder.
Specifically, we design two self-adaptive aggregators to extract local
information from low-level visual features and integrate global information
from high-level visual features, respectively. These key information are
progressively incorporated into textual representations through a
stage-by-stage interaction mechanism, significantly enhancing the model's
perception capability for fine-grained local visual information. Additionally,
MSCI dynamically adjusts the attention weights between global and local visual
information based on different combinations, as well as different elements
within the same combination, allowing it to flexibly adapt to diverse
scenarios. Experiments on three widely used datasets fully validate the
effectiveness and superiority of the proposed model. Data and code are
available at https://github.com/ltpwy/MSCI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global
  Marine Fog Detection and Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengqiu Xu, Kaixin Chen, Heng Guo, Yixiang Huang, Ming Wu, Zhenwei Shi, Chuang Zhang, Jun Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning approaches for marine fog detection and forecasting have
outperformed traditional methods, demonstrating significant scientific and
practical importance. However, the limited availability of open-source datasets
remains a major challenge. Existing datasets, often focused on a single region
or satellite, restrict the ability to evaluate model performance across diverse
conditions and hinder the exploration of intrinsic marine fog characteristics.
To address these limitations, we introduce \textbf{MFogHub}, the first
multi-regional and multi-satellite dataset to integrate annotated marine fog
observations from 15 coastal fog-prone regions and six geostationary
satellites, comprising over 68,000 high-resolution samples. By encompassing
diverse regions and satellite perspectives, MFogHub facilitates rigorous
evaluation of both detection and forecasting methods under varying conditions.
Extensive experiments with 16 baseline models demonstrate that MFogHub can
reveal generalization fluctuations due to regional and satellite discrepancy,
while also serving as a valuable resource for the development of targeted and
scalable fog prediction techniques. Through MFogHub, we aim to advance both the
practical monitoring and scientific understanding of marine fog dynamics on a
global scale. The dataset and code are at
\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall
  Probabilities Over 8 Hours 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Pablos Sarabia, Joachim Nyborg, Morten Birk, Jeppe Liborius Sjørup, Anders Lillevang Vesterholt, Ira Assent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a deep learning model for high-resolution probabilistic
precipitation forecasting over an 8-hour horizon in Europe, overcoming the
limitations of radar-only deep learning models with short forecast lead times.
Our model efficiently integrates multiple data sources - including radar,
satellite, and physics-based numerical weather prediction (NWP) - while
capturing long-range interactions, resulting in accurate forecasts with robust
uncertainty quantification through consistent probabilistic maps. Featuring a
compact architecture, it enables more efficient training and faster inference
than existing models. Extensive experiments demonstrate that our model
surpasses current operational NWP systems, extrapolation-based methods, and
deep-learning nowcasting models, setting a new standard for high-resolution
precipitation forecasting in Europe, ensuring a balance between accuracy,
interpretability, and computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HandReader: Advanced Techniques for Efficient Fingerspelling Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Korotaev, Petr Surovtsev, Alexander Kapitanov, Karina Kvanchiani, Aleksandr Nagaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fingerspelling is a significant component of Sign Language (SL), allowing the
interpretation of proper names, characterized by fast hand movements during
signing. Although previous works on fingerspelling recognition have focused on
processing the temporal dimension of videos, there remains room for improving
the accuracy of these approaches. This paper introduces HandReader, a group of
three architectures designed to address the fingerspelling recognition task.
HandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to
process RGB features from videos of varying lengths while preserving important
sequential information. HandReader$_{KP}$ is built on the proposed Temporal
Pose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition
in a batch allows the encoder to pass them through 2D and 3D convolution
layers, utilizing temporal and spatial information and accumulating keypoints
coordinates. We also introduce HandReader_RGB+KP - architecture with a joint
encoder to benefit from RGB and keypoint modalities. Each HandReader model
possesses distinct advantages and achieves state-of-the-art results on the
ChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate
high performance on the first open dataset for Russian fingerspelling, Znaki,
presented in this paper. The Znaki dataset and HandReader pre-trained models
are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/ai-forever/handreader</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inferring Driving Maps by Deep Learning-based Trail Map Extraction <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hubbertz, Pascal Colling, Qi Han, Tobias Meisen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-definition (HD) maps offer extensive and accurate environmental
information about the driving scene, making them a crucial and essential
element for planning within autonomous driving systems. To avoid extensive
efforts from manual labeling, methods for automating the map creation have
emerged. Recent trends have moved from offline mapping to online mapping,
ensuring availability and actuality of the utilized maps. While the performance
has increased in recent years, online mapping still faces challenges regarding
temporal consistency, sensor occlusion, runtime, and generalization. We propose
a novel offline mapping approach that integrates trails - informal routes used
by drivers - into the map creation process. Our method aggregates trail data
from the ego vehicle and other traffic participants to construct a
comprehensive global map using transformer-based deep learning models. Unlike
traditional offline mapping, our approach enables continuous updates while
remaining sensor-agnostic, facilitating efficient data transfer. Our method
demonstrates superior performance compared to state-of-the-art online mapping
approaches, achieving improved generalization to previously unseen environments
and sensor configurations. We validate our approach on two benchmark datasets,
highlighting its robustness and applicability in autonomous driving systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted at the CVPR WAD 2025 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Lu, Jiaqi Tang, Jiyao Wang, Yunfan LU, Xu Cao, Qingyong Hu, Yin Wang, Yuting Zhang, Tianxin Xie, Yunpeng Zhang, Yong Chen, Jiayu. Gao, Bin Huang, Dengbo He, Shuiguang Deng, Hao Chen, Ying-Cong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intelligent driving cockpit, an important part of intelligent driving,
needs to match different users' comfort, interaction, and safety needs. This
paper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.
Sage Deer achieves three highlights: (1) Super alignment: It achieves different
reactions according to different people's preferences and biases. (2)
Generalist: It can understand the multi-view and multi-mode inputs to reason
the user's physiological indicators, facial emotions, hand movements, body
movements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It
can elicit implicit thought chains in the language space to further increase
generalist and super-aligned abilities. Besides, we collected multiple data
sets and built a large-scale benchmark. This benchmark measures the deer's
perceptual decision-making ability and the super alignment's accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct
  Preference Optimization <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Shen, Wanqi Yin, Xiaofeng Yang, Cheng Chen, Chaoyue Song, Zhongang Cai, Lei Yang, Hao Wang, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mesh recovery (HMR) from a single image is inherently ill-posed due to
depth ambiguity and occlusions. Probabilistic methods have tried to solve this
by generating numerous plausible 3D human mesh predictions, but they often
exhibit misalignment with 2D image observations and weak robustness to
in-the-wild images. To address these issues, we propose ADHMR, a framework that
Aligns a Diffusion-based HMR model in a preference optimization manner. First,
we train a human mesh prediction assessment model, HMR-Scorer, capable of
evaluating predictions even for in-the-wild images without 3D annotations. We
then use HMR-Scorer to create a preference dataset, where each input image has
a pair of winner and loser mesh predictions. This dataset is used to finetune
the base model using direct preference optimization. Moreover, HMR-Scorer also
helps improve existing HMR models by data cleaning, even with fewer training
samples. Extensive experiments show that ADHMR outperforms current
state-of-the-art methods. Code is available at:
https://github.com/shenwenhao01/ADHMR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025. Code: https://github.com/shenwenhao01/ADHMR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbo Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are provided in the supplementary material and at this anonymous GitHub link:
https://anonymous.4open.science/r/MTVCrafter-1B13.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Interplay of Human-AI Alignment,Fairness, and Performance
  Trade-offs in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Luo, Ziyu Zhou, Zixin Shu, Aurélie Pahud de Mortanges, Robert Berke, Mauricio Reyes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks excel in medical imaging but remain prone to biases,
leading to fairness gaps across demographic groups. We provide the first
systematic exploration of Human-AI alignment and fairness in this domain. Our
results show that incorporating human insights consistently reduces fairness
gaps and enhances out-of-domain generalization, though excessive alignment can
introduce performance trade-offs, emphasizing the need for calibrated
strategies. These findings highlight Human-AI alignment as a promising approach
for developing fair, robust, and generalizable medical AI systems, striking a
balance between expert guidance and automated efficiency. Our code is available
at https://github.com/Roypic/Aligner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution
  Generalisation in MRI Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puru Vaish, Felix Meister, Tobias Heimann, Christoph Brune, Jelmer M. Wolterink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation models are often trained on curated datasets,
leading to performance degradation when deployed in real-world clinical
settings due to mismatches between training and test distributions. While data
augmentation techniques are widely used to address these challenges,
traditional visually consistent augmentation strategies lack the robustness
needed for diverse real-world scenarios. In this work, we systematically
evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary
Fourier Augmentation. These methods mitigate the effects of multiple variations
without explicitly targeting specific sources of distribution shifts. We
demonstrate how these techniques significantly improve out-of-distribution
generalization and robustness to imaging variations across a wide range of
transformations in cardiac cine MRI and prostate MRI segmentation. We
quantitatively find that these augmentation methods enhance learned feature
representations by promoting separability and compactness. Additionally, we
highlight how their integration into nnU-Net training pipelines provides an
easy-to-implement, effective solution for enhancing the reliability of medical
segmentation models in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MIDL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umair Haroon, Ahmad AlMughrabi, Thanasis Zoumpekas, Ricardo Marques, Petia Radeva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate food volume estimation is crucial for medical nutrition management
and health monitoring applications, but current food volume estimation methods
are often limited by mononuclear data, leveraging single-purpose hardware such
as 3D scanners, gathering sensor-oriented information such as depth
information, or relying on camera calibration using a reference object. In this
paper, we present VolE, a novel framework that leverages mobile device-driven
3D reconstruction to estimate food volume. VolE captures images and camera
locations in free motion to generate precise 3D models, thanks to AR-capable
mobile devices. To achieve real-world measurement, VolE is a reference- and
depth-free framework that leverages food video segmentation for food mask
generation. We also introduce a new food dataset encompassing the challenging
scenarios absent in the previous benchmarks. Our experiments demonstrate that
VolE outperforms the existing volume estimation techniques across multiple
datasets by achieving 2.22 % MAPE, highlighting its superior performance in
food volume estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Saliency <span class="highlight-title">Dataset</span> Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Kümmerer, Harneet Khanuja, Matthias Bethge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in image-based saliency prediction are approaching gold
standard performance levels on existing benchmarks. Despite this success, we
show that predicting fixations across multiple saliency datasets remains
challenging due to dataset bias. We find a significant performance drop (around
40%) when models trained on one dataset are applied to another. Surprisingly,
increasing dataset diversity does not resolve this inter-dataset gap, with
close to 60% attributed to dataset-specific biases. To address this remaining
generalization gap, we propose a novel architecture extending a mostly
dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific
parameters that govern interpretable mechanisms such as multi-scale structure,
center bias, and fixation spread. Adapting only these parameters to new data
accounts for more than 75% of the generalization gap, with a large fraction of
the improvement achieved with as few as 50 samples. Our model sets a new
state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark
(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from
unrelated datasets, but with a substantial boost when adapting to the
respective training datasets. The model also provides valuable insights into
spatial saliency properties, revealing complex multi-scale effects that combine
both absolute and relative sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Source Collaborative Style Augmentation and Domain-Invariant
  Learning for Federated Domain Generalization <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikang Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated domain generalization aims to learn a generalizable model from
multiple decentralized source domains for deploying on the unseen target
domain. The style augmentation methods have achieved great progress on domain
generalization. However, the existing style augmentation methods either explore
the data styles within isolated source domain or interpolate the style
information across existing source domains under the data decentralization
scenario, which leads to limited style space. To address this issue, we propose
a Multi-source Collaborative Style Augmentation and Domain-invariant learning
method (MCSAD) for federated domain generalization. Specifically, we propose a
multi-source collaborative style augmentation module to generate data in the
broader style space. Furthermore, we conduct domain-invariant learning between
the original data and augmented data by cross-domain feature alignment within
the same class and classes relation ensemble distillation between different
classes to learn a domain-invariant model. By alternatively conducting
collaborative style augmentation and domain-invariant learning, the model can
generalize well on unseen target domain. Extensive experiments on multiple
domain generalization datasets indicate that our method significantly
outperforms the state-of-the-art federated domain generalization methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuechang Tu, Lukas Radl, Michael Steiner, Markus Steinberger, Bernhard Kerbl, Fernando de la Torre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has rapidly become a leading technique for
novel-view synthesis, providing exceptional performance through efficient
software-based GPU rasterization. Its versatility enables real-time
applications, including on mobile and lower-powered devices. However, 3DGS
faces key challenges in virtual reality (VR): (1) temporal artifacts, such as
popping during head movements, (2) projection-based distortions that result in
disturbing and view-inconsistent floaters, and (3) reduced framerates when
rendering large numbers of Gaussians, falling below the critical threshold for
VR. Compared to desktop environments, these issues are drastically amplified by
large field-of-view, constant head movements, and high resolution of
head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine
and extend several recent advancements in 3DGS to address challenges of VR
holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal
Projection can complement each other, by modifying the individual techniques
and core 3DGS rasterizer. Additionally, we propose an efficient foveated
rasterizer that handles focus and peripheral areas in a single GPU launch,
avoiding redundant computations and improving GPU utilization. Our method also
incorporates a fine-tuning step that optimizes Gaussian parameters based on
StopThePop depth evaluations and Optimal Projection. We validate our method
through a controlled user study with 25 participants, showing a strong
preference for VRSplat over other configurations of Mini-Splatting. VRSplat is
the first, systematically evaluated 3DGS approach capable of supporting modern
VR applications, achieving 72+ FPS while eliminating popping and
stereo-disrupting floaters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I3D'25 (PACMCGIT); Project Page: https://cekavis.site/VRSplat/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IMITATE: Image Registration with Context for unknown time frame recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziad Kheil, Lucas Robinet, Laurent Risser, Soleakhena Ken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we formulate a novel image registration formalism dedicated to
the estimation of unknown condition-related images, based on two or more known
images and their associated conditions. We show how to practically model this
formalism by using a new conditional U-Net architecture, which fully takes into
account the conditional information and does not need any fixed image. Our
formalism is then applied to image moving tumors for radiotherapy treatment at
different breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal
regions. This driving application is particularly complex as it requires to
stitch a collection of sequential 2D slices into several 3D volumes at
different organ positions. Movement interpolation with standard methods then
generates well known reconstruction artefacts in the assembled volumes due to
irregular patient breathing, hysteresis and poor correlation of breathing
signal to internal motion. Results obtained on 4D-CT clinical data showcase
artefact-free volumes achieved through real-time latencies. The code is
publicly available at https://github.com/Kheil-Z/IMITATE .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE ISBI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via
  Multi-Objective Balanced Covering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangfu Li, Hongjian Zhan, Tianyi Chen, Qi Liu, Yue Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing visual token pruning methods target prompt alignment and visual
preservation with static strategies, overlooking the varying relative
importance of these objectives across tasks, which leads to inconsistent
performance. To address this, we derive the first closed-form error bound for
visual token pruning based on the Hausdorff distance, uniformly characterizing
the contributions of both objectives. Moreover, leveraging $\epsilon$-covering
theory, we reveal an intrinsic trade-off between these objectives and quantify
their optimal attainment levels under a fixed budget. To practically handle
this trade-off, we propose Multi-Objective Balanced Covering (MoB), which
reformulates visual token pruning as a bi-objective covering problem. In this
framework, the attainment trade-off reduces to budget allocation via greedy
radius trading. MoB offers a provable performance bound and linear scalability
with respect to the number of input visual tokens, enabling adaptation to
challenging pruning scenarios. Extensive experiments show that MoB preserves
96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual
tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible
performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm
that MoB integrates seamlessly into advanced MLLMs and diverse vision-language
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages,9 figures,conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMRL++: Parameter-Efficient and Interaction-Aware Representation
  Learning for Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuncheng Guo, Xiaodong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pre-trained Vision-Language Models (VLMs) have significantly
advanced transfer learning across diverse tasks. However, adapting these models
with limited few-shot data often leads to overfitting, undermining their
ability to generalize to new tasks. To address this, we propose Multi-Modal
Representation Learning (MMRL), which introduces a shared, learnable,
modality-agnostic representation space. MMRL generates space tokens projected
into both text and image encoders as representation tokens, enabling more
effective cross-modal interactions. Unlike prior methods that mainly optimize
class token features, MMRL inserts representation tokens into higher encoder
layers--where task-specific features are more prominent--while preserving
general knowledge in the lower layers. During training, both class and
representation features are jointly optimized: a trainable projection layer is
applied to representation tokens for task adaptation, while the projection
layer for class token remains frozen to retain pre-trained knowledge. To
further promote generalization, we introduce a regularization term aligning
class and text features with the frozen VLM's zero-shot features. At inference,
a decoupling strategy uses both class and representation features for base
tasks, but only class features for novel tasks due to their stronger
generalization. Building upon this, we propose MMRL++, a parameter-efficient
and interaction-aware extension that significantly reduces trainable parameters
and enhances intra-modal interactions--particularly across the layers of
representation tokens--allowing gradient sharing and instance-specific
information to propagate more effectively through the network. Extensive
experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently
outperform state-of-the-art methods, achieving a strong balance between
task-specific adaptation and generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Due to the limitation "The abstract field cannot be longer than 1,920
  characters", the abstract appearing here is slightly shorter than that in the
  PDF file</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowDreamer: A RGB-D World Model with Flow-based Motion Representations
  for Robot Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Guo, Xiaojian Ma, Yikai Wang, Min Yang, Huaping Liu, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates training better visual world models for robot
manipulation, i.e., models that can predict future visual observations by
conditioning on past frames and robot actions. Specifically, we consider world
models that operate on RGB-D frames (RGB-D world models). As opposed to
canonical approaches that handle dynamics prediction mostly implicitly and
reconcile it with visual rendering in a single model, we introduce FlowDreamer,
which adopts 3D scene flow as explicit motion representations. FlowDreamer
first predicts 3D scene flow from past frame and action conditions with a
U-Net, and then a diffusion model will predict the future frame utilizing the
scene flow. FlowDreamer is trained end-to-end despite its modularized nature.
We conduct experiments on 4 different benchmarks, covering both video
prediction and visual planning tasks. The results demonstrate that FlowDreamer
achieves better performance compared to other baseline RGB-D world models by 7%
on semantic similarity, 11% on pixel quality, and 6% on success rate in various
robot manipulation domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: see https://sharinka0715.github.io/FlowDreamer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head
  Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui-Yang Ju, Sheng-Yen Huang, Yi-Ping Hung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of 3D Gaussian blendshapes has enabled the real-time
reconstruction of animatable head avatars from monocular video. Toonify, a
StyleGAN-based framework, has become widely used for facial image stylization.
To extend Toonify for synthesizing diverse stylized 3D head avatars using
Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.
In Stage 1 (stylized video generation), we employ an improved StyleGAN to
generate the stylized video from the input video frames, which addresses the
limitation of cropping aligned faces at a fixed resolution as preprocessing for
normal StyleGAN. This process provides a more stable video, which enables
Gaussian blendshapes to better capture the high-frequency details of the video
frames, and efficiently generate high-quality animation in the next stage. In
Stage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head
model and a set of expression blendshapes from the generated video. By
combining the neutral head model with expression blendshapes, ToonifyGB can
efficiently render stylized avatars with arbitrary expressions. We validate the
effectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane
and Pixar.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PsOCR: Benchmarking Large Multimodal Models for Optical Character
  Recognition in Low-resource Pashto Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ijazul Haq, Yingjie Zhang, Irfan Ali Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper evaluates the performance of Large Multimodal Models (LMMs) on
Optical Character Recognition (OCR) in the low-resource Pashto language.
Natural Language Processing (NLP) in Pashto faces several challenges due to the
cursive nature of its script and a scarcity of structured datasets. To address
this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one
million images annotated with bounding boxes at word, line, and document
levels, suitable for training and evaluating models based on different
architectures, including Convolutional Neural Networks (CNNs) and Transformers.
PsOCR covers variations across 1,000 unique font families, colors, image sizes,
and layouts. A benchmark subset of 10K images was selected to evaluate the
performance of several LMMs, including seven open-source models: DeepSeek's
Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four
closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results
demonstrate that Gemini achieves the best performance among all models, whereas
among open-source models, Qwen-7B stands out. This work provides an insightful
assessment of the capabilities and limitations of current LMMs for OCR tasks in
Pashto and establishes a foundation for further research not only in Pashto OCR
but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is
available at https://github.com/zirak-ai/PashtoOCR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advances in Radiance Field for Dynamic Scene: From Neural Field to
  Gaussian Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong Fan, Xuepu Zeng, Jing Zhang, Mingming Gong, Yuxiang Yang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic scene representation and reconstruction have undergone transformative
advances in recent years, catalyzed by breakthroughs in neural radiance fields
and 3D Gaussian splatting techniques. While initially developed for static
environments, these methodologies have rapidly evolved to address the
complexities inherent in 4D dynamic scenes through an expansive body of
research. Coupled with innovations in differentiable volumetric rendering,
these approaches have significantly enhanced the quality of motion
representation and dynamic scene reconstruction, thereby garnering substantial
attention from the computer vision and graphics communities. This survey
presents a systematic analysis of over 200 papers focused on dynamic scene
representation using radiance field, spanning the spectrum from implicit neural
representations to explicit Gaussian primitives. We categorize and evaluate
these works through multiple critical lenses: motion representation paradigms,
reconstruction techniques for varied scene dynamics, auxiliary information
integration strategies, and regularization approaches that ensure temporal
consistency and physical plausibility. We organize diverse methodological
approaches under a unified representational framework, concluding with a
critical examination of persistent challenges and promising research
directions. By providing this comprehensive overview, we aim to establish a
definitive reference for researchers entering this rapidly evolving field while
offering experienced practitioners a systematic understanding of both
conceptual principles and practical frontiers in dynamic scene reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Deep Fusion of Large Language Models and Diffusion
  <span class="highlight-title">Transformer</span>s for Text-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingda Tang, Boyang Zheng, Xichen Pan, Sayak Paul, Saining Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper does not describe a new method; instead, it provides a thorough
exploration of an important yet understudied design space related to recent
advances in text-to-image synthesis -- specifically, the deep fusion of large
language models (LLMs) and diffusion transformers (DiTs) for multi-modal
generation. Previous studies mainly focused on overall system performance
rather than detailed comparisons with alternative methods, and key design
details and training recipes were often left undisclosed. These gaps create
uncertainty about the real potential of this approach. To fill these gaps, we
conduct an empirical study on text-to-image generation, performing controlled
comparisons with established baselines, analyzing important design choices, and
providing a clear, reproducible recipe for training at scale. We hope this work
offers meaningful data points and practical guidelines for future research in
multi-modal generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection
  of Diseases in Cocos nucifera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miit Daga, Dhriti Parikh, Swarna Priya Ramu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coconut tree diseases are a serious risk to agricultural yield, particularly
in developing countries where conventional farming practices restrict early
diagnosis and intervention. Current disease identification methods are manual,
labor-intensive, and non-scalable. In response to these limitations, we come up
with DeepSeqCoco, a deep learning based model for accurate and automatic
disease identification from coconut tree images. The model was tested under
various optimizer settings, such as SGD, Adam, and hybrid configurations, to
identify the optimal balance between accuracy, minimization of loss, and
computational cost. Results from experiments indicate that DeepSeqCoco can
achieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than
existing models) with the hybrid SGD-Adam showing the lowest validation loss of
2.81%. It also shows a drop of up to 18% in training time and up to 85% in
prediction time for input images. The results point out the promise of the
model to improve precision agriculture through an AI-based, scalable, and
efficient disease monitoring system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted for publication in IEEE Access journal and is
  currently pending revisions before publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model
  Super-Resolution Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of remote sensing technology, super-resolution
image reconstruction is of great research and practical significance. Existing
deep learning methods have made progress but still face limitations in handling
complex scenes and preserving image details. This paper proposes a
reinforcement learning-based latent diffusion model (LDM) fine-tuning method
for remote sensing image super-resolution. The method constructs a
reinforcement learning environment with states, actions, and rewards,
optimizing decision objectives through proximal policy optimization (PPO)
during the reverse denoising process of the LDM model. Experiments on the
RESISC45 dataset show significant improvements over the baseline model in PSNR,
SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,
and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural
scenes. The results demonstrate the method's effectiveness in enhancing
super-resolution quality and adaptability across scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 4th International Conference on Computing Innovation
  and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community
  Research Series-CORE or Theoretical and Natural Science (TNS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application of YOLOv8 in monocular downward multiple Car Target
  detection <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving technology is progressively transforming traditional car
driving methods, marking a significant milestone in modern transportation.
Object detection serves as a cornerstone of autonomous systems, playing a vital
role in enhancing driving safety, enabling autonomous functionality, improving
traffic efficiency, and facilitating effective emergency responses. However,
current technologies such as radar for environmental perception, cameras for
road perception, and vehicle sensor networks face notable challenges, including
high costs, vulnerability to weather and lighting conditions, and limited
resolution.To address these limitations, this paper presents an improved
autonomous target detection network based on YOLOv8. By integrating structural
reparameterization technology, a bidirectional pyramid structure network model,
and a novel detection pipeline into the YOLOv8 framework, the proposed approach
achieves highly efficient and precise detection of multi-scale, small, and
remote objects. Experimental results demonstrate that the enhanced model can
effectively detect both large and small objects with a detection accuracy of
65%, showcasing significant advancements over traditional methods.This improved
model holds substantial potential for real-world applications and is
well-suited for autonomous driving competitions, such as the Formula Student
Autonomous China (FSAC), particularly excelling in scenarios involving
single-target and small-object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 5th International Conference on Signal Processing and
  Machine Learning (CONF-SPML 2025), to appear in Applied and Computational
  Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive
  3D Sketching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Zang, Yuanqi Hu, Xinyu Chen, Yuxia Xu, Suhui Wang, Chunan Yu, Lanyun Zhu, Deyi Ji, Xin Xu, Tianrun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of immersive consumer electronics, such as AR/VR headsets and
smart devices, people increasingly seek ways to express their identity through
virtual fashion. However, existing 3D garment design tools remain inaccessible
to everyday users due to steep technical barriers and limited data. In this
work, we introduce a 3D sketch-driven 3D garment generation framework that
empowers ordinary users - even those without design experience - to create
high-quality digital clothing through simple 3D sketches in AR/VR environments.
By combining a conditional diffusion model, a sketch encoder trained in a
shared latent space, and an adaptive curriculum learning strategy, our system
interprets imprecise, free-hand input and produces realistic, personalized
garments. To address the scarcity of training data, we also introduce
KO3DClothes, a new dataset of paired 3D garments and user-created sketches.
Extensive experiments and user studies confirm that our method significantly
outperforms existing baselines in both fidelity and usability, demonstrating
its promise for democratized fashion design on next-generation consumer
platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Descriptive Image-Text Matching with Graded Contextual Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhyun Jang, Jiyeong Lee, Kwanghoon Sohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-text matching aims to build correspondences between visual and textual
data by learning their pairwise similarities. Most existing approaches have
adopted sparse binary supervision, indicating whether a pair of images and
sentences matches or not. However, such sparse supervision covers a limited
subset of image-text relationships, neglecting their inherent many-to-many
correspondences; an image can be described in numerous texts at different
descriptive levels. Moreover, existing approaches overlook the implicit
connections from general to specific descriptions, which form the underlying
rationale for the many-to-many relationships between vision and language. In
this work, we propose descriptive image-text matching, called DITM, to learn
the graded contextual similarity between image and text by exploring the
descriptive flexibility of language. We formulate the descriptiveness score of
each sentence with cumulative term frequency-inverse document frequency
(TF-IDF) to balance the pairwise similarity according to the keywords in the
sentence. Our method leverages sentence descriptiveness to learn robust
image-text matching in two key ways: (1) to refine the false negative labeling,
dynamically relaxing the connectivity between positive and negative pairs, and
(2) to build more precise matching, aligning a set of relevant sentences in a
generic-to-specific order. By moving beyond rigid binary supervision, DITM
enhances the discovery of both optimal matches and potential positive pairs.
Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the
effectiveness of our method in representing complex image-text relationships
compared to state-of-the-art approaches. In addition, DITM enhances the
hierarchical reasoning ability of the model, supported by the extensive
analysis on HierarCaps benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PointArena: Probing Multimodal Grounding Through Language-Guided
  Pointing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Cheng, Jiafei Duan, Yi Ru Wang, Haoquan Fang, Boyang Li, Yushan Huang, Elvis Wang, Ainaz Eftekhar, Jason Lee, Wentao Yuan, Rose Hendrix, Noah A. Smith, Fei Xia, Dieter Fox, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pointing serves as a fundamental and intuitive mechanism for grounding
language within visual contexts, with applications spanning robotics, assistive
technologies, and interactive AI systems. While recent multimodal models have
started to support pointing capabilities, existing benchmarks typically focus
only on referential object localization tasks. We introduce PointArena, a
comprehensive platform for evaluating multimodal pointing across diverse
reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a
curated dataset containing approximately 1,000 pointing tasks across five
reasoning categories; (2) Point-Battle, an interactive, web-based arena
facilitating blind, pairwise model comparisons, which has already gathered over
4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation
system allowing users to directly evaluate multimodal model pointing
capabilities in practical settings. We conducted extensive evaluations of both
state-of-the-art open-source and proprietary multimodal models. Results
indicate that Molmo-72B consistently outperforms other models, though
proprietary models increasingly demonstrate comparable performance.
Additionally, we find that supervised training specifically targeting pointing
tasks significantly enhances model performance. Across our multi-stage
evaluation pipeline, we also observe strong correlations, underscoring the
critical role of precise pointing capabilities in enabling multimodal models to
effectively bridge abstract reasoning with concrete, real-world actions.
Project page: https://pointarena.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages, Dataset and code:https://pointarena.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High Quality Underwater Image Compression with Adaptive Correction and
  Codebook-based Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimin Zhou, Yichong Xia, Sicheng Pan, Bin Chen, Baoyi An, Haoqian Wang, Zhi Wang, Yaowei Wang, Zikun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing exploration and exploitation of the underwater world,
underwater images have become a critical medium for human interaction with
marine environments, driving extensive research into their efficient
transmission and storage. However, contemporary underwater image compression
algorithms fail to fully leverage the unique characteristics distinguishing
underwater scenes from terrestrial images, resulting in suboptimal performance.
To address this limitation, we introduce HQUIC, designed to exploit
underwater-image-specific features for enhanced compression efficiency. HQUIC
employs an ALTC module to adaptively predict the attenuation coefficients and
global light information of the images, which effectively mitigates the issues
caused by the differences in lighting and tone existing in underwater images.
Subsequently, HQUIC employs a codebook as an auxiliary branch to extract the
common objects within underwater images and enhances the performance of the
main branch. Furthermore, HQUIC dynamically weights multi-scale frequency
components, prioritizing information critical for distortion quality while
discarding redundant details. Extensive evaluations on diverse underwater
datasets demonstrate that HQUIC outperforms state-of-the-art compression
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Yu, Bin Huang, Minghui Zhang, Weiwen Wu, Shaoyu Wang, Qiegen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score-based diffusion models have shown significant promise in the field of
sparse-view CT reconstruction. However, the projection dataset is large and
riddled with redundancy. Consequently, applying the diffusion model to
unprocessed data results in lower learning effectiveness and higher learning
difficulty, frequently leading to reconstructed images that lack fine details.
To address these issues, we propose the ordered-subsets multi-diffusion model
(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT
projection data into equal subsets and employs multi-subsets diffusion model
(MSDM) to learn from each subset independently. This targeted learning approach
reduces complexity and enhances the reconstruction of fine details.
Furthermore, the integration of one-whole diffusion model (OWDM) with complete
sinogram data acts as a global information constraint, which can reduce the
possibility of generating erroneous or inconsistent sinogram information.
Moreover, the OSMM's unsupervised learning framework provides strong robustness
and generalizability, adapting seamlessly to varying sparsity levels of CT
sinograms. This ensures consistent and reliable performance across different
clinical scenarios. Experimental results demonstrate that OSMM outperforms
traditional diffusion models in terms of image quality and noise resilience,
offering a powerful and versatile solution for advanced CT imaging in
sparse-view scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of
  Airborne LiDAR Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gao, Shaobo Xia, Sheng Nie, Cheng Wang, Xiaohuan Xi, Bisheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Airborne laser scanning (ALS) point cloud segmentation is a fundamental task
for large-scale 3D scene understanding. In real-world applications, models are
typically fixed after training. However, domain shifts caused by changes in the
environment, sensor types, or sensor degradation often lead to a decline in
model performance. Continuous Test-Time Adaptation (CTTA) offers a solution by
adapting a source-pretrained model to evolving, unlabeled target domains.
Despite its potential, research on ALS point clouds remains limited, facing
challenges such as the absence of standardized datasets and the risk of
catastrophic forgetting and error accumulation during prolonged adaptation. To
tackle these challenges, we propose APCoTTA, the first CTTA method tailored for
ALS point cloud semantic segmentation. We propose a dynamic trainable layer
selection module. This module utilizes gradient information to select
low-confidence layers for training, and the remaining layers are kept frozen,
mitigating catastrophic forgetting. To further reduce error accumulation, we
propose an entropy-based consistency loss. By losing such samples based on
entropy, we apply consistency loss only to the reliable samples, enhancing
model stability. In addition, we propose a random parameter interpolation
mechanism, which randomly blends parameters from the selected trainable layers
with those of the source model. This approach helps balance target adaptation
and source knowledge retention, further alleviating forgetting. Finally, we
construct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA
benchmarks for ALS point cloud segmentation. Experimental results demonstrate
that APCoTTA achieves the best performance on two benchmarks, with mIoU
improvements of approximately 9% and 14% over direct inference. The new
benchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages,12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liqian Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression recognition (FER) in the wild remains a challenging task
due to the subtle and localized nature of expression-related features, as well
as the complex variations in facial appearance. In this paper, we introduce a
novel framework that explicitly focuses on Texture Key Driver Factors (TKDF),
localized texture regions that exhibit strong discriminative power across
emotional categories. By carefully observing facial image patterns, we identify
that certain texture cues, such as micro-changes in skin around the brows,
eyes, and mouth, serve as primary indicators of emotional dynamics. To
effectively capture and leverage these cues, we propose a FER architecture
comprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual
Information Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced
with multi-branch attention to extract fine-grained texture representations,
while DCIF refines these features by filtering context through adaptive pooling
and attention mechanisms. Experimental results on RAF-DB and KDEF datasets
demonstrate that our method achieves state-of-the-art performance, verifying
the effectiveness and robustness of incorporating TKDFs into FER pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier
  Refinement for Diffusion-Based Disease Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Tao Tan, Shuai Tan, Weiqin Yang, Kunyan Cai, Calvin Chen, Yue Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modelling disease progression in precision medicine requires capturing
complex spatio-temporal dynamics while preserving anatomical integrity.
Existing methods often struggle with longitudinal dependencies and structural
consistency in progressive disorders. To address these limitations, we
introduce MambaControl, a novel framework that integrates selective state-space
modelling with diffusion processes for high-fidelity prediction of medical
image trajectories. To better capture subtle structural changes over time while
maintaining anatomical consistency, MambaControl combines Mamba-based
long-range modelling with graph-guided anatomical control to more effectively
represent anatomical correlations. Furthermore, we introduce Fourier-enhanced
spectral graph representations to capture spatial coherence and multiscale
detail, enabling MambaControl to achieve state-of-the-art performance in
Alzheimer's disease prediction. Quantitative and regional evaluations
demonstrate improved progression prediction quality and anatomical fidelity,
highlighting its potential for personalised prognosis and clinical decision
support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSPENet: Contour-Aware and Saliency Priors Embedding Network for
  Infrared Small Target Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiakun Deng, Kexuan Li, Xingye Cui, Jiaxuan Li, Chang Long, Tian Pu, Zhenming Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared small target detection (ISTD) plays a critical role in a wide range
of civilian and military applications. Existing methods suffer from
deficiencies in the localization of dim targets and the perception of contour
information under dense clutter environments, severely limiting their detection
performance. To tackle these issues, we propose a contour-aware and saliency
priors embedding network (CSPENet) for ISTD. We first design a
surround-convergent prior extraction module (SCPEM) that effectively captures
the intrinsic characteristic of target contour pixel gradients converging
toward their center. This module concurrently extracts two collaborative
priors: a boosted saliency prior for accurate target localization and
multi-scale structural priors for comprehensively enriching contour detail
representation. Building upon this, we propose a dual-branch priors embedding
architecture (DBPEA) that establishes differentiated feature fusion pathways,
embedding these two priors at optimal network positions to achieve performance
enhancement. Finally, we develop an attention-guided feature enhancement module
(AGFEM) to refine feature representations and improve saliency estimation
accuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and
NUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art
methods in detection performance. The code is available at
https://github.com/IDIP2025/CSPENet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Registration Change Detection: A Novel Change Detection Task and
  Benchmark <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Shan, Lei Zhou, Liu Mao, Shaofan Chen, Chuanqiu Ren, Xia Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a novel remote sensing change detection task,
non-registration change detection, to address the increasing number of
emergencies such as natural disasters, anthropogenic accidents, and military
strikes. First, in light of the limited discourse on the issue of
non-registration change detection, we systematically propose eight scenarios
that could arise in the real world and potentially contribute to the occurrence
of non-registration problems. Second, we develop distinct image transformation
schemes tailored to various scenarios to convert the available registration
change detection dataset into a non-registration version. Finally, we
demonstrate that non-registration change detection can cause catastrophic
damage to the state-of-the-art methods. Our code and dataset are available at
https://github.com/ShanZard/NRCD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IGARSS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VRU-CIPI: Crossing Intention Prediction at Intersections for Improving
  Vulnerable Road Users Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Quoc Dai Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and predicting human behavior in-thewild, particularly at urban
intersections, remains crucial for enhancing interaction safety between road
users. Among the most critical behaviors are crossing intentions of Vulnerable
Road Users (VRUs), where misinterpretation may result in dangerous conflicts
with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a
sequential attention-based model designed to predict VRU crossing intentions at
intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal
dynamics in VRU movements, combined with a multi-head Transformer
self-attention mechanism to encode contextual and spatial dependencies critical
for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed
achieves state-of-the-art performance with an accuracy of 96.45% and achieving
real-time inference speed reaching 33 frames per second. Furthermore, by
integrating with Infrastructure-to-Vehicles (I2V) communication, our approach
can proactively enhance intersection safety through timely activation of
crossing signals and providing early warnings to connected vehicles, ensuring
smoother and safer interactions for all road users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDFP: Data-dependent Frequency <span class="highlight-title">Prompt</span> for Source Free Domain Adaptation
  of Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Yin, Shaolei Liu, Manning Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptation addresses the challenge of model performance degradation
caused by domain gaps. In the typical setup for unsupervised domain adaptation,
labeled data from a source domain and unlabeled data from a target domain are
used to train a target model. However, access to labeled source domain data,
particularly in medical datasets, can be restricted due to privacy policies. As
a result, research has increasingly shifted to source-free domain adaptation
(SFDA), which requires only a pretrained model from the source domain and
unlabeled data from the target domain data for adaptation. Existing SFDA
methods often rely on domain-specific image style translation and
self-supervision techniques to bridge the domain gap and train the target
domain model. However, the quality of domain-specific style-translated images
and pseudo-labels produced by these methods still leaves room for improvement.
Moreover, training the entire model during adaptation can be inefficient under
limited supervision. In this paper, we propose a novel SFDA framework to
address these challenges. Specifically, to effectively mitigate the impact of
domain gap in the initial training phase, we introduce preadaptation to
generate a preadapted model, which serves as an initialization of target model
and allows for the generation of high-quality enhanced pseudo-labels without
introducing extra parameters. Additionally, we propose a data-dependent
frequency prompt to more effectively translate target domain images into a
source-like style. To further enhance adaptation, we employ a style-related
layer fine-tuning strategy, specifically designed for SFDA, to train the target
model using the prompted target domain images and pseudo-labels. Extensive
experiments on cross-modality abdominal and cardiac SFDA segmentation tasks
demonstrate that our proposed method outperforms existing state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin-Bin Gao, Yue Zhu, Jiangtao Yan, Yuezhi Cai, Weixi Zhang, Meng Wang, Jun Liu, Yong Liu, Lei Wang, Chengjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal visual anomaly detection aims to identify anomalies from novel or
unseen vision domains without additional fine-tuning, which is critical in open
scenarios. Recent studies have demonstrated that pre-trained vision-language
models like CLIP exhibit strong generalization with just zero or a few normal
images. However, existing methods struggle with designing prompt templates,
complex token interactions, or requiring additional fine-tuning, resulting in
limited flexibility. In this work, we present a simple yet effective method
called AdaptCLIP based on two key insights. First, adaptive visual and textual
representations should be learned alternately rather than jointly. Second,
comparative learning between query and normal image prompt should incorporate
both contextual and aligned residual features, rather than relying solely on
residual features. AdaptCLIP treats CLIP models as a foundational service,
adding only three simple adapters, visual adapter, textual adapter, and
prompt-query adapter, at its input or output ends. AdaptCLIP supports
zero-/few-shot generalization across domains and possesses a training-free
manner on target domains once trained on a base dataset. AdaptCLIP achieves
state-of-the-art performance on 12 anomaly detection benchmarks from industrial
and medical domains, significantly outperforming existing competitive methods.
We will make the code and model of AdaptCLIP available at
https://github.com/gaobb/AdaptCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 15 figures, 22 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-Scale Gaussian Splatting SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Xin, Chenyang Wu, Penghui Huang, Yanyong Zhang, Yinian Mao, Guoquan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniSkill: Imitating Human Videos via Cross-Embodiment Skill
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanjung Kim, Jaehyun Kang, Hyolim Kang, Meedeum Cho, Seon Joo Kim, Youngwoon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mimicry is a fundamental learning mechanism in humans, enabling individuals
to learn new tasks by observing and imitating experts. However, applying this
ability to robots presents significant challenges due to the inherent
differences between human and robot embodiments in both their visual appearance
and physical capabilities. While previous methods bridge this gap using
cross-embodiment datasets with shared scenes and tasks, collecting such aligned
data between humans and robots at scale is not trivial. In this paper, we
propose UniSkill, a novel framework that learns embodiment-agnostic skill
representations from large-scale cross-embodiment video data without any
labels, enabling skills extracted from human video prompts to effectively
transfer to robot policies trained only on robot data. Our experiments in both
simulation and real-world environments show that our cross-embodiment skills
successfully guide robots in selecting appropriate actions, even with unseen
video prompts. The project website can be found at:
https://kimhanjung.github.io/UniSkill.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://kimhanjung.github.io/UniSkill/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A portable diagnosis model for Keratoconus using a smartphone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Li, Peter Ho, Jo Woon Chong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keratoconus (KC) is a corneal disorder that results in blurry and distorted
vision. Traditional diagnostic tools, while effective, are often bulky, costly,
and require professional operation. In this paper, we present a portable and
innovative methodology for diagnosing. Our proposed approach first captures the
image reflected on the eye's cornea when a smartphone screen-generated Placido
disc sheds its light on an eye, then utilizes a two-stage diagnosis for
identifying the KC cornea and pinpointing the location of the KC on the cornea.
The first stage estimates the height and width of the Placido disc extracted
from the captured image to identify whether it has KC. In this KC
identification, k-means clustering is implemented to discern statistical
characteristics, such as height and width values of extracted Placido discs,
from non-KC (control) and KC-affected groups. The second stage involves the
creation of a distance matrix, providing a precise localization of KC on the
cornea, which is critical for efficient treatment planning. The analysis of
these distance matrices, paired with a logistic regression model and robust
statistical analysis, reveals a clear distinction between control and KC
groups. The logistic regression model, which classifies small areas on the
cornea as either control or KC-affected based on the corresponding inter-disc
distances in the distance matrix, reported a classification accuracy of 96.94%,
which indicates that we can effectively pinpoint the protrusion caused by KC.
This comprehensive, smartphone-based method is expected to detect KC and
streamline timely treatment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Learning-Driven Inhalation Injury Grading Assistant Using
  Bronchoscopy Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Li, Alan W Pang, Jo Woon Chong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inhalation injuries present a challenge in clinical diagnosis and grading due
to Conventional grading methods such as the Abbreviated Injury Score (AIS)
being subjective and lacking robust correlation with clinical parameters like
mechanical ventilation duration and patient mortality. This study introduces a
novel deep learning-based diagnosis assistant tool for grading inhalation
injuries using bronchoscopy images to overcome subjective variability and
enhance consistency in severity assessment. Our approach leverages data
augmentation techniques, including graphic transformations, Contrastive
Unpaired Translation (CUT), and CycleGAN, to address the scarcity of medical
imaging data. We evaluate the classification performance of two deep learning
models, GoogLeNet and Vision Transformer (ViT), across a dataset significantly
expanded through these augmentation methods. The results demonstrate GoogLeNet
combined with CUT as the most effective configuration for grading inhalation
injuries through bronchoscopy images and achieves a classification accuracy of
97.8%. The histograms and frequency analysis evaluations reveal variations
caused by the augmentation CUT with distribution changes in the histogram and
texture details of the frequency spectrum. PCA visualizations underscore the
CUT substantially enhances class separability in the feature space. Moreover,
Grad-CAM analyses provide insight into the decision-making process; mean
intensity for CUT heatmaps is 119.6, which significantly exceeds 98.8 of the
original datasets. Our proposed tool leverages mechanical ventilation periods
as a novel grading standard, providing comprehensive diagnostic support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An unsupervised method for MRI recovery: Deep image prior with
  structured sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ahmad Sultan, Chong Chen, Yingmin Liu, Katarzyna Gil, Karolina Zareba, Rizwan Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: To propose and validate an unsupervised MRI reconstruction method
that does not require fully sampled k-space data. Materials and Methods: The
proposed method, deep image prior with structured sparsity (DISCUS), extends
the deep image prior (DIP) by introducing group sparsity to frame-specific code
vectors, enabling the discovery of a low-dimensional manifold for capturing
temporal variations. \discus was validated using four studies: (I) simulation
of a dynamic Shepp-Logan phantom to demonstrate its manifold discovery
capabilities, (II) comparison with compressed sensing and DIP-based methods
using simulated single-shot late gadolinium enhancement (LGE) image series from
six distinct digital cardiac phantoms in terms of normalized mean square error
(NMSE) and structural similarity index measure (SSIM), (III) evaluation on
retrospectively undersampled single-shot LGE data from eight patients, and (IV)
evaluation on prospectively undersampled single-shot LGE data from eight
patients, assessed via blind scoring from two expert readers. Results: DISCUS
outperformed competing methods, demonstrating superior reconstruction quality
in terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study
IV). Discussion: An unsupervised image reconstruction method is presented and
validated on simulated and measured data. These developments can benefit
applications where acquiring fully sampled data is challenging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Magn Reson Mater Phy (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02147v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02147v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Tang, Lijun Zhou, Pengkun Hao, Zihang He, Kalok Ho, Shuo Gu, Zhihui Hao, Haiyang Sun, Kun Zhan, Peng Jia, XianPeng Lang, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D multiple object tracking (MOT) plays a crucial role in autonomous driving
perception. Recent end-to-end query-based trackers simultaneously detect and
track objects, which have shown promising potential for the 3D MOT task.
However, existing methods are still in the early stages of development and lack
systematic improvements, failing to track objects in certain complex scenarios,
like occlusions and the small size of target object's situations. In this
paper, we first summarize the current end-to-end 3D MOT framework by
decomposing it into three constituent parts: query initialization, query
propagation, and query matching. Then we propose corresponding improvements,
which lead to a strong yet simple tracker: S2-Track. Specifically, for query
initialization, we present 2D-Prompted Query Initialization, which leverages
predicted 2D object and depth information to prompt an initial estimate of the
object's 3D location. For query propagation, we introduce an Uncertainty-aware
Probabilistic Decoder to capture the uncertainty of complex environment in
object prediction with probabilistic attention. For query matching, we propose
a Hierarchical Query Denoising strategy to enhance training robustness and
convergence. As a result, our S2-Track achieves state-of-the-art performance on
nuScenes benchmark, i.e., 66.3% AMOTA on test split, surpassing the previous
best end-to-end solution by a significant margin of 8.9% AMOTA. We achieve 1st
place on the nuScenes tracking task leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DINO-X: A Unified Vision Model for Open-World Object Detection and
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14347v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14347v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, Xingyu Chen, Zhuheng Song, Yuhong Zhang, Hongjie Huang, Han Gao, Shilong Liu, Hao Zhang, Feng Li, Kent Yu, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce DINO-X, which is a unified object-centric vision
model developed by IDEA Research with the best open-world object detection
performance to date. DINO-X employs the same Transformer-based encoder-decoder
architecture as Grounding DINO 1.5 to pursue an object-level representation for
open-world object understanding. To make long-tailed object detection easy,
DINO-X extends its input options to support text prompt, visual prompt, and
customized prompt. With such flexible prompt options, we develop a universal
object prompt to support prompt-free open-world detection, making it possible
to detect anything in an image without requiring users to provide any prompt.
To enhance the model's core grounding capability, we have constructed a
large-scale dataset with over 100 million high-quality grounding samples,
referred to as Grounding-100M, for advancing the model's open-vocabulary
detection performance. Pre-training on such a large-scale grounding dataset
leads to a foundational object-level representation, which enables DINO-X to
integrate multiple perception heads to simultaneously support multiple object
perception and understanding tasks, including detection, segmentation, pose
estimation, object captioning, object-based QA, etc. Experimental results
demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro
model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and
LVIS-val zero-shot object detection benchmarks, respectively. Notably, it
scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val
benchmarks, improving the previous SOTA performance by 5.8 AP and 5.0 AP. Such
a result underscores its significantly improved capacity for recognizing
long-tailed objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Examining the Source of Defects from a Mechanical Perspective for 3D
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhe Liang, Aoran Wang, Jie Zhou, Xin Jin, Can Gao, Jinbao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore a novel approach to 3D anomaly detection (AD) that
goes beyond merely identifying anomalies based on structural characteristics.
Our primary perspective is that most anomalies arise from unpredictable
defective forces originating from both internal and external sources. To
address these anomalies, we seek out opposing forces that can help correct
them. Therefore, we introduce the Mechanics Complementary Model-based Framework
for the 3D-AD task (MC4AD), which generates internal and external corrective
forces for each point. We first propose a Diverse Anomaly-Generation (DA-Gen)
module designed to simulate various types of anomalies. Next, we present the
Corrective Force Prediction Network (CFP-Net), which uses complementary
representations for point-level analysis to simulate the different
contributions from internal and external corrective forces. To ensure the
corrective forces are constrained effectively, we have developed a combined
loss function that includes a new symmetric loss and an overall loss. Notably,
we implement a Hierarchical Quality Control (HQC) strategy based on a three-way
decision process and contribute a dataset titled Anomaly-IntraVariance, which
incorporates intraclass variance to evaluate our model. As a result, the
proposed MC4AD has been proven effective through theory and experimentation.
The experimental results demonstrate that our approach yields nine
state-of-the-art performances, achieving optimal results with minimal
parameters and the fastest inference speed across five existing datasets, in
addition to the proposed Anomaly-IntraVariance dataset. The source is available
at https://github.com/hzzzzzhappy/MC4AD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PEP-GS: Perceptually-Enhanced Precise Structured 3D Gaussians for
  View-Adaptive Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05731v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05731v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxi Jin, Xiulai Li, Haiping Huang, Lianjun Liu, Yujie Sun, Logan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, 3D Gaussian Splatting (3D-GS) has achieved significant success in
real-time, high-quality 3D scene rendering. However, it faces several
challenges, including Gaussian redundancy, limited ability to capture
view-dependent effects, and difficulties in handling complex lighting and
specular reflections. Additionally, methods that use spherical harmonics for
color representation often struggle to effectively capture anisotropic
components, especially when modeling view-dependent colors under complex
lighting conditions, leading to insufficient contrast and unnatural color
saturation. To address these limitations, we introduce PEP-GS, a
perceptually-enhanced framework that dynamically predicts Gaussian attributes,
including opacity, color, and covariance. We replace traditional spherical
harmonics with a Hierarchical Granular-Structural Attention mechanism, which
enables more accurate modeling of complex view-dependent color effects. By
employing a stable and interpretable framework for opacity and covariance
estimation, PEP-GS avoids the removal of essential Gaussians prematurely,
ensuring a more accurate scene representation. Furthermore, perceptual
optimization is applied to the final rendered images, enhancing perceptual
consistency across different views and ensuring high-quality renderings with
improved texture fidelity and fine-scale detail preservation. Experimental
results demonstrate that PEP-GS outperforms state-of-the-art methods,
particularly in challenging scenarios involving view-dependent effects and
fine-scale details.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage
  Estimation in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16147v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16147v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannik Elsäßer, Laura Weihl, Veronika Cheplygina, Lisbeth Tangaa Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seagrass meadows play a crucial role in marine ecosystems, providing benefits
such as carbon sequestration, water quality improvement, and habitat provision.
Monitoring the distribution and abundance of seagrass is essential for
environmental impact assessments and conservation efforts. However, the current
manual methods of analyzing underwater video data to assess seagrass coverage
are time-consuming and subjective. This work explores the use of deep learning
models to automate the process of seagrass detection and coverage estimation
from underwater video data. We create a new dataset of over 8,300 annotated
underwater images, and subsequently evaluate several deep learning
architectures, including ResNet, InceptionNetV3, DenseNet, and Vision
Transformer for the task of binary classification on the presence and absence
of seagrass by transfer learning. The results demonstrate that deep learning
models, particularly Vision Transformers, can achieve high performance in
predicting eelgrass presence, with AUROC scores exceeding 0.95 on the final
test dataset. The application of underwater image enhancement further improved
the models' prediction capabilities. Furthermore, we introduce a novel approach
for estimating seagrass coverage from video data, showing promising preliminary
results that align with expert manual labels, and indicating potential for
consistent and scalable monitoring. The proposed methodology allows for the
efficient processing of large volumes of video data, enabling the acquisition
of much more detailed information on seagrass distributions in comparison to
current manual methods. This information is crucial for environmental impact
assessments and monitoring programs, as seagrasses are important indicators of
coastal ecosystem health. This project demonstrates the value that deep
learning can bring to the field of marine ecology and environmental monitoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile
  Graphics for Individuals with Vision Impairment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adnan Khan, Alireza Choubineh, Mai A. Shaaban, Abbas Akkasi, Majid Komeili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile graphics are essential for providing access to visual information for
the 43 million people globally living with vision loss. Traditional methods for
creating these graphics are labor-intensive and cannot meet growing demand. We
introduce TactileNet, the first comprehensive dataset and AI-driven framework
for generating embossing-ready 2D tactile templates using text-to-image Stable
Diffusion (SD) models. By integrating Low-Rank Adaptation (LoRA) and
DreamBooth, our method fine-tunes SD models to produce high-fidelity,
guideline-compliant graphics while reducing computational costs. Quantitative
evaluations with tactile experts show 92.86% adherence to accessibility
standards. Structural fidelity analysis revealed near-human design similarity,
with an SSIM of 0.538 between generated graphics and expert-designed tactile
images. Notably, our method preserves object silhouettes better than human
designs (SSIM = 0.259 vs. 0.215 for binary masks), addressing a key limitation
of manual tactile abstraction. The framework scales to 32,000 images (7,050
high-quality) across 66 classes, with prompt editing enabling customizable
outputs (e.g., adding or removing details). By automating the 2D template
generation step-compatible with standard embossing workflows-TactileNet
accelerates production while preserving design flexibility. This work
demonstrates how AI can augment (not replace) human expertise to bridge the
accessibility gap in education and beyond. Code, data, and models will be
publicly released to foster further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at
  Intermediate Resolution with Structure-Aware Multimodal U-Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenwei Zhang, Khanh Dao Duc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at
intermediate resolution (4-8 {\AA}) is crucial in protein structure
determination. Recent advances in deep learning have led to the development of
automated approaches for enhancing experimental cryo-EM density maps. Yet,
these methods are not optimized for intermediate-resolution maps and rely on
map density features alone. To address this, we propose CryoSAMU, a novel
method designed to enhance 3D cryo-EM density maps of protein structures using
structure-aware multimodal U-Nets and trained on curated
intermediate-resolution density maps. We comprehensively evaluate CryoSAMU
across various metrics and demonstrate its competitive performance compared to
state-of-the-art methods. Notably, CryoSAMU achieves significantly faster
processing speed, showing promise for future practical applications. Our code
is available at https://github.com/chenwei-zhang/CryoSAMU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4
  supplementary tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Scalable IoT Deployment for Visual Anomaly Detection via
  Efficient Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arianna Stropeni, Francesco Borsatti, Manuel Barusco, Davide Dalle Pezze, Marco Fabris, Gian Antonio Susto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Anomaly Detection (VAD) is a key task in industrial settings, where
minimizing operational costs is essential. Deploying deep learning models
within Internet of Things (IoT) environments introduces specific challenges due
to limited computational power and bandwidth of edge devices. This study
investigates how to perform VAD effectively under such constraints by
leveraging compact, efficient processing strategies. We evaluate several data
compression techniques, examining the tradeoff between system latency and
detection accuracy. Experiments on the MVTec AD benchmark demonstrate that
significant compression can be achieved with minimal loss in anomaly detection
performance compared to uncompressed data. Current results show up to 80%
reduction in end-to-end inference time, including edge processing,
transmission, and server computation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teaching Humans Subtle Differences with DIFFusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mia Chiquier, Orr Avrech, Yossi Gandelsman, Berthy Feng, Katherine Bouman, Carl Vondrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific expertise often requires recognizing subtle visual differences
that remain challenging to articulate even for domain experts. We present a
system that leverages generative models to automatically discover and visualize
minimal discriminative features between categories while preserving instance
identity. Our method generates counterfactual visualizations with subtle,
targeted transformations between classes, performing well even in domains where
data is sparse, examples are unpaired, and category boundaries resist verbal
description. Experiments across six domains, including black hole simulations,
butterfly taxonomy, and medical imaging, demonstrate accurate transitions with
limited training data, highlighting both established discriminative features
and novel subtle distinctions that measurably improved category
differentiation. User studies confirm our generated counterfactuals
significantly outperform traditional approaches in teaching humans to correctly
differentiate between fine-grained classes, showing the potential of generative
models to advance visual learning and scientific research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Highly Efficient 3D Human Pose Tracking from Events with Spiking
  Spatiotemporal <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09681v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09681v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Zou, Yuxuan Mu, Wei Ji, Zi-An Wang, Xinxin Zuo, Sen Wang, Weixin Si, Li Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event camera, as an asynchronous vision sensor capturing scene dynamics,
presents new opportunities for highly efficient 3D human pose tracking.
Existing approaches typically adopt modern-day Artificial Neural Networks
(ANNs), such as CNNs or Transformer, where sparse events are converted into
dense images or paired with additional gray-scale images as input. Such
practices, however, ignore the inherent sparsity of events, resulting in
redundant computations, increased energy consumption, and potentially degraded
performance. Motivated by these observations, we introduce the first sparse
Spiking Neural Networks (SNNs) framework for 3D human pose tracking based
solely on events. Our approach eliminates the need to convert sparse data to
dense formats or incorporate additional images, thereby fully exploiting the
innate sparsity of input events. Central to our framework is a novel Spiking
Spatiotemporal Transformer, which enables bi-directional spatiotemporal fusion
of spike pose features and provides a guaranteed similarity measurement between
binary spike features in spiking attention. Moreover, we have constructed a
large-scale synthetic dataset, SynEventHPD, that features a broad and diverse
set of 3D human motions, as well as much longer hours of event streams.
Empirical experiments demonstrate the superiority of our approach over existing
state-of-the-art (SOTA) ANN-based methods, requiring only 19.1% FLOPs and 3.6%
energy cost. Furthermore, our approach outperforms existing SNN-based
benchmarks in this task, highlighting the effectiveness of our proposed SNN
framework. The dataset will be released upon acceptance, and code can be found
at https://github.com/JimmyZou/HumanPoseTracking_SNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TCSVT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WildFireCan-MMD: A Multimodal <span class="highlight-title">Dataset</span> for Classification of
  User-Generated Content During Wildfires in Canada 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Braeden Sherritt, Isar Nejadgholi, Marzieh Amini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid information access is vital during wildfires, yet traditional data
sources are slow and costly. Social media offers real-time updates, but
extracting relevant insights remains a challenge. We present WildFireCan-MMD, a
new multimodal dataset of X posts from recent Canadian wildfires, annotated
across twelve key themes. Evaluating both vision-language models and
custom-trained classifiers, we show that while zero-shot prompting offers quick
deployment, even simple trained models outperform them when labelled data is
available. Our best-performing transformer-based fine-tuned model reaches 83%
f-score, outperforming gpt4 by 23%. As a use case, we demonstrate how this
model can be used to uncover trends during wildfires. Our findings highlight
the enduring importance of tailored datasets and task-specific training.
Importantly, such datasets should be localized, as disaster response
requirements vary across regions and contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pose Priors from Language Models <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjay Subramanian, Evonne Ng, Lea Müller, Dan Klein, Shiry Ginosar, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is often used to describe physical interaction, yet most 3D human
pose estimation methods overlook this rich source of information. We bridge
this gap by leveraging large multimodal models (LMMs) as priors for
reconstructing contact poses, offering a scalable alternative to traditional
methods that rely on human annotations or motion capture data. Our approach
extracts contact-relevant descriptors from an LMM and translates them into
tractable losses to constrain 3D human pose optimization. Despite its
simplicity, our method produces compelling reconstructions for both two-person
interactions and self-contact scenarios, accurately capturing the semantics of
physical and social interactions. Our results demonstrate that LMMs can serve
as powerful tools for contact prediction and pose estimation, offering an
alternative to costly manual human annotations or motion capture data. Our code
is publicly available at https://prosepose.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Student Behavioral Engagement using Histogram of Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Abdelkawy, Aly Farag, Islam Alkabbany, Asem Ali, Chris Foreman, Thomas Tretter, Nicholas Hindy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel technique for measuring behavioral
engagement through students' actions recognition. The proposed approach
recognizes student actions then predicts the student behavioral engagement
level. For student action recognition, we use human skeletons to model student
postures and upper body movements. To learn the dynamics of student upper body,
a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions
within every 2minute video segment then these actions are used to build a
histogram of actions which encodes the student actions and their frequencies.
This histogram is utilized as an input to SVM classifier to classify whether
the student is engaged or disengaged. To evaluate the proposed framework, we
build a dataset consisting of 1414 2-minute video segments annotated with 13
actions and 112 video segments annotated with two engagement levels.
Experimental results indicate that student actions can be recognized with top 1
accuracy 83.63% and the proposed framework can capture the average engagement
of the class.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating the Diameter at Breast Height of Trees in a Forest With a
  Single 360 Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siming He, Zachary Osman, Fernando Cladera, Dexter Ong, Nitant Rai, Patrick Corey Green, Vijay Kumar, Pratik Chaudhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forest inventories rely on accurate measurements of the diameter at breast
height (DBH) for ecological monitoring, resource management, and carbon
accounting. While LiDAR-based techniques can achieve centimeter-level
precision, they are cost-prohibitive and operationally complex. We present a
low-cost alternative that only needs a consumer-grade 360 video camera. Our
semi-automated pipeline comprises of (i) a dense point cloud reconstruction
using Structure from Motion (SfM) photogrammetry software called Agisoft
Metashape, (ii) semantic trunk segmentation by projecting Grounded Segment
Anything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based
technique to estimate cross section shape and DBH. We introduce an interactive
visualization tool for inspecting segmented trees and their estimated DBH. On
61 acquisitions of 43 trees under a variety of conditions, our method attains
median absolute relative errors of 5-9% with respect to "ground-truth" manual
measurements. This is only 2-4% higher than LiDAR-based estimates, while
employing a single 360 camera that costs orders of magnitude less, requires
minimal setup, and is widely available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Fine-Grained Control via Aggregation of Multiple Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conghan Yue, Zhengwei Peng, Shiyan Du, Zhi Ji, Chuangjian Cai, Le Wan, Dongyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While many diffusion models perform well when controlling for particular
aspect among style, character, and interaction, they struggle with fine-grained
control due to dataset limitations and intricate model architecture design.
This paper first introduces a novel training-free algorithm in fine-grained
generation, Aggregation of Multiple Diffusion Models (AMDM), which integrates
features from multiple diffusion models into a specified model to activate
specific features and enable fine-grained control. Experimental results
demonstrate that AMDM significantly improves fine-grained control without
training, validating its effectiveness. Additionally, it reveals that diffusion
models initially focus on features such as position, attributes, and style,
with later stages improving generation quality and consistency. AMDM offers a
new perspective for tackling the challenges of fine-grained conditional control
generation in diffusion models: We can fully utilize existing or develop new
conditional diffusion models that control specific aspects, and then aggregate
them using AMDM algorithm. This eliminates the need for constructing complex
datasets, designing intricate model architectures, and incurring high training
costs. Code is available at: https://github.com/Hammour-steak/AMDM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniCAD: Efficient and Extendable Architecture for Multi-Task
  Computer-Aided Diagnosis System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitao Zhu, Yuan Yin, Zhenrong Shen, Zihao Zhao, Haiyu Song, Sheng Wang, Dinggang Shen, Qian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing complexity and scale of visual model pre-training have made
developing and deploying multi-task computer-aided diagnosis (CAD) systems
increasingly challenging and resource-intensive. Furthermore, the medical
imaging community lacks an open-source CAD platform to enable the rapid
creation of efficient and extendable diagnostic models. To address these
issues, we propose UniCAD, a unified architecture that leverages the robust
capabilities of pre-trained vision foundation models to seamlessly handle both
2D and 3D medical images while requiring only minimal task-specific parameters.
UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation
strategy is employed to adapt a pre-trained visual model to the medical image
domain, achieving performance on par with fully fine-tuned counterparts while
introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular
architecture that combines a frozen foundation model with multiple
plug-and-play experts, enabling diverse tasks and seamless functionality
expansion. Building on this unified CAD architecture, we establish an
open-source platform where researchers can share and access lightweight CAD
experts, fostering a more equitable and efficient research ecosystem.
Comprehensive experiments across 12 diverse medical datasets demonstrate that
UniCAD consistently outperforms existing methods in both accuracy and
deployment efficiency. The source code and project page are available at
https://mii-laboratory.github.io/UniCAD/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Action <span class="highlight-title">Pretrain</span>ing from Videos <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, Dieter Fox, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Latent Action Pretraining for general Action models (LAPA), an
unsupervised method for pretraining Vision-Language-Action (VLA) models without
ground-truth robot action labels. Existing Vision-Language-Action models
require action labels typically collected by human teleoperators during
pretraining, which significantly limits possible data sources and scale. In
this work, we propose a method to learn from internet-scale videos that do not
have robot action labels. We first train an action quantization model
leveraging VQ-VAE-based objective to learn discrete latent actions between
image frames, then pretrain a latent VLA model to predict these latent actions
from observations and task descriptions, and finally finetune the VLA on
small-scale robot manipulation data to map from latent to robot actions.
Experimental results demonstrate that our method significantly outperforms
existing techniques that train robot manipulation policies from large-scale
videos. Furthermore, it outperforms the state-of-the-art VLA model trained with
robotic action labels on real-world manipulation tasks that require language
conditioning, generalization to unseen objects, and semantic generalization to
unseen instructions. Training only on human manipulation videos also shows
positive transfer, opening up the potential for leveraging web-scale data for
robotics foundation model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Website: https://latentactionpretraining.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Quantum Convolutional Neural Networks for Image
  Classification: Overcoming Hardware Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Röseler, Oliver Schaudt, Helmut Berg, Christian Bauckhage, Matthias Koch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While classical convolutional neural networks (CNNs) have revolutionized
image classification, the emergence of quantum computing presents new
opportunities for enhancing neural network architectures. Quantum CNNs (QCNNs)
leverage quantum mechanical properties and hold potential to outperform
classical approaches. However, their implementation on current noisy
intermediate-scale quantum (NISQ) devices remains challenging due to hardware
limitations. In our research, we address this challenge by introducing an
encoding scheme that significantly reduces the input dimensionality. We
demonstrate that a primitive QCNN architecture with 49 qubits is sufficient to
directly process $28\times 28$ pixel MNIST images, eliminating the need for
classical dimensionality reduction pre-processing. Additionally, we propose an
automated framework based on expressibility, entanglement, and complexity
characteristics to identify the building blocks of QCNNs, parameterized quantum
circuits (PQCs). Our approach demonstrates advantages in accuracy and
convergence speed with a similar parameter count compared to both hybrid QCNNs
and classical CNNs. We validated our experiments on IBM's Heron r2 quantum
processor, achieving $96.08\%$ classification accuracy, surpassing the
$71.74\%$ benchmark of traditional approaches under identical training
conditions. These results represent one of the first implementations of image
classifications on real quantum hardware and validate the potential of quantum
computing in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative <span class="highlight-title">Pre-train</span>ed Autoregressive Diffusion <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhang, Jiacheng Jiang, Guoqing Ma, Zhiying Lu, Haoyang Huang, Jianlong Yuan, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present GPDiT, a Generative Pre-trained Autoregressive
Diffusion Transformer that unifies the strengths of diffusion and
autoregressive modeling for long-range video synthesis, within a continuous
latent space. Instead of predicting discrete tokens, GPDiT autoregressively
predicts future latent frames using a diffusion loss, enabling natural modeling
of motion dynamics and semantic consistency across frames. This continuous
autoregressive framework not only enhances generation quality but also endows
the model with representation capabilities. Additionally, we introduce a
lightweight causal attention variant and a parameter-free rotation-based
time-conditioning mechanism, improving both the training and inference
efficiency. Extensive experiments demonstrate that GPDiT achieves strong
performance in video generation quality, video representation ability, and
few-shot learning tasks, highlighting its potential as an effective framework
for video modeling in continuous space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis with
  Multimodal Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14066v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14066v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li, Chongyang Ma, Xiu Li, Tong-Yee Lee, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although remarkable progress has been made in image style transfer, style is
just one of the components of artistic paintings. Directly transferring
extracted style features to natural images often results in outputs with
obvious synthetic traces. This is because key painting attributes including
layout, perspective, shape, and semantics often cannot be conveyed and
expressed through style transfer. Large-scale pretrained text-to-image
generation models have demonstrated their capability to synthesize a vast
amount of high-quality images. However, even with extensive textual
descriptions, it is challenging to fully express the unique visual properties
and details of paintings. Moreover, generic models often disrupt the overall
artistic effect when modifying specific areas, making it more complicated to
achieve a unified aesthetic in artworks. Our main novel idea is to integrate
multimodal semantic information as a synthesis guide into artworks, rather than
transferring style to the real world. We also aim to reduce the disruption to
the harmony of artworks while simplifying the guidance conditions.
Specifically, we propose an innovative multi-task unified framework called
CreativeSynth, based on the diffusion model with the ability to coordinate
multimodal inputs. CreativeSynth combines multimodal features with customized
attention mechanisms to seamlessly integrate real-world semantic content into
the art domain through Cross-Art-Attention for aesthetic maintenance and
semantic fusion. We demonstrate the results of our method across a wide range
of different art categories, proving that CreativeSynth bridges the gap between
generative models and artistic expression. Code and results are available at
https://github.com/haha-lisa/CreativeSynth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Charm: The Missing Piece in ViT fine-tuning for Image Aesthetic
  Assessment <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.02522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.02522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Behrad, Tinne Tuytelaars, Johan Wagemans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capacity of Vision transformers (ViTs) to handle variable-sized inputs is
often constrained by computational complexity and batch processing limitations.
Consequently, ViTs are typically trained on small, fixed-size images obtained
through downscaling or cropping. While reducing computational burden, these
methods result in significant information loss, negatively affecting tasks like
image aesthetic assessment. We introduce Charm, a novel tokenization approach
that preserves Composition, High-resolution, Aspect Ratio, and Multi-scale
information simultaneously. Charm prioritizes high-resolution details in
specific regions while downscaling others, enabling shorter fixed-size input
sequences for ViTs while incorporating essential information. Charm is designed
to be compatible with pre-trained ViTs and their learned positional embeddings.
By providing multiscale input and introducing variety to input tokens, Charm
improves ViT performance and generalizability for image aesthetic assessment.
We avoid cropping or changing the aspect ratio to further preserve information.
Extensive experiments demonstrate significant performance improvements on
various image aesthetic and quality assessment datasets (up to 8.1 %) using a
lightweight ViT backbone. Code and pre-trained models are available at
https://github.com/FBehrad/Charm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IntrinsicEdit: Precise generative image manipulation in intrinsic space <span class="chip">SIGGRAPH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linjie Lyu, Valentin Deschaintre, Yannick Hold-Geoffroy, Miloš Hašan, Jae Shin Yoon, Thomas Leimkühler, Christian Theobalt, Iliyan Georgiev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative diffusion models have advanced image editing with high-quality
results and intuitive interfaces such as prompts and semantic drawing. However,
these interfaces lack precise control, and the associated methods typically
specialize on a single editing task. We introduce a versatile, generative
workflow that operates in an intrinsic-image latent space, enabling semantic,
local manipulation with pixel precision for a range of editing operations.
Building atop the RGB-X diffusion framework, we address key challenges of
identity preservation and intrinsic-channel entanglement. By incorporating
exact diffusion inversion and disentangled channel manipulation, we enable
precise, efficient editing with automatic resolution of global illumination
effects -- all without additional data collection or model fine-tuning. We
demonstrate state-of-the-art performance across a variety of tasks on complex
images, including color and texture adjustments, object insertion and removal,
global relighting, and their combinations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH 2025 Journal track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungho Lee, Dogyoon Lee, Minhyeok Lee, Donghyung Kim, Sangyoun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRF) has attracted considerable attention for their
exceptional ability in synthesizing novel views with high fidelity. However,
the presence of motion blur, resulting from slight camera movements during
extended shutter exposures, poses a significant challenge, potentially
compromising the quality of the reconstructed 3D scenes. To effectively handle
this issue, we propose sequential motion understanding radiance fields (SMURF),
a novel approach that models continuous camera motion and leverages the
explicit volumetric representation method for robustness to motion-blurred
input images. The core idea of the SMURF is continuous motion blurring kernel
(CMBK), a module designed to model a continuous camera movements for processing
blurry inputs. Our model is evaluated against benchmark datasets and
demonstrates state-of-the-art performance both quantitatively and
qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPRW 2025, Neural Fields Beyond Conventional Cameras, Project Page:
  https://jho-yonsei.github.io/SMURF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Illegal Waste Detection in Remote Sensing Images: A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06607v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06607v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Gibellini, Piero Fraternali, Giacomo Boracchi, Luca Morandini, Thomas Martinoli, Andrea Diecidue, Simona Malegori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environmental crime is the third largest criminal activity worldwide, with
significant revenues coming from illegal management of solid waste. Thanks to
the increasing availability and the decreasing cost of Very High Resolution
Remote Sensing (VHR RS) images, the fight against environmental crime can
nowadays rely on modern image-analysis tools to support photo-interpretation
for scanning vast territories in search of illegal waste disposal sites. This
paper illustrates a semi-automatic waste detection pipeline, developed in
collaboration with a regional environmental protection agency, for detecting
candidate illegal dumping sites in VHR RS images. To optimize the effectiveness
of the waste detector, extensive experiments evaluate such design choices as
the network architecture, the ground resolution and geographic span of the
input images, as well as the pretraining procedures. The best model attains
remarkable performance, achieving 92.02% F1-Score and 94.56% Accuracy. A
generalization study assesses the performance variation when the detector
processes images from a territory substantially different from the one used
during training, incurring only a moderate performance loss, i.e., 6.5%
decrease in the F1-Score. Finally, an exercise in which photo interpreters
compare the territory scanning effort with and without the support of the waste
detector assesses the concrete benefit of using a computer-aided image analysis
tool in a professional environment protection agency. Results show that a
reduction up to 30% of the time spent for waste site detection can be attained.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Multi-Modal Information to Enhance <span class="highlight-title">Dataset</span> Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08605v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08605v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Li, Hadrien Reynaud, Bernhard Kainz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation aims to create a compact and highly representative
synthetic dataset that preserves the knowledge of a larger real dataset. While
existing methods primarily focus on optimizing visual representations,
incorporating additional modalities and refining object-level information can
significantly improve the quality of distilled datasets. In this work, we
introduce two key enhancements to dataset distillation: caption-guided
supervision and object-centric masking. To integrate textual information, we
propose two strategies for leveraging caption features: the feature
concatenation, where caption embeddings are fused with visual features at the
classification stage, and caption matching, which introduces a caption-based
alignment loss during training to ensure semantic coherence between real and
synthetic data. Additionally, we apply segmentation masks to isolate target
objects and remove background distractions, introducing two loss functions
designed for object-centric learning: masked feature alignment loss and masked
gradient matching loss. Comprehensive evaluations demonstrate that integrating
caption-based guidance and object-centric masking enhances dataset
distillation, leading to synthetic datasets that achieve superior performance
on downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Laws for Black box Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan Liu, Huanran Chen, Yichi Zhang, Yinpeng Dong, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial examples usually exhibit good cross-model transferability,
enabling attacks on black-box models with limited information about their
architectures and parameters, which are highly threatening in commercial
black-box scenarios. Model ensembling is an effective strategy to improve the
transferability of adversarial examples by attacking multiple surrogate models.
However, since prior studies usually adopt few models in the ensemble, there
remains an open question of whether scaling the number of models can further
improve black-box attacks. Inspired by the scaling law of large foundation
models, we investigate the scaling laws of black-box adversarial attacks in
this work. Through theoretical analysis and empirical evaluations, we conclude
with clear scaling laws that using more surrogate models enhances adversarial
transferability. Comprehensive experiments verify the claims on standard image
classifiers, diverse defended models and multimodal large language models using
various adversarial attack methods. Specifically, by scaling law, we achieve
90%+ transfer attack success rate on even proprietary models like GPT-4o.
Further visualization indicates that there is also a scaling law on the
interpretability and semantics of adversarial perturbations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from
  Defocused Images <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungho Lee, Suhwan Cho, Taeoh Kim, Ho-Deok Jang, Minhyeok Lee, Geonho Cha, Dongyoon Wee, Dogyoon Lee, Sangyoun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has attracted significant attention for its
high-quality novel view rendering, inspiring research to address real-world
challenges. While conventional methods depend on sharp images for accurate
scene reconstruction, real-world scenarios are often affected by defocus blur
due to finite depth of field, making it essential to account for realistic 3D
scene representation. In this study, we propose CoCoGaussian, a Circle of
Confusion-aware Gaussian Splatting that enables precise 3D scene representation
using only defocused images. CoCoGaussian addresses the challenge of defocus
blur by modeling the Circle of Confusion (CoC) through a physically grounded
approach based on the principles of photographic defocus. Exploiting 3D
Gaussians, we compute the CoC diameter from depth and learnable aperture
information, generating multiple Gaussians to precisely capture the CoC shape.
Furthermore, we introduce a learnable scaling factor to enhance robustness and
provide more flexibility in handling unreliable depth in scenes with reflective
or refractive surfaces. Experiments on both synthetic and real-world datasets
demonstrate that CoCoGaussian achieves state-of-the-art performance across
multiple benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025, Project Page: https://Jho-Yonsei.github.io/CoCoGaussian/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Single View Garment Reconstruction Using Diffusion Mapping Via Pattern
  Coordinates <span class="chip">SIGGRAPH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ren Li, Cong Cao, Corentin Dumery, Yingxuan You, Hao Li, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing 3D clothed humans from images is fundamental to applications
like virtual try-on, avatar creation, and mixed reality. While recent advances
have enhanced human body recovery, accurate reconstruction of garment geometry
-- especially for loose-fitting clothing -- remains an open challenge. We
present a novel method for high-fidelity 3D garment reconstruction from single
images that bridges 2D and 3D representations. Our approach combines Implicit
Sewing Patterns (ISP) with a generative diffusion model to learn rich garment
shape priors in a 2D UV space. A key innovation is our mapping model that
establishes correspondences between 2D image pixels, UV pattern coordinates,
and 3D geometry, enabling joint optimization of both 3D garment meshes and the
corresponding 2D patterns by aligning learned priors with image observations.
Despite training exclusively on synthetically simulated cloth data, our method
generalizes effectively to real-world images, outperforming existing approaches
on both tight- and loose-fitting garments. The reconstructed garments maintain
physical plausibility while capturing fine geometric details, enabling
downstream applications including garment retargeting and texture manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-R1: Reinforcing Video Reasoning in MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21776v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21776v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by DeepSeek-R1's success in eliciting reasoning abilities through
rule-based reinforcement learning (RL), we introduce Video-R1 as the first
attempt to systematically explore the R1 paradigm for incentivizing video
reasoning within multimodal large language models (MLLMs). However, directly
applying RL training with the GRPO algorithm to video reasoning presents two
primary challenges: (i) a lack of temporal modeling for video reasoning, and
(ii) the scarcity of high-quality video-reasoning data. To address these
issues, we first propose the T-GRPO algorithm, which encourages models to
utilize temporal information in videos for reasoning. Additionally, instead of
relying solely on video data, we incorporate high-quality image-reasoning data
into the training process. We have constructed two datasets: Video-R1-CoT-165k
for SFT cold start and Video-R1-260k for RL training, both comprising image and
video data. Experimental results demonstrate that Video-R1 achieves significant
improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as
well as on general video benchmarks including MVBench and TempCompass, etc.
Notably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning
benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All
code, models, and data are released in: https://github.com/tulerfeng/Video-R1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/tulerfeng/Video-R1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoGenAV: Versatile Audio-Visual Representation Learning via
  Contrastive-Generative Synchronization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Detao Bai, Zhiheng Ma, Xihan Wei, Liefeng Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inherent synchronization between a speaker's lip movements, voice, and
the underlying linguistic content offers a rich source of information for
improving speech processing tasks, especially in challenging conditions where
traditional audio-only systems falter. We introduce CoGenAV, a powerful and
data-efficient model designed to learn versatile audio-visual representations
applicable across a wide range of speech and audio-visual tasks. CoGenAV is
trained by optimizing a dual objective derived from natural audio-visual
synchrony, contrastive feature alignment and generative text prediction, using
only 223 hours of labeled data from the LRS2 dataset. This
contrastive-generative synchronization strategy effectively captures
fundamental cross-modal correlations. We showcase the effectiveness and
versatility of the learned CoGenAV representations on multiple benchmarks. When
utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these
representations contribute to achieving a state-of-the-art Word Error Rate
(WER) of 1.27. They also enable strong performance in Visual Speech Recognition
(VSR) with a WER of 20.5 on LRS2, and significantly improve performance in
noisy environments by over 70%. Furthermore, CoGenAV representations benefit
speech reconstruction tasks, boosting performance in Speech Enhancement and
Separation, and achieve competitive results in audio-visual synchronization
tasks like Active Speaker Detection (ASD). Our model will be open-sourced to
facilitate further development and collaboration within both academia and
industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19159v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19159v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Ding, Rui Sun, Yunjian Zhang, Xiu Yan, Yueqi Zhou, Kaihao Huang, Suzhong Fu, Angelica I Aviles-Rivero, Chuanlong Xie, Yao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to width-wise pruning, depth-wise pruning can significantly
accelerate inference in resource-constrained scenarios. However, treating the
entire Transformer layer as the minimum pruning unit may degrade model
performance by indiscriminately discarding the entire information of the layer.
This paper reveals the ``Patch-like'' feature relationship between layers in
large language models by analyzing the correlation of the outputs of different
layers in the reproducing kernel Hilbert space. Building on this observation,
we propose a sliding layer merging method that dynamically selects and fuses
consecutive layers from top to bottom according to a pre-defined similarity
threshold, thereby simplifying the model structure while maintaining its
performance. Extensive experiments on LLMs with various architectures and
different parameter scales show that our method outperforms existing pruning
techniques in both zero-shot inference performance and retraining recovery
quality after pruning. In particular, in the experiment with 35% pruning on the
Vicuna-7B model, our method achieved a 1.654% improvement in average
performance on zero-shot tasks compared to the existing method. Moreover, we
further reveal the potential of combining depth pruning with width pruning to
enhance the pruning effect. Our codes are available at
https://github.com/920927/SLM-a-sliding-layer-merging-method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Convolutional Neural Networks for Rice Grain Classification:
  An Explainable AI Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Junaid Asif, Hamza Khan, Rabia Tehseen, Syed Tahir Hussain Rizvi, Mujtaba Asad, Shazia Saqib, Rana Fayyaz Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rice is an essential staple food worldwide that is important in promoting
international trade, economic growth, and nutrition. Asian countries such as
China, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their
significant contribution to the cultivation and utilization of rice. These
nations are also known for cultivating different rice grains, including short
and long grains. These sizes are further classified as basmati, jasmine, kainat
saila, ipsala, arborio, etc., catering to diverse culinary preferences and
cultural traditions. For both local and international trade, inspecting and
maintaining the quality of rice grains to satisfy customers and preserve a
country's reputation is necessary. Manual quality check and classification is
quite a laborious and time-consuming process. It is also highly prone to
mistakes. Therefore, an automatic solution must be proposed for the effective
and efficient classification of different varieties of rice grains. This
research paper presents an automatic framework based on a convolutional neural
network (CNN) for classifying different varieties of rice grains. We evaluated
the proposed model based on performance metrics such as accuracy, recall,
precision, and F1-Score. The CNN model underwent rigorous training and
validation, achieving a remarkable accuracy rate and a perfect area under each
class's Receiver Operating Characteristic (ROC) curve. The confusion matrix
analysis confirmed the model's effectiveness in distinguishing between the
different rice varieties, indicating minimal misclassifications. Additionally,
the integration of explainability techniques such as LIME (Local Interpretable
Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided
valuable insights into the model's decision-making process, revealing how
specific features of the rice grains influenced classification outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified theory for joint covariance properties under geometric image
  transformations for spatio-temporal receptive fields according to the
  generalized Gaussian derivative model for visual receptive fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10543v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10543v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Lindeberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The influence of natural image transformations on receptive field responses
is crucial for modelling visual operations in computer vision and biological
vision. In this regard, covariance properties with respect to geometric image
transformations in the earliest layers of the visual hierarchy are essential
for expressing robust image operations, and for formulating invariant visual
operations at higher levels.
  This paper defines and proves a set of joint covariance properties for
spatio-temporal receptive fields in terms of spatio-temporal derivative
operators applied to spatio-temporally smoothed image data under compositions
of spatial scaling transformations, spatial affine transformations, Galilean
transformations and temporal scaling transformations. Specifically, the derived
relations show how the parameters of the receptive fields need to be
transformed, in order to match the output from spatio-temporal receptive fields
under composed spatio-temporal image transformations.
  For this purpose, we also fundamentally extend the notion of scale-normalized
derivatives to affine-normalized derivatives, that are computed based on
spatial smoothing with affine Gaussian kernels, and analyze the covariance
properties of the resulting affine-normalized derivatives for the affine group
as well as for important subgroups thereof.
  We conclude with a geometric analysis, showing how the derived joint
covariance properties make it possible to relate or match spatio-temporal
receptive field responses, when observing, possibly moving, local surface
patches from different views, under locally linearized perspective or
projective transformations, as well as when observing different instances of
spatio-temporal events, that may occur either faster or slower between
different views of similar spatio-temporal events.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 19 figures. Note: From version 4, this paper considers a
  different form of joint composition of the geometric image transformations
  than in the earlier versions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EndoMamba: An Efficient Foundation Model for Endoscopic Videos via
  Hierarchical <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Dongdong Lei, Sebastien Ourselin, Hongbin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endoscopic video-based tasks, such as visual navigation and surgical phase
recognition, play a crucial role in minimally invasive surgeries by providing
real-time assistance. While recent video foundation models have shown promise,
their applications are hindered by (1) computational inefficiencies and (2)
suboptimal performance caused by limited data for pre-training in endoscopy. To
address these issues, we present EndoMamba, a foundation model designed for
real-time inference while learning generalized spatiotemporal representations.
First, to mitigate computational inefficiencies, we propose the EndoMamba
backbone, optimized for real-time inference. Inspired by recent advancements in
state space models, EndoMamba integrates Bidirectional Mamba blocks for spatial
modeling within individual frames and vanilla Mamba blocks for past-to-present
reasoning across the temporal domain. This design enables both strong
spatiotemporal modeling and efficient inference in online video streams.
Second, we propose a self-supervised hierarchical pre-training diagram to
enhance EndoMamba's representation learning using endoscopic videos and
incorporating general video domain knowledge. Specifically, our approach
combines masked reconstruction with auxiliary supervision, leveraging low-level
reconstruction to capture spatial-temporal structures and high-level alignment
to transfer broader knowledge from a pretrained general-video domain foundation
model. Extensive experiments on four downstream tasks--classification,
segmentation, surgical phase recognition, and localization--demonstrate that
EndoMamba outperforms existing foundation models and task-specific methods
while maintaining real-time inference speed. The source code is available at
https://github.com/TianCuteQY/EndoMamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging
  Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study AI
  for ECG to CMR Translation Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyao Ding, Ziyu Li, Yujian Hu, Youyao Xu, Chengchen Zhao, Yiheng Mao, Haitao Li, Zhikang Li, Qian Li, Jing Wang, Yue Chen, Mengjia Chen, Longbo Wang, Xuesen Chu, Weichao Pan, Ziyi Liu, Fei Wu, Hongkun Zhang, Ting Chen, Zhengxing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiovascular diseases (CVDs) are the leading cause of global mortality,
necessitating accessible and accurate diagnostic tools. While cardiac magnetic
resonance imaging (CMR) provides gold-standard insights into cardiac structure
and function, its clinical utility is limited by high cost and complexity. In
contrast, electrocardiography (ECG) is inexpensive and widely available but
lacks the granularity of CMR. We propose CardioNets, a deep learning framework
that translates 12-lead ECG signals into CMR-level functional parameters and
synthetic images, enabling scalable cardiac assessment. CardioNets integrates
cross-modal contrastive learning and generative pretraining, aligning ECG with
CMR-derived cardiac phenotypes and synthesizing high-resolution CMR images via
a masked autoregressive model. Trained on 159,819 samples from five cohorts,
including the UK Biobank (n=42,483) and MIMIC-IV-ECG (n=164,550), and
externally validated on independent clinical datasets (n=3,767), CardioNets
achieved strong performance across disease screening and phenotype estimation
tasks. In the UK Biobank, it improved cardiac phenotype regression R2 by 24.8%
and cardiomyopathy AUC by up to 39.3% over baseline models. In MIMIC, it
increased AUC for pulmonary hypertension detection by 5.6%. Generated CMR
images showed 36.6% higher SSIM and 8.7% higher PSNR than prior approaches. In
a reader study, ECG-only CardioNets achieved 13.9% higher accuracy than human
physicians using both ECG and real CMR. These results suggest that CardioNets
offers a promising, low-cost alternative to CMR for large-scale CVD screening,
particularly in resource-limited settings. Future efforts will focus on
clinical deployment and regulatory validation of ECG-based synthetic imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards user-centered interactive medical image segmentation in VR with
  an assistive AI agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Spiegler, Arash Harirpoush, Yiming Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crucial in disease analysis and surgical planning, manual segmentation of
volumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and
challenging to master, while fully automatic algorithms can benefit from user
feedback. Therefore, with the complementary power of the latest radiological AI
foundation models and virtual reality (VR)'s intuitive data interaction, we
propose SAMIRA, a novel conversational AI agent that assists users with
localizing, segmenting, and visualizing 3D medical concepts in VR. Through
speech-based interaction, the agent helps users understand radiological
features, locate clinical targets, and generate segmentation masks that can be
refined with just a few point prompts. The system also supports true-to-scale
3D visualization of segmented pathology to enhance patient-specific anatomical
understanding. Furthermore, to determine the optimal interaction paradigm under
near-far attention-switching for refining segmentation masks in an immersive,
human-in-the-loop workflow, we compare VR controller pointing, head pointing,
and eye tracking as input modes. With a user study, evaluations demonstrated a
high usability score (SUS=90.0 $\pm$ 9.0), low overall task load, as well as
strong support for the proposed VR system's guidance, training potential, and
integration of AI in radiological segmentation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cyclic 2.5D Perceptual Loss for Cross-Modal 3D Medical Image Synthesis:
  T1w MRI to Tau PET 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Moon, Symac Kim, Haejun Chung, Ikbeom Jang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a demand for medical image synthesis or translation to generate
synthetic images of missing modalities from available data. This need stems
from challenges such as restricted access to high-cost imaging devices,
government regulations, or failure to follow up with patients or study
participants. In medical imaging, preserving high-level semantic features is
often more critical than achieving pixel-level accuracy. Perceptual loss
functions are widely employed to train medical image synthesis or translation
models, as they quantify differences in high-level image features using a
pre-trained feature extraction network. While 3D and 2.5D perceptual losses are
used in 3D medical image synthesis, they face challenges, such as the lack of
pre-trained 3D models or difficulties in balancing loss reduction across
different planes. In this work, we focus on synthesizing 3D tau PET images from
3D T1-weighted MR images. We propose a cyclic 2.5D perceptual loss that
sequentially computes the 2D average perceptual loss for each of the axial,
coronal, and sagittal planes over epochs, with the cycle duration gradually
decreasing. Additionally, we process tau PET images using by-manufacturer
standardization to enhance the preservation of high-SUVR regions indicative of
tau pathology and mitigate SUVR variability caused by inter-manufacturer
differences. We combine the proposed loss with SSIM and MSE losses and
demonstrate its effectiveness in improving both quantitative and qualitative
performance across various generative models, including U-Net, UNETR,
SwinUNETR, CycleGAN, and Pix2Pix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Saliency-Motion Guided Trunk-Collateral Network for Unsupervised Video
  Object Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zheng, Wanyun Li, Songcheng He, Jianping Fan, Xiaoqiang Li, We Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent mainstream unsupervised video object segmentation (UVOS)
motion-appearance approaches use either the bi-encoder structure to separately
encode motion and appearance features, or the uni-encoder structure for joint
encoding. However, these methods fail to properly balance the motion-appearance
relationship. Consequently, even with complex fusion modules for
motion-appearance integration, the extracted suboptimal features degrade the
models' overall performance. Moreover, the quality of optical flow varies
across scenarios, making it insufficient to rely solely on optical flow to
achieve high-quality segmentation results. To address these challenges, we
propose the Saliency-Motion guided Trunk-Collateral Network (SMTC-Net), which
better balances the motion-appearance relationship and incorporates model's
intrinsic saliency information to enhance segmentation performance.
Specifically, considering that optical flow maps are derived from RGB images,
they share both commonalities and differences. Accordingly, we propose a novel
Trunk-Collateral structure for motion-appearance UVOS. The shared trunk
backbone captures the motion-appearance commonality, while the collateral
branch learns the uniqueness of motion features. Furthermore, an Intrinsic
Saliency guided Refinement Module (ISRM) is devised to efficiently leverage the
model's intrinsic saliency information to refine high-level features, and
provide pixel-level guidance for motion-appearance fusion, thereby enhancing
performance without additional input. Experimental results show that SMTC-Net
achieved state-of-the-art performance on three UVOS datasets ( 89.2% J&F on
DAVIS-16, 76% J on YouTube-Objects, 86.4% J on FBMS ) and four standard video
salient object detection (VSOD) benchmarks with the notable increase,
demonstrating its effectiveness and superiority over previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Behind Maya: Building a Multilingual Vision Language Model <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nahid Alam, Karthik Reddy Kanjula, Surya Guthikonda, Timothy Chung, Bala Krishna S Vegesna, Abhipsha Das, Anthony Susevski, Ryan Sze-Yin Chan, S M Iftekhar Uddin, Shayekh Bin Islam, Roshan Santhosh, Snegha A, Drishti Sharma, Chen Liu, Isha Chaturvedi, Genta Indra Winata, Ashvanth. S, Snehanshu Mukherjee, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, we have seen a rapid development of large Vision-Language
Models (VLMs). They have shown impressive results on academic benchmarks,
primarily in widely spoken languages but lack performance on low-resource
languages and varied cultural contexts. To address these limitations, we
introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a
multilingual image-text pretraining dataset in eight languages, based on the
LLaVA pretraining dataset; and 2) a multilingual image-text model supporting
these languages, enhancing cultural and linguistic comprehension in
vision-language tasks. Code available at https://github.com/nahidalam/maya.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at VLMs4ALL CVPR 2025 Workshop; corrected workshop name
  spelling</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Trust-Guided Approach to MR Image Reconstruction with Side Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03021v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03021v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arda Atalık, Sumit Chopra, Daniel K. Sodickson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reducing MRI scan times can improve patient care and lower healthcare costs.
Many acceleration methods are designed to reconstruct diagnostic-quality images
from sparse k-space data, via an ill-posed or ill-conditioned linear inverse
problem (LIP). To address the resulting ambiguities, it is crucial to
incorporate prior knowledge into the optimization problem, e.g., in the form of
regularization. Another form of prior knowledge less commonly used in medical
imaging is the readily available auxiliary data (a.k.a. side information)
obtained from sources other than the current acquisition. In this paper, we
present the Trust- Guided Variational Network (TGVN), an end-to-end deep
learning framework that effectively and reliably integrates side information
into LIPs. We demonstrate its effectiveness in multi-coil, multi-contrast MRI
reconstruction, where incomplete or low-SNR measurements from one contrast are
used as side information to reconstruct high-quality images of another contrast
from heavily under-sampled data. TGVN is robust across different contrasts,
anatomies, and field strengths. Compared to baselines utilizing side
information, TGVN achieves superior image quality while preserving subtle
pathological features even at challenging acceleration levels, drastically
speeding up acquisition while minimizing hallucinations. Source code and
dataset splits are available on github.com/sodicksonlab/TGVN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learned Image Compression with Dictionary-based Entropy Model <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.00496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.00496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingbo Lu, Leheng Zhang, Xingyu Zhou, Mu Li, Wen Li, Shuhang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned image compression methods have attracted great research interest and
exhibited superior rate-distortion performance to the best classical image
compression standards of the present. The entropy model plays a key role in
learned image compression, which estimates the probability distribution of the
latent representation for further entropy coding. Most existing methods
employed hyper-prior and auto-regressive architectures to form their entropy
models. However, they only aimed to explore the internal dependencies of latent
representation while neglecting the importance of extracting prior from
training data. In this work, we propose a novel entropy model named
Dictionary-based Cross Attention Entropy model, which introduces a learnable
dictionary to summarize the typical structures occurring in the training
dataset to enhance the entropy model. Extensive experimental results have
demonstrated that the proposed model strikes a better balance between
performance and latency, achieving state-of-the-art results on various
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OSMLoc: Single Image-Based Visual Localization in OpenStreetMap with
  Fused Geometric and Semantic Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youqi Liao, Xieyuanli Chen, Shuhao Kang, Jianping Li, Zhen Dong, Hongchao Fan, Bisheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenStreetMap (OSM), a rich and versatile source of volunteered geographic
information (VGI), facilitates human self-localization and scene understanding
by integrating nearby visual observations with vectorized map data. However,
the disparity in modalities and perspectives poses a major challenge for
effectively matching camera imagery with compact map representations, thereby
limiting the full potential of VGI data in real-world localization
applications.
  Inspired by the fact that the human brain relies on the fusion of geometric
and semantic understanding for spatial localization tasks, we propose the
OSMLoc in this paper. OSMLoc is a brain-inspired visual localization approach
based on first-person-view images against the OSM maps. It integrates semantic
and geometric guidance to significantly improve accuracy, robustness, and
generalization capability. First, we equip the OSMLoc with the visual
foundational model to extract powerful image features. Second, a
geometry-guided depth distribution adapter is proposed to bridge the monocular
depth estimation and camera-to-BEV transform. Thirdly, the semantic embeddings
from the OSM data are utilized as auxiliary guidance for image-to-OSM feature
matching. To validate the proposed OSMLoc, we collect a worldwide cross-area
and cross-condition (CC) benchmark for extensive evaluation. Experiments on the
MGL dataset, CC validation benchmark, and KITTI dataset have demonstrated the
superiority of our method. Code, pre-trained models, CC validation benchmark,
and additional results are available at: https://github.com/WHU-USI3DV/OSMLoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiECVC: Gated Diversification of Bidirectional Contexts for Learned
  Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jiang, Junru Li, Kai Zhang, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent forward prediction-based learned video compression (LVC) methods have
achieved impressive results, even surpassing VVC reference software VTM under
the Low Delay B (LDB) configuration. In contrast, learned bidirectional video
compression (BVC) remains underexplored and still lags behind its forward-only
counterparts. This performance gap is mainly due to the limited ability to
extract diverse and accurate contexts: most existing BVCs primarily exploit
temporal motion while neglecting non-local correlations across frames.
Moreover, they lack the adaptability to dynamically suppress harmful contexts
arising from fast motion or occlusion. To tackle these challenges, we propose
BiECVC, a BVC framework that incorporates diversified local and non-local
context modeling along with adaptive context gating. For local context
enhancement, BiECVC reuses high-quality features from lower layers and aligns
them using decoded motion vectors without introducing extra motion overhead. To
model non-local dependencies efficiently, we adopt a linear attention mechanism
that balances performance and complexity. To further mitigate the impact of
inaccurate context prediction, we introduce Bidirectional Context Gating,
inspired by data-dependent decay in recent autoregressive language models, to
dynamically filter contextual information based on conditional coding results.
Extensive experiments demonstrate that BiECVC achieves state-of-the-art
performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2
under the Random Access (RA) configuration with intra periods of 32 and 64,
respectively. To our knowledge, BiECVC is the first learned video codec to
surpass VTM 13.2 RA across all standard test datasets. Code will be available
at https://github.com/JiangWeibeta/ECVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first learned video codec that surpasses VTM 13.2 RA across all
  standard test datasets. Code will be available at
  https://github.com/JiangWeibeta/ECVC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06512v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06512v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Wang, Zhi-Qi Cheng, Chenhao Lin, Chao Shen, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image synthesis has progressed to the point where models can generate
visually compelling images from natural language prompts. Yet, existing methods
often fail to reconcile high-level semantic fidelity with explicit spatial
control, particularly in scenes involving multiple objects, nuanced relations,
or complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal
Alignment (HCMA) framework for grounded text-to-image generation. HCMA
integrates two alignment modules into each diffusion sampling step: a global
module that continuously aligns latent representations with textual
descriptions to ensure scene-level coherence, and a local module that employs
bounding-box layouts to anchor objects at specified locations, enabling
fine-grained spatial control. Extensive experiments on the MS-COCO 2014
validation set show that HCMA surpasses state-of-the-art baselines, achieving a
0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP
Score. These results demonstrate HCMA's effectiveness in faithfully capturing
intricate textual semantics while adhering to user-defined spatial constraints,
offering a robust solution for semantically grounded image generation. Our code
is available at https://github.com/hwang-cs-ime/HCMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Memorize Recommendation <span class="highlight-title">Dataset</span>s? A Preliminary Study on
  MovieLens-1M 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Di Palma, Felice Antonio Merra, Maurizio Sfilio, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become increasingly central to
recommendation scenarios due to their remarkable natural language understanding
and generation capabilities. Although significant research has explored the use
of LLMs for various recommendation tasks, little effort has been dedicated to
verifying whether they have memorized public recommendation dataset as part of
their training data. This is undesirable because memorization reduces the
generalizability of research findings, as benchmarking on memorized datasets
does not guarantee generalization to unseen datasets. Furthermore, memorization
can amplify biases, for example, some popular items may be recommended more
frequently than others.
  In this work, we investigate whether LLMs have memorized public
recommendation datasets. Specifically, we examine two model families (GPT and
Llama) across multiple sizes, focusing on one of the most widely used dataset
in recommender systems: MovieLens-1M. First, we define dataset memorization as
the extent to which item attributes, user profiles, and user-item interactions
can be retrieved by prompting the LLMs. Second, we analyze the impact of
memorization on recommendation performance. Lastly, we examine whether
memorization varies across model families and model sizes. Our results reveal
that all models exhibit some degree of memorization of MovieLens-1M, and that
recommendation performance is related to the extent of memorization. We have
made all the code publicly available at:
https://github.com/sisinflab/LLM-MemoryInspector
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Text-to-Chart Retrieval through Training with Synthesized
  Semantic Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wu, Lutao Yan, Yizhang Zhu, Yinan Mei, Jiannan Wang, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Charts are crucial for data analysis and decision-making.Text-to-chart
retrieval systems have become increasingly important for Business Intelligence
(BI), where users need to find relevant charts that match their analytical
needs. These needs can be categorized into precise queries that are
well-specified and fuzzy queries that are more exploratory -- both require
understanding the semantics and context of the charts. However, existing
text-to-chart retrieval solutions often fail to capture the semantic content
and contextual information of charts, primarily due to the lack of
comprehensive metadata (or semantic insights). To address this limitation, we
propose a training data development pipeline that automatically synthesizes
hierarchical semantic insights for charts, covering visual patterns
(visual-oriented), statistical properties (statistics-oriented), and practical
applications (task-oriented), which produces 207,498 semantic insights for
69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to
learn better representations of charts for text-to-chart retrieval. Our method
leverages rich semantic insights during the training phase to develop a model
that understands both visual and semantic aspects of charts.To evaluate
text-to-chart retrieval performance, we curate the first benchmark, CRBench,
for this task with 21,862 charts and 326 text queries from real-world BI
applications, with ground-truth labels verified by the crowd
workers.Experiments show that ChartFinder significantly outperforms existing
methods in text-to-chart retrieval tasks across various settings. For precise
queries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than
state-of-the-art models. In fuzzy query tasks, our method also demonstrates
consistent improvements, with an average increase of 5% across nearly all
metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedPCL-CDR: A Federated Prototype-based Contrastive Learning Framework
  for Privacy-Preserving Cross-domain Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Wang, Qiang Wu, Min Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-domain recommendation (CDR) aims to improve recommendation accuracy in
sparse domains by transferring knowledge from data-rich domains. However,
existing CDR approaches often assume that user-item interaction data across
domains is publicly available, neglecting user privacy concerns. Additionally,
they experience performance degradation with sparse overlapping users due to
their reliance on a large number of fully shared users for knowledge transfer.
To address these challenges, we propose a Federated Prototype-based Contrastive
Learning (CL) framework for Privacy Preserving CDR, called FedPCL-CDR. This
approach utilizes non-overlapping user information and differential prototypes
to improve model performance within a federated learning framework. FedPCL-CDR
comprises two key modules: local domain (client) learning and global server
aggregation. In the local domain, FedPCL-CDR first clusters all user data and
utilizes local differential privacy (LDP) to learn differential prototypes,
effectively utilizing non-overlapping user information and protecting user
privacy. It then conducts knowledge transfer by employing both local and global
prototypes returned from the server in a CL manner. Meanwhile, the global
server aggregates differential prototypes sent from local domains to learn both
local and global prototypes. Extensive experiments on four CDR tasks across
Amazon and Douban datasets demonstrate that FedPCL-CDR surpasses SOTA
baselines. We release our code at https://github.com/Lili1013/FedPCL CDR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal
  Perspective <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19458v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19458v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taoyu Su, Jiawei Sheng, Duohe Ma, Xiaodong Li, Juwei Yue, Mengxiao Song, Yingkai Tang, Tingwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from
different Multi-Modal Knowledge Graphs (MMKGs), a critical information
retrieval task. Existing studies have explored various fusion paradigms and
consistency constraints to improve the alignment of equivalent entities, while
overlooking that the visual modality may not always contribute positively.
Empirically, entities with low-similarity images usually generate
unsatisfactory performance, highlighting the limitation of overly relying on
visual features. We believe the model can be biased toward the visual modality,
leading to a shortcut image-matching task. To address this, we propose a
counterfactual debiasing framework for MMEA, termed CDMEA, which investigates
visual modality bias from a causal perspective. Our approach aims to leverage
both visual and graph modalities to enhance MMEA while suppressing the direct
causal effect of the visual modality on model predictions. By estimating the
Total Effect (TE) of both modalities and excluding the Natural Direct Effect
(NDE) of the visual modality, we ensure that the model predicts based on the
Total Indirect Effect (TIE), effectively utilizing both modalities and reducing
visual modality bias. Extensive experiments on 9 benchmark datasets show that
CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,
high-noise, and low-resource data scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAKR: Enhancing Retrieval-Augmented Generation via Streaming Algorithm
  and K-Means Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21300v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21300v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Kang, Yuzhou Zhu, Yukun Zhong, Ke Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has achieved significant success in
information retrieval to assist large language models LLMs because it builds an
external knowledge database. However, it also has many problems, it consumes a
lot of memory because of the enormous database, and it cannot update the
established index database in time when confronted with massive streaming data.
To reduce the memory required for building the database and maintain accuracy
simultaneously, we proposed a new approach integrating a streaming algorithm
with k-means clustering into RAG. Our approach applied a streaming algorithm to
update the index dynamically and reduce memory consumption. Additionally, the
k-means algorithm clusters highly similar documents, and the query time would
be shortened. We conducted comparative experiments on four methods, and the
results indicated that RAG with streaming algorithm and k-means clusters
outperforms traditional RAG in accuracy and memory, particularly when dealing
with large-scale streaming data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shallow AutoEncoding Recommender with Cold Start Handling via Side
  Features <span class="chip">CIKM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.02288v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.02288v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward DongBo Cui, Lu Zhang, William Ping-hsun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User and item cold starts present significant challenges in industrial
applications of recommendation systems. Supplementing user-item interaction
data with metadata is a common solution-but often at the cost of introducing
additional biases. In this work, we introduce an augmented EASE model that
seamlessly integrates both user and item side information to address these cold
start issues. Our straightforward, autoencoder-based method produces a
closed-form solution that leverages rich content signals for cold items while
refining user representations in data-sparse environments. Importantly, our
method strikes a balance by effectively recommending cold start items and
handling cold start users without incurring extra bias, and it maintains strong
performance in warm settings. Experimental results demonstrate improved
recommendation accuracy and robustness compared to previous collaborative
filtering approaches. Moreover, our model serves as a strong baseline for
future comparative studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preparing submission to CIKM 2025; 2 Figures; 4 Tables; 13 pages;
  Python code implementation example</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Thermodynamic Laws for Large Language Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Liu, Yizhou Liu, Jeff Gore, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beyond neural scaling laws, little is known about the laws underlying large
language models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new
framework that offers fresh insights into LLM training dynamics. On the
theoretical side, we demonstrate that key thermodynamic quantities (e.g.,
temperature, entropy, heat capacity, thermal conduction) and classical
thermodynamic principles (e.g., the three laws of thermodynamics and the
equipartition theorem) naturally emerge under river-valley loss landscape
assumptions. On the practical side, this scientific perspective yields
intuitive guidelines for designing learning rate schedules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An AI-driven framework for the prediction of personalised health
  response to air pollution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nazanin Zounemat Kermani, Sadjad Naderi, Claire H. Dilliway, Claire E. Heaney, Shrreya Behll, Boyang Chen, Hisham Abubakar-Waziri, Alexandra E. Porter, Marc Chadeau-Hyam, Fangxin Fang, Ian M. Adcock, Kian Fan Chung, Christopher C. Pain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Air pollution poses a significant threat to public health, causing or
exacerbating many respiratory and cardiovascular diseases. In addition, climate
change is bringing about more extreme weather events such as wildfires and
heatwaves, which can increase levels of pollution and worsen the effects of
pollution exposure. Recent advances in personal sensing have transformed the
collection of behavioural and physiological data, leading to the potential for
new improvements in healthcare. We wish to capitalise on this data, alongside
new capabilities in AI for making time series predictions, in order to monitor
and predict health outcomes for an individual. Thus, we present a novel
workflow for predicting personalised health responses to pollution by
integrating physiological data from wearable fitness devices with real-time
environmental exposures. The data is collected from various sources in a secure
and ethical manner, and is used to train an AI model to predict individual
health responses to pollution exposure within a cloud-based, modular framework.
We demonstrate that the AI model -- an Adversarial Autoencoder neural network
in this case -- accurately reconstructs time-dependent health signals and
captures nonlinear responses to pollution. Transfer learning is applied using
data from a personal smartwatch, which increases the generalisation abilities
of the AI model and illustrates the adaptability of the approach to real-world,
user-generated data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Kermani and Naderi share first authorship. 20 pages, 6 figures and 1
  table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug
  Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amira Alakhdar, Barnabas Poczos, Newell Washburn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing bioactive molecules remains a central, time- and cost-heavy
challenge in drug discovery, particularly for novel targets lacking structural
or functional data. Pharmacophore modeling presents an alternative for
capturing the key features required for molecular bioactivity against a
biological target. In this work, we present PharmaDiff, a
pharmacophore-conditioned diffusion model for 3D molecular generation.
PharmaDiff employs a transformer-based architecture to integrate an atom-based
representation of the 3D pharmacophore into the generative process, enabling
the precise generation of 3D molecular graphs that align with predefined
pharmacophore hypotheses. Through comprehensive testing, PharmaDiff
demonstrates superior performance in matching 3D pharmacophore constraints
compared to ligand-based drug design methods. Additionally, it achieves higher
docking scores across a range of proteins in structure-based drug design,
without the need for target protein structures. By integrating pharmacophore
modeling with 3D generative techniques, PharmaDiff offers a powerful and
flexible framework for rational drug design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Multi-Image Question Answering via Submodular Subset Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaryan Sharma, Shivansh Gupta, Samar Agarwal, Vishak Prasad C., Ganesh Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) have achieved high performance in
vision-language tasks involving single image but they struggle when presented
with a collection of multiple images (Multiple Image Question Answering
scenario). These tasks, which involve reasoning over large number of images,
present issues in scalability (with increasing number of images) and retrieval
performance. In this work, we propose an enhancement for retriever framework
introduced in MIRAGE model using submodular subset selection techniques. Our
method leverages query-aware submodular functions, such as GraphCut, to
pre-select a subset of semantically relevant images before main retrieval
component. We demonstrate that using anchor-based queries and augmenting the
data improves submodular-retriever pipeline effectiveness, particularly in
large haystack sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative
  Decoding of Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mugilan Ganesan, Shane Segal, Ankur Aggarwal, Nish Sinnadurai, Sean Lie, Vithursan Thangarasa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge capture, adaptation and composition (KCAC): A framework for
  cross-task curriculum learning in robotic manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinrui Wang, Yan Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has demonstrated remarkable potential in robotic
manipulation but faces challenges in sample inefficiency and lack of
interpretability, limiting its applicability in real world scenarios. Enabling
the agent to gain a deeper understanding and adapt more efficiently to diverse
working scenarios is crucial, and strategic knowledge utilization is a key
factor in this process. This paper proposes a Knowledge Capture, Adaptation,
and Composition (KCAC) framework to systematically integrate knowledge transfer
into RL through cross-task curriculum learning. KCAC is evaluated using a two
block stacking task in the CausalWorld benchmark, a complex robotic
manipulation environment. To our knowledge, existing RL approaches fail to
solve this task effectively, reflecting deficiencies in knowledge capture. In
this work, we redesign the benchmark reward function by removing rigid
constraints and strict ordering, allowing the agent to maximize total rewards
concurrently and enabling flexible task completion. Furthermore, we define two
self-designed sub-tasks and implement a structured cross-task curriculum to
facilitate efficient learning. As a result, our KCAC approach achieves a 40
percent reduction in training time while improving task success rates by 10
percent compared to traditional RL methods. Through extensive evaluation, we
identify key curriculum design parameters subtask selection, transition timing,
and learning rate that optimize learning efficiency and provide conceptual
guidance for curriculum based RL frameworks. This work offers valuable insights
into curriculum design in RL and robotic learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Token Prediction Needs Registers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasios Gerontopoulos, Spyros Gidaris, Nikos Komodakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PnPXAI: A Universal XAI Framework Providing Automatic Explanations
  Across Diverse Modalities and Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongun Kim, Sol A Kim, Geonhyeong Kim, Enver Menadjiev, Chanwoo Lee, Seongwook Chung, Nari Kim, Jaesik Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, post hoc explanation methods have emerged to enhance model
transparency by attributing model outputs to input features. However, these
methods face challenges due to their specificity to certain neural network
architectures and data modalities. Existing explainable artificial intelligence
(XAI) frameworks have attempted to address these challenges but suffer from
several limitations. These include limited flexibility to diverse model
architectures and data modalities due to hard-coded implementations, a
restricted number of supported XAI methods because of the requirements for
layer-specific operations of attribution methods, and sub-optimal
recommendations of explanations due to the lack of evaluation and optimization
phases. Consequently, these limitations impede the adoption of XAI technology
in real-world applications, making it difficult for practitioners to select the
optimal explanation method for their domain. To address these limitations, we
introduce \textbf{PnPXAI}, a universal XAI framework that supports diverse data
modalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI
automatically detects model architectures, recommends applicable explanation
methods, and optimizes hyperparameters for optimal explanations. We validate
the framework's effectiveness through user surveys and showcase its versatility
across various domains, including medicine and finance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Nonlinear Dynamics in Physical Modelling Synthesis using Neural
  Ordinary Differential Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Zheleznov, Stefan Bilbao, Alec Wright, Simon King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modal synthesis methods are a long-standing approach for modelling
distributed musical systems. In some cases extensions are possible in order to
handle geometric nonlinearities. One such case is the high-amplitude vibration
of a string, where geometric nonlinear effects lead to perceptually important
effects including pitch glides and a dependence of brightness on striking
amplitude. A modal decomposition leads to a coupled nonlinear system of
ordinary differential equations. Recent work in applied machine learning
approaches (in particular neural ordinary differential equations) has been used
to model lumped dynamic systems such as electronic circuits automatically from
data. In this work, we examine how modal decomposition can be combined with
neural ordinary differential equations for modelling distributed musical
systems. The proposed model leverages the analytical solution for linear
vibration of system's modes and employs a neural network to account for
nonlinear dynamic behaviour. Physical parameters of a system remain easily
accessible after the training without the need for a parameter encoder in the
network architecture. As an initial proof of concept, we generate synthetic
data for a nonlinear transverse string and show that the model can be trained
to reproduce the nonlinear dynamics of the system. Sound examples are
presented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Proceedings of the 28th International
  Conference on Digital Audio Effects (DAFx25), Ancona, Italy, September 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Batched Nonparametric Bandits via k-Nearest Neighbor UCB 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sakshi Arya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study sequential decision-making in batched nonparametric contextual
bandits, where actions are selected over a finite horizon divided into a small
number of batches. Motivated by constraints in domains such as medicine and
marketing -- where online feedback is limited -- we propose a nonparametric
algorithm that combines adaptive k-nearest neighbor (k-NN) regression with the
upper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully
nonparametric, adapts to the context dimension, and is simple to implement.
Unlike prior work relying on parametric or binning-based estimators, BaNk-UCB
uses local geometry to estimate rewards and adaptively balances exploration and
exploitation. We provide near-optimal regret guarantees under standard
Lipschitz smoothness and margin assumptions, using a theoretically motivated
batch schedule that balances regret across batches and achieves minimax-optimal
rates. Empirical evaluations on synthetic and real-world datasets demonstrate
that BaNk-UCB consistently outperforms binning-based baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RouteNator: A Router-Based Multi-Modal Architecture for Generating
  Synthetic Training Data for Function Calling LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vibha Belavadi, Tushar Vatsa, Dewang Sultania, Suhas Suresha, Ishita Verma, Cheng Chen, Tracy Holloway King, Michael Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses fine-tuning Large Language Models (LLMs) for function
calling tasks when real user interaction data is unavailable. In digital
content creation tools, where users express their needs through natural
language queries that must be mapped to API calls, the lack of real-world
task-specific data and privacy constraints for training on it necessitate
synthetic data generation. Existing approaches to synthetic data generation
fall short in diversity and complexity, failing to replicate real-world data
distributions and leading to suboptimal performance after LLM fine-tuning. We
present a novel router-based architecture that leverages domain resources like
content metadata and structured knowledge graphs, along with text-to-text and
vision-to-text language models to generate high-quality synthetic training
data. Our architecture's flexible routing mechanism enables synthetic data
generation that matches observed real-world distributions, addressing a
fundamental limitation of traditional approaches. Evaluation on a comprehensive
set of real user queries demonstrates significant improvements in both function
classification accuracy and API parameter selection. Models fine-tuned with our
synthetic data consistently outperform traditional approaches, establishing new
benchmarks for function calling tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 4th International Workshop on Knowledge-Augmented
  Methods for Natural Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fixing Incomplete Value Function Decomposition for Multi-Agent
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Baisero, Rupali Bhati, Shuo Liu, Aathira Pillai, Christopher Amato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Value function decomposition methods for cooperative multi-agent
reinforcement learning compose joint values from individual per-agent
utilities, and train them using a joint objective. To ensure that the action
selection process between individual utilities and joint values remains
consistent, it is imperative for the composition to satisfy the
individual-global max (IGM) property. Although satisfying IGM itself is
straightforward, most existing methods (e.g., VDN, QMIX) have limited
representation capabilities and are unable to represent the full class of IGM
values, and the one exception that has no such limitation (QPLEX) is
unnecessarily complex. In this work, we present a simple formulation of the
full class of IGM values that naturally leads to the derivation of QFIX, a
novel family of value function decomposition models that expand the
representation capabilities of prior models by means of a thin "fixing" layer.
We derive multiple variants of QFIX, and implement three variants in two
well-known multi-agent frameworks. We perform an empirical evaluation on
multiple SMACv2 and Overcooked environments, which confirms that QFIX (i)
succeeds in enhancing the performance of prior methods, (ii) learns more stably
and performs better than its main competitor QPLEX, and (iii) achieves this
while employing the simplest and smallest mixing models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-tuning Diffusion Policies with Backpropagation Through Diffusion
  Timesteps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyuan Yang, Jiaxuan Gao, Feng Gao, Yi Wu, Chao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion policies, widely adopted in decision-making scenarios such as
robotics, gaming and autonomous driving, are capable of learning diverse skills
from demonstration data due to their high representation power. However, the
sub-optimal and limited coverage of demonstration data could lead to diffusion
policies that generate sub-optimal trajectories and even catastrophic failures.
While reinforcement learning (RL)-based fine-tuning has emerged as a promising
solution to address these limitations, existing approaches struggle to
effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This
challenge stems from the computational intractability of action likelihood
estimation during the denoising process, which leads to complicated
optimization objectives. In our experiments starting from randomly initialized
policies, we find that online tuning of Diffusion Policies demonstrates much
lower sample efficiency compared to directly applying PPO on MLP policies
(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework
that reformulates Diffusion Policy as a noise-conditioned deterministic policy.
By treating each denoising step as a differentiable transformation conditioned
on pre-sampled noise, NCDPO enables tractable likelihood evaluation and
gradient backpropagation through all diffusion timesteps. Our experiments
demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when
training from scratch, outperforming existing methods in both sample efficiency
and final performance across diverse benchmarks, including continuous robot
control and multi-agent game scenarios. Furthermore, our experimental results
show that our method is robust to the number denoising timesteps in the
Diffusion Policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages for main text, 23 pages in total, submitted to Neurips, 13
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Scaling Law for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouxiang Chen, Binyuan Hui, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Jianling Sun, Junyang Lin, Zhongxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is commonly believed that scaling language models should commit a
significant space or time cost, by increasing the parameters (parameter
scaling) or output tokens (inference-time scaling). We introduce the third and
more inference-efficient scaling paradigm: increasing the model's parallel
computation during both training and inference time. We apply $P$ diverse and
learnable transformations to the input, execute forward passes of the model in
parallel, and dynamically aggregate the $P$ outputs. This method, namely
parallel scaling (ParScale), scales parallel computation by reusing existing
parameters and can be applied to any model structure, optimization procedure,
data, or task. We theoretically propose a new scaling law and validate it
through large-scale pre-training, which shows that a model with $P$ parallel
streams is similar to scaling the parameters by $O(\log P)$ while showing
superior inference efficiency. For example, ParScale can use up to 22$\times$
less memory increase and 6$\times$ less latency increase compared to parameter
scaling that achieves the same performance improvement. It can also recycle an
off-the-shelf pre-trained model into a parallelly scaled one by post-training
on a small amount of tokens, further reducing the training budget. The new
scaling law we discovered potentially facilitates the deployment of more
powerful models in low-resource scenarios, and provides an alternative
perspective for the role of computation in machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models for Cancer Communication: Evaluating Linguistic
  Quality, Safety, and Accessibility in Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agnik Saha, Victoria Churchill, Anny D. Rodriguez, Ugur Kursuncu, Muhammed Y. Idris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective communication about breast and cervical cancers remains a
persistent health challenge, with significant gaps in public understanding of
cancer prevention, screening, and treatment, potentially leading to delayed
diagnoses and inadequate treatments. This study evaluates the capabilities and
limitations of Large Language Models (LLMs) in generating accurate, safe, and
accessible cancer-related information to support patient understanding. We
evaluated five general-purpose and three medical LLMs using a mixed-methods
evaluation framework across linguistic quality, safety and trustworthiness, and
communication accessibility and affectiveness. Our approach utilized
quantitative metrics, qualitative expert ratings, and statistical analysis
using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that
general-purpose LLMs produced outputs of higher linguistic quality and
affectiveness, while medical LLMs demonstrate greater communication
accessibility. However, medical LLMs tend to exhibit higher levels of potential
harm, toxicity, and bias, reducing their performance in safety and
trustworthiness. Our findings indicate a duality between domain-specific
knowledge and safety in health communications. The results highlight the need
for intentional model design with targeted improvements, particularly in
mitigating harm and bias, and improving safety and affectiveness. This study
provides a comprehensive evaluation of LLMs for cancer communication, offering
critical insights for improving AI-generated health content and informing
future development of accurate, safe, and accessible digital health tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant
  Tempering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juehang Qin, Shixiao Liang, Christopher Tunnell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal and high-dimensional posteriors present significant challenges
for variational inference, causing mode-seeking behavior and collapse despite
the theoretical expressiveness of normalizing flows. Traditional annealing
methods require temperature schedules and hyperparameter tuning, falling short
of the goal of truly black-box variational inference. We introduce FlowVAT, a
conditional tempering approach for normalizing flow variational inference that
addresses these limitations. Our method tempers both the base and target
distributions simultaneously, maintaining affine-invariance under tempering. By
conditioning the normalizing flow on temperature, we leverage overparameterized
neural networks' generalization capabilities to train a single flow
representing the posterior across a range of temperatures. This preserves modes
identified at higher temperatures when sampling from the variational posterior
at $T = 1$, mitigating standard variational methods' mode-seeking behavior. In
experiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT
outperforms traditional and adaptive annealing methods, finding more modes and
achieving better ELBO values, particularly in higher dimensions where existing
approaches fail. Our method requires minimal hyperparameter tuning and does not
require an annealing schedule, advancing toward fully-automatic black-box
variational inference for complicated posteriors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, and 2 tables in main text, two appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superposition Yields Robust Neural Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou liu, Ziming Liu, Jeff Gore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of today's large language models (LLMs) depends on the
observation that larger models perform better. However, the origin of this
neural scaling law -- the finding that loss decreases as a power law with model
size -- remains unclear. Starting from two empirical principles -- that LLMs
represent more things than the model dimensions (widths) they have (i.e.,
representations are superposed), and that words or concepts in language occur
with varying frequencies -- we constructed a toy model to study the loss
scaling with model size. We found that when superposition is weak, meaning only
the most frequent features are represented without interference, the scaling of
loss with model size depends on the underlying feature frequency; if feature
frequencies follow a power law, so does the loss. In contrast, under strong
superposition, where all features are represented but overlap with each other,
the loss becomes inversely proportional to the model dimension across a wide
range of feature frequency distributions. This robust scaling behavior is
explained geometrically: when many more vectors are packed into a lower
dimensional space, the interference (squared overlaps) between vectors scales
inversely with that dimension. We then analyzed four families of open-sourced
LLMs and found that they exhibit strong superposition and quantitatively match
the predictions of our toy model. The Chinchilla scaling law turned out to also
agree with our results. We conclude that representation superposition is an
important mechanism underlying the observed neural scaling laws. We anticipate
that these insights will inspire new training strategies and model
architectures to achieve better performance with less computation and fewer
parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEAL: Searching Expandable Architectures for Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Gambella, Vicente Javier Castro Solar, Manuel Roveri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental learning is a machine learning paradigm where a model learns from
a sequential stream of tasks. This setting poses a key challenge: balancing
plasticity (learning new tasks) and stability (preserving past knowledge).
Neural Architecture Search (NAS), a branch of AutoML, automates the design of
the architecture of Deep Neural Networks and has shown success in static
settings. However, existing NAS-based approaches to incremental learning often
rely on expanding the model at every task, making them impractical in
resource-constrained environments. In this work, we introduce SEAL, a NAS-based
framework tailored for data-incremental learning, a scenario where disjoint
data samples arrive sequentially and are not stored for future access. SEAL
adapts the model structure dynamically by expanding it only when necessary,
based on a capacity estimation metric. Stability is preserved through
cross-distillation training after each expansion step. The NAS component
jointly searches for both the architecture and the optimal expansion policy.
Experiments across multiple benchmarks demonstrate that SEAL effectively
reduces forgetting and enhances accuracy while maintaining a lower model size
compared to prior methods. These results highlight the promise of combining NAS
and selective expansion for efficient, adaptive learning in incremental
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient MCMC Sampling with Expensive-to-Compute and Irregular
  Likelihoods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conor Rosato, Harvinder Lehal, Simon Maskell, Lee Devlin, Malcolm Strens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when
the likelihood function is irregular and expensive to compute. We explore
several sampling algorithms that make use of subset evaluations to reduce
computational overhead. We adapt the subset samplers for this setting where
gradient information is not available or is unreliable. To achieve this, we
introduce data-driven proxies in place of Taylor expansions and define a novel
computation-cost aware adaptive controller. We undertake an extensive
evaluation for a challenging disease modelling task and a configurable task
with similar irregularity in the likelihood surface. We find our improved
version of Hierarchical Importance with Nested Training Samples (HINTS), with
adaptive proposals and a data-driven proxy, obtains the best sampling error in
a fixed computational budget. We conclude that subset evaluations can provide
cheap and naturally-tempered exploration, while a data-driven proxy can
pre-screen proposals successfully in explored regions of the state space. These
two elements combine through hierarchical delayed acceptance to achieve
efficient, exact sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inferring entropy production in many-body systems using nonequilibrium
  MaxEnt 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Aguilera, Sosuke Ito, Artemy Kolchinsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method for inferring entropy production (EP) in high-dimensional
stochastic systems, including many-body systems and non-Markovian systems with
long memory. Standard techniques for estimating EP become intractable in such
systems due to computational and statistical limitations. We infer
trajectory-level EP and lower bounds on average EP by exploiting a
nonequilibrium analogue of the Maximum Entropy principle, along with convex
duality. Our approach uses only samples of trajectory observables (such as
spatiotemporal correlation functions). It does not require reconstruction of
high-dimensional probability distributions or rate matrices, nor any special
assumptions such as discrete states or multipartite dynamics. It may be used to
compute a hierarchical decomposition of EP, reflecting contributions from
different kinds of interactions, and it has an intuitive physical
interpretation as a thermodynamic uncertainty relation. We demonstrate its
numerical performance on a disordered nonequilibrium spin model with 1000 spins
and a large neural spike-train dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIF: Anomaly detection via preference embedding <span class="chip">ICPR
  2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Leveni, Luca Magri, Giacomo Boracchi, Cesare Alippi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of detecting anomalies with respect to structured
patterns. To this end, we conceive a novel anomaly detection method called PIF,
that combines the advantages of adaptive isolation methods with the flexibility
of preference embedding. Specifically, we propose to embed the data in a high
dimensional space where an efficient tree-based method, PI-Forest, is employed
to compute an anomaly score. Experiments on synthetic and real datasets
demonstrate that PIF favorably compares with state-of-the-art anomaly detection
techniques, and confirm that PI-Forest is better at measuring arbitrary
distances and isolate points in the preference space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Pattern Recognition (ICPR
  2020)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identification and Optimal Nonlinear Control of Turbojet Engine Using
  Koopman Eigenfunction Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Grasev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gas turbine engines represent complex highly nonlinear dynamical systems.
Deriving their physics-based models can be challenging as it requires
performance characteristics, that are not always available, and one often has
to make many simplifying assumptions. In this paper, the limitations of
conventional experimental methods used to derive component-level and locally
linear parameter-varying models are discussed and addressed by employing
identification techniques based on data collected from standard engine
operation under closed-loop control. The rotor dynamics were estimated using
the sparse identification of nonlinear dynamics. Subsequently, the autonomous
part of the dynamics was mapped into an optimally constructed Koopman
eigenfunction space. The process included eigenvalue optimization using
metaheuristic algorithms and temporal projection, followed by gradient-based
eigenfunction identification. The resulting Koopman model was validated against
an in-house reference component-level model. A globally optimal nonlinear
feedback controller and a Kalman estimator were then designed in the
eigenfunction space and compared to the classical and gain-scheduled
proportional-integral controllers, as well as a proposed internal model control
approach. The eigenmode structure allowed targeting individual modes during the
optimization process, resulting in a better performance tuning. The results
showed that the Koopman-based controller outperformed the other benchmark
controllers in both reference tracking and disturbance rejection, under
sea-level and varying flight conditions, due to its global nature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 28 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Score-based diffusion nowcasting of GOES imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Randy J. Chase, Katherine Haynes, Lander Ver Hoef, Imme Ebert-Uphoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clouds and precipitation are important for understanding weather and climate.
Simulating clouds and precipitation with traditional numerical weather
prediction is challenging because of the sub-grid parameterizations required.
Machine learning has been explored for forecasting clouds and precipitation,
but early machine learning methods often created blurry forecasts. In this
paper we explore a newer method, named score-based diffusion, to nowcast (zero
to three hour forecast) clouds and precipitation. We discuss the background and
intuition of score-based diffusion models - thus providing a starting point for
the community - while exploring the methodology's use for nowcasting
geostationary infrared imagery. We experiment with three main types of
diffusion models: a standard score-based diffusion model (Diff); a residual
correction diffusion model (CorrDiff); and a latent diffusion model (LDM). Our
results show that the diffusion models are able to not only advect existing
clouds, but also generate and decay clouds, including convective initiation.
These results are surprising because the forecasts are initiated with only the
past 20 mins of infrared satellite imagery. A case study qualitatively shows
the preservation of high resolution features longer into the forecast than a
conventional mean-squared error trained U-Net. The best of the three diffusion
models tested was the CorrDiff approach, outperforming all other diffusion
models, the traditional U-Net, and a persistence forecast by one to two kelvin
on root mean squared error. The diffusion models also enable out-of-the-box
ensemble generation, which shows skillful calibration, with the spread of the
ensemble correlating well to the error.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyao Wang, Wenwen Qiang, Zeen Song, Changwen Zheng, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Power of Random Features and the Limits of Distribution-Free
  Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ari Karchmer, Eran Malach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the relationship between gradient-based optimization of parametric
models (e.g., neural networks) and optimization of linear combinations of
random features. Our main result shows that if a parametric model can be
learned using mini-batch stochastic gradient descent (bSGD) without making
assumptions about the data distribution, then with high probability, the target
function can also be approximated using a polynomial-sized combination of
random features. The size of this combination depends on the number of gradient
steps and numerical precision used in the bSGD process. This finding reveals
fundamental limitations of distribution-free learning in neural networks
trained by gradient descent, highlighting why making assumptions about data
distributions is often crucial in practice. Along the way, we also introduce a
new theoretical framework called average probabilistic dimension complexity
(adc), which extends the probabilistic dimension complexity developed by Kamath
et al. (2020). We prove that adc has a polynomial relationship with statistical
query dimension, and use this relationship to demonstrate an infinite
separation between adc and standard dimension complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decomposed Inductive Procedure Learning: Learning Academic Tasks with
  Human-Like Data Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Weitekamp, Christopher MacLellan, Erik Harpstead, Kenneth Koedinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human learning relies on specialization -- distinct cognitive mechanisms
working together to enable rapid learning. In contrast, most modern neural
networks rely on a single mechanism: gradient descent over an objective
function. This raises the question: might human learners' relatively rapid
learning from just tens of examples instead of tens of thousands in data-driven
deep learning arise from our ability to use multiple specialized mechanisms of
learning in combination? We investigate this question through an ablation
analysis of inductive human learning simulations in online tutoring
environments. Comparing reinforcement learning to a more data-efficient
3-mechanism symbolic rule induction approach, we find that decomposing learning
into multiple distinct mechanisms significantly improves data efficiency,
bringing it in line with human learning. Furthermore, we show that this
decomposition has a greater impact on efficiency than the distinction between
symbolic and subsymbolic learning alone. Efforts to align data-driven machine
learning with human learning often overlook the stark difference in learning
efficiency. Our findings suggest that integrating multiple specialized learning
mechanisms may be key to bridging this gap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CogSci 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-Stage Generative Model for Intracranial Aneurysm Meshes with
  Morphological Marker Conditioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Ding, Choon Hwai Yap, Kangjun Ji, Simão Castro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A generative model for the mesh geometry of intracranial aneurysms (IA) is
crucial for training networks to predict blood flow forces in real time, which
is a key factor affecting disease progression. This need is necessitated by the
absence of a large IA image datasets. Existing shape generation methods
struggle to capture realistic IA features and ignore the relationship between
IA pouches and parent vessels, limiting physiological realism and their
generation cannot be controlled to have specific morphological measurements. We
propose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh
generator. In the first stage, AneuG generates low-dimensional Graph Harmonic
Deformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,
constrained to morphing energy statistics truths. GHD enables more accurate
shape encoding than alternatives. In the second stage, AneuG generates parent
vessels conditioned on GHD tokens, by generating vascular centreline and
propagating the cross-section. AneuG's IA shape generation can further be
conditioned to have specific clinically relevant morphological measurements.
This is useful for studies to understand shape variations represented by
clinical measurements, and for flow simulation studies to understand effects of
specific clinical shape parameters on fluid dynamics. Source code and
implementation details are available at
https://github.com/anonymousaneug/AneuG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Fidelity Index for Generative Semantic Communications with
  Critical Information Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Huang, Qunsong Zeng, Kaibin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative semantic communication (Gen-SemCom) with large artificial
intelligence (AI) model promises a transformative paradigm for 6G networks,
which reduces communication costs by transmitting low-dimensional prompts
rather than raw data. However, purely prompt-driven generation loses
fine-grained visual details. Additionally, there is a lack of systematic
metrics to evaluate the performance of Gen-SemCom systems. To address these
issues, we develop a hybrid Gen-SemCom system with a critical information
embedding (CIE) framework, where both text prompts and semantically critical
features are extracted for transmissions. First, a novel approach of semantic
filtering is proposed to select and transmit the semantically critical features
of images relevant to semantic label. By integrating the text prompt and
critical features, the receiver reconstructs high-fidelity images using a
diffusion-based generative model. Next, we propose the generative visual
information fidelity (GVIF) metric to evaluate the visual quality of the
generated image. By characterizing the statistical models of image features,
the GVIF metric quantifies the mutual information between the distorted
features and their original counterparts. By maximizing the GVIF metric, we
design a channel-adaptive Gen-SemCom system that adaptively control the volume
of features and compression rate according to the channel state. Experimental
results validate the GVIF metric's sensitivity to visual fidelity, correlating
with both the PSNR and critical information volume. In addition, the optimized
system achieves superior performance over benchmarking schemes in terms of
higher PSNR and lower FID scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Repetition Problems of LLMs in Code Generation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Dong, Yuchen Liu, Xue Jiang, Zhi Jin, Ge Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of neural language models, the performance of code generation
has been significantly boosted. However, the problem of repetitions during the
generation process continues to linger. Previous work has primarily focused on
content repetition, which is merely a fraction of the broader repetition
problem in code generation. A more prevalent and challenging problem is
structural repetition. In structural repetition, the repeated code appears in
various patterns but possesses a fixed structure, which can be inherently
reflected in grammar. In this paper, we formally define structural repetition
and propose an efficient decoding approach called RPG, which stands for
Repetition Penalization based on Grammar, to alleviate the repetition problems
in code generation for LLMs. Specifically, RPG first leverages grammar rules to
identify repetition problems during code generation, and then strategically
decays the likelihood of critical tokens that contribute to repetitions,
thereby mitigating them in code generation. To facilitate this study, we
construct a new dataset CodeRepetEval to comprehensively evaluate approaches
for mitigating the repetition problems in code generation. Extensive
experimental results demonstrate that RPG substantially outperforms the
best-performing baselines on CodeRepetEval dataset as well as HumanEval and
MBPP benchmarks, effectively reducing repetitions and enhancing the quality of
generated code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 (main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Model Explanations without Ground Truth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaivalya Rawal, Zihao Fu, Eoin Delaney, Chris Russell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There can be many competing and contradictory explanations for a single model
prediction, making it difficult to select which one to use. Current explanation
evaluation frameworks measure quality by comparing against ideal "ground-truth"
explanations, or by verifying model sensitivity to important inputs. We outline
the limitations of these approaches, and propose three desirable principles to
ground the future development of explanation evaluation strategies for local
feature importance explanations. We propose a ground-truth Agnostic eXplanation
Evaluation framework (AXE) for evaluating and comparing model explanations that
satisfies these principles. Unlike prior approaches, AXE does not require
access to ideal ground-truth explanations for comparison, or rely on model
sensitivity - providing an independent measure of explanation quality. We
verify AXE by comparing with baselines, and show how it can be used to detect
explanation fairwashing. Our code is available at
https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoCam: Hierarchical Path Planning for an Autonomous Auxiliary Camera
  in Surgical Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Banks, Randy Moore, Sayem Nazmuz Zaman, Alaa Eldin Abdelaal, Septimiu E. Salcudean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating an autonomous auxiliary camera into robot-assisted minimally
invasive surgery (RAMIS) enhances spatial awareness and eliminates manual
viewpoint control. Existing path planning methods for auxiliary cameras track
two-dimensional surgical features but do not simultaneously account for camera
orientation, workspace constraints, and robot joint limits. This study presents
AutoCam: an automatic auxiliary camera placement method to improve
visualization in RAMIS. Implemented on the da Vinci Research Kit, the system
uses a priority-based, workspace-constrained control algorithm that combines
heuristic geometric placement with nonlinear optimization to ensure robust
camera tracking. A user study (N=6) demonstrated that the system maintained
99.84% visibility of a salient feature and achieved a pose error of 4.36 $\pm$
2.11 degrees and 1.95 $\pm$ 5.66 mm. The controller was computationally
efficient, with a loop time of 6.8 $\pm$ 12.8 ms. An additional pilot study
(N=6), where novices completed a Fundamentals of Laparoscopic Surgery training
task, suggests that users can teleoperate just as effectively from AutoCam's
viewpoint as from the endoscope's while still benefiting from AutoCam's
improved visual coverage of the scene. These results indicate that an auxiliary
camera can be autonomously controlled using the da Vinci patient-side
manipulators to track a salient feature, laying the groundwork for new
multi-camera visualization methods in RAMIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Schreier-Coset Graph Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryan Mishra, Lizhen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) offer a principled framework for learning over
graph-structured data, yet their expressive capacity is often hindered by
over-squashing, wherein information from distant nodes is compressed into
fixed-size vectors. Existing solutions, including graph rewiring and
bottleneck-resistant architectures such as Cayley and expander graphs, avoid
this problem but introduce scalability bottlenecks. In particular, the Cayley
graphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoretical
properties, yet suffer from cubic node growth $O(n^3)$, leading to high memory
usage. To address this, this work introduces Schrier-Coset Graph Propagation
(SCGP), a group-theoretic augmentation method that enriches node features
through Schreier-coset embeddings without altering the input graph topology.
SCGP embeds bottleneck-free connectivity patterns into a compact feature space,
improving long-range message passing while maintaining computational
efficiency. Empirical evaluations across standard node and graph classification
benchmarks demonstrate that SCGP achieves performance comparable to, or
exceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits
particular advantages in processing hierarchical and modular graph structures,
offering reduced inference latency, improved scalability, and a low memory
footprint, making it suitable for real-time and resource-constrained
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figure , preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Sparse Autoencoders Useful for Java Function Bug Detection? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Melo, Claudia Mamede, Andre Catarino, Rui Abreu, Henrique Lopes Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software vulnerabilities such as buffer overflows and SQL injections are a
major source of security breaches. Traditional methods for vulnerability
detection remain essential but are limited by high false positive rates,
scalability issues, and reliance on manual effort. These constraints have
driven interest in AI-based approaches to automated vulnerability detection and
secure code generation. While Large Language Models (LLMs) have opened new
avenues for classification tasks, their complexity and opacity pose challenges
for interpretability and deployment. Sparse Autoencoder offer a promising
solution to this problem. We explore whether SAEs can serve as a lightweight,
interpretable alternative for bug detection in Java functions. We evaluate the
effectiveness of SAEs when applied to representations from GPT-2 Small and
Gemma 2B, examining their capacity to highlight buggy behaviour without
fine-tuning the underlying LLMs. We found that SAE-derived features enable bug
detection with an F1 score of up to 89%, consistently outperforming fine-tuned
transformer encoder baselines. Our work provides the first empirical evidence
that SAEs can be used to detect software bugs directly from the internal
representations of pretrained LLMs, without any fine-tuning or task-specific
supervision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ILIF: Temporal Inhibitory Leaky Integrate-and-Fire Neuron for
  Overactivation in Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Sun, Peibo Duan, Levin Kuhlmann, Beilun Wang, Bin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Spiking Neural Network (SNN) has drawn increasing attention for its
energy-efficient, event-driven processing and biological plausibility. To train
SNNs via backpropagation, surrogate gradients are used to approximate the
non-differentiable spike function, but they only maintain nonzero derivatives
within a narrow range of membrane potentials near the firing threshold,
referred to as the surrogate gradient support width gamma. We identify a major
challenge, termed the dilemma of gamma: a relatively large gamma leads to
overactivation, characterized by excessive neuron firing, which in turn
increases energy consumption, whereas a small gamma causes vanishing gradients
and weakens temporal dependencies. To address this, we propose a temporal
Inhibitory Leaky Integrate-and-Fire (ILIF) neuron model, inspired by biological
inhibitory mechanisms. This model incorporates interconnected inhibitory units
for membrane potential and current, effectively mitigating overactivation while
preserving gradient propagation. Theoretical analysis demonstrates ILIF
effectiveness in overcoming the gamma dilemma, and extensive experiments on
multiple datasets show that ILIF improves energy efficiency by reducing firing
rates, stabilizes training, and enhances accuracy. The code is available at
github.com/kaisun1/ILIF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid Strategy for Aggregated Probabilistic Forecasting and Energy
  Trading in HEFTCom2024 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanqing Pu, Feilong Fan, Nengling Tai, Songyuan Liu, Jinming Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obtaining accurate probabilistic energy forecasts and making effective
decisions amid diverse uncertainties are routine challenges in future energy
systems. This paper presents the solution of team GEB, which ranked 3rd in
trading, 4th in forecasting, and 1st among student teams in the IEEE Hybrid
Energy Forecasting and Trading Competition 2024 (HEFTCom2024). The solution
provides accurate probabilistic forecasts for a wind-solar hybrid system, and
achieves substantial trading revenue in the day-ahead electricity market. Key
components include: (1) a stacking-based approach combining sister forecasts
from various Numerical Weather Predictions (NWPs) to provide wind power
forecasts, (2) an online solar post-processing model to address the
distribution shift in the online test set caused by increased solar capacity,
(3) a probabilistic aggregation method for accurate quantile forecasts of
hybrid generation, and (4) a stochastic trading strategy to maximize expected
trading revenue considering uncertainties in electricity prices. This paper
also explores the potential of end-to-end learning to further enhance the
trading revenue by adjusting the distribution of forecast errors. Detailed case
studies are provided to validate the effectiveness of these proposed methods.
Code for all mentioned methods is available for reproduction and further
research in both industry and academia.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Solution description of IEEE Hybrid Energy Forecasting and Trading
  Competition (HEFTCom)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plasticity as the Mirror of Empowerment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Abel, Michael Bowling, André Barreto, Will Dabney, Shi Dong, Steven Hansen, Anna Harutyunyan, Khimya Khetarpal, Clare Lyle, Razvan Pascanu, Georgios Piliouras, Doina Precup, Jonathan Richens, Mark Rowland, Tom Schaul, Satinder Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents are minimally entities that are influenced by their past observations
and act to influence future observations. This latter capacity is captured by
empowerment, which has served as a vital framing concept across artificial
intelligence and cognitive science. This former capacity, however, is equally
foundational: In what ways, and to what extent, can an agent be influenced by
what it observes? In this paper, we ground this concept in a universal
agent-centric measure that we refer to as plasticity, and reveal a fundamental
connection to empowerment. Following a set of desiderata on a suitable
definition, we define plasticity using a new information-theoretic quantity we
call the generalized directed information. We show that this new quantity
strictly generalizes the directed information introduced by Massey (1990) while
preserving all of its desirable properties. Our first finding is that
plasticity is the mirror of empowerment: The agent's plasticity is identical to
the empowerment of the environment, and vice versa. Our second finding
establishes a tension between the plasticity and empowerment of an agent,
suggesting that agent design needs to be mindful of both characteristics. We
explore the implications of these findings, and suggest that plasticity,
empowerment, and their relationship are essential to understanding agency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FactsR: A Safer Method for Producing High Quality Healthcare
  Documentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Petrén Bach Hansen, Lasse Krogsbøll, Jonas Lyngsø, Mathias Baltzersen, Andreas Motzfeldt, Kevin Pelgrims, Lars Maaløe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are now a multitude of AI-scribing solutions for healthcare promising
the utilization of large language models for ambient documentation. However,
these AI scribes still rely on one-shot, or few-shot prompts for generating
notes after the consultation has ended, employing little to no reasoning. This
risks long notes with an increase in hallucinations, misrepresentation of the
intent of the clinician, and reliance on the proofreading of the clinician to
catch errors. A dangerous combination for patient safety if vigilance is
compromised by workload and fatigue. In this paper, we introduce a method for
extracting salient clinical information in real-time alongside the healthcare
consultation, denoted Facts, and use that information recursively to generate
the final note. The FactsR method results in more accurate and concise notes by
placing the clinician-in-the-loop of note generation, while opening up new use
cases within real-time decision support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikeVideoFormer: An Efficient Spike-Driven Video <span class="highlight-title">Transformer</span> with
  Hamming Attention and $\mathcal{O}(T)$ Complexity <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Zou, Qingfeng Li, Wei Ji, Jingjing Li, Yongkui Yang, Guoqi Li, Chao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) have shown competitive performance to
Artificial Neural Networks (ANNs) in various vision tasks, while offering
superior energy efficiency. However, existing SNN-based Transformers primarily
focus on single-image tasks, emphasizing spatial features while not effectively
leveraging SNNs' efficiency in video-based vision tasks. In this paper, we
introduce SpikeVideoFormer, an efficient spike-driven video Transformer,
featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design
a spike-driven Hamming attention (SDHA) which provides a theoretically guided
adaptation from traditional real-valued attention to spike-driven attention.
Building on SDHA, we further analyze various spike-driven space-time attention
designs and identify an optimal scheme that delivers appealing performance for
video tasks, while maintaining only linear temporal complexity. The
generalization ability and efficiency of our model are demonstrated across
diverse downstream video tasks, including classification, human pose tracking,
and semantic segmentation. Empirical results show our method achieves
state-of-the-art (SOTA) performance compared to existing SNN approaches, with
over 15\% improvement on the latter two tasks. Additionally, it matches the
performance of recent ANN-based methods while offering significant efficiency
gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the
three tasks. https://github.com/JimmyZou/SpikeVideoFormer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uniform Loss vs. Specialized Optimization: A Comparative Analysis in
  Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel S. Gama, Valdir Grassi Jr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task
Learning by addressing issues like conflicting gradients and differing gradient
norms, which hinder equal-weighted task training. However, recent critiques
suggest that equally weighted tasks can achieve competitive results compared to
SMTOs, arguing that previous SMTO results were influenced by poor
hyperparameter optimization and lack of regularization. In this work, we
evaluate these claims through an extensive empirical evaluation of SMTOs,
including some of the latest methods, on more complex multi-task problems to
clarify this behavior. Our findings indicate that SMTOs perform well compared
to uniform loss and that fixed weights can achieve competitive performance
compared to SMTOs. Furthermore, we demonstrate why uniform loss perform
similarly to SMTOs in some instances. The code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Introduction to Discrete Variational Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Jeffares, Liyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational Autoencoders (VAEs) are well-established as a principled approach
to probabilistic unsupervised learning with neural networks. Typically, an
encoder network defines the parameters of a Gaussian distributed latent space
from which we can sample and pass realizations to a decoder network. This model
is trained to reconstruct its inputs and is optimized through the evidence
lower bound. In recent years, discrete latent spaces have grown in popularity,
suggesting that they may be a natural choice for many data modalities (e.g.
text). In this tutorial, we provide a rigorous, yet practical, introduction to
discrete variational autoencoders -- specifically, VAEs in which the latent
space is made up of latent variables that follow a categorical distribution. We
assume only a basic mathematical background with which we carefully derive each
step from first principles. From there, we develop a concrete training recipe
and provide an example implementation, hosted at
https://github.com/alanjeffares/discreteVAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tutorial paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergence of Structure in Ensembles of Random Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Muscarnera, Luigi Loreti, Giovanni Todeschini, Alessio Fumagalli, Francesco Regazzoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Randomness is ubiquitous in many applications across data science and machine
learning. Remarkably, systems composed of random components often display
emergent global behaviors that appear deterministic, manifesting a transition
from microscopic disorder to macroscopic organization. In this work, we
introduce a theoretical model for studying the emergence of collective
behaviors in ensembles of random classifiers. We argue that, if the ensemble is
weighted through the Gibbs measure defined by adopting the classification loss
as an energy, then there exists a finite temperature parameter for the
distribution such that the classification is optimal, with respect to the loss
(or the energy). Interestingly, for the case in which samples are generated by
a Gaussian distribution and labels are constructed by employing a teacher
perceptron, we analytically prove and numerically confirm that such optimal
temperature does not depend neither on the teacher classifier (which is, by
construction of the learning problem, unknown), nor on the number of random
classifiers, highlighting the universal nature of the observed behavior.
Experiments on the MNIST dataset underline the relevance of this phenomenon in
high-quality, noiseless, datasets. Finally, a physical analogy allows us to
shed light on the self-organizing nature of the studied phenomenon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Adaptation of Reinforcement Learning Agents to Sudden
  Environmental Change 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Clifford Balloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world autonomous decision-making systems, from robots to recommendation
engines, must operate in environments that change over time. While deep
reinforcement learning (RL) has shown an impressive ability to learn optimal
policies in stationary environments, most methods are data intensive and assume
a world that does not change between training and test time. As a result,
conventional RL methods struggle to adapt when conditions change. This poses a
fundamental challenge: how can RL agents efficiently adapt their behavior when
encountering novel environmental changes during deployment without
catastrophically forgetting useful prior knowledge? This dissertation
demonstrates that efficient online adaptation requires two key capabilities:
(1) prioritized exploration and sampling strategies that help identify and
learn from relevant experiences, and (2) selective preservation of prior
knowledge through structured representations that can be updated without
disruption to reusable components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Dissertation, 131 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Representation Learning Approach to Feature Drift Detection in
  Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Athanasios Tziouvaras, Blaz Bertalanic, George Floros, Kostas Kolomvatsos, Panagiotis Sarigiannidis, Carolina Fortuna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI is foreseen to be a centerpiece in next generation wireless networks
enabling enabling ubiquitous communication as well as new services. However, in
real deployment, feature distribution changes may degrade the performance of AI
models and lead to undesired behaviors. To counter for undetected model
degradation, we propose ALERT; a method that can detect feature distribution
changes and trigger model re-training that works well on two wireless network
use cases: wireless fingerprinting and link anomaly detection. ALERT includes
three components: representation learning, statistical testing and utility
assessment. We rely on MLP for designing the representation learning component,
on Kolmogorov-Smirnov and Population Stability Index tests for designing the
statistical testing and a new function for utility assessment. We show the
superiority of the proposed method against ten standard drift detection methods
available in the literature on two wireless network use cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate
  Descent Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijie Zhou, Shi Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized optimization has become vital for leveraging distributed data
without central control, enhancing scalability and privacy. However, practical
deployments face fundamental challenges due to heterogeneous computation speeds
and unpredictable communication delays. This paper introduces a refined model
of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under
practical assumptions of bounded computation and communication times. To
understand the convergence of ADSGD, we first analyze Asynchronous Stochastic
Block Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges
under computation-delay-independent step sizes. The convergence result is
established without assuming bounded data heterogeneity. Empirical experiments
reveal that ADSGD outperforms existing methods in wall-clock convergence time
across various scenarios. With its simplicity, efficiency in memory and
communication, and resilience to communication and computation delays, ADSGD is
well-suited for real-world decentralized learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, Swarnadeep Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 tables, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deconstructing Subset Construction -- Reducing While Determinizing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Nicol, Markus Frohme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel perspective on the NFA canonization problem, which
introduces intermediate minimization steps to reduce the exploration space
on-the-fly. Essential to our approach are so-called equivalence registries
which manage information about equivalent states and allow for incorporating
further optimization techniques such as convexity closures or simulation to
boost performance. Due to the generality of our approach, these concepts can be
embedded in classic subset construction or Brzozowski's approach. We evaluate
our approach on a set of real-world examples from automatic sequences and
observe that we are able to improve especially worst-case scenarios. We
implement our approach in an open-source library for users to experiment with.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Negative Metric Learning for Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Zhao, Chengpei Wu, Lilin Zhang, Ning Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph contrastive learning (GCL) often suffers from false negatives, which
degrades the performance on downstream tasks. The existing methods addressing
the false negative issue usually rely on human prior knowledge, still leading
GCL to suboptimal results. In this paper, we propose a novel Negative Metric
Learning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative
Metric Network (NMN) to build a negative metric space, in which false negatives
can be distinguished better from true negatives based on their distance to
anchor node. To overcome the lack of explicit supervision signals for NML, we
propose a joint training scheme with bi-level optimization objective, which
implicitly utilizes the self-supervision signals to iteratively optimize the
encoder and the negative metric network. The solid theoretical analysis and the
extensive experiments conducted on widely used benchmarks verify the
superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defending the Edge: Representative-Attention for Mitigating Backdoor
  Attacks in Federated Learning <span class="chip">ESORICS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chibueze Peace Obioma, Youcheng Sun, Mustafa A. Mustafa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enhances privacy and reduces communication cost for
resource-constrained edge clients by supporting distributed model training at
the edge. However, the heterogeneous nature of such devices produces diverse,
non-independent, and identically distributed (non-IID) data, making the
detection of backdoor attacks more challenging. In this paper, we propose a
novel federated representative-attention-based defense mechanism, named FeRA,
that leverages cross-client attention over internal feature representations to
distinguish benign from malicious clients. FeRA computes an anomaly score based
on representation reconstruction errors, effectively identifying clients whose
internal activations significantly deviate from the group consensus. Our
evaluation demonstrates FeRA's robustness across various FL scenarios,
including challenging non-IID data distributions typical of edge devices.
Experimental results show that it effectively reduces backdoor attack success
rates while maintaining high accuracy on the main task. The method is
model-agnostic, attack-agnostic, and does not require labeled reference data,
making it well suited to heterogeneous and resource-limited edge deployments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ESORICS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Electric Bus Charging Scheduling with Uncertainties Using
  Hierarchical Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaju Qi, Lei Lei, Thorsteinn Jonsson, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing adoption of Electric Buses (EBs) represents a significant step
toward sustainable development. By utilizing Internet of Things (IoT) systems,
charging stations can autonomously determine charging schedules based on
real-time data. However, optimizing EB charging schedules remains a critical
challenge due to uncertainties in travel time, energy consumption, and
fluctuating electricity prices. Moreover, to address real-world complexities,
charging policies must make decisions efficiently across multiple time scales
and remain scalable for large EB fleets. In this paper, we propose a
Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the
original Markov Decision Process (MDP) into two augmented MDPs. To solve these
MDPs and enable multi-timescale decision-making, we introduce a novel HDRL
algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization
Enhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic
(DAC) algorithm for large-scale EB fleets are addressed through enhancements at
both decision levels. At the high level, we redesign the decentralized actor
network and integrate an attention mechanism to extract relevant global state
information for each EB, decreasing the size of neural networks. At the low
level, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is
incorporated into the DAC framework, enabling decentralized and coordinated
charging power decisions, reducing computational complexity and enhancing
convergence speed. Extensive experiments with real-world data demonstrate the
superior performance and scalability of DAC-MAPPO-E in optimizing EB fleet
charging schedules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating the number of household TV profiles based in customer
  behaviour using Gaussian mixture model averaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel R. Palma, Sally McClean, Brahim Allan, Zeeshan Tariq, Rafael A. Moral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TV customers today face many choices from many live channels and on-demand
services. Providing a personalised experience that saves customers time when
discovering content is essential for TV providers. However, a reliable
understanding of their behaviour and preferences is key. When creating
personalised recommendations for TV, the biggest challenge is understanding
viewing behaviour within households when multiple people are watching. The
objective is to detect and combine individual profiles to make
better-personalised recommendations for group viewing. Our challenge is that we
have little explicit information about who is watching the devices at any time
(individuals or groups). Also, we do not have a way to combine more than one
individual profile to make better recommendations for group viewing. We propose
a novel framework using a Gaussian mixture model averaging to obtain point
estimates for the number of household TV profiles and a Bayesian random walk
model to introduce uncertainty. We applied our approach using data from real
customers whose TV-watching data totalled approximately half a million
observations. Our results indicate that combining our framework with the
selected features provides a means to estimate the number of household TV
profiles and their characteristics, including shifts over time and
quantification of uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spike-timing-dependent Hebbian learning as noisy gradient descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Dexheimer, Sascha Gaudlitz, Johannes Schmidt-Hieber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hebbian learning is a key principle underlying learning in biological neural
networks. It postulates that synaptic changes occur locally, depending on the
activities of pre- and postsynaptic neurons. While Hebbian learning based on
neuronal firing rates is well explored, much less is known about learning rules
that account for precise spike-timing. We relate a Hebbian
spike-timing-dependent plasticity rule to noisy gradient descent with respect
to a natural loss function on the probability simplex. This connection allows
us to prove that the learning rule eventually identifies the presynaptic neuron
with the highest activity. We also discover an intrinsic connection to noisy
mirror descent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall
  Probabilities Over 8 Hours 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Pablos Sarabia, Joachim Nyborg, Morten Birk, Jeppe Liborius Sjørup, Anders Lillevang Vesterholt, Ira Assent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a deep learning model for high-resolution probabilistic
precipitation forecasting over an 8-hour horizon in Europe, overcoming the
limitations of radar-only deep learning models with short forecast lead times.
Our model efficiently integrates multiple data sources - including radar,
satellite, and physics-based numerical weather prediction (NWP) - while
capturing long-range interactions, resulting in accurate forecasts with robust
uncertainty quantification through consistent probabilistic maps. Featuring a
compact architecture, it enables more efficient training and faster inference
than existing models. Extensive experiments demonstrate that our model
surpasses current operational NWP systems, extrapolation-based methods, and
deep-learning nowcasting models, setting a new standard for high-resolution
precipitation forecasting in Europe, ensuring a balance between accuracy,
interpretability, and computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HandReader: Advanced Techniques for Efficient Fingerspelling Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Korotaev, Petr Surovtsev, Alexander Kapitanov, Karina Kvanchiani, Aleksandr Nagaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fingerspelling is a significant component of Sign Language (SL), allowing the
interpretation of proper names, characterized by fast hand movements during
signing. Although previous works on fingerspelling recognition have focused on
processing the temporal dimension of videos, there remains room for improving
the accuracy of these approaches. This paper introduces HandReader, a group of
three architectures designed to address the fingerspelling recognition task.
HandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to
process RGB features from videos of varying lengths while preserving important
sequential information. HandReader$_{KP}$ is built on the proposed Temporal
Pose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition
in a batch allows the encoder to pass them through 2D and 3D convolution
layers, utilizing temporal and spatial information and accumulating keypoints
coordinates. We also introduce HandReader_RGB+KP - architecture with a joint
encoder to benefit from RGB and keypoint modalities. Each HandReader model
possesses distinct advantages and achieves state-of-the-art results on the
ChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate
high performance on the first open dataset for Russian fingerspelling, Znaki,
presented in this paper. The Znaki dataset and HandReader pre-trained models
are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/ai-forever/handreader</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack
  in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Diana, André Nusser, Chuan Xu, Giovanni Neglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Electric Bus Charging Schedules Relying on Real Data-Driven Targets
  Based on Hierarchical Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaju Qi, Lei Lei, Thorsteinn Jonsson, Lajos Hanzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The charging scheduling problem of Electric Buses (EBs) is investigated based
on Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is
conceived, where the time horizon includes multiple charging and operating
periods in a day, while each period is further divided into multiple time
steps. To overcome the challenge of long-range multi-phase planning with sparse
reward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP
into a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical
Double Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is
proposed for simultaneously solving the decision problems arising at different
temporal resolutions. As a result, the high-level agent learns an effective
policy for prescribing the charging targets for every charging period, while
the low-level agent learns an optimal policy for setting the charging power of
every time step within a single charging period, with the aim of minimizing the
charging costs while meeting the charging target. It is proved that the flat
policy constructed by superimposing the optimal high-level policy and the
optimal low-level policy performs as well as the optimal policy of the original
MDP. Since jointly learning both levels of policies is challenging due to the
non-stationarity of the high-level agent and the sampling inefficiency of the
low-level agent, we divide the joint learning process into two phases and
exploit our new HER algorithm to manipulate the experience replay buffers for
both levels of agents. Numerical experiments are performed with the aid of
real-world data to evaluate the performance of the proposed algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on
  Resource-Constrained Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangwen Zhuge, Xu Shen, Zeyu Wang, Fan Dang, Xuan Ding, Danyang Li, Yahui Han, Tianxiang Hao, Zheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient LLM inference on resource-constrained devices presents significant
challenges in compute and memory utilization. Due to limited GPU memory,
existing systems offload model weights to CPU memory, incurring substantial I/O
overhead between the CPU and GPU. This leads to two major inefficiencies: (1)
GPU cores are underutilized, often remaining idle while waiting for data to be
loaded; and (2) GPU memory has low impact on performance, as reducing its
capacity has minimal effect on overall throughput.In this paper, we propose
SpecOffload, a high-throughput inference engine that embeds speculative
decoding into offloading. Our key idea is to unlock latent GPU resources for
storing and executing a draft model used for speculative decoding, thus
accelerating inference at near-zero additional cost. To support this, we
carefully orchestrate the interleaved execution of target and draft models in
speculative decoding within the offloading pipeline, and propose a planner to
manage tensor placement and select optimal parameters. Compared to the best
baseline, SpecOffload improves GPU core utilization by 4.49x and boosts
inference throughput by 2.54x. Our code is available at
https://github.com/MobiSense/SpecOffload .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution
  Generalisation in MRI Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puru Vaish, Felix Meister, Tobias Heimann, Christoph Brune, Jelmer M. Wolterink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation models are often trained on curated datasets,
leading to performance degradation when deployed in real-world clinical
settings due to mismatches between training and test distributions. While data
augmentation techniques are widely used to address these challenges,
traditional visually consistent augmentation strategies lack the robustness
needed for diverse real-world scenarios. In this work, we systematically
evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary
Fourier Augmentation. These methods mitigate the effects of multiple variations
without explicitly targeting specific sources of distribution shifts. We
demonstrate how these techniques significantly improve out-of-distribution
generalization and robustness to imaging variations across a wide range of
transformations in cardiac cine MRI and prostate MRI segmentation. We
quantitatively find that these augmentation methods enhance learned feature
representations by promoting separability and compactness. Additionally, we
highlight how their integration into nnU-Net training pipelines provides an
easy-to-implement, effective solution for enhancing the reliability of medical
segmentation models in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MIDL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComplexFormer: Disruptively Advancing <span class="highlight-title">Transformer</span> Inference Ability via
  Head-Specific Complex Vector Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintian Shao, Hongyi Huang, Jiayi Wu, Beiwen Zhang, ZhiYu Wu, You Shan, MingKai Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models rely on self-attention to capture token dependencies but
face challenges in effectively integrating positional information while
allowing multi-head attention (MHA) flexibility. Prior methods often model
semantic and positional differences disparately or apply uniform positional
adjustments across heads, potentially limiting representational capacity. This
paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.
CMHA empowers each head to independently model semantic and positional
differences unified within the complex plane, representing interactions as
rotations and scaling. ComplexFormer incorporates two key improvements: (1) a
per-head Euler transformation, converting real-valued query/key projections
into polar-form complex vectors for head-specific complex subspace operation;
and (2) a per-head adaptive differential rotation mechanism,
exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct
strategies for integrating semantic angle differences (ASmn,i) with relative
positional encodings (Delta(Pmn),i). Extensive experiments on language
modeling, text generation, code generation, and mathematical reasoning show
ComplexFormer achieves superior performance, significantly lower generation
perplexity , and improved long-context coherence compared to strong baselines
like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,
offering a more expressive, adaptable attention mechanism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM
  Performance on Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadmahdi Ghasemloo, Alireza Moradi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread adoption of Large Language Models (LLMs), there is a
growing need to establish best practices for leveraging their capabilities
beyond traditional natural language tasks. In this paper, a novel cross-domain
knowledge transfer framework is proposed to enhance the performance of LLMs in
time series forecasting -- a task of increasing relevance in fields such as
energy systems, finance, and healthcare. The approach systematically infuses
LLMs with structured temporal information to improve their forecasting
accuracy. This study evaluates the proposed method on a real-world time series
dataset and compares it to a naive baseline where the LLM receives no auxiliary
information. Results show that knowledge-informed forecasting significantly
outperforms the uninformed baseline in terms of predictive accuracy and
generalization. These findings highlight the potential of knowledge transfer
strategies to bridge the gap between LLMs and domain-specific forecasting
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A multi-head deep fusion model for recognition of cattle foraging events
  using sound and movement signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariano Ferrero, José Omar Chelotti, Luciano Sebastián Martinez-Rau, Leandro Vignolo, Martín Pires, Julio Ricardo Galli, Leonardo Luis Giovanini, Hugo Leonardo Rufiner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring feeding behaviour is a relevant task for efficient herd management
and the effective use of available resources in grazing cattle. The ability to
automatically recognise animals' feeding activities through the identification
of specific jaw movements allows for the improvement of diet formulation, as
well as early detection of metabolic problems and symptoms of animal
discomfort, among other benefits. The use of sensors to obtain signals for such
monitoring has become popular in the last two decades. The most frequently
employed sensors include accelerometers, microphones, and cameras, each with
its own set of advantages and drawbacks. An unexplored aspect is the
simultaneous use of multiple sensors with the aim of combining signals in order
to enhance the precision of the estimations. In this direction, this work
introduces a deep neural network based on the fusion of acoustic and inertial
signals, composed of convolutional, recurrent, and dense layers. The main
advantage of this model is the combination of signals through the automatic
extraction of features independently from each of them. The model has emerged
from an exploration and comparison of different neural network architectures
proposed in this work, which carry out information fusion at different levels.
Feature-level fusion has outperformed data and decision-level fusion by at
least a 0.14 based on the F1-score metric. Moreover, a comparison with
state-of-the-art machine learning methods is presented, including traditional
and deep learning approaches. The proposed model yielded an F1-score value of
0.802, representing a 14% increase compared to previous methods. Finally,
results from an ablation study and post-training quantization evaluation are
also reported.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint submitted to Engineering Applications of Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defect Detection in Photolithographic Patterns Using Deep Learning
  Models Trained on Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant P. Shinde, Priyadarshini P. Pai, Shashishekar P. Adiga, K. Subramanya Mayya, Yongbeom Seo, Myungsoo Hwang, Heeyoung Go, Changmin Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the photolithographic process vital to semiconductor manufacturing,
various types of defects appear during EUV pattering. Due to ever-shrinking
pattern size, these defects are extremely small and cause false or missed
detection during inspection. Specifically, the lack of defect-annotated quality
data with good representation of smaller defects has prohibited deployment of
deep learning based defect detection models in fabrication lines. To resolve
the problem of data unavailability, we artificially generate scanning electron
microscopy (SEM) images of line patterns with known distribution of defects and
autonomously annotate them. We then employ state-of-the-art object detection
models to investigate defect detection performance as a function of defect
size, much smaller than the pitch width. We find that the real-time object
detector YOLOv8 has the best mean average precision of 96% as compared to
EfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We
report the smallest defect size that can be detected reliably. When tested on
real SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and
78.3% of Break defects across all relevant instances. These promising results
suggest that synthetic data can be used as an alternative to real-world data in
order to develop robust machine-learning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Zheng, Qi Shao, Guijun Han, Wei Li, Hong Li, Xuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mesoscale eddies dominate the spatiotemporal multiscale variability of the
ocean, and their impact on the energy cascade of the global ocean cannot be
ignored. Eddy-resolving ocean forecasting is providing more reliable protection
for fisheries and navigational safety, but also presents significant scientific
challenges and high computational costs for traditional numerical models.
Artificial intelligence (AI)-based weather and ocean forecasting systems are
becoming powerful tools that balance forecast performance with computational
efficiency. However, the complex multiscale features in the ocean dynamical
system make AI models still face many challenges in mesoscale eddy forecasting
(especially regional modelling). Here, we develop LanTu, a regional
eddy-resolving ocean forecasting system based on dynamics-enhanced deep
learning. We incorporate cross-scale interactions into LanTu and construct
multiscale physical constraint for optimising LanTu guided by knowledge of eddy
dynamics in order to improve the forecasting skill of LanTu for mesoscale
evolution. The results show that LanTu outperforms the existing advanced
operational numerical ocean forecasting system (NOFS) and AI-based ocean
forecasting system (AI-OFS) in temperature, salinity, sea level anomaly and
current prediction, with a lead time of more than 10 days. Our study highlights
that dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for
eddy-resolving ocean forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining Hidden Thoughts from Texts: Evaluating Continual <span class="highlight-title">Pretrain</span>ing with
  Synthetic Data for LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoichi Ishibashi, Taro Yano, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Scaling Law Apply in Time Series Forecasting? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyan Li, Libing Chen, Yin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid expansion of model size has emerged as a key challenge in time series
forecasting. From early Transformer with tens of megabytes to recent
architectures like TimesNet with thousands of megabytes, performance gains have
often come at the cost of exponentially increasing parameter counts. But is
this scaling truly necessary? To question the applicability of the scaling law
in time series forecasting, we propose Alinear, an ultra-lightweight
forecasting model that achieves competitive performance using only k-level
parameters. We introduce a horizon-aware adaptive decomposition mechanism that
dynamically rebalances component emphasis across different forecast lengths,
alongside a progressive frequency attenuation strategy that achieves stable
prediction in various forecasting horizons without incurring the computational
overhead of attention mechanisms. Extensive experiments on seven benchmark
datasets demonstrate that Alinear consistently outperforms large-scale models
while using less than 1% of their parameters, maintaining strong accuracy
across both short and ultra-long forecasting horizons. Moreover, to more fairly
evaluate model efficiency, we propose a new parameter-aware evaluation metric
that highlights the superiority of ALinear under constrained model budgets. Our
analysis reveals that the relative importance of trend and seasonal components
varies depending on data characteristics rather than following a fixed pattern,
validating the necessity of our adaptive design. This work challenges the
prevailing belief that larger models are inherently better and suggests a
paradigm shift toward more efficient time series modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Saliency <span class="highlight-title">Dataset</span> Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Kümmerer, Harneet Khanuja, Matthias Bethge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in image-based saliency prediction are approaching gold
standard performance levels on existing benchmarks. Despite this success, we
show that predicting fixations across multiple saliency datasets remains
challenging due to dataset bias. We find a significant performance drop (around
40%) when models trained on one dataset are applied to another. Surprisingly,
increasing dataset diversity does not resolve this inter-dataset gap, with
close to 60% attributed to dataset-specific biases. To address this remaining
generalization gap, we propose a novel architecture extending a mostly
dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific
parameters that govern interpretable mechanisms such as multi-scale structure,
center bias, and fixation spread. Adapting only these parameters to new data
accounts for more than 75% of the generalization gap, with a large fraction of
the improvement achieved with as few as 50 samples. Our model sets a new
state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark
(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from
unrelated datasets, but with a substantial boost when adapting to the
respective training datasets. The model also provides valuable insights into
spatial saliency properties, revealing complex multi-scale effects that combine
both absolute and relative sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QuXAI: Explainers for Hybrid Quantum Machine Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saikat Barua, Mostafizur Rahman, Shehenaz Khaled, Md Jafor Sadek, Rafiul Islam, Shahnewaz Siddique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
quantized feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures, 7 equations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Stage Top-$k$ Learning-to-Defer: Score-Based Surrogates with
  Theoretical Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannis Montreuil, Axel Carlier, Lai Xing Ng, Wei Tsang Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the first one-stage Top-$k$ Learning-to-Defer framework, which
unifies prediction and deferral by learning a shared score-based model that
selects the $k$ most cost-effective entities-labels or experts-per input. While
existing one-stage L2D methods are limited to deferring to a single expert, our
approach jointly optimizes prediction and deferral across multiple entities
through a single end-to-end objective. We define a cost-sensitive loss and
derive a novel convex surrogate that is independent of the cardinality
parameter $k$, enabling generalization across Top-$k$ regimes without
retraining. Our formulation recovers the Top-1 deferral policy of prior
score-based methods as a special case, and we prove that our surrogate is both
Bayes-consistent and $\mathcal{H}$-consistent under mild assumptions. We
further introduce an adaptive variant, Top-$k(x)$, which dynamically selects
the number of consulted entities per input to balance predictive accuracy and
consultation cost. Experiments on CIFAR-10 and SVHN confirm that our one-stage
Top-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves
superior accuracy-cost trade-offs by tailoring allocations to input complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Near Optimal Best Arm Identification for Clustered Bandits <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Yash, Nikhil Karamchandani, Avishek Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates the problem of best arm identification for multi-agent
multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where
each cluster solves a stochastic bandit problem. The mapping between agents and
bandits is a priori unknown. Each bandit is associated with $K$ arms, and the
goal is to identify the best arm for each agent under a $\delta$-probably
correct ($\delta$-PC) framework, while minimizing sample complexity and
communication overhead.
  We propose two novel algorithms: Clustering then Best Arm Identification
(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a
two-phase approach that first clusters agents based on the bandit problems they
are learning, followed by identifying the best arm for each cluster. BAI-Cl
reverses the sequence by identifying the best arms first and then clustering
agents accordingly. Both algorithms leverage the successive elimination
framework to ensure computational efficiency and high accuracy.
  We establish $\delta$-PC guarantees for both methods, derive bounds on their
sample complexity, and provide a lower bound for this problem class. Moreover,
when $M$ is small (a constant), we show that the sample complexity of a variant
of BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic
and real-world datasets (MovieLens, Yelp) demonstrate the superior performance
of the proposed algorithms in terms of sample and communication efficiency,
particularly in settings where $M \ll N$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Path Gradients after Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenz Vaitl, Leon Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Boltzmann Generators have emerged as a promising machine learning tool for
generating samples from equilibrium distributions of molecular systems using
Normalizing Flows and importance weighting. Recently, Flow Matching has helped
speed up Continuous Normalizing Flows (CNFs), scale them to more complex
molecular systems, and minimize the length of the flow integration
trajectories. We investigate the benefits of using path gradients to fine-tune
CNFs initially trained by Flow Matching, in the setting where a target energy
is known. Our experiments show that this hybrid approach yields up to a
threefold increase in sampling efficiency for molecular systems, all while
using the same model, a similar computational budget and without the need for
additional sampling. Furthermore, by measuring the length of the flow
trajectories during fine-tuning, we show that path gradients largely preserve
the learned structure of the flow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Wireless Localization Model (LWLM): A Foundation Model for
  Positioning in 6G Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangjin Pan, Kaixuan Huang, Hui Chen, Shunqing Zhang, Christian Häger, Henk Wymeersch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and robust localization is a critical enabler for emerging 5G and 6G
applications, including autonomous driving, extended reality (XR), and smart
manufacturing. While data-driven approaches have shown promise, most existing
models require large amounts of labeled data and struggle to generalize across
deployment scenarios and wireless configurations. To address these limitations,
we propose a foundation-model-based solution tailored for wireless
localization. We first analyze how different self-supervised learning (SSL)
tasks acquire general-purpose and task-specific semantic features based on
information bottleneck (IB) theory. Building on this foundation, we design a
pretraining methodology for the proposed Large Wireless Localization Model
(LWLM). Specifically, we propose an SSL framework that jointly optimizes three
complementary objectives: (i) spatial-frequency masked channel modeling
(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)
position-invariant contrastive learning (PICL). These objectives jointly
capture the underlying semantics of wireless channel from multiple
perspectives. We further design lightweight decoders for key downstream tasks,
including time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,
single base station (BS) localization, and multiple BS localization.
Comprehensive experimental results confirm that LWLM consistently surpasses
both model-based and supervised learning baselines across all localization
tasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer
models without pretraining, and exhibits strong generalization under
label-limited fine-tuning and unseen BS configurations, confirming its
potential as a foundation model for wireless localization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,16 figures.This work has been submitted to the IEEE for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Federated Learning on Edge Devices with Domain Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Q. Le, Latif U. Khan, Choong Seon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) allows collaborative training while ensuring data
privacy across distributed edge devices, making it a popular solution for
privacy-sensitive applications. However, FL faces significant challenges due to
statistical heterogeneity, particularly domain heterogeneity, which impedes the
global mode's convergence. In this study, we introduce a new framework to
address this challenge by improving the generalization ability of the FL global
model under domain heterogeneity, using prototype augmentation. Specifically,
we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a
prototype-based FL framework designed to enhance feature diversity and model
robustness. FedAPC leverages prototypes derived from the mean features of
augmented data to capture richer representations. By aligning local features
with global prototypes, we enable the model to learn meaningful semantic
features while reducing overfitting to any specific domain. Experimental
results on the Office-10 and Digits datasets illustrate that our framework
outperforms SOTA baselines, demonstrating superior performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWCMC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing the Performance of Global Model by Improving the Adaptability
  of Local Models in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wujun Zhou, Shu Ding, ZeLin Li, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning enables the clients to collaboratively train a global
model, which is aggregated from local models. Due to the heterogeneous data
distributions over clients and data privacy in federated learning, it is
difficult to train local models to achieve a well-performed global model. In
this paper, we introduce the adaptability of local models, i.e., the average
performance of local models on data distributions over clients, and enhance the
performance of the global model by improving the adaptability of local models.
Since each client does not know the data distributions over other clients, the
adaptability of the local model cannot be directly optimized. First, we provide
the property of an appropriate local model which has good adaptability on the
data distributions over clients. Then, we formalize the property into the local
training objective with a constraint and propose a feasible solution to train
the local model. Extensive experiments on federated learning benchmarks
demonstrate that our method significantly improves the adaptability of local
models and achieves a well-performed global model that consistently outperforms
the baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All You Need Is Synthetic Task Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Godin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Injecting rule-based models like Random Forests into differentiable neural
network frameworks remains an open challenge in machine learning. Recent
advancements have demonstrated that pretrained models can generate efficient
molecular embeddings. However, these approaches often require extensive
pretraining and additional techniques, such as incorporating posterior
probabilities, to boost performance. In our study, we propose a novel strategy
that jointly trains a single Graph Transformer neural network on both sparse
multitask molecular property experimental targets and synthetic targets derived
from XGBoost models trained on Osmordred molecular descriptors. These synthetic
tasks serve as independent auxiliary tasks. Our results show consistent and
significant performance improvement across all 19 molecular property prediction
tasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms
the XGBoost single-task learner. This demonstrates that synthetic task
augmentation is an effective method for enhancing neural model performance in
multitask molecular property prediction without the need for feature injection
or pretraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 Figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Virtual Machine Scheduling in Cloud Computing through Language
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JieHao Wu, Ziwei Wang, Junjie Sheng, Wenhao Li, Xiangfei Wang, Jun Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In cloud services, virtual machine (VM) scheduling is a typical Online
Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by
large-scale complexity and fluctuating demands. Traditional optimization
methods struggle to adapt to real-time changes, domain-expert-designed
heuristic approaches suffer from rigid strategies, and existing learning-based
methods often lack generalizability and interpretability. To address these
limitations, this paper proposes a hierarchical language agent framework named
MiCo, which provides a large language model (LLM)-driven heuristic design
paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov
Decision Process with Options (SMDP-Option), enabling dynamic scheduling
through a two-stage architecture, i.e., Option Miner and Option Composer.
Option Miner utilizes LLMs to discover diverse and useful non-context-aware
strategies by interacting with constructed environments. Option Composer
employs LLMs to discover a composing strategy that integrates the
non-context-aware strategies with the contextual ones. Extensive experiments on
real-world enterprise datasets demonstrate that MiCo achieves a 96.9\%
competitive ratio in large-scale scenarios involving more than 10,000 virtual
machines. It maintains high performance even under nonstationary request flows
and diverse configurations, thus validating its effectiveness in complex and
large-scale cloud environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Scalable Gradient-Based Optimization Framework for Sparse
  Minimum-Variance Portfolio Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarat Moka, Matias Quiroz, Vali Asimit, Samuel Muller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Portfolio optimization involves selecting asset weights to minimize a
risk-reward objective, such as the portfolio variance in the classical
minimum-variance framework. Sparse portfolio selection extends this by imposing
a cardinality constraint: only $k$ assets from a universe of $p$ may be
included. The standard approach models this problem as a mixed-integer
quadratic program and relies on commercial solvers to find the optimal
solution. However, the computational costs of such methods increase
exponentially with $k$ and $p$, making them too slow for problems of even
moderate size. We propose a fast and scalable gradient-based approach that
transforms the combinatorial sparse selection problem into a constrained
continuous optimization task via Boolean relaxation, while preserving
equivalence with the original problem on the set of binary points. Our
algorithm employs a tunable parameter that transmutes the auxiliary objective
from a convex to a concave function. This allows a stable convex starting
point, followed by a controlled path toward a sparse binary solution as the
tuning parameter increases and the objective moves toward concavity. In
practice, our method matches commercial solvers in asset selection for most
instances and, in rare instances, the solution differs by a few assets whilst
showing a negligible error in portfolio variance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChronoSteer: Bridging Large Language Model and Time Series Foundation
  Model via Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengsen Wang, Qi Qi, Zhongwen Rao, Lujia Pan, Jingyu Wang, Jianxin Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional forecasting methods rely on unimodal time series data, limiting
their ability to exploit rich textual information. Recently, large language
models (LLMs) and time series foundation models (TSFMs) have demonstrated
powerful capability in textual reasoning and temporal modeling, respectively.
Integrating the strengths of both to construct a multimodal model that
concurrently leverages both temporal and textual information for future
inference has emerged as a critical research challenge. To address the scarcity
of event-series paired data, we propose a decoupled framework: an LLM is
employed to transform textual events into revision instructions, which are then
used to steer the output of TSFM. To implement this framework, we introduce
ChronoSteer, a multimodal TSFM that can be steered through textual revision
instructions, effectively bridging LLM and TSFM. Moreover, to mitigate the
shortage of cross-modal instruction-series paired data, we devise a two-stage
training strategy based on synthetic data. In addition, we also construct a
high-quality multimodal time series forecasting benchmark to address the
information leakage concerns during evaluation. After integrating with an LLM,
ChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%
improvement in prediction accuracy compared to the unimodal backbone and a
22.5% gain over the previous state-of-the-art multimodal method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Role of scrambling and noise in temporal information processing with
  quantum systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Xiong, Zoë Holmes, Armando Angrisani, Yudai Suzuki, Thiparat Chotibut, Supanut Thanasilp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scrambling quantum systems have been demonstrated as effective substrates for
temporal information processing. While their role in providing rich feature
maps has been widely studied, a theoretical understanding of their performance
in temporal tasks is still lacking. Here we consider a general quantum
reservoir processing framework that captures a broad range of physical
computing models with quantum systems. We examine the scalability and memory
retention of the model with scrambling reservoirs modelled by high-order
unitary designs in both noiseless and noisy settings. In the former regime, we
show that measurement readouts become exponentially concentrated with
increasing reservoir size, yet strikingly do not worsen with the reservoir
iterations. Thus, while repeatedly reusing a small scrambling reservoir with
quantum data might be viable, scaling up the problem size deteriorates
generalization unless one can afford an exponential shot overhead. In contrast,
the memory of early inputs and initial states decays exponentially in both
reservoir size and reservoir iterations. In the noisy regime, we also prove
exponential memory decays with iterations for local noisy channels. Proving
these results required us to introduce new proof techniques for bounding
concentration in temporal quantum learning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14+35 pages, 6+5 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dark LLMs: The Growing Threat of Unaligned AI Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Fire, Yitzhak Elbazis, Adi Wasenstein, Lior Rokach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) rapidly reshape modern life, advancing fields
from healthcare to education and beyond. However, alongside their remarkable
capabilities lies a significant threat: the susceptibility of these models to
jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems
from the very data they learn from. As long as this training data includes
unfiltered, problematic, or 'dark' content, the models can inherently learn
undesirable patterns or weaknesses that allow users to circumvent their
intended safety controls. Our research identifies the growing threat posed by
dark LLMs models deliberately designed without ethical guardrails or modified
through jailbreak techniques. In our research, we uncovered a universal
jailbreak attack that effectively compromises multiple state-of-the-art models,
enabling them to answer almost any question and produce harmful outputs upon
request. The main idea of our attack was published online over seven months
ago. However, many of the tested LLMs were still vulnerable to this attack.
Despite our responsible disclosure efforts, responses from major LLM providers
were often inadequate, highlighting a concerning gap in industry practices
regarding AI safety. As model training becomes more accessible and cheaper, and
as open-source LLMs proliferate, the risk of widespread misuse escalates.
Without decisive intervention, LLMs may continue democratizing access to
dangerous knowledge, posing greater risks than anticipated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JointDistill: Adaptive Multi-Task Distillation for Joint Depth
  Estimation and Scene Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiancong Cheng, Ying Zhang, Yuxuan Liang, Roger Zimmermann, Zhiwen Yu, Bin Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation and scene segmentation are two important tasks in
intelligent transportation systems. A joint modeling of these two tasks will
reduce the requirement for both the storage and training efforts. This work
explores how the multi-task distillation could be used to improve such unified
modeling. While existing solutions transfer multiple teachers' knowledge in a
static way, we propose a self-adaptive distillation method that can dynamically
adjust the knowledge amount from each teacher according to the student's
current learning ability. Furthermore, as multiple teachers exist, the
student's gradient update direction in the distillation is more prone to be
erroneous where knowledge forgetting may occur. To avoid this, we propose a
knowledge trajectory to record the most essential information that a model has
learnt in the past, based on which a trajectory-based distillation loss is
designed to guide the student to follow the learning curve similarly in a
cost-effective way. We evaluate our method on multiple benchmarking datasets
including Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,
our method achieves a clearly improvement. The code is provided in the
supplementary materials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Financial Fraud Detection Using Explainable AI and Stacking Ensemble
  Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fahad Almalki, Mehedi Masud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional machine learning models often prioritize predictive accuracy,
often at the expense of model transparency and interpretability. The lack of
transparency makes it difficult for organizations to comply with regulatory
requirements and gain stakeholders trust. In this research, we propose a fraud
detection framework that combines a stacking ensemble of well-known gradient
boosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable
artificial intelligence (XAI) techniques are used to enhance the transparency
and interpretability of the model's decisions. We used SHAP (SHapley Additive
Explanations) for feature selection to identify the most important features.
Further efforts were made to explain the model's predictions using Local
Interpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots
(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection
dataset, which includes more than 590,000 real transaction records, was used to
evaluate the proposed model. The model achieved a high performance with an
accuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent
related approaches. These results indicate that combining high prediction
accuracy with transparent interpretability is possible and could lead to a more
ethical and trustworthy solution in financial fraud detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Song, Jiaxing Li, Shihan Guan, Youyong Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their
capacity to preserve previously acquired knowledge amid the assimilation of
novel information. Rehearsal-based techniques revisit historical examples,
adopted as a principal strategy to alleviate this phenomenon. However, memory
explosion and privacy infringements impose significant constraints on their
utility. Non-Exemplar methods circumvent the prior issues through Prototype
Replay (PR), yet feature drift presents new challenges. In this paper, our
empirical findings reveal that Prototype Contrastive Learning (PCL) exhibits
less pronounced drift than conventional PR. Drawing upon PCL, we propose
Instance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar
Continual Graph Learning (NECGL). Exploiting graph structural information, we
formulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature
distributions towards high-impact nodes to augment the model's capacity for
assimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)
safeguards task memory by regularizing discontinuities in class relationships.
Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,
fostering greater inter-class discriminability. Evaluations on four node
classification benchmark datasets demonstrate that our method outperforms
existing state-of-the-art methods, achieving a better trade-off between
plasticity and stability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER
  Gates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Chen, Jiaying Zhu, Xinyu Yang, Wenya Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Circuit discovery has gradually become one of the prominent methods for
mechanistic interpretability, and research on circuit completeness has also
garnered increasing attention. Methods of circuit discovery that do not
guarantee completeness not only result in circuits that are not fixed across
different runs but also cause key mechanisms to be omitted. The nature of
incompleteness arises from the presence of OR gates within the circuit, which
are often only partially detected in standard circuit discovery methods. To
this end, we systematically introduce three types of logic gates: AND, OR, and
ADDER gates, and decompose the circuit into combinations of these logical
gates. Through the concept of these gates, we derive the minimum requirements
necessary to achieve faithfulness and completeness. Furthermore, we propose a
framework that combines noising-based and denoising-based interventions, which
can be easily integrated into existing circuit discovery methods without
significantly increasing computational complexity. This framework is capable of
fully identifying the logic gates and distinguishing them within the circuit.
In addition to the extensive experimental validation of the framework's ability
to restore the faithfulness, completeness, and sparsity of circuits, using this
framework, we uncover fundamental properties of the three logic gates, such as
their proportions and contributions to the output, and explore how they behave
among the functionalities of language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal normalization in quantum-classical hybrid models for anti-cancer
  drug response prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takafumi Ito, Lysenko Artem, Tatsuhiko Tsunoda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum-classical Hybrid Machine Learning (QHML) models are recognized for
their robust performance and high generalization ability even for relatively
small datasets. These qualities offer unique advantages for anti-cancer drug
response prediction, where the number of available samples is typically small.
However, such hybrid models appear to be very sensitive to the data encoding
used at the interface of a neural network and a quantum circuit, with
suboptimal choices leading to stability issues. To address this problem, we
propose a novel strategy that uses a normalization function based on a
moderated gradient version of the $\tanh$. This method transforms the outputs
of the neural networks without concentrating them at the extreme value ranges.
Our idea was evaluated on a dataset of gene expression and drug response
measurements for various cancer cell lines, where we compared the prediction
performance of a classical deep learning model and several QHML models. These
results confirmed that QHML performed better than the classical models when
data was optimally normalized. This study opens up new possibilities for
biomedical data analysis using quantum computers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Robustness of Deep Reinforcement Learning for Autonomous
  Surface Vehicle Control in Field Tests <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis F. W. Batista, Stéphanie Aravecchia, Seth Hutchinson, Cédric Pradalier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in Deep Reinforcement Learning (DRL) for
Autonomous Surface Vehicles (ASVs), their robustness in real-world conditions,
particularly under external disturbances, remains insufficiently explored. In
this paper, we evaluate the resilience of a DRL-based agent designed to capture
floating waste under various perturbations. We train the agent using domain
randomization and evaluate its performance in real-world field tests, assessing
its ability to handle unexpected disturbances such as asymmetric drag and an
off-center payload. We assess the agent's performance under these perturbations
in both simulation and real-world experiments, quantifying performance
degradation and benchmarking it against an MPC baseline. Results indicate that
the DRL agent performs reliably despite significant disturbances. Along with
the open-source release of our implementation, we provide insights into
effective training strategies, real-world challenges, and practical
considerations for deploying DRLbased ASV controllers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Field Robotics at ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection
  of Diseases in Cocos nucifera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miit Daga, Dhriti Parikh, Swarna Priya Ramu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coconut tree diseases are a serious risk to agricultural yield, particularly
in developing countries where conventional farming practices restrict early
diagnosis and intervention. Current disease identification methods are manual,
labor-intensive, and non-scalable. In response to these limitations, we come up
with DeepSeqCoco, a deep learning based model for accurate and automatic
disease identification from coconut tree images. The model was tested under
various optimizer settings, such as SGD, Adam, and hybrid configurations, to
identify the optimal balance between accuracy, minimization of loss, and
computational cost. Results from experiments indicate that DeepSeqCoco can
achieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than
existing models) with the hybrid SGD-Adam showing the lowest validation loss of
2.81%. It also shows a drop of up to 18% in training time and up to 85% in
prediction time for input images. The results point out the promise of the
model to improve precision agriculture through an AI-based, scalable, and
efficient disease monitoring system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted for publication in IEEE Access journal and is
  currently pending revisions before publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImagineBench: Evaluating Reinforcement Learning with Large Language
  Model Rollouts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing-Cheng Pang, Kaiyuan Li, Yidi Wang, Si-Hang Yang, Shengyi Jiang, Yang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central challenge in reinforcement learning (RL) is its dependence on
extensive real-world interaction data to learn task-specific policies. While
recent work demonstrates that large language models (LLMs) can mitigate this
limitation by generating synthetic experience (noted as imaginary rollouts) for
mastering novel tasks, progress in this emerging field is hindered due to the
lack of a standard benchmark. To bridge this gap, we introduce ImagineBench,
the first comprehensive benchmark for evaluating offline RL algorithms that
leverage both real rollouts and LLM-imaginary rollouts. The key features of
ImagineBench include: (1) datasets comprising environment-collected and
LLM-imaginary rollouts; (2) diverse domains of environments covering
locomotion, robotic manipulation, and navigation tasks; and (3) natural
language task instructions with varying complexity levels to facilitate
language-conditioned policy learning. Through systematic evaluation of
state-of-the-art offline RL algorithms, we observe that simply applying
existing offline RL algorithms leads to suboptimal performance on unseen tasks,
achieving 35.44% success rate in hard tasks in contrast to 64.37% of method
training on real rollouts for hard tasks. This result highlights the need for
algorithm advancements to better leverage LLM-imaginary rollouts. Additionally,
we identify key opportunities for future research: including better utilization
of imaginary rollouts, fast online adaptation and continual learning, and
extension to multi-modal tasks. Our code is publicly available at
https://github.com/LAMDA-RL/ImagineBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample Complexity of Distributionally Robust Average-Reward
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Chen, Shengbo Wang, Nian Si
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by practical applications where stable long-term performance is
critical-such as robotics, operations research, and healthcare-we study the
problem of distributionally robust (DR) average-reward reinforcement learning.
We propose two algorithms that achieve near-optimal sample complexity. The
first reduces the problem to a DR discounted Markov decision process (MDP),
while the second, Anchored DR Average-Reward MDP, introduces an anchoring state
to stabilize the controlled transition kernels within the uncertainty set.
Assuming the nominal MDP is uniformly ergodic, we prove that both algorithms
attain a sample complexity of $\widetilde{O}\left(|\mathbf{S}||\mathbf{A}|
t_{\mathrm{mix}}^2\varepsilon^{-2}\right)$ for estimating the optimal policy as
well as the robust average reward under KL and $f_k$-divergence-based
uncertainty sets, provided the uncertainty radius is sufficiently small. Here,
$\varepsilon$ is the target accuracy, $|\mathbf{S}|$ and $|\mathbf{A}|$ denote
the sizes of the state and action spaces, and $t_{\mathrm{mix}}$ is the mixing
time of the nominal MDP. This represents the first finite-sample convergence
guarantee for DR average-reward reinforcement learning. We further validate the
convergence rates of our algorithms through numerical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom
  Domain Large Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Jiao, Zhuoran Xiao, Yihang Huang, Chenhui Ye, Yijia Feng, Liyu Cai, Jiang Chang, Fangkun Liu, Yin Xu, Dazhi He, Yunfeng Guan, Wenjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing a 6G-oriented universal model capable of processing multi-modal
data and executing diverse air interface tasks has emerged as a common goal in
future wireless systems. Building on our prior work in communication
multi-modal alignment and telecom large language model (LLM), we propose a
scalable, task-aware artificial intelligence-air interface multi-modal
universal model (AI2MMUM), which flexibility and effectively perform various
physical layer tasks according to subtle task instructions. The LLM backbone
provides robust contextual comprehension and generalization capabilities, while
a fine-tuning approach is adopted to incorporate domain-specific knowledge. To
enhance task adaptability, task instructions consist of fixed task keywords and
learnable, implicit prefix prompts. Frozen radio modality encoders extract
universal representations and adapter layers subsequently bridge radio and
language modalities. Moreover, lightweight task-specific heads are designed to
directly output task objectives. Comprehensive evaluations demonstrate that
AI2MMUM achieves SOTA performance across five representative physical
environment/wireless channel-based downstream tasks using the WAIR-D and
DeepMIMO datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sybil-based Virtual Data Poisoning Attacks in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changxun Zhu, Qilong Wu, Lingjuan Lyu, Shibei Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is vulnerable to poisoning attacks by malicious
adversaries. Existing methods often involve high costs to achieve effective
attacks. To address this challenge, we propose a sybil-based virtual data
poisoning attack, where a malicious client generates sybil nodes to amplify the
poisoning model's impact. To reduce neural network computational complexity, we
develop a virtual data generation method based on gradient matching. We also
design three schemes for target model acquisition, applicable to online local,
online global, and offline scenarios. In simulation, our method outperforms
other attack algorithms since our method can obtain a global target model under
non-independent uniformly distributed data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, accepted by IEEE Codit 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Who Said What WSW 2.0? Enhanced Automated Analysis of Preschool
  Classroom Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anchen Sun, Tiantian Feng, Gabriela Gutierrez, Juan J Londono, Anfeng Xu, Batya Elbaum, Shrikanth Narayanan, Lynn K Perry, Daniel S Messinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an automated framework WSW2.0 for analyzing vocal
interactions in preschool classrooms, enhancing both accuracy and scalability
through the integration of wav2vec2-based speaker classification and Whisper
(large-v2 and large-v3) speech transcription. A total of 235 minutes of audio
recordings (160 minutes from 12 children and 75 minutes from 5 teachers), were
used to compare system outputs to expert human annotations. WSW2.0 achieves a
weighted F1 score of .845, accuracy of .846, and an error-corrected kappa of
.672 for speaker classification (child vs. teacher). Transcription quality is
moderate to high with word error rates of .119 for teachers and .238 for
children. WSW2.0 exhibits relatively high absolute agreement intraclass
correlations (ICC) with expert transcriptions for a range of classroom language
features. These include teacher and child mean utterance length, lexical
diversity, question asking, and responses to questions and other utterances,
which show absolute agreement intraclass correlations between .64 and .98. To
establish scalability, we apply the framework to an extensive dataset spanning
two years and over 1,592 hours of classroom audio recordings, demonstrating the
framework's robustness for broad real-world applications. These findings
highlight the potential of deep learning and natural language processing
techniques to revolutionize educational research by providing accurate measures
of key features of preschool classroom speech, ultimately guiding more
effective intervention strategies and supporting early childhood language
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Machine Learning Framework for Heart Disease Prediction:
  Performance Evaluation and Future Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Azimi Lamir, Shiva Razzagzadeh, Zeynab Rezaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a machine learning-based framework for heart disease
prediction using the heart-disease dataset, comprising 303 samples with 14
features. The methodology involves data preprocessing, model training, and
evaluation using three classifiers: Logistic Regression, K-Nearest Neighbors
(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and
RandomizedSearchCV was employed to enhance model performance. The Random Forest
classifier outperformed other models, achieving an accuracy of 91% and an
F1-score of 0.89. Evaluation metrics, including precision, recall, and
confusion matrix, revealed balanced performance across classes. The proposed
model demonstrates strong potential for aiding clinical decision-making by
effectively predicting heart disease. Limitations such as dataset size and
generalizability underscore the need for future studies using larger and more
diverse datasets. This work highlights the utility of machine learning in
healthcare, offering insights for further advancements in predictive
diagnostics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARR: Question Answering with Large Language Models via Analyzing,
  Retrieving, and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04689v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04689v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Yin, Giuseppe Carenini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive capabilities on
complex evaluation benchmarks, many of which are formulated as
question-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts
is becoming increasingly vital for advancing their development and
applicability. This paper introduces ARR, an intuitive, effective, and general
QA solving method that explicitly incorporates three key steps: analyzing the
intent of the question, retrieving relevant information, and reasoning step by
step. Notably, this paper is the first to introduce intent analysis in QA,
which plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA
tasks demonstrate that ARR consistently outperforms the baseline methods.
Ablation and case studies further validate the positive contributions of each
ARR component. Furthermore, experiments involving variations in prompt design
indicate that ARR maintains its effectiveness regardless of the specific prompt
formulation. Additionally, extensive evaluations across various model sizes,
LLM series, and generation settings solidify the effectiveness, robustness, and
generalizability of ARR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages. Code: https://github.com/YuweiYin/ARR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lightspeed Geometric <span class="highlight-title">Dataset</span> Distance via Sliced Optimal Transport <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khai Nguyen, Hai Nguyen, Tuan Pham, Nhat Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce sliced optimal transport dataset distance (s-OTDD), a
model-agnostic, embedding-agnostic approach for dataset comparison that
requires no training, is robust to variations in the number of classes, and can
handle disjoint label sets. The core innovation is Moment Transform Projection
(MTP), which maps a label, represented as a distribution over features, to a
real number. Using MTP, we derive a data point projection that transforms
datasets into one-dimensional distributions. The s-OTDD is defined as the
expected Wasserstein distance between the projected distributions, with respect
to random projection parameters. Leveraging the closed form solution of
one-dimensional optimal transport, s-OTDD achieves (near-)linear computational
complexity in the number of data points and feature dimensions and is
independent of the number of classes. With its geometrically meaningful
projection, s-OTDD strongly correlates with the optimal transport dataset
distance while being more efficient than existing dataset discrepancy measures.
Moreover, it correlates well with the performance gap in transfer learning and
classification accuracy in data augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025, 16 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A portable diagnosis model for Keratoconus using a smartphone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Li, Peter Ho, Jo Woon Chong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keratoconus (KC) is a corneal disorder that results in blurry and distorted
vision. Traditional diagnostic tools, while effective, are often bulky, costly,
and require professional operation. In this paper, we present a portable and
innovative methodology for diagnosing. Our proposed approach first captures the
image reflected on the eye's cornea when a smartphone screen-generated Placido
disc sheds its light on an eye, then utilizes a two-stage diagnosis for
identifying the KC cornea and pinpointing the location of the KC on the cornea.
The first stage estimates the height and width of the Placido disc extracted
from the captured image to identify whether it has KC. In this KC
identification, k-means clustering is implemented to discern statistical
characteristics, such as height and width values of extracted Placido discs,
from non-KC (control) and KC-affected groups. The second stage involves the
creation of a distance matrix, providing a precise localization of KC on the
cornea, which is critical for efficient treatment planning. The analysis of
these distance matrices, paired with a logistic regression model and robust
statistical analysis, reveals a clear distinction between control and KC
groups. The logistic regression model, which classifies small areas on the
cornea as either control or KC-affected based on the corresponding inter-disc
distances in the distance matrix, reported a classification accuracy of 96.94%,
which indicates that we can effectively pinpoint the protrusion caused by KC.
This comprehensive, smartphone-based method is expected to detect KC and
streamline timely treatment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Learning-Driven Inhalation Injury Grading Assistant Using
  Bronchoscopy Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Li, Alan W Pang, Jo Woon Chong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inhalation injuries present a challenge in clinical diagnosis and grading due
to Conventional grading methods such as the Abbreviated Injury Score (AIS)
being subjective and lacking robust correlation with clinical parameters like
mechanical ventilation duration and patient mortality. This study introduces a
novel deep learning-based diagnosis assistant tool for grading inhalation
injuries using bronchoscopy images to overcome subjective variability and
enhance consistency in severity assessment. Our approach leverages data
augmentation techniques, including graphic transformations, Contrastive
Unpaired Translation (CUT), and CycleGAN, to address the scarcity of medical
imaging data. We evaluate the classification performance of two deep learning
models, GoogLeNet and Vision Transformer (ViT), across a dataset significantly
expanded through these augmentation methods. The results demonstrate GoogLeNet
combined with CUT as the most effective configuration for grading inhalation
injuries through bronchoscopy images and achieves a classification accuracy of
97.8%. The histograms and frequency analysis evaluations reveal variations
caused by the augmentation CUT with distribution changes in the histogram and
texture details of the frequency spectrum. PCA visualizations underscore the
CUT substantially enhances class separability in the feature space. Moreover,
Grad-CAM analyses provide insight into the decision-making process; mean
intensity for CUT heatmaps is 119.6, which significantly exceeds 98.8 of the
original datasets. Our proposed tool leverages mechanical ventilation periods
as a novel grading standard, providing comprehensive diagnostic support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An unsupervised method for MRI recovery: Deep image prior with
  structured sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ahmad Sultan, Chong Chen, Yingmin Liu, Katarzyna Gil, Karolina Zareba, Rizwan Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: To propose and validate an unsupervised MRI reconstruction method
that does not require fully sampled k-space data. Materials and Methods: The
proposed method, deep image prior with structured sparsity (DISCUS), extends
the deep image prior (DIP) by introducing group sparsity to frame-specific code
vectors, enabling the discovery of a low-dimensional manifold for capturing
temporal variations. \discus was validated using four studies: (I) simulation
of a dynamic Shepp-Logan phantom to demonstrate its manifold discovery
capabilities, (II) comparison with compressed sensing and DIP-based methods
using simulated single-shot late gadolinium enhancement (LGE) image series from
six distinct digital cardiac phantoms in terms of normalized mean square error
(NMSE) and structural similarity index measure (SSIM), (III) evaluation on
retrospectively undersampled single-shot LGE data from eight patients, and (IV)
evaluation on prospectively undersampled single-shot LGE data from eight
patients, assessed via blind scoring from two expert readers. Results: DISCUS
outperformed competing methods, demonstrating superior reconstruction quality
in terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study
IV). Discussion: An unsupervised image reconstruction method is presented and
validated on simulated and measured data. These developments can benefit
applications where acquiring fully sampled data is challenging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Magn Reson Mater Phy (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Federated Learning under Model Dissimilarity Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07575v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07575v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Erickson, Mikael Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the defining challenges in federated learning is that of statistical
heterogeneity among clients. We address this problem with KARULA, a regularized
strategy for personalized federated learning, which constrains the pairwise
model dissimilarities between clients based on the difference in their
distributions, as measured by a surrogate for the 1-Wasserstein distance
adapted for the federated setting. This allows the strategy to adapt to highly
complex interrelations between clients, that e.g., clustered approaches fail to
capture. We propose an inexact projected stochastic gradient algorithm to solve
the constrained problem that the strategy defines, and show theoretically that
it converges with smooth, possibly non-convex losses to a neighborhood of a
stationary point with rate O(1/K). We demonstrate the effectiveness of KARULA
on synthetic and real federated data sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SceneGenAgent: Precise Industrial Scene Generation with Coding Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xia, Dan Zhang, Zibo Liao, Zhenyu Hou, Tianrui Sun, Jing Li, Ling Fu, Yuxiao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The modeling of industrial scenes is essential for simulations in industrial
manufacturing. While large language models (LLMs) have shown significant
progress in generating general 3D scenes from textual descriptions, generating
industrial scenes with LLMs poses a unique challenge due to their demand for
precise measurements and positioning, requiring complex planning over spatial
arrangement. To address this challenge, we introduce SceneGenAgent, an
LLM-based agent for generating industrial scenes through C# code. SceneGenAgent
ensures precise layout planning through a structured and calculable format,
layout verification, and iterative refinement to meet the quantitative
requirements of industrial scenarios. Experiment results demonstrate that LLMs
powered by SceneGenAgent exceed their original performance, reaching up to
81.0% success rate in real-world industrial scene generation tasks and
effectively meeting most scene generation requirements. To further enhance
accessibility, we construct SceneInstruct, a dataset designed for fine-tuning
open-source LLMs to integrate into SceneGenAgent. Experiments show that
fine-tuning open-source LLMs on SceneInstruct yields significant performance
improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our
code and data are available at https://github.com/THUDM/SceneGenAgent .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Modeling Language Code Generation from Diagram Images Using
  Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Averi Bates, Ryan Vavricka, Shane Carleton, Ruosi Shao, Chongle Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Unified Modeling Language is a standardized visual language widely used
for modeling and documenting the design of software systems. Although many
tools generate UML diagrams from UML code, generating executable UML code from
image-based UML diagrams remains challenging. This paper proposes a new
approach to generate UML code using a large multimodal language model
automatically. Synthetic UML activity and sequence diagram datasets were
created to train and test the model. We compared standard fine-tuning with LoRA
techniques to optimize base models. The experiments measured code generation
accuracy across different model sizes and training strategies. These results
demonstrated that domain-adapted MM-LLMs perform for UML code generation
automation, whereby, at the best model, it achieved BLEU and SSIM scores of
0.779 and 0.942 on sequence diagrams. This will enable the modernization of
legacy systems and decrease the manual effort in software development
workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the Journal of Machine Learning with Applications,
  Author Contributions: Averi Bates: Methodology, Development, Analysis, Data
  Curation, Drafting, Review. Ryan Vavricka: Data Curation, Development,
  Review. Shane Carleton: Supervision, Funding. Ruosi Shao: Review. Chongle
  Pan: Supervision, Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Driven Calibration of Prediction Sets in Large Vision-Language
  Models Based on Inductive Conformal Prediction <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17671v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17671v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchang Ye, Weiyan Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the critical challenge of hallucination mitigation in
Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks
through a Split Conformal Prediction (SCP) framework. While LVLMs excel in
multi-modal reasoning, their outputs often exhibit hallucinated content with
high confidence, posing risks in safety-critical applications. We propose a
model-agnostic uncertainty quantification method that integrates dynamic
threshold calibration and cross-modal consistency verification. By partitioning
data into calibration and test sets, the framework computes nonconformity
scores to construct prediction sets with statistical guarantees under
user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous
control of \textbf{marginal coverage} to ensure empirical error rates remain
strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes
inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of
prior distribution assumptions and retraining requirements. Evaluations on
benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces
theoretical guarantees across all $\alpha$ values. The framework achieves
stable performance across varying calibration-to-test split ratios,
underscoring its robustness for real-world deployment in healthcare, autonomous
systems, and other safety-sensitive domains. This work bridges the gap between
theoretical reliability and practical applicability in multi-modal AI systems,
offering a scalable solution for hallucination detection and uncertainty-aware
decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICIPCA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Objective Optimization-Based Anonymization of Structured Data for
  Machine Learning Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01002v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01002v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusi Wei, Hande Y. Benson, Joseph K. Agor, Muge Capan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Organizations are collecting vast amounts of data, but they often lack the
capabilities needed to fully extract insights. As a result, they increasingly
share data with external experts, such as analysts or researchers, to gain
value from it. However, this practice introduces significant privacy risks.
Various techniques have been proposed to address privacy concerns in data
sharing. However, these methods often degrade data utility, impacting the
performance of machine learning (ML) models. Our research identifies key
limitations in existing optimization models for privacy preservation,
particularly in handling categorical variables, and evaluating effectiveness
across diverse datasets. We propose a novel multi-objective optimization model
that simultaneously minimizes information loss and maximizes protection against
attacks. This model is empirically validated using diverse datasets and
compared with two existing algorithms. We assess information loss, the number
of individuals subject to linkage or homogeneity attacks, and ML performance
after anonymization. The results indicate that our model achieves lower
information loss and more effectively mitigates the risk of attacks, reducing
the number of individuals susceptible to these attacks compared to alternative
algorithms in some cases. Additionally, our model maintains comparable ML
performance relative to the original data or data anonymized by other methods.
Our findings highlight significant improvements in privacy protection and ML
model performance, offering a comprehensive and extensible framework for
balancing privacy and utility in data sharing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can AI weather models predict out-of-distribution gray swan tropical
  cyclones? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14932v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14932v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Y. Qiang Sun, Pedram Hassanzadeh, Mohsen Zand, Ashesh Chattopadhyay, Jonathan Weare, Dorian S. Abbot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting gray swan weather extremes, which are possible but so rare that
they are absent from the training dataset, is a major concern for AI weather
models and long-term climate emulators. An important open question is whether
AI models can extrapolate from weaker weather events present in the training
set to stronger, unseen weather extremes. To test this, we train independent
versions of the AI model FourCastNet on the 1979-2015 ERA5 dataset with all
data, or with Category 3-5 tropical cyclones (TCs) removed, either globally or
only over the North Atlantic or Western Pacific basin. We then test these
versions of FourCastNet on 2018-2023 Category 5 TCs (gray swans). All versions
yield similar accuracy for global weather, but the one trained without Category
3-5 TCs cannot accurately forecast Category 5 TCs, indicating that these models
cannot extrapolate from weaker storms. The versions trained without Category
3-5 TCs in one basin show some skill forecasting Category 5 TCs in that basin,
suggesting that FourCastNet can generalize across tropical basins. This is
encouraging and surprising because regional information is implicitly encoded
in inputs. Given that current state-of-the-art AI weather and climate models
have similar learning strategies, we expect our findings to apply to other
models. Other types of weather extremes need to be similarly investigated. Our
work demonstrates that novel learning strategies are needed for AI models to
reliably provide early warning or estimated statistics for the rarest, most
impactful TCs, and, possibly, other weather extremes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Graph Representation of Agent Diffusers <span class="chip">AAMAS2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youcef Djenouri, Nassim Belmecheri, Tomasz Michalak, Jan Dubiński, Ahmed Nabil Belbachir, Anis Yazidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based generative models have significantly advanced text-to-image
synthesis, demonstrating impressive text comprehension and zero-shot
generalization. These models refine images from random noise based on textual
prompts, with initial reliance on text input shifting towards enhanced visual
fidelity over time. This transition suggests that static model parameters might
not optimally address the distinct phases of generation. We introduce LGR-AD
(Learning Graph Representation of Agent Diffusers), a novel multi-agent system
designed to improve adaptability in dynamic computer vision tasks. LGR-AD
models the generation process as a distributed system of interacting agents,
each representing an expert sub-model. These agents dynamically adapt to
varying conditions and collaborate through a graph neural network that encodes
their relationships and performance metrics. Our approach employs a
coordination mechanism based on top-$k$ maximum spanning trees, optimizing the
generation process. Each agent's decision-making is guided by a meta-model that
minimizes a novel loss function, balancing accuracy and diversity. Theoretical
analysis and extensive empirical evaluations show that LGR-AD outperforms
traditional diffusion models across various benchmarks, highlighting its
potential for scalable and flexible solutions in complex image generation
tasks. Code is available at: https://github.com/YousIA/LGR_AD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAMAS2025 International Conference on Autonomous Agents
  and Multiagent Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unitless Unrestricted Markov-Consistent SCM Generation: Better Benchmark
  <span class="highlight-title">Dataset</span>s for Causal Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17037v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17037v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rebecca J. Herman, Jonas Wahl, Urmi Ninad, Jakob Runge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal discovery aims to extract qualitative causal knowledge in the form of
causal graphs from data. Because causal ground truth is rarely known in the
real world, simulated data plays a vital role in evaluating the performance of
the various causal discovery algorithms proposed in the literature. But recent
work highlighted certain artifacts of commonly used data generation techniques
for a standard class of structural causal models (SCM) that may be nonphysical,
including var- and R2-sortability, where the variables' variance and
coefficients of determination (R2) after regressing on all other variables,
respectively, increase along the causal order. Some causal methods exploit such
artifacts, leading to unrealistic expectations for their performance on
real-world data. Some modifications have been proposed to remove these
artifacts; notably, the internally-standardized structural causal model (iSCM)
avoids varsortability and largely alleviates R2-sortability on sparse causal
graphs, but exhibits a reversed R2-sortability pattern for denser graphs not
featured in their work. We analyze which sortability patterns we expect to see
in real data, and propose a method for drawing coefficients that we argue more
effectively samples the space of SCMs. Finally, we propose a novel extension of
our SCM generation method to the time series setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4th Conference on Causal Learning and Reasoning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Double Successive Over-Relaxation Q-Learning with an Extension to Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyas S R
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Q-learning is a widely used algorithm in reinforcement learning (RL), but its
convergence can be slow, especially when the discount factor is close to one.
Successive Over-Relaxation (SOR) Q-learning, which introduces a relaxation
factor to speed up convergence, addresses this issue but has two major
limitations: In the tabular setting, the relaxation parameter depends on
transition probability, making it not entirely model-free, and it suffers from
overestimation bias. To overcome these limitations, we propose a sample-based,
model-free double SOR Q-learning algorithm. Theoretically and empirically, this
algorithm is shown to be less biased than SOR Q-learning. Further, in the
tabular setting, the convergence analysis under boundedness assumptions on
iterates is discussed. The proposed algorithm is extended to large-scale
problems using deep RL. Finally, the tabular version of the proposed algorithm
is compared using roulette and grid world environments, while the deep RL
version is tested on a maximization bias example and OpenAI Gym environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Harris, Fan Grayson, Felix Feldman, Timothy Laurence, Toby Nonnenmacher, Oliver Higgins, Leo Loman, Selina Patel, Thomas Finnie, Samuel Collins, Michael Borowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) become widely accessible, a detailed
understanding of their knowledge within specific domains becomes necessary for
successful real world use. This is particularly critical in public health,
where failure to retrieve relevant, accurate, and current information could
significantly impact UK residents. However, currently little is known about LLM
knowledge of UK Government public health information. To address this issue,
this paper introduces a new benchmark, PubHealthBench, with over 8000 questions
for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form
responses to public health queries. To create PubHealthBench we extract free
text from 687 current UK government guidance documents and implement an
automated pipeline for generating MCQA samples. Assessing 24 LLMs on
PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a
high degree of knowledge, achieving >90% accuracy in the MCQA setup, and
outperform humans with cursory search engine use. However, in the free form
setup we see lower performance with no model scoring >75%. Importantly we find
in both setups LLMs have higher accuracy on guidance intended for the general
public. Therefore, there are promising signs that state of the art (SOTA) LLMs
are an increasingly accurate source of public health information, but
additional safeguards or tools may still be needed when providing free form
responses on public health topics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 pages main text</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge
  Distillation via $α$-$β$-Divergence <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.04560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.04560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghui Wang, Zhiyong Yang, Zitai Wang, Shi Wang, Qianqian Xu, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Distillation (KD) transfers knowledge from a large teacher model to
a smaller student model by minimizing the divergence between their output
distributions, typically using forward Kullback-Leibler divergence (FKLD) or
reverse KLD (RKLD). It has become an effective training paradigm due to the
broader supervision information provided by the teacher distribution compared
to one-hot labels. We identify that the core challenge in KD lies in balancing
two mode-concentration effects: the \textbf{\textit{Hardness-Concentration}}
effect, which refers to focusing on modes with large errors, and the
\textbf{\textit{Confidence-Concentration}} effect, which refers to focusing on
modes with high student confidence. Through an analysis of how probabilities
are reassigned during gradient updates, we observe that these two effects are
entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too
weak in FKLD, causing the student to fail to concentrate on the target class.
In contrast, both are too strong in RKLD, causing the student to overly
emphasize the target class while ignoring the broader distributional
information from the teacher. To address this imbalance, we propose ABKD, a
generic framework with $\alpha$-$\beta$-divergence. Our theoretical results
show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving
an effective trade-off between these effects. Extensive experiments on 17
language/vision datasets with 12 teacher-student settings confirm its efficacy.
The code is available at https://github.com/ghwang-s/abkd.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at
  Intermediate Resolution with Structure-Aware Multimodal U-Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenwei Zhang, Khanh Dao Duc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at
intermediate resolution (4-8 {\AA}) is crucial in protein structure
determination. Recent advances in deep learning have led to the development of
automated approaches for enhancing experimental cryo-EM density maps. Yet,
these methods are not optimized for intermediate-resolution maps and rely on
map density features alone. To address this, we propose CryoSAMU, a novel
method designed to enhance 3D cryo-EM density maps of protein structures using
structure-aware multimodal U-Nets and trained on curated
intermediate-resolution density maps. We comprehensively evaluate CryoSAMU
across various metrics and demonstrate its competitive performance compared to
state-of-the-art methods. Notably, CryoSAMU achieves significantly faster
processing speed, showing promise for future practical applications. Our code
is available at https://github.com/chenwei-zhang/CryoSAMU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4
  supplementary tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Uncertain to Safe: Conformal Fine-Tuning of Diffusion Models for
  Safe PDE Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyan Hu, Xiaowei Qian, Wenhao Deng, Rui Wang, Haodong Feng, Ruiqi Feng, Tao Zhang, Long Wei, Yue Wang, Zhi-Ming Ma, Tailin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of deep learning for partial differential equation
(PDE)-constrained control is gaining increasing attention. However, existing
methods rarely consider safety requirements crucial in real-world applications.
To address this limitation, we propose Safe Diffusion Models for PDE Control
(SafeDiffCon), which introduce the uncertainty quantile as model uncertainty
quantification to achieve optimal control under safety constraints through both
post-training and inference phases. Firstly, our approach post-trains a
pre-trained diffusion model to generate control sequences that better satisfy
safety constraints while achieving improved control objectives via a reweighted
diffusion loss, which incorporates the uncertainty quantile estimated using
conformal prediction. Secondly, during inference, the diffusion model
dynamically adjusts both its generation process and parameters through
iterative guidance and fine-tuning, conditioned on control targets while
simultaneously integrating the estimated uncertainty quantile. We evaluate
SafeDiffCon on three control tasks: 1D Burgers' equation, 2D incompressible
fluid, and controlled nuclear fusion problem. Results demonstrate that
SafeDiffCon is the only method that satisfies all safety constraints, whereas
other classical and deep learning baselines fail. Furthermore, while adhering
to safety constraints, SafeDiffCon achieves the best control performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient
  Fine-Tuning of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyegang Son, Yonglak Son, Changhoon Kim, Young Geun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large-scale pre-trained models achieve great success.
Fine-tuning is the standard practice for leveraging these models in downstream
tasks. Among the fine-tuning methods, adapter-tuning provides a
parameter-efficient fine-tuning by introducing lightweight trainable modules
while keeping most pre-trained parameters frozen. However, existing
adapter-tuning methods still impose substantial resource usage. Through our
investigation, we show that each adapter unequally contributes to both task
performance and resource usage. Motivated by this insight, we propose Selective
Adapter FrEezing (SAFE), which gradually freezes less important adapters early
to reduce unnecessary resource usage while maintaining performance. In our
experiments, SAFE reduces memory usage, computation amount, and training time
by 42.85\%, 34.59\%, and 11.82\%, respectively, while achieving comparable or
better task performance compared to the baseline. We also demonstrate that SAFE
induces regularization effect, thereby smoothing the loss landscape, which
enables the model to generalize better by avoiding sharp minima.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>URL: https://aclanthology.org/2025.naacl-long.480/ Volume:
  Proceedings of the 2025 Conference of the Nations of the Americas Chapter of
  the Association for Computational Linguistics: Human Language Technologies
  (Volume 1: Long Papers) Year: 2025 Address: Albuquerque, New Mexico</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Graph Foundation Models: Training on Knowledge Graphs Enables
  Transferability to General Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12609v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12609v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Wang, Siqiang Luo, Caihua Shan, Yifei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the success of large language models, there is a trend toward
developing graph foundation models to conduct diverse downstream tasks in
various domains. However, current models often require extra fine-tuning to
apply their learned structural and semantic representations to new graphs,
which limits their versatility. Recent breakthroughs in zero-shot inductive
reasoning on knowledge graphs (KGs), offer us a new perspective on extending KG
reasoning to general graph applications. In this paper, we introduce SCR, a
unified graph reasoning framework designed to train on knowledge graphs and
effectively generalize across a wide range of graph tasks and domains. We begin
by designing the task-specific KG structures to establish a unified topology
for different task formats. Then we propose semantic-conditioned message
passing, a novel mechanism addressing the inherent semantic isolation in
traditional KG reasoning, by jointly modeling structural and semantic
invariance patterns in graph representations. To demonstrate the effectiveness,
we evaluate the inductive reasoning capability of SCR using 38 diverse graph
datasets, covering node-level, link-level, and graph-level tasks across
multiple domains. Our results show substantial performance gains over existing
foundation models and supervised baselines, highlighting the efficacy and
adaptability of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 Pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Power Grid Topologies with Reinforcement Learning: A <span class="highlight-title">Survey</span>
  of Methods and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erica van der Sar, Alessandro Zocca, Sandjai Bhulai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Power grid operation is becoming increasingly complex due to the rising
integration of renewable energy sources and the need for more adaptive control
strategies. Reinforcement Learning (RL) has emerged as a promising approach to
power network control (PNC), offering the potential to enhance decision-making
in dynamic and uncertain environments. The Learning To Run a Power Network
(L2RPN) competitions have played a key role in accelerating research by
providing standardized benchmarks and problem formulations, leading to rapid
advancements in RL-based methods. This survey provides a comprehensive and
structured overview of RL applications for power grid topology optimization,
categorizing existing techniques, highlighting key design choices, and
identifying gaps in current research. Additionally, we present a comparative
numerical study evaluating the impact of commonly applied RL-based methods,
offering insights into their practical effectiveness. By consolidating existing
research and outlining open challenges, this survey aims to provide a
foundation for future advancements in RL-driven power grid optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 pages, 26 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning with Physics Knowledge for Prediction: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joe Watson, Chen Song, Oliver Weeger, Theo Gruner, An T. Le, Kay Pompetzki, Ahmed Hendawy, Oleg Arenz, Will Trojak, Miles Cranmer, Carlo D'Eramo, Fabian Bülow, Tanmay Goyal, Jan Peters, Martin W. Hoffman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey examines the broad suite of methods and models for combining
machine learning with physics knowledge for prediction and forecast, with a
focus on partial differential equations. These methods have attracted
significant interest due to their potential impact on advancing scientific
research and industrial practices by improving predictive models with small- or
large-scale datasets and expressive predictive models with useful inductive
biases. The survey has two parts. The first considers incorporating physics
knowledge on an architectural level through objective functions, structured
predictive models, and data augmentation. The second considers data as physics
knowledge, which motivates looking at multi-task, meta, and contextual learning
as an alternative approach to incorporating physics knowledge in a data-driven
fashion. Finally, we also provide an industrial perspective on the application
of these methods and a survey of the open-source ecosystem for physics-informed
machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>61 pages, 8 figures, 2 tables. Accepted at the Transactions of
  Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FitCF: A Framework for Automatic Feature Importance-guided
  Counterfactual Example Generation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianli Wang, Nils Feldhus, Simon Ostermann, Luis Felipe Villa-Arenas, Sebastian Möller, Vera Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual examples are widely used in natural language processing (NLP)
as valuable data to improve models, and in explainable artificial intelligence
(XAI) to understand model behavior. The automated generation of counterfactual
examples remains a challenging task even for large language models (LLMs),
despite their impressive performance on many tasks. In this paper, we first
introduce ZeroCF, a faithful approach for leveraging important words derived
from feature attribution methods to generate counterfactual examples in a
zero-shot setting. Second, we present a new framework, FitCF, which further
verifies aforementioned counterfactuals by label flip verification and then
inserts them as demonstrations for few-shot prompting, outperforming two
state-of-the-art baselines. Through ablation studies, we identify the
importance of each of FitCF's core components in improving the quality of
counterfactuals, as assessed through flip rate, perplexity, and similarity
measures. Furthermore, we show the effectiveness of LIME and Integrated
Gradients as backbone attribution methods for FitCF and find that the number of
demonstrations has the largest effect on performance. Finally, we reveal a
strong correlation between the faithfulness of feature attribution scores and
the quality of generated counterfactuals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings; camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Foundation Model for Chemical Reactor Modeling: Meta-Learning
  with Physics-Informed Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11752v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11752v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, Zhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing accurate models for chemical reactors is often challenging due to
the complexity of reaction kinetics and process dynamics. Traditional
approaches require retraining models for each new system, limiting
generalizability and efficiency. In this work, we take a step toward foundation
models for chemical reactor modeling by introducing a neural network framework
that generalizes across diverse reactor types and rapidly adapts to new
chemical processes. Our approach leverages meta-learning to pretrain the model
on a broad set of reactor dynamics, enabling efficient adaptation to unseen
reactions with minimal data. To further enhance generalizability, we
incorporate physics-informed fine-tuning, ensuring physically consistent
adaptation to new reactor conditions. Our framework is evaluated across three
integer-order fundamental reactor types - continuous stirred tank reactors,
batch reactors, and plug flow reactors - demonstrating superior few-shot
adaptation compared to conventional data-driven, physics-informed, and transfer
learning approaches. By combining meta-learning with physics-informed
adaptation, this work lays the foundation for a generalizable modeling
framework, advancing the development of foundation models for chemical
engineering applications. Source code is available at
https://github.com/killingbear999/chemical-reactor-foundation-model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Chemical Engineering Research and Design</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intelligently Augmented Contrastive Tensor Factorization: Empowering
  Multi-dimensional Time Series Classification in Low-Data Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anushiya Arunan, Yan Qin, Xiaoli Li, Yuen Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification of multi-dimensional time series from real-world systems
require fine-grained learning of complex features such as cross-dimensional
dependencies and intra-class variations-all under the practical challenge of
low training data availability. However, standard deep learning (DL) struggles
to learn generalizable features in low-data environments due to model
overfitting. We propose a versatile yet data-efficient framework, Intelligently
Augmented Contrastive Tensor Factorization (ITA-CTF), to learn effective
representations from multi-dimensional time series. The CTF module learns core
explanatory components of the time series (e.g., sensor factors, temporal
factors), and importantly, their joint dependencies. Notably, unlike standard
tensor factorization (TF), the CTF module incorporates a new contrastive loss
optimization to induce similarity learning and class-awareness into the learnt
representations for better classification performance. To strengthen this
contrastive learning, the preceding ITA module generates targeted but
informative augmentations that highlight realistic intra-class patterns in the
original data, while preserving class-wise properties. This is achieved by
dynamically sampling a "soft" class prototype to guide the warping of each
query data sample, which results in an augmentation that is intelligently
pattern-mixed between the "soft" class prototype and the query sample. These
augmentations enable the CTF module to recognize complex intra-class variations
despite the limited original training data, and seek out invariant class-wise
properties for accurate classification performance. The proposed method is
comprehensively evaluated on five different classification tasks. Compared to
standard TF and several DL benchmarks, notable performance improvements up to
18.7% were achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Expert Systems with Applications
  (DOI:https://doi.org/10.1016/j.eswa.2025.127889)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Provable Scaling Laws for the Test-Time Compute of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19477v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19477v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose two simple, principled and practical algorithms that enjoy
provable scaling laws for the test-time compute of large language models
(LLMs). The first one is a two-stage knockout-style algorithm: given an input
problem, it first generates multiple candidate solutions, and then aggregate
them via a knockout tournament for the final output. Assuming that the LLM can
generate a correct solution with non-zero probability and do better than a
random guess in comparing a pair of correct and incorrect solutions, we prove
theoretically that the failure probability of this algorithm decays to zero
exponentially or by a power law (depending on the specific way of scaling) as
its test-time compute grows. The second one is a two-stage league-style
algorithm, where each candidate is evaluated by its average win rate against
multiple opponents, rather than eliminated upon loss to a single opponent.
Under analogous but more robust assumptions, we prove that its failure
probability also decays to zero exponentially with more test-time compute. Both
algorithms require a black-box LLM and nothing else (e.g., no verifier or
reward model) for a minimalistic implementation, which makes them appealing for
practical applications and easy to adapt for different tasks. Through extensive
experiments with diverse models and datasets, we validate the proposed theories
and demonstrate the outstanding scaling properties of both algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Statistical Taylor Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01223v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01223v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengpu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical Taylor expansion replaces the input precise variables in a
conventional Taylor expansion with random variables each with known
distribution, to calculate the result mean and deviation. It is based on the
uncorrelated uncertainty assumption: Each input variable is measured
independently with fine enough statistical precision, so that their
uncertainties are independent of each other. Statistical Taylor expansion
reviews that the intermediate analytic expressions can no longer be regarded as
independent of each other, and the result of analytic expression should be path
independent. This conclusion differs fundamentally from the conventional common
approach in applied mathematics to find the best execution path for a result.
This paper also presents an implementation of statistical Taylor expansion
called variance arithmetic, and the tests on variance arithmetic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>65 pages, 53 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ System Log Parsing with Large Language Models: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Beck, Max Landauer, Markus Wurzenberger, Florian Skopik, Andreas Rauber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Log data provides crucial insights for tasks like monitoring, root cause
analysis, and anomaly detection. Due to the vast volume of logs, automated log
parsing is essential to transform semi-structured log messages into structured
representations. Recent advances in large language models (LLMs) have
introduced the new research field of LLM-based log parsing. Despite promising
results, there is no structured overview of the approaches in this relatively
new research field with the earliest advances published in late 2023. This work
systematically reviews 29 LLM-based log parsing methods. We benchmark seven of
them on public datasets and critically assess their comparability and the
reproducibility of their reported results. Our findings summarize the advances
of this new research field, with insights on how to report results, which data
sets, metrics and which terminology to use, and which inconsistencies to avoid,
with code and results made publicly available for transparency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeBridge: Non-Stationarity Matters for Long-term Time Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04442v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04442v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyuan Liu, Beiliang Wu, Yifan Hu, Naiqi Li, Tao Dai, Jigang Bao, Shu-tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-stationarity poses significant challenges for multivariate time series
forecasting due to the inherent short-term fluctuations and long-term trends
that can lead to spurious regressions or obscure essential long-term
relationships. Most existing methods either eliminate or retain
non-stationarity without adequately addressing its distinct impacts on
short-term and long-term modeling. Eliminating non-stationarity is essential
for avoiding spurious regressions and capturing local dependencies in
short-term modeling, while preserving it is crucial for revealing long-term
cointegration across variates. In this paper, we propose TimeBridge, a novel
framework designed to bridge the gap between non-stationarity and dependency
modeling in long-term time series forecasting. By segmenting input series into
smaller patches, TimeBridge applies Integrated Attention to mitigate short-term
non-stationarity and capture stable dependencies within each variate, while
Cointegrated Attention preserves non-stationarity to model long-term
cointegration across variates. Extensive experiments show that TimeBridge
consistently achieves state-of-the-art performance in both short-term and
long-term forecasting. Additionally, TimeBridge demonstrates exceptional
performance in financial forecasting on the CSI 500 and S&P 500 indices,
further validating its robustness and effectiveness. Code is available at
https://github.com/Hank0626/TimeBridge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Fine-Grained Control via Aggregation of Multiple Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conghan Yue, Zhengwei Peng, Shiyan Du, Zhi Ji, Chuangjian Cai, Le Wan, Dongyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While many diffusion models perform well when controlling for particular
aspect among style, character, and interaction, they struggle with fine-grained
control due to dataset limitations and intricate model architecture design.
This paper first introduces a novel training-free algorithm in fine-grained
generation, Aggregation of Multiple Diffusion Models (AMDM), which integrates
features from multiple diffusion models into a specified model to activate
specific features and enable fine-grained control. Experimental results
demonstrate that AMDM significantly improves fine-grained control without
training, validating its effectiveness. Additionally, it reveals that diffusion
models initially focus on features such as position, attributes, and style,
with later stages improving generation quality and consistency. AMDM offers a
new perspective for tackling the challenges of fine-grained conditional control
generation in diffusion models: We can fully utilize existing or develop new
conditional diffusion models that control specific aspects, and then aggregate
them using AMDM algorithm. This eliminates the need for constructing complex
datasets, designing intricate model architectures, and incurring high training
costs. Code is available at: https://github.com/Hammour-steak/AMDM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R2VF: A Two-Step Regularization Algorithm to Cluster Categories in GLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuval Ben Dror
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over recent decades, extensive research has aimed to overcome the restrictive
underlying assumptions required for a Generalized Linear Model to generate
accurate and meaningful predictions. These efforts include regularizing
coefficients, selecting features, and clustering ordinal categories, among
other approaches. Despite these advances, efficiently clustering nominal
categories in GLMs without incurring high computational costs remains a
challenge. This paper introduces Ranking to Variable Fusion (R2VF), a two-step
method designed to efficiently fuse nominal and ordinal categories in GLMs. By
first transforming nominal features into an ordinal framework via regularized
regression and then applying variable fusion, R2VF strikes a balance between
model complexity and interpretability. We demonstrate the effectiveness of R2VF
through comparisons with other methods, highlighting its performance in
addressing overfitting and identifying an appropriate set of covariates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and
  Compression in LLMs <span class="chip">IJCNN 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Gu, Wuyang Zhou, Giorgos Iacovides, Danilo Mandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reasoning abilities of Large Language Models (LLMs) can be improved by
structurally denoising their weights, yet existing techniques primarily focus
on denoising the feed-forward network (FFN) of the transformer block, and can
not efficiently utilise the Multi-head Attention (MHA) block, which is the core
of transformer architectures. To address this issue, we propose a novel
intuitive framework that, at its very core, performs MHA compression through a
multi-head tensorisation process and the Tucker decomposition. This enables
both higher-dimensional structured denoising and compression of the MHA
weights, by enforcing a shared higher-dimensional subspace across the weights
of the multiple attention heads. We demonstrate that this approach consistently
enhances the reasoning capabilities of LLMs across multiple benchmark datasets,
and for both encoder-only and decoder-only architectures, while achieving
compression rates of up to $\sim 250$ times in the MHA weights, all without
requiring any additional data, training, or fine-tuning. Furthermore, we show
that the proposed method can be seamlessly combined with existing
FFN-only-based denoising techniques to achieve further improvements in LLM
reasoning performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpeted for IEEE International Joint Conference on Neural Networks
  (IJCNN 2025). The code is available at https://github.com/guyuxuan9/TensorLLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Convergence: Mutual Distillation is Secretly a Form of
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02481v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02481v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengpeng Xie, Jiahang Cao, Qiang Zhang, Jianxiong Zhang, Changwei Wang, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we argue that mutual distillation between reinforcement
learning policies serves as an implicit regularization, preventing them from
overfitting to irrelevant features. We highlight two key contributions: (a)
Theoretically, for the first time, we prove that enhancing the policy
robustness to irrelevant features leads to improved generalization performance.
(b) Empirically, we demonstrate that mutual distillation between policies
contributes to such robustness, enabling the spontaneous emergence of invariant
representations over pixel inputs. Overall, our findings challenge the
conventional view of distillation as merely a means of knowledge transfer,
offering a novel perspective on the generalization in deep reinforcement
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aggregating Concepts of Accuracy and Fairness in Prediction Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Kinney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An algorithm that outputs predictions about the state of the world will
almost always be designed with the implicit or explicit goal of outputting
accurate predictions (i.e., predictions that are likely to be true). In
addition, the rise of increasingly powerful predictive algorithms brought about
by the recent revolution in artificial intelligence has led to an emphasis on
building predictive algorithms that are fair, in the sense that their
predictions do not systematically evince bias or bring about harm to certain
individuals or groups. This state of affairs presents two conceptual
challenges. First, the goals of accuracy and fairness can sometimes be in
tension, and there are no obvious normative guidelines for managing the
trade-offs between these two desiderata when they arise. Second, there are many
distinct ways of measuring both the accuracy and fairness of a predictive
algorithm; here too, there are no obvious guidelines on how to aggregate our
preferences for predictive algorithms that satisfy disparate measures of
fairness and accuracy to various extents. The goal of this paper is to address
these challenges by arguing that there are good reasons for using a linear
combination of accuracy and fairness metrics to measure the
all-things-considered value of a predictive algorithm for agents who care about
both accuracy and fairness. My argument depends crucially on a classic result
in the preference aggregation literature due to Harsanyi. After making this
formal argument, I apply my result to an analysis of accuracy-fairness
trade-offs using the COMPAS dataset compiled by Angwin et al.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Action <span class="highlight-title">Pretrain</span>ing from Videos <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, Dieter Fox, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Latent Action Pretraining for general Action models (LAPA), an
unsupervised method for pretraining Vision-Language-Action (VLA) models without
ground-truth robot action labels. Existing Vision-Language-Action models
require action labels typically collected by human teleoperators during
pretraining, which significantly limits possible data sources and scale. In
this work, we propose a method to learn from internet-scale videos that do not
have robot action labels. We first train an action quantization model
leveraging VQ-VAE-based objective to learn discrete latent actions between
image frames, then pretrain a latent VLA model to predict these latent actions
from observations and task descriptions, and finally finetune the VLA on
small-scale robot manipulation data to map from latent to robot actions.
Experimental results demonstrate that our method significantly outperforms
existing techniques that train robot manipulation policies from large-scale
videos. Furthermore, it outperforms the state-of-the-art VLA model trained with
robotic action labels on real-world manipulation tasks that require language
conditioning, generalization to unseen objects, and semantic generalization to
unseen instructions. Training only on human manipulation videos also shows
positive transfer, opening up the potential for leveraging web-scale data for
robotics foundation model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Website: https://latentactionpretraining.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Speculative Inference for Efficient LLM Inference Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyao Gao, Jianchun Liu, Hongli Xu, Xichong Zhang, Yunming Liao, Liusheng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative inference is a promising paradigm employing small speculative
models (SSMs) as drafters to generate draft tokens, which are subsequently
verified in parallel by the target large language model (LLM). This approach
enhances the efficiency of inference serving by reducing LLM inference latency
and costs while preserving generation quality. However, existing speculative
methods face critical challenges, including inefficient resource utilization
and limited draft acceptance, which constrain their scalability and overall
effectiveness. To overcome these obstacles, we present CoSine, a novel
speculative inference system that decouples sequential speculative decoding
from parallel verification, enabling efficient collaboration among multiple
nodes. Specifically, CoSine routes inference requests to specialized drafters
based on their expertise and incorporates a confidence-based token fusion
mechanism to synthesize outputs from cooperating drafters, ensuring
high-quality draft generation. Additionally, CoSine dynamically orchestrates
the execution of speculative decoding and verification in a pipelined manner,
employing batch scheduling to selectively group requests and adaptive
speculation control to minimize idle periods. By optimizing parallel workflows
through heterogeneous node collaboration, CoSine balances draft generation and
verification throughput in real-time, thereby maximizing resource utilization.
Experimental results demonstrate that CoSine achieves superior performance
compared to state-of-the-art speculative approaches. Notably, with equivalent
resource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5%
increase in throughput compared to baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A systematic <span class="highlight-title">review</span> of challenges and proposed solutions in modeling
  multimodal data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Farhadizadeh, Maria Weymann, Michael Blaß, Johann Kraus, Christopher Gundler, Sebastian Walter, Noah Hempen, Harald Binder, Nadine Binder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal data modeling has emerged as a powerful approach in clinical
research, enabling the integration of diverse data types such as imaging,
genomics, wearable sensors, and electronic health records. Despite its
potential to improve diagnostic accuracy and support personalized care,
modeling such heterogeneous data presents significant technical challenges.
This systematic review synthesizes findings from 69 studies to identify common
obstacles, including missing modalities, limited sample sizes, dimensionality
imbalance, interpretability issues, and finding the optimal fusion techniques.
We highlight recent methodological advances, such as transfer learning,
generative models, attention mechanisms, and neural architecture search that
offer promising solutions. By mapping current trends and innovations, this
review provides a comprehensive overview of the field and offers practical
insights to guide future research and development in multimodal modeling for
medical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Progress Driven Multi-Agent Curriculum <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10016v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10016v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenshuai Zhao, Zhiyuan Li, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The number of agents can be an effective curriculum variable for controlling
the difficulty of multi-agent reinforcement learning (MARL) tasks. Existing
work typically uses manually defined curricula such as linear schemes. We
identify two potential flaws while applying existing reward-based automatic
curriculum learning methods in MARL: (1) The expected episode return used to
measure task difficulty has high variance; (2) Credit assignment difficulty can
be exacerbated in tasks where increasing the number of agents yields higher
returns which is common in many MARL tasks. To address these issues, we propose
to control the curriculum by using a TD-error based *learning progress* measure
and by letting the curriculum proceed from an initial context distribution to
the final task specific one. Since our approach maintains a distribution over
the number of agents and measures learning progress rather than absolute
performance, which often increases with the number of agents, we alleviate
problem (2). Moreover, the learning progress measure naturally alleviates
problem (1) by aggregating returns. In three challenging sparse-reward MARL
benchmarks, our approach outperforms state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Objective Hyperparameter Selection via Hypothesis Testing on
  Reliability Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirmohammad Farzaneh, Osvaldo Simeone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The selection of hyperparameters, such as prompt templates in large language
models (LLMs), must often strike a balance between reliability and cost. In
many cases, structural relationships between the expected reliability levels of
the hyperparameters can be inferred from prior information and held-out data --
e.g., longer prompt templates may be more detailed and thus more reliable.
However, existing hyperparameter selection methods either do not provide formal
reliability guarantees or are unable to incorporate structured knowledge in the
hyperparameter space. This paper introduces reliability graph-based Pareto
testing (RG-PT), a novel multi-objective hyperparameter selection framework
that maintains formal reliability guarantees in terms of false discovery rate
(FDR), while accounting for known relationships among hyperparameters via a
directed acyclic graph. Edges in the graph reflect expected reliability and
cost trade-offs among hyperparameters, which are inferred via the Bradley-Terry
(BT) ranking model from prior information and held-out data. Experimental
evaluations demonstrate that RG-PT significantly outperforms existing methods
such as learn-then-test (LTT) and Pareto testing (PT) through a more efficient
exploration of the hyperparameter space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Quantum Convolutional Neural Networks for Image
  Classification: Overcoming Hardware Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Röseler, Oliver Schaudt, Helmut Berg, Christian Bauckhage, Matthias Koch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While classical convolutional neural networks (CNNs) have revolutionized
image classification, the emergence of quantum computing presents new
opportunities for enhancing neural network architectures. Quantum CNNs (QCNNs)
leverage quantum mechanical properties and hold potential to outperform
classical approaches. However, their implementation on current noisy
intermediate-scale quantum (NISQ) devices remains challenging due to hardware
limitations. In our research, we address this challenge by introducing an
encoding scheme that significantly reduces the input dimensionality. We
demonstrate that a primitive QCNN architecture with 49 qubits is sufficient to
directly process $28\times 28$ pixel MNIST images, eliminating the need for
classical dimensionality reduction pre-processing. Additionally, we propose an
automated framework based on expressibility, entanglement, and complexity
characteristics to identify the building blocks of QCNNs, parameterized quantum
circuits (PQCs). Our approach demonstrates advantages in accuracy and
convergence speed with a similar parameter count compared to both hybrid QCNNs
and classical CNNs. We validated our experiments on IBM's Heron r2 quantum
processor, achieving $96.08\%$ classification accuracy, surpassing the
$71.74\%$ benchmark of traditional approaches under identical training
conditions. These results represent one of the first implementations of image
classifications on real quantum hardware and validate the potential of quantum
computing in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniVLA: Learning to Act Anywhere with Task-centric Latent Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, Hongyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A generalist robot should perform effectively across various environments.
However, most existing approaches heavily rely on scaling action-annotated data
to enhance their capabilities. Consequently, they are often limited to single
physical specification and struggle to learn transferable knowledge across
different embodiments and environments. To confront these limitations, we
propose UniVLA, a new framework for learning cross-embodiment
vision-language-action (VLA) policies. Our key innovation is to derive
task-centric action representations from videos with a latent action model.
This enables us to exploit extensive data across a wide spectrum of embodiments
and perspectives. To mitigate the effect of task-irrelevant dynamics, we
incorporate language instructions and establish a latent action model within
the DINO feature space. Learned from internet-scale videos, the generalist
policy can be deployed to various robots through efficient latent action
decoding. We obtain state-of-the-art results across multiple manipulation and
navigation benchmarks, as well as real-robot deployments. UniVLA achieves
superior performance over OpenVLA with less than 1/20 of pretraining compute
and 1/10 of downstream data. Continuous performance improvements are observed
as heterogeneous data, even including human videos, are incorporated into the
training pipeline. The results underscore UniVLA's potential to facilitate
scalable and efficient robot policy learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to RSS 2025. Code is available at
  https://github.com/OpenDriveLab/UniVLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to the
  Top-$k$ Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.12988v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.12988v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannis Montreuil, Axel Carlier, Lai Xing Ng, Wei Tsang Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although existing Learning-to-Defer (L2D) frameworks support multiple
experts, they allocate each query to a single expert, limiting their ability to
leverage collective expertise in complex decision-making scenarios. To address
this, we introduce the first framework for Top-$k$ Learning-to-Defer, enabling
systems to defer each query to the $k$ most cost-effective experts. Our
formulation strictly generalizes classical two-stage L2D by supporting
multi-expert deferral-a capability absent in prior work. We further propose
Top-$k(x)$ Learning-to-Defer, an adaptive extension that learns the optimal
number of experts per query based on input complexity, expert quality, and
consultation cost. We introduce a novel surrogate loss that is
Bayes-consistent, $(\mathcal{R}, \mathcal{G})$-consistent, and independent of
the cardinality parameter $k$, enabling efficient reuse across different values
of $k$. We show that classical model cascades arise as a special case of our
method, situating our framework as a strict generalization of both selective
deferral and cascaded inference. Experiments on classification and regression
demonstrate that Top-$k$ and Top-$k(x)$ yield improved accuracy--cost
trade-offs, establishing a new direction for multi-expert deferral in
Learning-to-Defer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Mosaic Memory of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Shilov, Matthieu Meeus, Yves-Alexandre de Montjoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) become widely adopted, understanding how they
learn from, and memorize, training data becomes crucial. Memorization in LLMs
is widely assumed to only occur as a result of sequences being repeated in the
training data. Instead, we show that LLMs memorize by assembling information
from similar sequences, a phenomena we call mosaic memory. We show major LLMs
to exhibit mosaic memory, with fuzzy duplicates contributing to memorization as
much as 0.8 of an exact duplicate and even heavily modified sequences
contributing substantially to memorization. Despite models display reasoning
capabilities, we somewhat surprisingly show memorization to be predominantly
syntactic rather than semantic. We finally show fuzzy duplicates to be
ubiquitous in real-world data, untouched by deduplication techniques. Taken
together, our results challenge widely held beliefs and show memorization to be
a more complex, mosaic process, with real-world implications for privacy,
confidentiality, model utility and evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cape: Context-Aware <span class="highlight-title">Prompt</span> Perturbation Mechanism with Differential
  Privacy <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05922v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05922v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoqi Wu, Wei Dai, Li Wang, Qiang Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have gained significant popularity due to their
remarkable capabilities in text understanding and generation. However, despite
their widespread deployment in inference services such as ChatGPT, concerns
about the potential leakage of sensitive user data have arisen. Existing
solutions primarily rely on privacy-enhancing technologies to mitigate such
risks, facing the trade-off among efficiency, privacy, and utility. To narrow
this gap, we propose Cape, a context-aware prompt perturbation mechanism based
on differential privacy, to enable efficient inference with an improved
privacy-utility trade-off. Concretely, we introduce a hybrid utility function
that better captures the token similarity. Additionally, we propose a
bucketized sampling mechanism to handle large sampling space, which might lead
to long-tail phenomenons. Extensive experiments across multiple datasets, along
with ablation studies, demonstrate that Cape achieves a better privacy-utility
trade-off compared to prior state-of-the-art works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mirror Descent Under Generalized Smoothness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingzhi Yu, Wei Jiang, Yuanyu Wan, Lijun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smoothness is crucial for attaining fast rates in first-order optimization.
However, many optimization problems in modern machine learning involve
non-smooth objectives. Recent studies relax the smoothness assumption by
allowing the Lipschitz constant of the gradient to grow with respect to the
gradient norm, which accommodates a broad range of objectives in practice.
Despite this progress, existing generalizations of smoothness are restricted to
Euclidean geometry with $\ell_2$-norm and only have theoretical guarantees for
optimization in the Euclidean space. In this paper, we address this limitation
by introducing a new $\ell*$-smoothness concept that measures the norm of
Hessians in terms of a general norm and its dual, and establish convergence for
mirror-descent-type algorithms, matching the rates under the classic
smoothness. Notably, we propose a generalized self-bounding property that
facilitates bounding the gradients via controlling suboptimality gaps, serving
as a principal component for convergence analysis. Beyond deterministic
optimization, we establish an anytime convergence for stochastic mirror descent
based on a new bounded noise condition that encompasses the widely adopted
bounded or affine noise assumptions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flexible Graph Similarity Computation With A Proactive Optimization
  Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouyang Liu, Ning Liu, Yixin Chen, Jiezhong He, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Edit Distance (GED) offers a principled and flexible measure of graph
similarity, as it quantifies the minimum cost needed to transform one graph
into another with customizable edit operation costs. Despite recent
learning-based efforts to approximate GED via vector space representations,
existing methods struggle with adapting to varying operation costs.
Furthermore, they suffer from inefficient, reactive mapping refinements due to
reliance on isolated node-level distance as guidance. To address these issues,
we propose GEN, a novel learning-based approach for flexible GED approximation.
GEN addresses the varying costs adaptation by integrating operation costs prior
to match establishment, enabling mappings to dynamically adapt to cost
variations. Furthermore, GEN introduces a proactive guidance optimization
strategy that captures graph-level dependencies between matches, allowing
informed matching decisions in a single step without costly iterative
refinements. Extensive evaluations on real-world and synthetic datasets
demonstrate that GEN achieves up to 37.8% reduction in GED approximation error
and 72.7% reduction in inference time compared with state-of-the-art methods,
while consistently maintaining robustness under diverse cost settings and graph
sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Laws for Black box Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan Liu, Huanran Chen, Yichi Zhang, Yinpeng Dong, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial examples usually exhibit good cross-model transferability,
enabling attacks on black-box models with limited information about their
architectures and parameters, which are highly threatening in commercial
black-box scenarios. Model ensembling is an effective strategy to improve the
transferability of adversarial examples by attacking multiple surrogate models.
However, since prior studies usually adopt few models in the ensemble, there
remains an open question of whether scaling the number of models can further
improve black-box attacks. Inspired by the scaling law of large foundation
models, we investigate the scaling laws of black-box adversarial attacks in
this work. Through theoretical analysis and empirical evaluations, we conclude
with clear scaling laws that using more surrogate models enhances adversarial
transferability. Comprehensive experiments verify the claims on standard image
classifiers, diverse defended models and multimodal large language models using
various adversarial attack methods. Specifically, by scaling law, we achieve
90%+ transfer attack success rate on even proprietary models like GPT-4o.
Further visualization indicates that there is also a scaling law on the
interpretability and semantics of adversarial perturbations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Malliavin Calculus for Score-based Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Mirafzali, Utkarsh Gupta, Patrick Wyrod, Frank Proske, Daniele Venturi, Razvan Marinescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new framework based on Malliavin calculus to derive exact
analytical expressions for the score function $\nabla \log p_t(x)$, i.e., the
gradient of the log-density associated with the solution to stochastic
differential equations (SDEs). Our approach combines classical
integration-by-parts techniques with modern stochastic analysis tools, such as
Bismut's formula and Malliavin calculus, and it works for both linear and
nonlinear SDEs. In doing so, we establish a rigorous connection between the
Malliavin derivative, its adjoint, the Malliavin divergence (Skorokhod
integral), and diffusion generative models, thereby providing a systematic
method for computing $\nabla \log p_t(x)$. In the linear case, we present a
detailed analysis showing that our formula coincides with the analytical score
function derived from the solution of the Fokker--Planck equation. For
nonlinear SDEs with state-independent diffusion coefficients, we derive a
closed-form expression for $\nabla \log p_t(x)$. We evaluate the proposed
framework across multiple generative tasks and find that its performance is
comparable to state-of-the-art methods. These results can be generalised to
broader classes of SDEs, paving the way for new score-based diffusion
generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Single View Garment Reconstruction Using Diffusion Mapping Via Pattern
  Coordinates <span class="chip">SIGGRAPH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ren Li, Cong Cao, Corentin Dumery, Yingxuan You, Hao Li, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing 3D clothed humans from images is fundamental to applications
like virtual try-on, avatar creation, and mixed reality. While recent advances
have enhanced human body recovery, accurate reconstruction of garment geometry
-- especially for loose-fitting clothing -- remains an open challenge. We
present a novel method for high-fidelity 3D garment reconstruction from single
images that bridges 2D and 3D representations. Our approach combines Implicit
Sewing Patterns (ISP) with a generative diffusion model to learn rich garment
shape priors in a 2D UV space. A key innovation is our mapping model that
establishes correspondences between 2D image pixels, UV pattern coordinates,
and 3D geometry, enabling joint optimization of both 3D garment meshes and the
corresponding 2D patterns by aligning learned priors with image observations.
Despite training exclusively on synthetically simulated cloth data, our method
generalizes effectively to real-world images, outperforming existing approaches
on both tight- and loose-fitting garments. The reconstructed garments maintain
physical plausibility while capturing fine geometric details, enabling
downstream applications including garment retargeting and texture manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convolutional Neural Networks and Mixture of Experts for Intrusion
  Detection in 5G Networks and beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loukas Ilias, George Doukas, Vangelis Lamprou, Christos Ntanos, Dimitris Askounis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of 6G/NextG networks comes along with a series of benefits,
including extreme capacity, reliability, and efficiency. However, these
networks may become vulnerable to new security threats. Therefore, 6G/NextG
networks must be equipped with advanced Artificial Intelligence algorithms, in
order to evade these attacks. Existing studies on the intrusion detection task
rely on the train of shallow machine learning classifiers, including Logistic
Regression, Decision Trees, and so on, yielding suboptimal performance. Others
are based on deep neural networks consisting of static components, which are
not conditional on the input. This limits their representation power and
efficiency. To resolve these issues, we present the first study integrating
Mixture of Experts (MoE) for identifying malicious traffic. Specifically, we
use network traffic data and convert the 1D array of features into a 2D matrix.
Next, we pass this matrix through convolutional neural network (CNN) layers
followed by batch normalization and max pooling layers. After obtaining the
representation vector via the CNN layers, a sparsely gated MoE layer is used.
This layer consists of a set of experts (dense layers) and a router, where the
router assigns weights to the output of each expert. Sparsity is achieved by
choosing the most relevant experts of the total ones. Finally, we perform a
series of ablation experiments to prove the effectiveness of our proposed
model. Experiments are conducted on the 5G-NIDD dataset, a network intrusion
detection dataset generated from a real 5G test network. Results show that our
introduced approach reaches weighted F1-score up to 99.95% achieving comparable
performance to existing approaches. Findings also show that our proposed model
achieves multiple advantages over state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achref Doula, Max Mühlhäuser, Alejandro Sanchez Guinea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) show growing promise in autonomous driving by
reasoning over complex traffic scenarios to generate path plans. However, their
tendencies toward overconfidence, and hallucinations raise critical safety
concerns. We introduce SafePath, a modular framework that augments LLM-based
path planning with formal safety guarantees using conformal prediction.
SafePath operates in three stages. In the first stage, we use an LLM that
generates a set of diverse candidate paths, exploring possible trajectories
based on agent behaviors and environmental cues. In the second stage, SafePath
filters out high-risk trajectories while guaranteeing that at least one safe
option is included with a user-defined probability, through a multiple-choice
question-answering formulation that integrates conformal prediction. In the
final stage, our approach selects the path with the lowest expected collision
risk when uncertainty is low or delegates control to a human when uncertainty
is high. We theoretically prove that SafePath guarantees a safe trajectory with
a user-defined probability, and we show how its human delegation rate can be
tuned to balance autonomy and safety. Extensive experiments on nuScenes and
Highway-env show that SafePath reduces planning uncertainty by 77\% and
collision rates by up to 70\%, demonstrating effectiveness in making LLM-driven
path planning more safer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding In-context Learning of Addition via Activation Subspaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyan Hu, Kayo Yin, Michael I. Jordan, Jacob Steinhardt, Lijie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To perform in-context learning, language models must extract signals from
individual few-shot examples, aggregate these into a learned prediction rule,
and then apply this rule to new examples. How is this implemented in the
forward pass of modern transformer models? To study this, we consider a
structured family of few-shot learning tasks for which the true prediction rule
is to add an integer $k$ to the input. We find that Llama-3-8B attains high
accuracy on this task for a range of $k$, and localize its few-shot ability to
just three attention heads via a novel optimization approach. We further show
the extracted signals lie in a six-dimensional subspace, where four of the
dimensions track the unit digit and the other two dimensions track overall
magnitude. We finally examine how these heads extract information from
individual few-shot examples, identifying a self-correction mechanism in which
mistakes from earlier examples are suppressed by later examples. Our results
demonstrate how tracking low-dimensional subspaces across a forward pass can
provide insight into fine-grained computational structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSINR: Capturing Temporal Continuity via Implicit Neural Representations
  for Time Series Anomaly Detection <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11641v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11641v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengxuan Li, Ke Liu, Hongyang Chen, Jiajun Bu, Hongwei Wang, Haishuai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series anomaly detection aims to identify unusual patterns in data or
deviations from systems' expected behavior. The reconstruction-based methods
are the mainstream in this task, which learn point-wise representation via
unsupervised learning. However, the unlabeled anomaly points in training data
may cause these reconstruction-based methods to learn and reconstruct anomalous
data, resulting in the challenge of capturing normal patterns. In this paper,
we propose a time series anomaly detection method based on implicit neural
representation (INR) reconstruction, named TSINR, to address this challenge.
Due to the property of spectral bias, TSINR enables prioritizing low-frequency
signals and exhibiting poorer performance on high-frequency abnormal data.
Specifically, we adopt INR to parameterize time series data as a continuous
function and employ a transformer-based architecture to predict the INR of
given data. As a result, the proposed TSINR method achieves the advantage of
capturing the temporal continuity and thus is more sensitive to discontinuous
anomaly data. In addition, we further design a novel form of INR continuous
function to learn inter- and intra-channel information, and leverage a
pre-trained large language model to amplify the intense fluctuations in
anomalies. Extensive experiments demonstrate that TSINR achieves superior
overall performance on both univariate and multivariate time series anomaly
detection benchmarks compared to other state-of-the-art reconstruction-based
methods. Our codes are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards More Efficient, Robust, Instance-adaptive, and Generalizable
  Sequential Decision making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.09192v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.09192v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The primary goal of my Ph.D. study is to develop provably efficient and
practical algorithms for data-driven sequential decision-making under
uncertainty. My work focuses on reinforcement learning (RL), multi-armed
bandits, and their applications, including recommendation systems, computer
networks, video analytics, and large language models (LLMs). Sequential
decision-making methods, such as bandits and RL, have demonstrated remarkable
success - ranging from outperforming human players in complex games like Atari
and Go to advancing robotics, recommendation systems, and fine-tuning LLMs.
Despite these successes, many established algorithms rely on idealized models
that can fail under model misspecifications or adversarial perturbations,
particularly in settings where accurate prior knowledge of the underlying model
class is unavailable or where malicious users operate within dynamic systems.
These challenges are pervasive in real-world applications, where robust and
adaptive solutions are critical. Furthermore, while worst-case guarantees
provide theoretical reliability, they often fail to capture instance-dependent
performance, which can lead to more efficient and practical solutions. Another
key challenge lies in generalizing to new, unseen environments, a crucial
requirement for deploying these methods in dynamic and unpredictable settings.
To address these limitations, my research aims to develop more efficient,
robust, instance-adaptive, and generalizable sequential decision-making
algorithms for both reinforcement learning and bandits. Towards this end, I
focus on developing more efficient, robust, instance-adaptive, and
generalizable for both general reinforcement learning (RL) and bandits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ph.D. Thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Network-based Spectral Filtering Mechanism for Imbalance
  Classification in Network Digital Twin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abubakar Isah, Ibrahim Aliyu, Sulaiman Muhammad Rashid, Jaehyung Park, Minsoo Hahn, Jinsul Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks are gaining attention in fifth-generation (5G) core
network digital twins, which are data-driven complex systems with numerous
components. Analyzing these data can be challenging due to rare failure types,
leading to imbalanced classification in multiclass settings. Digital twins of
5G networks increasingly employ graph classification as the main method for
identifying failure types. However, the skewed distribution of failure
occurrences is a significant class-imbalance problem that prevents practical
graph data mining. Previous studies have not sufficiently addressed this
complex problem. This paper, proposes class-Fourier GNN (CF-GNN) that
introduces a class-oriented spectral filtering mechanism to ensure precise
classification by estimating a unique spectral filter for each class. This work
employs eigenvalue and eigenvector spectral filtering to capture and adapt to
variations in minority classes, ensuring accurate class-specific feature
discrimination, and adept at graph representation learning for complex local
structures among neighbors in an end-to-end setting. The extensive experiments
demonstrate that the proposed CF-GNN could help create new techniques for
enhancing classifiers and investigate the characteristics of the multiclass
imbalanced data in a network digital twin system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2406.06595</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Next Token Prediction: Patch-Level Training for Large Language
  Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12665v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12665v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenze Shao, Fandong Meng, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prohibitive training costs of Large Language Models (LLMs) have emerged
as a significant bottleneck in the development of next-generation LLMs. In this
paper, we show that it is possible to significantly reduce the training costs
of LLMs without sacrificing their performance. Specifically, we introduce
patch-level training for LLMs, in which multiple tokens are aggregated into a
unit of higher information density, referred to as a `patch', to serve as the
fundamental text unit for training LLMs. During patch-level training, we feed
the language model shorter sequences of patches and train it to predict the
next patch, thereby processing the majority of the training data at a
significantly reduced cost. Following this, the model continues token-level
training on the remaining training data to align with the inference mode.
Experiments on a diverse range of models (370M-2.7B parameters) demonstrate
that patch-level training can reduce the overall training costs to 0.5$\times$,
without compromising the model performance compared to token-level training.
Source code: https://github.com/shaochenze/PatchTrain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Commute Graph Neural Networks <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01635v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01635v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhuo, Han Yu, Guang Tan, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have shown remarkable success in learning from
graph-structured data. However, their application to directed graphs (digraphs)
presents unique challenges, primarily due to the inherent asymmetry in node
relationships. Traditional GNNs are adept at capturing unidirectional relations
but fall short in encoding the mutual path dependencies between nodes, such as
asymmetrical shortest paths typically found in digraphs. Recognizing this gap,
we introduce Commute Graph Neural Networks (CGNN), an approach that seamlessly
integrates node-wise commute time into the message passing scheme. The
cornerstone of CGNN is an efficient method for computing commute time using a
newly formulated digraph Laplacian. Commute time is then integrated into the
neighborhood aggregation process, with neighbor contributions weighted
according to their respective commute time to the central node in each layer.
It enables CGNN to directly capture the mutual, asymmetric relationships in
digraphs. Extensive experiments on 8 benchmarking datasets confirm the
superiority of CGNN against 13 state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in International Conference on Machine Learning (ICML),
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Saliency-Motion Guided Trunk-Collateral Network for Unsupervised Video
  Object Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zheng, Wanyun Li, Songcheng He, Jianping Fan, Xiaoqiang Li, We Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent mainstream unsupervised video object segmentation (UVOS)
motion-appearance approaches use either the bi-encoder structure to separately
encode motion and appearance features, or the uni-encoder structure for joint
encoding. However, these methods fail to properly balance the motion-appearance
relationship. Consequently, even with complex fusion modules for
motion-appearance integration, the extracted suboptimal features degrade the
models' overall performance. Moreover, the quality of optical flow varies
across scenarios, making it insufficient to rely solely on optical flow to
achieve high-quality segmentation results. To address these challenges, we
propose the Saliency-Motion guided Trunk-Collateral Network (SMTC-Net), which
better balances the motion-appearance relationship and incorporates model's
intrinsic saliency information to enhance segmentation performance.
Specifically, considering that optical flow maps are derived from RGB images,
they share both commonalities and differences. Accordingly, we propose a novel
Trunk-Collateral structure for motion-appearance UVOS. The shared trunk
backbone captures the motion-appearance commonality, while the collateral
branch learns the uniqueness of motion features. Furthermore, an Intrinsic
Saliency guided Refinement Module (ISRM) is devised to efficiently leverage the
model's intrinsic saliency information to refine high-level features, and
provide pixel-level guidance for motion-appearance fusion, thereby enhancing
performance without additional input. Experimental results show that SMTC-Net
achieved state-of-the-art performance on three UVOS datasets ( 89.2% J&F on
DAVIS-16, 76% J on YouTube-Objects, 86.4% J on FBMS ) and four standard video
salient object detection (VSOD) benchmarks with the notable increase,
demonstrating its effectiveness and superiority over previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Power of Learning-Augmented Search Trees <span class="chip">ICML25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09251v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09251v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingbang Chen, Xinyuan Cao, Alicia Stepin, Li Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study learning-augmented binary search trees (BSTs) via Treaps with
carefully designed priorities. The result is a simple search tree in which the
depth of each item $x$ is determined by its predicted weight $w_x$.
Specifically, each item $x$ is assigned a composite priority of
$-\lfloor\log\log(1/w_x)\rfloor + U(0, 1)$ where $U(0, 1)$ is the uniform
random variable. By choosing $w_x$ as the relative frequency of $x$, the
resulting search trees achieve static optimality. This approach generalizes the
recent learning-augmented BSTs [Lin-Luo-Woodruff ICML '22], which only work for
Zipfian distributions, by extending them to arbitrary input distributions.
Furthermore, we demonstrate that our method can be generalized to a B-Tree data
structure using the B-Treap approach [Golovin ICALP '09]. Our search trees are
also capable of leveraging localities in the access sequence through online
self-reorganization, thereby achieving the working-set property. Additionally,
they are robust to prediction errors and support dynamic operations, such as
insertions, deletions, and prediction updates. We complement our analysis with
an empirical study, demonstrating that our method outperforms prior work and
classic data structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and
  StyleGAN2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongmin Jung, Dasaem Jeong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces LAV (Latent Audio-Visual), a system that integrates
EnCodec's neural audio compression with StyleGAN2's generative capabilities to
produce visually dynamic outputs driven by pre-recorded audio. Unlike previous
works that rely on explicit feature mappings, LAV uses EnCodec embeddings as
latent representations, directly transformed into StyleGAN2's style latent
space via randomly initialized linear mapping. This approach preserves semantic
richness in the transformation, enabling nuanced and semantically coherent
audio-visual translations. The framework demonstrates the potential of using
pretrained audio compression models for artistic and computational
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at ISEA 2025, The 30th International Symposium on
  Electronic/Emerging Art, Seoul, Republic of Korea, 23 - 29 May 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CartoAgent: a multimodal large language model-powered multi-agent
  cartographic framework for map style transfer and evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglong Wang, Yuhao Kang, Zhaoya Gong, Pengjun Zhao, Yu Feng, Wenjia Zhang, Ge Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of generative artificial intelligence (GenAI) presents
new opportunities to advance the cartographic process. Previous studies have
either overlooked the artistic aspects of maps or faced challenges in creating
both accurate and informative maps. In this study, we propose CartoAgent, a
novel multi-agent cartographic framework powered by multimodal large language
models (MLLMs). This framework simulates three key stages in cartographic
practice: preparation, map design, and evaluation. At each stage, different
MLLMs act as agents with distinct roles to collaborate, discuss, and utilize
tools for specific purposes. In particular, CartoAgent leverages MLLMs' visual
aesthetic capability and world knowledge to generate maps that are both
visually appealing and informative. By separating style from geographic data,
it can focus on designing stylesheets without modifying the vector-based data,
thereby ensuring geographic accuracy. We applied CartoAgent to a specific task
centered on map restyling-namely, map style transfer and evaluation. The
effectiveness of this framework was validated through extensive experiments and
a human evaluation study. CartoAgent can be extended to support a variety of
cartographic design decisions and inform future integrations of GenAI in
cartography.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal
  Perspective <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19458v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19458v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taoyu Su, Jiawei Sheng, Duohe Ma, Xiaodong Li, Juwei Yue, Mengxiao Song, Yingkai Tang, Tingwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from
different Multi-Modal Knowledge Graphs (MMKGs), a critical information
retrieval task. Existing studies have explored various fusion paradigms and
consistency constraints to improve the alignment of equivalent entities, while
overlooking that the visual modality may not always contribute positively.
Empirically, entities with low-similarity images usually generate
unsatisfactory performance, highlighting the limitation of overly relying on
visual features. We believe the model can be biased toward the visual modality,
leading to a shortcut image-matching task. To address this, we propose a
counterfactual debiasing framework for MMEA, termed CDMEA, which investigates
visual modality bias from a causal perspective. Our approach aims to leverage
both visual and graph modalities to enhance MMEA while suppressing the direct
causal effect of the visual modality on model predictions. By estimating the
Total Effect (TE) of both modalities and excluding the Natural Direct Effect
(NDE) of the visual modality, we ensure that the model predicts based on the
Total Indirect Effect (TIE), effectively utilizing both modalities and reducing
visual modality bias. Extensive experiments on 9 benchmark datasets show that
CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,
high-noise, and low-resource data scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>Mobile: Efficient <span class="highlight-title">Prompt</span>us for Low Bandwidth Mobile Video
  Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16112v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16112v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liming Liu, Jiangkai Wu, Haoyang Wang, Peiheng Wang, Zongming Guo, Xinggong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional video compression algorithms exhibit significant quality
degradation at extremely low bitrates. Promptus emerges as a new paradigm for
video streaming, substantially cutting down the bandwidth essential for video
streaming. However, Promptus is computationally intensive and can not run in
real-time on mobile devices. This paper presents PromptMobile, an efficient
acceleration framework tailored for on-device Promptus. Specifically, we
propose (1) a two-stage efficient generation framework to reduce computational
cost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant
computations by 16.6%, (3) system-level optimizations to further enhance
efficiency. The evaluations demonstrate that compared with the original
Promptus, PromptMobile achieves a 13.6x increase in image generation speed.
Compared with other streaming methods, PromptMobile achives an average LPIPS
improvement of 0.016 (compared with H.265), reducing 60% of severely distorted
frames (compared to VQGAN).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages (excluding references), 10 figures, to appear in APNET 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-14T00:00:00Z">2025-05-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">50</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictability Shapes Adaptation: An Evolutionary Perspective on Modes
  of Learning in <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Y. Ku, Thomas L. Griffiths, Stephanie C. Y. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models learn in two distinct modes: in-weights learning (IWL),
encoding knowledge into model weights, and in-context learning (ICL), adapting
flexibly to context without weight modification. To better understand the
interplay between these learning modes, we draw inspiration from evolutionary
biology's analogous adaptive strategies: genetic encoding (akin to IWL,
adapting over generations and fixed within an individual's lifetime) and
phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to
environmental cues). In evolutionary biology, environmental predictability
dictates the balance between these strategies: stability favors genetic
encoding, while reliable predictive cues promote phenotypic plasticity. We
experimentally operationalize these dimensions of predictability and
systematically investigate their influence on the ICL/IWL balance in
Transformers. Using regression and classification tasks, we show that high
environmental stability decisively favors IWL, as predicted, with a sharp
transition at maximal stability. Conversely, high cue reliability enhances ICL
efficacy, particularly when stability is low. Furthermore, learning dynamics
reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift
occurs in some settings (e.g., classification with many classes), we
demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL
acquisition (e.g., regression) can exhibit an initial IWL phase later yielding
to ICL dominance. These findings support a relative-cost hypothesis for
explaining these learning mode transitions, establishing predictability as a
critical factor governing adaptive strategies in Transformers, and offering
novel insights for understanding ICL and guiding training methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Large Language Models Know Conflict? Investigating Parametric vs.
  Non-Parametric Knowledge of LLMs for Conflict Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apollinaire Poli Nemkova, Sarath Chandra Lingareddy, Sagnik Ray Choudhury, Mark V. Albert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive performance across natural
language tasks, but their ability to forecast violent conflict remains
underexplored. We investigate whether LLMs possess meaningful parametric
knowledge-encoded in their pretrained weights-to predict conflict escalation
and fatalities without external data. This is critical for early warning
systems, humanitarian planning, and policy-making. We compare this parametric
knowledge with non-parametric capabilities, where LLMs access structured and
unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent
news reports via Retrieval-Augmented Generation (RAG). Incorporating external
information could enhance model performance by providing up-to-date context
otherwise missing from pretrained weights. Our two-part evaluation framework
spans 2020-2024 across conflict-prone regions in the Horn of Africa and the
Middle East. In the parametric setting, LLMs predict conflict trends and
fatalities relying only on pretrained knowledge. In the non-parametric setting,
models receive summaries of recent conflict events, indicators, and
geopolitical developments. We compare predicted conflict trend labels (e.g.,
Escalate, Stable Conflict, De-escalate, Peace) and fatalities against
historical data. Our findings highlight the strengths and limitations of LLMs
for conflict forecasting and the benefits of augmenting them with structured
external knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiqi Sui, Juan Diego Rodriguez, Philippe Laban, Dean Murphy, Joseph P. Dexter, Richard Jean So, Samuel Baker, Pramit Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Each year, tens of millions of essays are written and graded in college-level
English courses. Students are asked to analyze literary and cultural texts
through a process known as close reading, in which they gather textual details
to formulate evidence-based arguments. Despite being viewed as a basis for
critical thinking and widely adopted as a required element of university
coursework, close reading has never been evaluated on large language models
(LLMs), and multi-discipline benchmarks like MMLU do not include literature as
a subject. To fill this gap, we present KRISTEVA, the first close reading
benchmark for evaluating interpretive reasoning, consisting of 1331
multiple-choice questions adapted from classroom data. With KRISTEVA, we
propose three progressively more difficult sets of tasks to approximate
different elements of the close reading process, which we use to test how well
LLMs may seem to understand and reason about literary works: 1) extracting
stylistic features, 2) retrieving relevant contextual information from
parametric knowledge, and 3) multi-hop reasoning between style and external
contexts. Our baseline results find that, while state-of-the-art LLMs possess
some college-level close reading competency (accuracy 49.7% - 69.7%), their
performances still trail those of experienced human evaluators on 10 out of our
11 tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the generalization of LLM truth directions on conversational
  formats 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timour Ichmoukhamedov, David Martens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several recent works argue that LLMs have a universal truth direction where
true and false statements are linearly separable in the activation space of the
model. It has been demonstrated that linear probes trained on a single hidden
state of the model already generalize across a range of topics and might even
be used for lie detection in LLM conversations. In this work we explore how
this truth direction generalizes between various conversational formats. We
find good generalization between short conversations that end on a lie, but
poor generalization to longer formats where the lie appears earlier in the
input prompt. We propose a solution that significantly improves this type of
generalization by adding a fixed key phrase at the end of each conversation.
Our results highlight the challenges towards reliable LLM lie detectors that
generalize to new settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Detection of Clinical Entities in Lung and Breast Cancer
  Reports Using NLP Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. Moreno-Casanova, J. M. Auñón, A. Mártinez-Pérez, M. E. Pérez-Martínez, M. E. Gas-López
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research projects, including those focused on cancer, rely on the manual
extraction of information from clinical reports. This process is time-consuming
and prone to errors, limiting the efficiency of data-driven approaches in
healthcare. To address these challenges, Natural Language Processing (NLP)
offers an alternative for automating the extraction of relevant data from
electronic health records (EHRs). In this study, we focus on lung and breast
cancer due to their high incidence and the significant impact they have on
public health. Early detection and effective data management in both types of
cancer are crucial for improving patient outcomes. To enhance the accuracy and
efficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels
at identifying relevant entities in clinical texts and converting them into
standardized formats such as SNOMED and OMOP. uQuery not only detects and
classifies entities but also associates them with contextual information,
including negated entities, temporal aspects, and patient-related details. In
this work, we explore the use of NLP techniques, specifically Named Entity
Recognition (NER), to automatically identify and extract key clinical
information from EHRs related to these two cancers. A dataset from Health
Research Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast
cancer and 400 lung cancer reports, was used, with eight clinical entities
manually labeled using the Doccano platform. To perform NER, we fine-tuned the
bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained
in Spanish. Fine-tuning was performed using the Transformers architecture,
enabling accurate recognition of clinical entities in these cancer types. Our
results demonstrate strong overall performance, particularly in identifying
entities like MET and PAT, although challenges remain with less frequent
entities like EVOL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Large Language Models in Multimodal Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejo Lopez-Avila, Jinhua Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal recommender systems (MRS) integrate heterogeneous user and item
data, such as text, images, and structured information, to enhance
recommendation performance. The emergence of large language models (LLMs)
introduces new opportunities for MRS by enabling semantic reasoning, in-context
learning, and dynamic input handling. Compared to earlier pre-trained language
models (PLMs), LLMs offer greater flexibility and generalisation capabilities
but also introduce challenges related to scalability and model accessibility.
This survey presents a comprehensive review of recent work at the intersection
of LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and
data adaptation techniques. We propose a novel taxonomy to characterise
integration patterns, identify transferable techniques from related
recommendation domains, provide an overview of evaluation metrics and datasets,
and point to possible future directions. We aim to clarify the emerging role of
LLMs in multimodal recommendation and support future research in this rapidly
evolving field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Achieving Tokenizer Flexibility in Language Models through Heuristic
  Adaptation and Supertoken Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaurya Sharthak, Vinayak Pahalwan, Adithya Kamath, Adarsh Shirawalmath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (LLMs) are often constrained by their fixed
tokenization schemes, leading to inefficiencies and performance limitations,
particularly for multilingual or specialized applications. This tokenizer
lock-in presents significant challenges. standard methods to overcome this
often require prohibitive computational resources. Although tokenizer
replacement with heuristic initialization aims to reduce this burden, existing
methods often require exhaustive residual fine-tuning and still may not fully
preserve semantic nuances or adequately address the underlying compression
inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a
model-agnostic tokenizer transplantation method, and second, novel
pre-tokenization learning for multi-word Supertokens to enhance compression and
reduce fragmentation. Tokenadapt initializes new unique token embeddings via a
hybrid heuristic that combines two methods: a local estimate based on subword
decomposition using the old tokenizer, and a global estimate utilizing the
top-k semantically similar tokens from the original vocabulary. This
methodology aims to preserve semantics while significantly minimizing
retraining requirements. Empirical investigations validate both contributions:
the transplantation heuristic successfully initializes unique tokens, markedly
outperforming conventional baselines and sophisticated methods including
Transtokenizer and ReTok, while our Supertokens achieve notable compression
gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid
initialization consistently yields lower perplexity ratios compared to both
ReTok and TransTokenizer baselines across different base models and newly
trained target tokenizers. TokenAdapt typically reduced the overall perplexity
ratio significantly compared to ReTok, yielding at least a 2-fold improvement
in these aggregate scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An AI-Powered Research Assistant in the Lab: A Practical Guide for Text
  Analysis Through Iterative Collaboration with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gino Carmona-Díaz, William Jiménez-Leal, María Alejandra Grisales, Chandra Sripada, Santiago Amaya, Michael Inzlicht, Juan Pablo Bermúdez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing texts such as open-ended responses, headlines, or social media
posts is a time- and labor-intensive process highly susceptible to bias. LLMs
are promising tools for text analysis, using either a predefined (top-down) or
a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we
present a step-by-step tutorial to efficiently develop, test, and apply
taxonomies for analyzing unstructured data through an iterative and
collaborative process between researchers and LLMs. Using personal goals
provided by participants as an example, we demonstrate how to write prompts to
review datasets and generate a taxonomy of life domains, evaluate and refine
the taxonomy through prompt and direct modifications, test the taxonomy and
assess intercoder agreements, and apply the taxonomy to categorize an entire
dataset with high intercoder reliability. We discuss the possibilities and
limitations of using LLMs for text analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact
  Extraction and Reference Facts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Liu, Lechen Zhang, Sheza Munir, Yiyang Gu, Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel at generating long-form responses, but
evaluating their factuality remains challenging due to complex inter-sentence
dependencies within the generated facts. Prior solutions predominantly follow a
decompose-decontextualize-verify pipeline but often fail to capture essential
context and miss key relational facts. In this paper, we introduce VeriFact, a
factuality evaluation framework designed to enhance fact extraction by
identifying and resolving incomplete and missing facts to support more accurate
verification results. Moreover, we introduce FactRBench , a benchmark that
evaluates both precision and recall in long-form model responses, whereas prior
work primarily focuses on precision. FactRBench provides reference fact sets
from advanced LLMs and human-written answers, enabling recall assessment.
Empirical evaluations show that VeriFact significantly enhances fact
completeness and preserves complex facts with critical relational information,
resulting in more accurate factuality evaluation. Benchmarking various open-
and close-weight LLMs on FactRBench indicate that larger models within same
model family improve precision and recall, but high precision does not always
correlate with high recall, underscoring the importance of comprehensive
factuality assessment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Agents Mirror Human Causal Reasoning Biases. How Can We Help
  Them Think Like Scientists? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony GX-Chen, Dongyan Lin, Mandana Samiei, Doina Precup, Blake A. Richards, Rob Fergus, Kenneth Marino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LM) agents are increasingly used as autonomous
decision-makers who need to actively gather information to guide their
decisions. A crucial cognitive skill for such agents is the efficient
exploration and understanding of the causal structure of the world -- key to
robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs
possess this capability or exhibit systematic biases leading to erroneous
conclusions. In this work, we examine LMs' ability to explore and infer causal
relationships, using the well-established "Blicket Test" paradigm from
developmental psychology. We find that LMs reliably infer the common, intuitive
disjunctive causal relationships but systematically struggle with the unusual,
yet equally (or sometimes even more) evidenced conjunctive ones. This
"disjunctive bias" persists across model families, sizes, and prompting
strategies, and performance further declines as task complexity increases.
Interestingly, an analogous bias appears in human adults, suggesting that LMs
may have inherited deep-seated reasoning heuristics from their training data.
To this end, we quantify similarities between LMs and humans, finding that LMs
exhibit adult-like inference profiles (but not children-like). Finally, we
propose a test-time sampling method which explicitly samples and eliminates
hypotheses about causal relationships from the LM. This scalable approach
significantly reduces the disjunctive bias and moves LMs closer to the goal of
scientific, causally rigorous reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Customizing a Large Language Model for VHDL Design of High-Performance
  Microprocessors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Dupuis, Ravi Nair, Shyam Ramji, Sean McClintock, Nishant Chauhan, Priyanka Nagpal, Bart Blaner, Ken Valk, Leon Stok, Ruchir Puri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of Large Language Models (LLMs) in hardware design has taken off in
recent years, principally through its incorporation in tools that increase chip
designer productivity. There has been considerable discussion about the use of
LLMs in RTL specifications of chip designs, for which the two most popular
languages are Verilog and VHDL. LLMs and their use in Verilog design has
received significant attention due to the higher popularity of the language,
but little attention so far has been given to VHDL despite its continued
popularity in the industry. There has also been little discussion about the
unique needs of organizations that engage in high-performance processor design,
and techniques to deploy AI solutions in these settings. In this paper, we
describe our journey in developing a Large Language Model (LLM) specifically
for the purpose of explaining VHDL code, a task that has particular importance
in an organization with decades of experience and assets in high-performance
processor design. We show how we developed test sets specific to our needs and
used them for evaluating models as we performed extended pretraining (EPT) of a
base LLM. Expert evaluation of the code explanations produced by the EPT model
increased to 69% compared to a base model rating of 43%. We further show how we
developed an LLM-as-a-judge to gauge models similar to expert evaluators. This
led us to deriving and evaluating a host of new models, including an
instruction-tuned version of the EPT model with an expected expert evaluator
rating of 71%. Our experiments also indicate that with the potential use of
newer base models, this rating can be pushed to 85% and beyond. We conclude
with a discussion on further improving the quality of hardware design LLMs
using exciting new developments in the Generative AI world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives
  in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Mushtaq, Imran Taj, Rafay Naeem, Ibrahim Ghaznavi, Junaid Qadir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are predominantly trained and aligned in ways
that reinforce Western-centric epistemologies and socio-cultural norms, leading
to cultural homogenization and limiting their ability to reflect global
civilizational plurality. Existing benchmarking frameworks fail to adequately
capture this bias, as they rely on rigid, closed-form assessments that overlook
the complexity of cultural inclusivity. To address this, we introduce
WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity
(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our
approach is grounded in the Multiplex Worldview proposed by Senturk et al.,
which distinguishes between Uniplex models, reinforcing cultural
homogenization, and Multiplex models, which integrate diverse perspectives.
WorldView-Bench measures Cultural Polarization, the exclusion of alternative
perspectives, through free-form generative evaluation rather than conventional
categorical benchmarks. We implement applied multiplexity through two
intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where
system prompts embed multiplexity principles, and (2) Multi-Agent System
(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing
distinct cultural perspectives collaboratively generate responses. Our results
demonstrate a significant increase in Perspectives Distribution Score (PDS)
entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,
alongside a shift toward positive sentiment (67.7%) and enhanced cultural
balance. These findings highlight the potential of multiplex-aware AI
evaluation in mitigating cultural bias in LLMs, paving the way for more
inclusive and ethically aligned AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Submitted to the Journal of Artificial Intelligence
  Research (JAIR) on April 29, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ System <span class="highlight-title">Prompt</span> Optimization with Meta-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumin Choi, Jinheon Baek, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities, with
optimizing their input prompts playing a pivotal role in maximizing their
performance. However, while LLM prompts consist of both the task-agnostic
system prompts and task-specific user prompts, existing work on prompt
optimization has focused on user prompts specific to individual queries or
tasks, and largely overlooked the system prompt that is, once optimized,
applicable across different tasks and domains. Motivated by this, we introduce
the novel problem of bilevel system prompt optimization, whose objective is to
design system prompts that are robust to diverse user prompts and transferable
to unseen tasks. To tackle this problem, we then propose a meta-learning
framework, which meta-learns the system prompt by optimizing it over various
user prompts across multiple datasets, while simultaneously updating the user
prompts in an iterative manner to ensure synergy between them. We conduct
experiments on 14 unseen datasets spanning 5 different domains, on which we
show that our approach produces system prompts that generalize effectively to
diverse user prompts. Also, our findings reveal that the optimized system
prompt enables rapid adaptation even to unseen tasks, requiring fewer
optimization steps for test-time user prompts while achieving improved
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns
  in Reddit via LLM-Enhanced Topic Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sulong Zhou, Qunying Huang, Shaoheng Zhou, Yun Hang, Xinyue Ye, Aodong Mei, Kathryn Phung, Yuning Ye, Uma Govindswamy, Zehan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wildfires have become increasingly frequent, irregular, and severe in recent
years. Understanding how affected populations perceive and respond during
wildfire crises is critical for timely and empathetic disaster response. Social
media platforms offer a crowd-sourced channel to capture evolving public
discourse, providing hyperlocal information and insight into public sentiment.
This study analyzes Reddit discourse during the 2025 Los Angeles wildfires,
spanning from the onset of the disaster to full containment. We collect 385
posts and 114,879 comments related to the Palisades and Eaton fires. We adopt
topic modeling methods to identify the latent topics, enhanced by large
language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we
develop a hierarchical framework to categorize latent topics, consisting of two
main categories, Situational Awareness (SA) and Crisis Narratives (CN). The
volume of SA category closely aligns with real-world fire progressions, peaking
within the first 2-5 days as the fires reach the maximum extent. The most
frequent co-occurring category set of public health and safety, loss and
damage, and emergency resources expands on a wide range of health-related
latent topics, including environmental health, occupational health, and one
health. Grief signals and mental health risks consistently accounted for 60
percentage and 40 percentage of CN instances, respectively, with the highest
total volume occurring at night. This study contributes the first annotated
social media dataset on the 2025 LA fires, and introduces a scalable
multi-layer framework that leverages topic modeling for crisis discourse
analysis. By identifying persistent public health concerns, our results can
inform more empathetic and adaptive strategies for disaster response, public
health communication, and future research in comparable climate-related
disaster events.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PT-MoE: An Efficient Finetuning Framework for Integrating
  Mixture-of-Experts into <span class="highlight-title">Prompt</span> Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongqian Li, Yixuan Su, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting
large language models, yet existing approaches exhibit counter-intuitive
phenomena: integrating router into prompt tuning (PT) increases training
efficiency yet does not improve performance universally; parameter reduction
through matrix decomposition can improve performance in specific domains.
Motivated by these observations and the modular nature of PT, we propose
PT-MoE, a novel framework that integrates matrix decomposition with
mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets
demonstrate that PT-MoE achieves state-of-the-art performance in both question
answering (QA) and mathematical problem solving tasks, improving F1 score by
1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing
mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all
while using 25% fewer parameters than LoRA. Our analysis reveals that while PT
methods generally excel in QA tasks and LoRA-based methods in math datasets,
the integration of matrix decomposition and MoE in PT-MoE yields complementary
benefits: decomposition enables efficient parameter sharing across experts
while MoE provides dynamic adaptation, collectively enabling PT-MoE to
demonstrate cross-task consistency and generalization abilities. These
findings, along with ablation studies on routing mechanisms and architectural
components, provide insights for future PEFT methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CXMArena: Unified <span class="highlight-title">Dataset</span> to benchmark performance in realistic CXM
  Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Garg, Kapil Sharma, Karan Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) hold immense potential for revolutionizing
Customer Experience Management (CXM), particularly in contact center
operations. However, evaluating their practical utility in complex operational
environments is hindered by data scarcity (due to privacy concerns) and the
limitations of current benchmarks. Existing benchmarks often lack realism,
failing to incorporate deep knowledge base (KB) integration, real-world noise,
or critical operational tasks beyond conversational fluency. To bridge this
gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset
specifically designed for evaluating AI in operational CXM contexts. Given the
diversity in possible contact center features, we have developed a scalable
LLM-powered pipeline that simulates the brand's CXM entities that form the
foundation of our datasets-such as knowledge articles including product
specifications, issue taxonomies, and contact center conversations. The
entities closely represent real-world distribution because of controlled noise
injection (informed by domain experts) and rigorous automated validation.
Building on this, we release CXMArena, which provides dedicated benchmarks
targeting five important operational tasks: Knowledge Base Refinement, Intent
Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with
Integrated Tools. Our baseline experiments underscore the benchmark's
difficulty: even state of the art embedding and generation models achieve only
68% accuracy on article search, while standard embedding methods yield a low F1
score of 0.3 for knowledge base refinement, highlighting significant challenges
for current models necessitating complex pipelines and solutions over
conventional techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Are More Persuasive Than Incentivized Human
  Persuaders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Schoenegger, Francesco Salvi, Jiacheng Liu, Xiaoli Nan, Ramit Debnath, Barbara Fasolo, Evelina Leivada, Gabriel Recchia, Fritz Günther, Ali Zarifhonarvar, Joe Kwon, Zahoor Ul Islam, Marco Dehnert, Daryl Y. H. Lee, Madeline G. Reinecke, David G. Kamper, Mert Kobaş, Adam Sandford, Jonas Kgomo, Luke Hewitt, Shreya Kapoor, Kerem Oktar, Eyup Engin Kucuk, Bo Feng, Cameron R. Jones, Izzy Gainsburg, Sebastian Olschewski, Nora Heinzelmann, Francisco Cruz, Ben M. Tappin, Tao Ma, Peter S. Park, Rayan Onyonka, Arthur Hjorth, Peter Slattery, Qingcheng Zeng, Lennart Finke, Igor Grossmann, Alessandro Salatiello, Ezra Karger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We directly compare the persuasion capabilities of a frontier large language
model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an
interactive, real-time conversational quiz setting. In this preregistered,
large-scale incentivized experiment, participants (quiz takers) completed an
online quiz where persuaders (either humans or LLMs) attempted to persuade quiz
takers toward correct or incorrect answers. We find that LLM persuaders
achieved significantly higher compliance with their directional persuasion
attempts than incentivized human persuaders, demonstrating superior persuasive
capabilities in both truthful (toward correct answers) and deceptive (toward
incorrect answers) contexts. We also find that LLM persuaders significantly
increased quiz takers' accuracy, leading to higher earnings, when steering quiz
takers toward correct answers, and significantly decreased their accuracy,
leading to lower earnings, when steering them toward incorrect answers.
Overall, our findings suggest that AI's persuasion capabilities already exceed
those of humans that have real-money bonuses tied to performance. Our findings
of increasingly capable AI persuaders thus underscore the urgency of emerging
alignment and governance frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Machine Translation with Quantum Encoder Decoder
  Attention-based Convolutional Variational Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subrit Dikshit, Ritu Tiwari, Priyank Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud-based multilingual translation services like Google Translate and
Microsoft Translator achieve state-of-the-art translation capabilities. These
services inherently use large multilingual language models such as GRU, LSTM,
BERT, GPT, T5, or similar encoder-decoder architectures with attention
mechanisms as the backbone. Also, new age natural language systems, for
instance ChatGPT and DeepSeek, have established huge potential in multiple
tasks in natural language processing. At the same time, they also possess
outstanding multilingual translation capabilities. However, these models use
the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder
Attention-based Convolutional Variational Circuits) is an alternate solution
that explores the quantum computing realm instead of the classical computing
realm to study and demonstrate multilingual machine translation. QEDACVC
introduces the quantum encoder-decoder architecture that simulates and runs on
quantum computing hardware via quantum convolution, quantum pooling, quantum
variational circuit, and quantum attention as software alterations. QEDACVC
achieves an Accuracy of 82% when trained on the OPUS dataset for English,
French, German, and Hindi corpora for multilingual translations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qwen3 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present Qwen3, the latest version of the Qwen model family.
Qwen3 comprises a series of large language models (LLMs) designed to advance
performance, efficiency, and multilingual capabilities. The Qwen3 series
includes models of both dense and Mixture-of-Expert (MoE) architectures, with
parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is
the integration of thinking mode (for complex, multi-step reasoning) and
non-thinking mode (for rapid, context-driven responses) into a unified
framework. This eliminates the need to switch between different models--such as
chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,
QwQ-32B)--and enables dynamic mode switching based on user queries or chat
templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing
users to allocate computational resources adaptively during inference, thereby
balancing latency and performance based on task complexity. Moreover, by
leveraging the knowledge from the flagship models, we significantly reduce the
computational resources required to build smaller-scale models, while ensuring
their highly competitive performance. Empirical evaluations demonstrate that
Qwen3 achieves state-of-the-art results across diverse benchmarks, including
tasks in code generation, mathematical reasoning, agent tasks, etc.,
competitive against larger MoE models and proprietary models. Compared to its
predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119
languages and dialects, enhancing global accessibility through improved
cross-lingual understanding and generation capabilities. To facilitate
reproducibility and community-driven research and development, all Qwen3 models
are publicly accessible under Apache 2.0.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment
  and Distraction in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingcheng Niu, Xingdi Yuan, Tong Wang, Hamidreza Saghir, Amir H. Abdi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We observe a novel phenomenon, contextual entrainment, across a wide range of
language models (LMs) and prompt settings, providing a new mechanistic
perspective on how LMs become distracted by ``irrelevant'' contextual
information in the input prompt. Specifically, LMs assign significantly higher
logits (or probabilities) to any tokens that have previously appeared in the
context prompt, even for random tokens. This suggests that contextual
entrainment is a mechanistic phenomenon, occurring independently of the
relevance or semantic relation of the tokens to the question or the rest of the
sentence. We find statistically significant evidence that the magnitude of
contextual entrainment is influenced by semantic factors. Counterfactual
prompts have a greater effect compared to factual ones, suggesting that while
contextual entrainment is a mechanistic phenomenon, it is modulated by semantic
factors.
  We hypothesise that there is a circuit of attention heads -- the entrainment
heads -- that corresponds to the contextual entrainment phenomenon. Using a
novel entrainment head discovery method based on differentiable masking, we
identify these heads across various settings. When we ``turn off'' these heads,
i.e., set their outputs to zero, the effect of contextual entrainment is
significantly attenuated, causing the model to generate output that capitulates
to what it would produce if no distracting context were provided. Our discovery
of contextual entrainment, along with our investigation into LM distraction via
the entrainment heads, marks a key step towards the mechanistic analysis and
mitigation of the distraction problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scent of Knowledge: Optimizing Search-Enhanced Reasoning with
  Information Foraging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjin Qian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Scalable Unsupervised Framework for multi-aspect labeling of
  Multilingual and Multi-Domain <span class="highlight-title">Review</span> Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiin Park, Misuk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively analyzing online review data is essential across industries.
However, many existing studies are limited to specific domains and languages or
depend on supervised learning approaches that require large-scale labeled
datasets. To address these limitations, we propose a multilingual, scalable,
and unsupervised framework for cross-domain aspect detection. This framework is
designed for multi-aspect labeling of multilingual and multi-domain review
data. In this study, we apply automatic labeling to Korean and English review
datasets spanning various domains and assess the quality of the generated
labels through extensive experiments. Aspect category candidates are first
extracted through clustering, and each review is then represented as an
aspect-aware embedding vector using negative sampling. To evaluate the
framework, we conduct multi-aspect labeling and fine-tune several pretrained
language models to measure the effectiveness of the automatically generated
labels. Results show that these models achieve high performance, demonstrating
that the labels are suitable for training. Furthermore, comparisons with
publicly available large language models highlight the framework's superior
consistency and scalability when processing large-scale data. A human
evaluation also confirms that the quality of the automatic labels is comparable
to those created manually. This study demonstrates the potential of a robust
multi-aspect labeling approach that overcomes limitations of supervised methods
and is adaptable to multilingual, multi-domain environments. Future research
will explore automatic review summarization and the integration of artificial
intelligence agents to further improve the efficiency and depth of review
analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How an unintended Side Effect of a Research Project led to Boosting the
  Power of UML 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ulrich Frank, Pierre Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the design, implementation and use of a new UML modeling
tool that represents a significant advance over conventional tools. Among other
things, it allows the integration of class diagrams and object diagrams as well
as the execution of objects. This not only enables new software architectures
characterized by the integration of software with corresponding object models,
but is also ideal for use in teaching, as it provides students with a
particularly stimulating learning experience. A special feature of the project
is that it has emerged from a long-standing international research project,
which is aimed at a comprehensive multi-level architecture. The project is
therefore an example of how research can lead to valuable results that arise as
a side effect of other work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Focus, Merge, Rank: Improved Question Answering Based on Semi-structured
  Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derian Boer, Stephen Roth, Stefan Kramer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world settings, machine learning models and interactive systems
have access to both structured knowledge, e.g., knowledge graphs or tables, and
unstructured content, e.g., natural language documents. However, most rely on
either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking
unstructured content to nodes within structured data, thereby enabling new
strategies for knowledge access and use. In this work, we present
FocusedRetriever, a modular SKB-based framework for multi-hop question
answering. It integrates components (VSS-based entity search, LLM-based
generation of Cypher queries and pairwise re-ranking) in a way that enables it
to outperform state-of-the-art methods across all three STaRK benchmark test
sets, covering diverse domains and multiple performance metrics. The average
first-hit rate exceeds that of the second-best method by 25.7%.
FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to
extract relational facts and entity attributes from unstructured text, (2) node
set joins to filter answer candidates based on these extracted triplets and
constraints, (3) vector similarity search to retrieve and rank relevant
unstructured content, and (4) the contextual capabilities of LLMs to finally
rank the top-k answers. For generality, we only incorporate base LLMs in
FocusedRetriever in our evaluation. However, our analysis of intermediate
results highlights several opportunities for further upgrades including
finetuning. The source code is publicly available at
https://github.com/kramerlab/FocusedRetriever .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ornithologist: Towards Trustworthy "Reasoning" about Central Bank
  Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominic Zaun Eu Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  I develop Ornithologist, a weakly-supervised textual classification system
and measure the hawkishness and dovishness of central bank text. Ornithologist
uses ``taxonomy-guided reasoning'', guiding a large language model with
human-authored decision trees. This increases the transparency and
explainability of the system and makes it accessible to non-experts. It also
reduces hallucination risk. Since it requires less supervision than traditional
classification systems, it can more easily be applied to other problems or
sources of text (e.g. news) without much modification. Ornithologist
measurements of hawkishness and dovishness of RBA communication carry
information about the future of the cash rate path and of market expectations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CEC-Zero: Chinese Error Correction Solution Based on LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophie Zhang, Zhiming Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) demonstrate exceptional
Chinese text processing capabilities, particularly in Chinese Spelling
Correction (CSC). While LLMs outperform traditional BERT-based models in
accuracy and robustness, challenges persist in reliability and generalization.
This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework
enabling LLMs to self-correct through autonomous error strategy learning
without external supervision. By integrating RL with LLMs' generative power,
the method eliminates dependency on annotated data or auxiliary models.
Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and
superior cross-domain generalization, offering a scalable solution for
reliability optimization in Chinese NLP applications. This breakthrough
facilitates LLM deployment in practical Chinese text correction scenarios while
establishing a new paradigm for self-improving language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent
  Thinking Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer Haase, Paul H. P. Hanel, Sebastian Pokutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces S-DAT (Synthetic-Divergent Association Task), a
scalable, multilingual framework for automated assessment of divergent thinking
(DT) -a core component of human creativity. Traditional creativity assessments
are often labor-intensive, language-specific, and reliant on subjective human
ratings, limiting their scalability and cross-cultural applicability. In
contrast, S-DAT leverages large language models and advanced multilingual
embeddings to compute semantic distance -- a language-agnostic proxy for DT. We
evaluate S-DAT across eleven diverse languages, including English, Spanish,
German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating
robust and consistent scoring across linguistic contexts. Unlike prior DAT
approaches, the S-DAT shows convergent validity with other DT measures and
correct discriminant validity with convergent thinking. This cross-linguistic
flexibility allows for more inclusive, global-scale creativity research,
addressing key limitations of earlier approaches. S-DAT provides a powerful
tool for fairer, more comprehensive evaluation of cognitive flexibility in
diverse populations and can be freely assessed online:
https://sdat.iol.zib.de/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like
  Training of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiwen Chen, Wenhui Zhu, Peijie Qiu, Xuanzhao Dong, Hao Wang, Haiyu Wu, Huayu Li, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in reinforcement learning for language model post-training,
such as Group Relative Policy Optimization (GRPO), have shown promise in
low-resource settings. However, GRPO typically relies on solution-level and
scalar reward signals that fail to capture the semantic diversity among sampled
completions. This leads to what we identify as a diversity-quality
inconsistency, where distinct reasoning paths may receive indistinguishable
rewards. To address this limitation, we propose $\textit{Diversity-aware Reward
Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity
into the reward computation. DRA uses Submodular Mutual Information (SMI) to
downweight redundant completions and amplify rewards for diverse ones. This
encourages better exploration during learning, while maintaining stable
exploitation of high-quality samples. Our method integrates seamlessly with
both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and
$\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning
benchmarks and find that it outperforms recent strong baselines. It achieves
state-of-the-art performance with an average accuracy of 58.2%, using only
7,000 fine-tuning samples and a total training cost of approximately $55. The
code is available at https://github.com/xiwenc1/DRA-GRPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Analysis of Large Language Model Outputs: Similarity,
  Diversity, and Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Smith, Mohamed Reda Bouadjenek, Tahsin Alamgir Kheya, Phillip Dawson, Sunil Aryal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) represent a major step toward artificial general
intelligence, significantly advancing our ability to interact with technology.
While LLMs perform well on Natural Language Processing tasks -- such as
translation, generation, code writing, and summarization -- questions remain
about their output similarity, variability, and ethical implications. For
instance, how similar are texts generated by the same model? How does this
compare across different models? And which models best uphold ethical
standards? To investigate, we used 5{,}000 prompts spanning diverse tasks like
generation, explanation, and rewriting. This resulted in approximately 3
million texts from 12 LLMs, including proprietary and open-source systems from
OpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs
from the same LLM are more similar to each other than to human-written texts;
(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4
produces more varied responses; (3) LLM writing styles differ significantly,
with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for
distinctiveness; (4) differences in vocabulary and tone underscore the
linguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate
greater gender balance and reduced bias. These results offer new insights into
the behavior and diversity of LLM outputs, helping guide future development and
ethical evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Atomic Consistency Preference Optimization for Long-Form Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingfeng Chen, Raghuveer Thirukovalluru, Junlin Wang, Kaiwei Luo, Bhuwan Dhingra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) frequently produce factoid hallucinations -
plausible yet incorrect answers. A common mitigation strategy is model
alignment, which improves factual accuracy by training on curated factual and
non-factual pairs. However, this approach often relies on a stronger model
(e.g., GPT-4) or an external knowledge base to assess factual correctness,
which may not always be accessible. To address this, we propose Atomic
Consistency Preference Optimization (ACPO), a self-supervised preference-tuning
method that enhances factual accuracy without external supervision. ACPO
leverages atomic consistency signals, i.e., the agreement of individual facts
across multiple stochastic responses, to identify high- and low-quality data
pairs for model alignment. By eliminating the need for costly GPT calls, ACPO
provides a scalable and efficient approach to improving factoid
question-answering. Despite being self-supervised, empirical results
demonstrate that ACPO outperforms FactAlign, a strong supervised alignment
baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its
effectiveness in enhancing factual reliability without relying on external
models or knowledge bases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from
  Multiple Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayoung Lee, Ryan Sungmo Kwon, Peter Railton, Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating high-stakes dilemmas involving conflicting values is challenging
even for humans, let alone for AI. Yet prior work in evaluating the reasoning
capabilities of large language models (LLMs) in such situations has been
limited to everyday scenarios. To close this gap, this work first introduces
CLASH (Character perspective-based LLM Assessments in Situations with
High-stakes), a meticulously curated dataset consisting of 345 high-impact
dilemmas along with 3,795 individual perspectives of diverse values. In
particular, we design CLASH in a way to support the study of critical aspects
of value-based decision-making processes which are missing from prior work,
including understanding decision ambivalence and psychological discomfort as
well as capturing the temporal shifts of values in characters' perspectives. By
benchmarking 10 open and closed frontier models, we uncover several key
findings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet,
achieve less than 50% accuracy in identifying situations where the decision
should be ambivalent, while they perform significantly better in clear-cut
scenarios. (2) While LLMs reasonably predict psychological discomfort as marked
by human, they inadequately comprehend perspectives involving value shifts,
indicating a need for LLMs to reason over complex values. (3) Our experiments
also reveal a significant correlation between LLMs' value preferences and their
steerability towards a given value. (4) Finally, LLMs exhibit greater
steerability when engaged in value reasoning from a third-party perspective,
compared to a first-person setup, though certain value pairs benefit uniquely
from the first-person framing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypernym Mercury: Token Optimization Through Semantic Field Constriction
  And Reconstruction From Hypernyms. A New Text Compression Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08058v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08058v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Forrester, Octavia Sulea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compute optimization using token reduction of LLM prompts is an emerging task
in the fields of NLP and next generation, agentic AI. In this white paper, we
introduce a novel (patent pending) text representation scheme and a
first-of-its-kind word-level semantic compression of paragraphs that can lead
to over 90% token reduction, while retaining high semantic similarity to the
source text. We explain how this novel compression technique can be lossless
and how the detail granularity is controllable. We discuss benchmark results
over open source data (i.e. Bram Stoker's Dracula available through Project
Gutenberg) and show how our results hold at the paragraph level, across
multiple genres and models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for
  Embodied Interactive Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, Weiming Lu, Peng Li, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep thinking models have demonstrated remarkable
reasoning capabilities on mathematical and coding tasks. However, their
effectiveness in embodied domains which require continuous interaction with
environments through image action interleaved trajectories remains largely
-unexplored. We present Embodied Reasoner, a model that extends o1 style
reasoning to interactive embodied search tasks. Unlike mathematical reasoning
that relies primarily on logical deduction, embodied scenarios demand spatial
understanding, temporal reasoning, and ongoing self-reflection based on
interaction history. To address these challenges, we synthesize 9.3k coherent
Observation-Thought-Action trajectories containing 64k interactive images and
90k diverse thinking processes (analysis, spatial reasoning, reflection,
planning, and verification). We develop a three-stage training pipeline that
progressively enhances the model's capabilities through imitation learning,
self-exploration via rejection sampling, and self-correction through reflection
tuning. The evaluation shows that our model significantly outperforms those
advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and
Claude-3.7 by +9\%, 24\%, and +13\%. Analysis reveals our model exhibits fewer
repeated searches and logical inconsistencies, with particular advantages in
complex long-horizon tasks. Real-world environments also show our superiority
while exhibiting fewer repeated searches and logical inconsistency cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/zwq2018/embodied_reasoner Dataset:
  https://huggingface.co/datasets/zwq2018/embodied_reasoner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Activation Steering in Neural Theorem Provers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Kirtania
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown promise in proving formal theorems
using proof assistants like Lean. However, current state of the art language
models struggles to predict next step in proofs leading practitioners to use
different sampling techniques to improve LLMs capabilities. We observe that the
LLM is capable of predicting the correct tactic; however, it faces challenges
in ranking it appropriately within the set of candidate tactics, affecting the
overall selection process. To overcome this hurdle, we use activation steering
to guide LLMs responses to improve the generations at the time of inference.
Our results suggest that activation steering offers a promising lightweight
alternative to specialized fine-tuning for enhancing theorem proving
capabilities in LLMs, particularly valuable in resource-constrained
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>incorrect explanation for a concept, need to revise and update!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OAEI-LLM-T: A TBox Benchmark <span class="highlight-title">Dataset</span> for Understanding Large Language
  Model Hallucinations in Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21813v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21813v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang, Jing Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations are often inevitable in downstream tasks using large language
models (LLMs). To tackle the substantial challenge of addressing hallucinations
for LLM-based ontology matching (OM) systems, we introduce a new benchmark
dataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the
Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of
ten different LLMs performing OM tasks. These OM-specific hallucinations are
organised into two primary categories and six sub-categories. We showcase the
usefulness of the dataset in constructing an LLM leaderboard for OM tasks and
for fine-tuning LLMs used in OM tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, 4 tables, 2 prompt templates</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Llama-Nemotron: Efficient Reasoning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00949v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00949v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Shaona Ghosh, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Chris Alexiuk, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Llama-Nemotron series of models, an open family of
heterogeneous reasoning models that deliver exceptional reasoning capabilities,
inference efficiency, and an open license for enterprise use. The family comes
in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs
competitively with state-of-the-art reasoning models such as DeepSeek-R1 while
offering superior inference throughput and memory efficiency. In this report,
we discuss the training procedure for these models, which entails using neural
architecture search from Llama 3 models for accelerated inference, knowledge
distillation, and continued pretraining, followed by a reasoning-focused
post-training stage consisting of two main parts: supervised fine-tuning and
large scale reinforcement learning. Llama-Nemotron models are the first
open-source models to support a dynamic reasoning toggle, allowing users to
switch between standard chat and reasoning modes during inference. To further
support open research and facilitate model development, we provide the
following resources: 1. We release the Llama-Nemotron reasoning models --
LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA
Open Model License Agreement. 2. We release the complete post-training dataset:
Llama-Nemotron-Post-Training-Dataset. 3. We also release our training
codebases: NeMo, NeMo-Aligner, and Megatron-LM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSLFormer: A Lightweight <span class="highlight-title">Transformer</span> Model for Turkish Sign Language
  Recognition Using Skeletal Landmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kutay Ertürk, Furkan Altınışık, İrem Sarıaltın, Ömer Nezih Gerek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents TSLFormer, a light and robust word-level Turkish Sign
Language (TSL) recognition model that treats sign gestures as ordered,
string-like language. Instead of using raw RGB or depth videos, our method only
works with 3D joint positions - articulation points - extracted using Google's
Mediapipe library, which focuses on the hand and torso skeletal locations. This
creates efficient input dimensionality reduction while preserving important
semantic gesture information.
  Our approach revisits sign language recognition as sequence-to-sequence
translation, inspired by the linguistic nature of sign languages and the
success of transformers in natural language processing. Since TSLFormer uses
the self-attention mechanism, it effectively captures temporal co-occurrence
within gesture sequences and highlights meaningful motion patterns as words
unfold.
  Evaluated on the AUTSL dataset with over 36,000 samples and 227 different
words, TSLFormer achieves competitive performance with minimal computational
cost. These results show that joint-based input is sufficient for enabling
real-time, mobile, and assistive communication systems for hearing-impaired
individuals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is analogy enough to draw novel adjective-noun inferences? <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.24293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.24293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hayley Ross, Kathryn Davidson, Najoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work (Ross et al., 2025, 2024) has argued that the ability of humans
and LLMs respectively to generalize to novel adjective-noun combinations shows
that they each have access to a compositional mechanism to determine the
phrase's meaning and derive inferences. We study whether these inferences can
instead be derived by analogy to known inferences, without need for
composition. We investigate this by (1) building a model of analogical
reasoning using similarity over lexical items, and (2) asking human
participants to reason by analogy. While we find that this strategy works well
for a large proportion of the dataset of Ross et al. (2025), there are novel
combinations for which both humans and LLMs derive convergent inferences but
which are not well handled by analogy. We thus conclude that the mechanism
humans and LLMs use to generalize in these cases cannot be fully reduced to
analogy, and likely involves composition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages (17 pages with appendix). Accepted to SCiL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hakim: Farsi Text Embedding Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehran Sarmadi, Morteza Alikhani, Erfan Zinvandi, Zahra Pourbahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text embedding have significantly improved natural
language understanding across many languages, yet Persian remains notably
underrepresented in large-scale embedding research. In this paper, we present
Hakim, a novel state-of-the-art Persian text embedding model that achieves a
8.5% performance improvement over existing approaches on the FaMTEB benchmark,
outperforming all previously developed Persian language models. As part of this
work, we introduce three new datasets - Corpesia, Pairsia-sup, and
Pairsia-unsup - to support supervised and unsupervised training scenarios.
Additionally, Hakim is designed for applications in chatbots and
retrieval-augmented generation (RAG) systems, particularly addressing retrieval
tasks that require incorporating message history within these systems. We also
propose a new baseline model built on the BERT architecture. Our language model
consistently achieves higher accuracy across various Persian NLP tasks, while
the RetroMAE-based model proves particularly effective for textual information
retrieval applications. Together, these contributions establish a new
foundation for advancing Persian language understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Features in <span class="highlight-title">Prompt</span>s Jailbreak LLMs? Investigating the Mechanisms
  Behind Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathalie Kirch, Constantin Weisser, Severin Field, Helen Yannakoudakis, Stephen Casper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Jailbreaks have been a central focus of research regarding the safety and
reliability of large language models (LLMs), yet the mechanisms underlying
these attacks remain poorly understood. While previous studies have
predominantly relied on linear methods to detect jailbreak attempts and model
refusals, we take a different approach by examining both linear and non-linear
features in prompts that lead to successful jailbreaks. First, we introduce a
novel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack
methods. Leveraging this dataset, we train probes to classify successful from
unsuccessful jailbreaks using the latent representations corresponding to
prompt tokens. Notably, we find that even when probes achieve high accuracy in
predicting the success of jailbreaks, their performance often fails to
generalize to unseen attack methods. This reveals that different jailbreaking
strategies exploit different non-linear, non-universal features. Next, we
demonstrate that non-linear probes provide a powerful tool for steering model
behavior. Specifically, we use these probes to guide targeted latent space
perturbations, enabling us to effectively modulate the model's robustness
against jailbreaks. Overall, our findings challenge the assumption that
jailbreaks can be fully understood through linear or simple universal prompt
features alone, highlighting the importance of a nuanced understanding of the
mechanisms behind LLM vulnerabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Clinical Competencies of Large Language Models with a General
  Practice Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheqing Li, Yiying Yang, Jiping Lang, Wenhao Jiang, Yuhang Zhao, Shuang Li, Dingqian Wang, Zhu Lin, Xuanna Li, Yuze Tang, Jiexian Qiu, Xiaolin Lu, Hongji Yu, Shuang Chen, Yuhua Bi, Xiaofei Zeng, Yixian Chen, Junrong Chen, Lin Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated considerable potential in
general practice. However, existing benchmarks and evaluation frameworks
primarily depend on exam-style or simplified question-answer formats, lacking a
competency-based structure aligned with the real-world clinical
responsibilities encountered in general practice. Consequently, the extent to
which LLMs can reliably fulfill the duties of general practitioners (GPs)
remains uncertain. In this work, we propose a novel evaluation framework to
assess the capability of LLMs to function as GPs. Based on this framework, we
introduce a general practice benchmark (GPBench), whose data are meticulously
annotated by domain experts in accordance with routine clinical practice
standards. We evaluate ten state-of-the-art LLMs and analyze their
competencies. Our findings indicate that current LLMs are not yet ready for
deployment in such settings without human oversight, and further optimization
specifically tailored to the daily responsibilities of GPs is essential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PropNet: a White-Box and Human-Like Network for Sentence Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10725v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10725v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based embedding methods have dominated the field of sentence
representation in recent years. Although they have achieved remarkable
performance on NLP missions, such as semantic textual similarity (STS) tasks,
their black-box nature and large-data-driven training style have raised
concerns, including issues related to bias, trust, and safety. Many efforts
have been made to improve the interpretability of embedding models, but these
problems have not been fundamentally resolved. To achieve inherent
interpretability, we propose a purely white-box and human-like sentence
representation network, PropNet. Inspired by findings from cognitive science,
PropNet constructs a hierarchical network based on the propositions contained
in a sentence. While experiments indicate that PropNet has a significant gap
compared to state-of-the-art (SOTA) embedding models in STS tasks, case studies
reveal substantial room for improvement. Additionally, PropNet enables us to
analyze and understand the human cognitive processes underlying STS benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Clarified some ambiguities in the previous version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-based NLG Evaluation: Current Status and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01383v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01383v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating natural language generation (NLG) is a vital but challenging
problem in natural language processing. Traditional evaluation metrics mainly
capturing content (e.g. n-gram) overlap between system outputs and references
are far from satisfactory, and large language models (LLMs) such as ChatGPT
have demonstrated great potential in NLG evaluation in recent years. Various
automatic evaluation methods based on LLMs have been proposed, including
metrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM
collaborative evaluation. In this survey, we first give a taxonomy of LLM-based
NLG evaluation methods, and discuss their pros and cons, respectively. Lastly,
we discuss several open problems in this area and point out future research
directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAS: Fast ANN-SNN Conversion for Spiking Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Chen, Xiaotian Song, Andy Song, BaDong Chen, Jiancheng Lv, Yanan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Large Language Models have been shown as a good alternative to LLMs
in various scenarios. Existing methods for creating Spiking LLMs, i.e., direct
training and ANN-SNN conversion, often suffer from performance degradation and
relatively high computational costs. To address these issues, we propose a
novel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking
LLMs in two stages. The first stage employs a full-parameter fine-tuning of
pre-trained models, so it does not need any direct training from scratch. The
second stage introduces a coarse-to-fine calibration method to reduce
conversion errors and improve accuracy. Experiments on both language and
vision-language tasks across four different scales of LLMs demonstrate that FAS
can achieve state-of-the-art performance yet with significantly reduced
inference latency and computational costs. Notably, FAS only takes eight
timesteps to achieve an accuracy of 3\% higher than that of the OPT-7B model,
while reducing energy consumption by 96.63\%. The source code is available at
https://github.com/lc783/FAS
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text
  Detection Framework via Multiscaled Conformal Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowei Zhu, Yubing Ren, Yanan Cao, Xixun Lin, Fang Fang, Yangxi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models has raised significant
concerns regarding their potential misuse by malicious actors. As a result,
developing effective detectors to mitigate these risks has become a critical
priority. However, most existing detection methods focus excessively on
detection accuracy, often neglecting the societal risks posed by high false
positive rates (FPRs). This paper addresses this issue by leveraging Conformal
Prediction (CP), which effectively constrains the upper bound of FPRs. While
directly applying CP constrains FPRs, it also leads to a significant reduction
in detection performance. To overcome this trade-off, this paper proposes a
Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal
Prediction (MCP), which both enforces the FPR constraint and improves detection
performance. This paper also introduces RealDet, a high-quality dataset that
spans a wide range of domains, ensuring realistic calibration and enabling
superior detection performance when combined with MCP. Empirical evaluations
demonstrate that MCP effectively constrains FPRs, significantly enhances
detection performance, and increases robustness against adversarial attacks
across multiple detectors and datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction
  covering Multi-Level Error with Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08037v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08037v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Liu, Feng Xiao, Ziyue Zhang, Yongbin Yu, Cheng Huang, Fan Gao, Xiangxiang Wang, Ma-bao Ban, Manping Fan, Thupten Tsering, Cheng Huang, Gadeng Luosang, Renzeng Duojie, Nyima Tashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-level Tibetan spelling correction addresses errors at both the
character and syllable levels within a unified model. Existing methods focus
mainly on single-level correction and lack effective integration of both
levels. Moreover, there are no open-source datasets or augmentation methods
tailored for this task in Tibetan. To tackle this, we propose a data
augmentation approach using unlabeled text to generate multi-level corruptions,
and introduce TiSpell, a semi-masked model capable of correcting both
character- and syllable-level errors. Although syllable-level correction is
more challenging due to its reliance on global context, our semi-masked
strategy simplifies this process. We synthesize nine types of corruptions on
clean sentences to create a robust training set. Experiments on both simulated
and real-world data demonstrate that TiSpell, trained on our dataset,
outperforms baseline models and matches the performance of state-of-the-art
approaches, confirming its effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent
  Evaluation of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidan Zhang, Yu Wan, Boyi Deng, Baosong Yang, Haoran Wei, Fei Huang, Bowen Yu, Junyang Lin, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) showcase varied
multilingual capabilities across tasks like translation, code generation, and
reasoning. Previous assessments often limited their scope to fundamental
natural language processing (NLP) or isolated capability-specific tasks. To
alleviate this drawback, we aim to present a comprehensive multilingual
multitask benchmark. First, we introduce P-MMEval, a large-scale benchmark
covering effective fundamental and capability-specialized datasets.
Furthermore, P-MMEval delivers consistent language coverage across various
datasets and provides parallel samples. Finally, we conduct extensive
experiments on representative multilingual model series to compare performances
across models and tasks, explore the relationship between multilingual
performances and factors such as tasks, model sizes, languages, and prompts,
and examine the effectiveness of knowledge transfer from English to other
languages. The resulting insights are intended to offer valuable guidance for
future research. The dataset is available at
https://huggingface.co/datasets/Qwen/P-MMEval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study
  on Audio Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11197v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11197v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gang Li, Jizhong Liu, Heinrich Dinkel, Yadong Niu, Junbo Zhang, Jian Luan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, reinforcement learning (RL) has been shown to greatly enhance the
reasoning capabilities of large language models (LLMs), and RL-based approaches
have been progressively applied to visual multimodal tasks. However, the audio
modality has largely been overlooked in these developments. Thus, we conduct a
series of RL explorations in audio understanding and reasoning, specifically
focusing on the audio question answering (AQA) task. We leverage the group
relative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and
our experiments demonstrated state-of-the-art performance on the MMAU Test-mini
benchmark, achieving an accuracy rate of 64.5%. The main findings in this
technical report are as follows: 1) The GRPO algorithm can be effectively
applied to large audio language models (LALMs), even when the model has only
8.2B parameters; 2) With only 38k post-training samples, RL significantly
outperforms supervised fine-tuning (SFT), indicating that RL-based approaches
can be effective without large datasets; 3) The explicit reasoning process has
not shown significant benefits for AQA tasks, and how to efficiently utilize
deep thinking remains an open question for further research; 4) LALMs still lag
far behind humans auditory-language reasoning, suggesting that the RL-based
approaches warrant further exploration. Our project is available at
https://github.com/xiaomi-research/r1-aqa and
https://huggingface.co/mispeech/r1-aqa.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Single-Turn: A <span class="highlight-title">Survey</span> on Multi-Turn Interactions with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04717v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04717v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Li, Xiaobin Shen, Xinyu Yao, Xueying Ding, Yidi Miao, Ramayya Krishnan, Rema Padman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have revolutionized their
ability to handle single-turn tasks, yet real-world applications demand
sophisticated multi-turn interactions. This survey provides a comprehensive
review of recent advancements in evaluating and enhancing multi-turn
interactions in LLMs. Focusing on task-specific scenarios, from instruction
following in diverse domains such as math and coding to complex conversational
engagements in roleplay, healthcare, education, and even adversarial jailbreak
settings, we systematically examine the challenges of maintaining context,
coherence, fairness, and responsiveness over prolonged dialogues. The paper
organizes current benchmarks and datasets into coherent categories that reflect
the evolving landscape of multi-turn dialogue evaluation. In addition, we
review a range of enhancement methodologies under multi-turn settings,
including model-centric strategies (contextual learning, supervised
fine-tuning, reinforcement learning, and new architectures), external
integration approaches (memory-augmented, retrieval-based methods, and
knowledge graph), and agent-based techniques for collaborative interactions.
Finally, we discuss open challenges and propose future directions for research
to further advance the robustness and effectiveness of multi-turn interactions
in LLMs. Related resources and papers are available at
https://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method
  for Enhancing Question-Answering Capabilities of Large Language Models for
  Chinese Intangible Cultural Heritage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruilin Liu, Zhixiao Zhao, Jieqiong Li, Chang Liu, Dongbo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs) has provided
significant support and opportunities for the advancement of domain-specific
LLMs. However, fine-tuning these large models using Intangible Cultural
Heritage (ICH) data inevitably faces challenges such as bias, incorrect
knowledge inheritance, and catastrophic forgetting. To address these issues, we
propose a novel training method that integrates a bidirectional chains of
thought and a reward mechanism. This method is built upon ICH-Qwen, a large
language model specifically designed for the field of intangible cultural
heritage. The proposed method enables the model to not only perform forward
reasoning but also enhances the accuracy of the generated answers by utilizing
reverse questioning and reverse reasoning to activate the model's latent
knowledge. Additionally, a reward mechanism is introduced during training to
optimize the decision-making process. This mechanism improves the quality of
the model's outputs through structural and content evaluations with different
weighting schemes. We conduct comparative experiments on ICH-Qwen, with results
demonstrating that our method outperforms 0-shot, step-by-step reasoning,
knowledge distillation, and question augmentation methods in terms of accuracy,
Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the
paper highlights the effectiveness of combining the bidirectional chains of
thought and reward mechanism through ablation experiments. In addition, a
series of generalizability experiments are conducted, with results showing that
the proposed method yields improvements on various domain-specific datasets and
advanced models in areas such as Finance, Wikidata, and StrategyQA. This
demonstrates that the method is adaptable to multiple domains and provides a
valuable approach for model training in future applications across diverse
fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">36</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Learning of Visual Compositional Concepts through Probabilistic
  Schema Induction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Jun Lee, Taylor Webb, Trevor Bihl, Keith Holyoak, Hongjing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to learn new visual concepts from limited examples is a hallmark
of human cognition. While traditional category learning models represent each
example as an unstructured feature vector, compositional concept learning is
thought to depend on (1) structured representations of examples (e.g., directed
graphs consisting of objects and their relations) and (2) the identification of
shared relational structure across examples through analogical mapping. Here,
we introduce Probabilistic Schema Induction (PSI), a prototype model that
employs deep learning to perform analogical mapping over structured
representations of only a handful of examples, forming a compositional concept
called a schema. In doing so, PSI relies on a novel conception of similarity
that weighs object-level similarity and relational similarity, as well as a
mechanism for amplifying relations relevant to classification, analogous to
selective attention parameters in traditional models. We show that PSI produces
human-like learning performance and outperforms two controls: a prototype model
that uses unstructured feature vectors extracted from a deep learning model,
and a variant of PSI with weaker structured representations. Notably, we find
that PSI's human-like performance is driven by an adaptive strategy that
increases relational similarity over object-level similarity and upweights the
contribution of relations that distinguish classes. These findings suggest that
structured representations and analogical mapping are critical to modeling
rapid human-like learning of compositional visual concepts, and demonstrate how
deep learning can be leveraged to create psychological models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Lee, A. J., Webb, T., Bihl, T., Holyoak, K. J., & Lu, H. (2025).
  Few-shot learning of visual compositional concepts through probabilistic
  schema induction. In A. Ruggeri, D. Barner, C. Walker, & N. Bramley (Eds.),
  Proceedings of the 47th Annual Conference of the Cognitive Science Society.
  Cognitive Science Society</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mission Balance: Generating Under-represented Class Samples using Video
  Diffusion Models <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danush Kumar Venkatesh, Isabel Funke, Micha Pfeiffer, Fiona Kolbinger, Hanna Maria Schmeiser, Juergen Weitz, Marius Distler, Stefanie Speidel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer-assisted interventions can improve intra-operative guidance,
particularly through deep learning methods that harness the spatiotemporal
information in surgical videos. However, the severe data imbalance often found
in surgical video datasets hinders the development of high-performing models.
In this work, we aim to overcome the data imbalance by synthesizing surgical
videos. We propose a unique two-stage, text-conditioned diffusion-based method
to generate high-fidelity surgical videos for under-represented classes. Our
approach conditions the generation process on text prompts and decouples
spatial and temporal modeling by utilizing a 2D latent diffusion model to
capture spatial content and then integrating temporal attention layers to
ensure temporal consistency. Furthermore, we introduce a rejection sampling
strategy to select the most suitable synthetic samples, effectively augmenting
existing datasets to address class imbalance. We evaluate our method on two
downstream tasks-surgical action recognition and intra-operative event
prediction-demonstrating that incorporating synthetic videos from our approach
substantially enhances model performance. We open-source our implementation at
https://gitlab.com/nct_tso_public/surgvgen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Early accept at MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImplicitStainer: Data-Efficient Medical Image Translation for Virtual
  Antibody-based Tissue Staining Using Local Implicit Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushar Kataria, Beatrice Knudsen, Shireen Y. Elhabian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hematoxylin and eosin (H&E) staining is a gold standard for microscopic
diagnosis in pathology. However, H&E staining does not capture all the
diagnostic information that may be needed. To obtain additional molecular
information, immunohistochemical (IHC) stains highlight proteins that mark
specific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells.
While IHC stains are vital for prognosis and treatment guidance, they are
typically only available at specialized centers and time consuming to acquire,
leading to treatment delays for patients. Virtual staining, enabled by deep
learning-based image translation models, provides a promising alternative by
computationally generating IHC stains from H&E stained images. Although many
GAN and diffusion based image to image (I2I) translation methods have been used
for virtual staining, these models treat image patches as independent data
points, which results in increased and more diverse data requirements for
effective generation. We present ImplicitStainer, a novel approach that
leverages local implicit functions to improve image translation, specifically
virtual staining performance, by focusing on pixel-level predictions. This
method enhances robustness to variations in dataset sizes, delivering
high-quality results even with limited data. We validate our approach on two
datasets using a comprehensive set of metrics and benchmark it against over
fifteen state-of-the-art GAN- and diffusion based models. Full Code and models
trained will be released publicly via Github upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image
  Segmentation Performance for Low Data Regimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushar Kataria, Shireen Y. Elhabian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obtaining large-scale medical data, annotated or unannotated, is challenging
due to stringent privacy regulations and data protection policies. In addition,
annotating medical images requires that domain experts manually delineate
anatomical structures, making the process both time-consuming and costly. As a
result, semi-supervised methods have gained popularity for reducing annotation
costs. However, the performance of semi-supervised methods is heavily dependent
on the availability of unannotated data, and their effectiveness declines when
such data are scarce or absent. To overcome this limitation, we propose a
simple, yet effective and computationally efficient approach for medical image
segmentation that leverages only existing annotations. We propose BoundarySeg ,
a multi-task framework that incorporates organ boundary prediction as an
auxiliary task to full organ segmentation, leveraging consistency between the
two task predictions to provide additional supervision. This strategy improves
segmentation accuracy, especially in low data regimes, allowing our method to
achieve performance comparable to or exceeding state-of-the-art semi supervised
approaches all without relying on unannotated data or increasing computational
demands. Code will be released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dyadic Mamba: Long-term Dyadic Human Motion Synthesis <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Tanke, Takashi Shibuya, Kengo Uchida, Koichi Saito, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating realistic dyadic human motion from text descriptions presents
significant challenges, particularly for extended interactions that exceed
typical training sequence lengths. While recent transformer-based approaches
have shown promising results for short-term dyadic motion synthesis, they
struggle with longer sequences due to inherent limitations in positional
encoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach
that leverages State-Space Models (SSMs) to generate high-quality dyadic human
motion of arbitrary length. Our method employs a simple yet effective
architecture that facilitates information flow between individual motion
sequences through concatenation, eliminating the need for complex
cross-attention mechanisms. We demonstrate that Dyadic Mamba achieves
competitive performance on standard short-term benchmarks while significantly
outperforming transformer-based approaches on longer sequences. Additionally,
we propose a new benchmark for evaluating long-term motion synthesis quality,
providing a standardized framework for future research. Our results demonstrate
that SSM-based architectures offer a promising direction for addressing the
challenging task of long-term dyadic human motion synthesis from text
descriptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025 HuMoGen Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Feedback of Pattern Separability Improves Myoelectric Decoding
  Performance of Upper Limb Prostheses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichen Yang, György M. Lévay, Christopher L. Hunt, Dániel Czeiner, Megan C. Hodgson, Damini Agarwal, Rahul R. Kaliki, Nitish V. Thakor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art upper limb myoelectric prostheses often use pattern
recognition (PR) control systems that translate electromyography (EMG) signals
into desired movements. As prosthesis movement complexity increases, users
often struggle to produce sufficiently distinct EMG patterns for reliable
classification. Existing training typically involves heuristic, trial-and-error
user adjustments to static decoder boundaries. Goal: We introduce the Reviewer,
a 3D visual interface projecting EMG signals directly into the decoder's
classification space, providing intuitive, real-time insight into PR algorithm
behavior. This structured feedback reduces cognitive load and fosters mutual,
data-driven adaptation between user-generated EMG patterns and decoder
boundaries. Methods: A 10-session study with 12 able-bodied participants
compared PR performance after motor-based training and updating using the
Reviewer versus conventional virtual arm visualization. Performance was
assessed using a Fitts law task that involved the aperture of the cursor and
the control of orientation. Results: Participants trained with the Reviewer
achieved higher completion rates, reduced overshoot, and improved path
efficiency and throughput compared to the standard visualization group.
Significance: The Reviewer introduces decoder-informed motor training,
facilitating immediate and consistent PR-based myoelectric control
improvements. By iteratively refining control through real-time feedback, this
approach reduces reliance on trial-and-error recalibration, enabling a more
adaptive, self-correcting training framework. Conclusion: The 3D visual
feedback significantly improves PR control in novice operators through
structured training, enabling feedback-driven adaptation and reducing reliance
on extensive heuristic adjustments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the
  Left Atrium 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xabier Morales, Ayah Elsayed, Debbie Zhao, Filip Loncaric, Ainhoa Aguado, Mireia Masias, Gina Quill, Marc Ramos, Ada Doltra, Ana Garcia, Marta Sitges, David Marlevi, Alistair Young, Martyn Nash, Bart Bijnens, Oscar Camara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The left atrium (LA) plays a pivotal role in modulating left ventricular
filling, but our comprehension of its hemodynamics is significantly limited by
the constraints of conventional ultrasound analysis. 4D flow magnetic resonance
imaging (4D Flow MRI) holds promise for enhancing our understanding of atrial
hemodynamics. However, the low velocities within the LA and the limited spatial
resolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,
the absence of dedicated computational frameworks, combined with diverse
acquisition protocols and vendors, complicates gathering large cohorts for
studying the prognostic value of hemodynamic parameters provided by 4D Flow
MRI. In this study, we introduce the first open-source computational framework
tailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive
qualitative and quantitative analysis of advanced hemodynamic parameters. Our
framework proves robust to data from different centers of varying quality,
producing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95
$<$ 3 mm), even with limited training data. Additionally, we conducted the
first comprehensive assessment of energy, vorticity, and pressure parameters in
the LA across a spectrum of disorders to investigate their potential as
prognostic biomarkers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yung-Hsuan Lai, Janek Ebbers, Yu-Chiang Frank Wang, François Germain, Michael Jeffrey Jones, Moitreya Chatterjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing
both uni-modal events (i.e., those occurring exclusively in either the visual
or acoustic modality of a video) and multi-modal events (i.e., those occurring
in both modalities concurrently). Moreover, the prohibitive cost of annotating
training data with the class labels of all these events, along with their start
and end times, imposes constraints on the scalability of AVVP techniques unless
they can be trained in a weakly-supervised setting, where only
modality-agnostic, video-level labels are available in the training data. To
this end, recently proposed approaches seek to generate segment-level
pseudo-labels to better guide model training. However, the absence of
inter-segment dependencies when generating these pseudo-labels and the general
bias towards predicting labels that are absent in a segment limit their
performance. This work proposes a novel approach towards overcoming these
weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video
Parsing (UWAV). Additionally, our innovative approach factors in the
uncertainty associated with these estimated pseudo-labels and incorporates a
feature mixup based training regularization for improved training. Empirical
results show that UWAV outperforms state-of-the-art methods for the AVVP task
on multiple metrics, across two different datasets, attesting to its
effectiveness and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightLab: Controlling Light Sources in Images with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Magar, Amir Hertz, Eric Tabellion, Yael Pritch, Alex Rav-Acha, Ariel Shamir, Yedid Hoshen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple, yet effective diffusion-based method for fine-grained,
parametric control over light sources in an image. Existing relighting methods
either rely on multiple input views to perform inverse rendering at inference
time, or fail to provide explicit control over light changes. Our method
fine-tunes a diffusion model on a small set of real raw photograph pairs,
supplemented by synthetically rendered images at scale, to elicit its
photorealistic prior for relighting. We leverage the linearity of light to
synthesize image pairs depicting controlled light changes of either a target
light source or ambient illumination. Using this data and an appropriate
fine-tuning scheme, we train a model for precise illumination changes with
explicit control over light intensity and color. Lastly, we show how our method
can achieve compelling light editing results, and outperforms existing methods
based on user preference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://nadmag.github.io/LightLab/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Visual Question Answering <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Jan Wieczorek, Nathalie Daun, Mohammad Emtiyaz Khan, Marcus Rohrbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite remarkable progress in multimodal models for Visual Question
Answering (VQA), there remain major reliability concerns because the models can
often be overconfident and miscalibrated, especially in out-of-distribution
(OOD) settings. Plenty has been done to address such issues for unimodal
models, but little work exists for multimodal cases. Here, we address
unreliability in multimodal models by proposing a Variational VQA approach.
Specifically, instead of fine-tuning vision-language models by using AdamW, we
employ a recently proposed variational algorithm called IVON, which yields a
posterior distribution over model parameters. Through extensive experiments, we
show that our approach improves calibration and abstentions without sacrificing
the accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce
Expected Calibration Error by more than 50% compared to the AdamW baseline and
raise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of
distribution shifts, the performance gain is even higher, achieving 8% Coverage
(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we
present variational learning as a viable option to enhance the reliability of
multimodal models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 16 figures, under review at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Forget your Inverse DDIM for Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Gomez-Trenado, Pablo Mesejo, Oscar Cordón, Stéphane Lathuilière
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of text-to-image generation has undergone significant advancements
with the introduction of diffusion models. Nevertheless, the challenge of
editing real images persists, as most methods are either computationally
intensive or produce poor reconstructions. This paper introduces SAGE
(Self-Attention Guidance for image Editing) - a novel technique leveraging
pre-trained diffusion models for image editing. SAGE builds upon the DDIM
algorithm and incorporates a novel guidance mechanism utilizing the
self-attention layers of the diffusion U-Net. This mechanism computes a
reconstruction objective based on attention maps generated during the inverse
DDIM process, enabling efficient reconstruction of unedited regions without the
need to precisely reconstruct the entire input image. Thus, SAGE directly
addresses the key challenges in image editing. The superiority of SAGE over
other methods is demonstrated through quantitative and qualitative evaluations
and confirmed by a statistically validated comprehensive user study, in which
all 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE
ranks as the top-performing method in seven out of 10 quantitative analyses and
secures second and third places in the remaining three.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures, code available at
  https://guillermogotre.github.io/sage/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,
  Training and <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, Ran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unifying image understanding and generation has gained growing attention in
recent research on multimodal models. Although design choices for image
understanding have been extensively studied, the optimal model architecture and
training recipe for a unified framework with image generation remain
underexplored. Motivated by the strong potential of autoregressive and
diffusion models for high-quality generation and scalability, we conduct a
comprehensive study of their use in unified multimodal settings, with emphasis
on image representations, modeling objectives, and training strategies.
Grounded in these investigations, we introduce a novel approach that employs a
diffusion transformer to generate semantically rich CLIP image features, in
contrast to conventional VAE-based representations. This design yields both
higher training efficiency and improved generative quality. Furthermore, we
demonstrate that a sequential pretraining strategy for unified models-first
training on image understanding and subsequently on image generation-offers
practical advantages by preserving image understanding capability while
developing strong image generation ability. Finally, we carefully curate a
high-quality instruction-tuning dataset BLIP3o-60k for image generation by
prompting GPT-4o with a diverse set of captions covering various scenes,
objects, human gestures, and more. Building on our innovative model design,
training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art
unified multimodal models. BLIP3-o achieves superior performance across most of
the popular benchmarks spanning both image understanding and generation tasks.
To facilitate future research, we fully open-source our models, including code,
model weights, training scripts, and pretraining and instruction tuning
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using
  Implicit Neural Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maik Dannecker, Thomas Sanchez, Meritxell Bach Cuadra, Özgün Turgut, Anthony N. Price, Lucilio Cordero-Grande, Vanessa Kyriakopoulou, Joseph V. Hajnal, Daniel Rueckert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution slice-to-volume reconstruction (SVR) from multiple
motion-corrupted low-resolution 2D slices constitutes a critical step in
image-based diagnostics of moving subjects, such as fetal brain Magnetic
Resonance Imaging (MRI). Existing solutions struggle with image artifacts and
severe subject motion or require slice pre-alignment to achieve satisfying
reconstruction performance. We propose a novel SVR method to enable fast and
accurate MRI reconstruction even in cases of severe image and motion
corruption. Our approach performs motion correction, outlier handling, and
super-resolution reconstruction with all operations being entirely based on
implicit neural representations. The model can be initialized with
task-specific priors through fully self-supervised meta-learning on either
simulated or real-world data. In extensive experiments including over 480
reconstructions of simulated and clinical MRI brain data from different
centers, we prove the utility of our method in cases of severe subject motion
and image artifacts. Our results demonstrate improvements in reconstruction
quality, especially in the presence of severe motion, compared to
state-of-the-art methods, and up to 50% reduction in reconstruction time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D
  Cardiac CT Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anne-Marie Rickmann, Stephanie L. Thorn, Shawn S. Ahn, Supum Lee, Selen Uman, Taras Lysyy, Rachel Burns, Nicole Guerrera, Francis G. Spinale, Jason A. Burdick, Albert J. Sinusas, James S. Duncan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac image segmentation is an important step in many cardiac image
analysis and modeling tasks such as motion tracking or simulations of cardiac
mechanics. While deep learning has greatly advanced segmentation in clinical
settings, there is limited work on pre-clinical imaging, notably in porcine
models, which are often used due to their anatomical and physiological
similarity to humans. However, differences between species create a domain
shift that complicates direct model transfer from human to pig data.
  Recently, foundation models trained on large human datasets have shown
promise for robust medical image segmentation; yet their applicability to
porcine data remains largely unexplored. In this work, we investigate whether
foundation models can generate sufficiently accurate pseudo-labels for pig
cardiac CT and propose a simple self-training approach to iteratively refine
these labels. Our method requires no manually annotated pig data, relying
instead on iterative updates to improve segmentation quality. We demonstrate
that this self-training process not only enhances segmentation accuracy but
also smooths out temporal inconsistencies across consecutive frames. Although
our results are encouraging, there remains room for improvement, for example by
incorporating more sophisticated self-training strategies and by exploring
additional foundation models and other cardiac imaging technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at FIMH 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through
  Differentiable Object Shapes <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Marinello, Simen Cassiman, Jonas Heylen, Marc Proesmans, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles need a complete map of their surroundings to plan and
act. This has sparked research into the tasks of 3D occupancy prediction, 3D
scene completion, and 3D panoptic scene completion, which predict a dense map
of the ego vehicle's surroundings as a voxel grid. Scene completion extends
occupancy prediction by predicting occluded regions of the voxel grid, and
panoptic scene completion further extends this task by also distinguishing
object instances within the same class; both aspects are crucial for path
planning and decision-making. However, 3D panoptic scene completion is
currently underexplored. This work introduces a novel framework for 3D panoptic
scene completion that extends existing 3D semantic scene completion models. We
propose an Object Module and Panoptic Module that can easily be integrated with
3D occupancy and scene completion methods presented in the literature. Our
approach leverages the available annotations in occupancy benchmarks, allowing
individual object shapes to be learned as a differentiable problem. The code is
available at https://github.com/nicolamarinello/OffsetOcc .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025 Workshop on Autonomous Driving</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contactless Cardiac Pulse Monitoring Using Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Moustafa, Joseph Lemley, Peter Corcoran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time event cameras are a novel technology for recording scene information at
extremely low latency and with low power consumption. Event cameras output a
stream of events that encapsulate pixel-level light intensity changes within
the scene, capturing information with a higher dynamic range and temporal
resolution than traditional cameras. This study investigates the contact-free
reconstruction of an individual's cardiac pulse signal from time event
recording of their face using a supervised convolutional neural network (CNN)
model. An end-to-end model is trained to extract the cardiac signal from a
two-dimensional representation of the event stream, with model performance
evaluated based on the accuracy of the calculated heart rate. The experimental
results confirm that physiological cardiac information in the facial region is
effectively preserved within the event stream, showcasing the potential of this
novel sensor for remote heart rate monitoring. The model trained on event
frames achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)
compared to the RMSE of 2.92 bpm achieved by the baseline model trained on
standard camera frames. Furthermore, models trained on event frames generated
at 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an
RMSE of 2.54 and 2.13 bpm, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is a preprint of a paper submitted to IEEE Access and is
  currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Bounds on Full-Reference Image Quality for Imaging Inverse
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Wen, Rizwan Ahmad, Philip Schniter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In imaging inverse problems, we would like to know how close the recovered
image is to the true image in terms of full-reference image quality (FRIQ)
metrics like PSNR, SSIM, LPIPS, etc. This is especially important in
safety-critical applications like medical imaging, where knowing that, say, the
SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't
know the true image, computing FRIQ is non-trivial. In this work, we combine
conformal prediction with approximate posterior sampling to construct bounds on
FRIQ that are guaranteed to hold up to a user-specified error probability. We
demonstrate our approach on image denoising and accelerated magnetic resonance
imaging (MRI) problems. Code is available at
https://github.com/jwen307/quality_uq.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI
  Reconstruction based on Multi-directional Time-Frequency Convolutional
  Attention Encoder and Vision-Mamba U-Net 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyi He, Shiyang Li, Bin Jiang, He Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution functional magnetic resonance imaging (fMRI) is essential for
mapping human brain activity; however, it remains costly and logistically
challenging. If comparable volumes could be generated directly from widely
available scalp electroencephalography (EEG), advanced neuroimaging would
become significantly more accessible. Existing EEG-to-fMRI generators rely on
plain CNNs that fail to capture cross-channel time-frequency cues or on heavy
transformer/GAN decoders that strain memory and stability. We propose
Spec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts
these issues via a Multi-directional Time-Frequency Convolutional Attention
Encoder, stacking temporal, spectral and joint convolutions with
self-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space
blocks enable efficient long-range spatial modelling. Trained end-to-end with a
hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on
three public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball
and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%
respectively over previous best SSIM scores. Furthermore, it achieves
competitive PSNR scores, particularly excelling on the CN-EPFL dataset with a
4.6% improvement over the previous best PSNR, thus striking a better balance in
reconstruction quality. The proposed model is lightweight and efficient, making
it suitable for real-time applications in clinical and research settings. The
code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low
  Latency and High Throughput 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Zhang, Shuo Li, Runhe Tian, Yang Yang, Jixin Tang, Jinhao Zhou, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Flash-VL 2B, a novel approach to optimizing
Vision-Language Models (VLMs) for real-time applications, targeting ultra-low
latency and high throughput without sacrificing accuracy. Leveraging advanced
architectural enhancements and efficient computational strategies, Flash-VL 2B
is designed to maximize throughput by reducing processing time while
maintaining competitive performance across multiple vision-language benchmarks.
Our approach includes tailored architectural choices, token compression
mechanisms, data curation, training schemes, and a novel image processing
technique called implicit semantic stitching that effectively balances
computational load and model performance. Through extensive evaluations on 11
standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves
state-of-the-art results in both speed and accuracy, making it a promising
solution for deployment in resource-constrained environments and large-scale
real-time applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising and Alignment: Rethinking Domain Generalization for Multimodal
  Face Anti-Spoofing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingjie Ma, Xun Lin, Zitong Yu, Xin Liu, Xiaochen Yuan, Weicheng Xie, Linlin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face Anti-Spoofing (FAS) is essential for the security of facial recognition
systems in diverse scenarios such as payment processing and surveillance.
Current multimodal FAS methods often struggle with effective generalization,
mainly due to modality-specific biases and domain shifts. To address these
challenges, we introduce the \textbf{M}ulti\textbf{m}odal \textbf{D}enoising
and \textbf{A}lignment (\textbf{MMDA}) framework. By leveraging the zero-shot
generalization capability of CLIP, the MMDA framework effectively suppresses
noise in multimodal data through denoising and alignment mechanisms, thereby
significantly enhancing the generalization performance of cross-modal
alignment. The \textbf{M}odality-\textbf{D}omain Joint \textbf{D}ifferential
\textbf{A}ttention (\textbf{MD2A}) module in MMDA concurrently mitigates the
impacts of domain and modality noise by refining the attention mechanism based
on extracted common noise features. Furthermore, the \textbf{R}epresentation
\textbf{S}pace \textbf{S}oft (\textbf{RS2}) Alignment strategy utilizes the
pre-trained CLIP model to align multi-domain multimodal data into a generalized
representation space in a flexible manner, preserving intricate representations
and enhancing the model's adaptability to various unseen conditions. We also
design a \textbf{U}-shaped \textbf{D}ual \textbf{S}pace \textbf{A}daptation
(\textbf{U-DSA}) module to enhance the adaptability of representations while
maintaining generalization performance. These improvements not only enhance the
framework's generalization capabilities but also boost its ability to represent
complex representations. Our experimental results on four benchmark datasets
under different evaluation protocols demonstrate that the MMDA framework
outperforms existing state-of-the-art methods in terms of cross-domain
generalization and multimodal detection accuracy. The code will be released
soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A 2D Semantic-Aware Position Encoding for Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Shiyang Zhou, Muqi Huang, Jiaxu Feng, Yun Xiong, Kun Zhou, Biao Yang, Yuhui Zhang, Huishuai Bao, Sijia Peng, Chuan Li, Feng Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers have demonstrated significant advantages in computer
vision tasks due to their ability to capture long-range dependencies and
contextual relationships through self-attention. However, existing position
encoding techniques, which are largely borrowed from natural language
processing, fail to effectively capture semantic-aware positional relationships
between image patches. Traditional approaches like absolute position encoding
and relative position encoding primarily focus on 1D linear position
relationship, often neglecting the semantic similarity between distant yet
contextually related patches. These limitations hinder model generalization,
translation equivariance, and the ability to effectively handle repetitive or
structured patterns in images. In this paper, we propose 2-Dimensional
Semantic-Aware Position Encoding ($\text{SaPE}^2$), a novel position encoding
method with semantic awareness that dynamically adapts position representations
by leveraging local content instead of fixed linear position relationship or
spatial coordinates. Our method enhances the model's ability to generalize
across varying image resolutions and scales, improves translation equivariance,
and better aggregates features for visually similar but spatially distant
patches. By integrating $\text{SaPE}^2$ into vision transformers, we bridge the
gap between position encoding and perceptual similarity, thereby improving
performance on computer vision tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Pixels: Leveraging the Language of Soccer to Improve
  Spatio-Temporal Action Detection in Broadcast Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremie Ochin, Raphael Chekroun, Bogdan Stanciulescu, Sotiris Manitsaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art spatio-temporal action detection (STAD) methods show
promising results for extracting soccer events from broadcast videos. However,
when operated in the high-recall, low-precision regime required for exhaustive
event coverage in soccer analytics, their lack of contextual understanding
becomes apparent: many false positives could be resolved by considering a
broader sequence of actions and game-state information. In this work, we
address this limitation by reasoning at the game level and improving STAD
through the addition of a denoising sequence transduction task. Sequences of
noisy, context-free player-centric predictions are processed alongside clean
game state information using a Transformer-based encoder-decoder model. By
modeling extended temporal context and reasoning jointly over team-level
dynamics, our method leverages the "language of soccer" - its tactical
regularities and inter-player dependencies - to generate "denoised" sequences
of actions. This approach improves both precision and recall in low-confidence
regimes, enabling more reliable event extraction from broadcast video and
complementing existing pixel-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, submitted to Advanced Concepts for Intelligent Vision
  Systems 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating
  Motion during Ultrasound-Guided Aspiration Biopsy <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelin Zhang, Qingpeng Ding, Long Lei, Yongxuan Feng, Raymond Shing-Yan Tang, Shing Shin Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally
invasive diagnostic procedure. However, an aspiration needle tracker addressing
rapid reciprocating motion is still missing. MrTrack, an aspiration needle
tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a
Mamba-based register extractor to sequentially distill global context from each
historical search map, storing these temporal cues in a register bank. The
Mamba-based register retriever then retrieves temporal prompts from the
register bank to provide external cues when current vision features are
temporarily unusable due to rapid reciprocating motion and imaging degradation.
A self-supervised register diversify loss is proposed to encourage feature
diversity and dimension independence within the learned register, mitigating
feature collapse. Comprehensive experiments conducted on both motorized and
manual aspiration datasets demonstrate that MrTrack not only outperforms
state-of-the-art trackers in accuracy and robustness but also achieves superior
inference efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Early Accepted by MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Endo-CLIP: Progressive <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-train</span>ing on Raw Colonoscopy
  Records <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yili He, Yan Zhu, Peiyao Fu, Ruijie Yang, Tianyi Chen, Zhihua Wang, Quanlin Li, Pinghong Zhou, Xian Yang, Shuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training on image-text colonoscopy records offers substantial potential
for improving endoscopic image analysis, but faces challenges including
non-informative background images, complex medical terminology, and ambiguous
multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised
framework that enhances Contrastive Language-Image Pre-training (CLIP) for this
domain. Endo-CLIP's three-stage framework--cleansing, attunement, and
unification--addresses these challenges by (1) removing background frames, (2)
leveraging large language models to extract clinical attributes for
fine-grained contrastive learning, and (3) employing patient-level
cross-attention to resolve multi-polyp ambiguities. Extensive experiments
demonstrate that Endo-CLIP significantly outperforms state-of-the-art
pre-training methods in zero-shot and few-shot polyp detection and
classification, paving the way for more accurate and clinically relevant
endoscopic analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Early accepted to MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient LiDAR Reflectance Compression via Scanning Serialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Zhu, Kang You, Dandan Ding, Zhan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reflectance attributes in LiDAR point clouds provide essential information
for downstream tasks but remain underexplored in neural compression methods. To
address this, we introduce SerLiC, a serialization-based neural compression
framework to fully exploit the intrinsic characteristics of LiDAR reflectance.
SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order
serialization, offering a device-centric perspective for reflectance analysis.
Each point is then tokenized into a contextual representation comprising its
sensor scanning index, radial distance, and prior reflectance, for effective
dependencies exploration. For efficient sequential modeling, Mamba is
incorporated with a dual parallelization scheme, enabling simultaneous
autoregressive dependency capture and fast processing. Extensive experiments
demonstrate that SerLiC attains over 2x volume reduction against the original
reflectance data, outperforming the state-of-the-art method by up to 22%
reduction of compressed bits while using only 2% of its parameters. Moreover, a
lightweight version of SerLiC achieves > 10 fps (frames per second) with just
111K parameters, which is attractive for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Video Highlight Detection by Learning from Audio and Visual
  Recurrence <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahidul Islam, Sujoy Paul, Mrigank Rochan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the exponential growth of video content, the need for automated video
highlight detection to extract key moments or highlights from lengthy videos
has become increasingly pressing. This technology has the potential to enhance
user experiences by allowing quick access to relevant content across diverse
domains. Existing methods typically rely either on expensive manually labeled
frame-level annotations, or on a large external dataset of videos for weak
supervision through category information. To overcome this, we focus on
unsupervised video highlight detection, eliminating the need for manual
annotations. We propose a novel unsupervised approach which capitalizes on the
premise that significant moments tend to recur across multiple videos of the
similar category in both audio and visual modalities. Surprisingly, audio
remains under-explored, especially in unsupervised algorithms, despite its
potential to detect key moments. Through a clustering technique, we identify
pseudo-categories of videos and compute audio pseudo-highlight scores for each
video by measuring the similarities of audio features among audio clips of all
the videos within each pseudo-category. Similarly, we also compute visual
pseudo-highlight scores for each video using visual features. Then, we combine
audio and visual pseudo-highlights to create the audio-visual pseudo
ground-truth highlight of each video for training an audio-visual highlight
detection network. Extensive experiments and ablation studies on three
benchmarks showcase the superior performance of our method over prior work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2025 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peizhi Yan, Rabab K. Ward, Dan Wang, Qiang Tang, Shan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For 3D face modeling, the recently developed 3D-aware neural rendering
methods are able to render photorealistic face images with arbitrary viewing
directions. The training of the parametric controllable 3D-aware face models,
however, still relies on a large-scale dataset that is lab-collected. To
address this issue, this paper introduces "StyleMorpheus", the first
style-based neural 3D Morphable Face Model (3DMM) that is trained on
in-the-wild images. It inherits 3DMM's disentangled controllability (over face
identity, expression, and appearance) but without the need for accurately
reconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder
structure. The encoder aims at learning a representative disentangled
parametric code space and the decoder improves the disentanglement using shape
and appearance-related style codes in the different sub-modules of the network.
Furthermore, we fine-tune the decoder through style-based generative
adversarial learning to achieve photorealistic 3D rendering quality. The
proposed style-based design enables StyleMorpheus to achieve state-of-the-art
3D-aware face reconstruction results, while also allowing disentangled control
of the reconstructed face. Our model achieves real-time rendering speed,
allowing its use in virtual reality applications. We also demonstrate the
capability of the proposed style-based design in face editing applications such
as style mixing and color editing. Project homepage:
https://github.com/ubc-3d-vision-lab/StyleMorpheus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, work was completed in 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HaHeAE: Learning Generalisable Joint Representations of Human Hand and
  Head Movements in Extended Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Hu, Guanhua Zhang, Zheming Yin, Daniel Haeufle, Syn Schmitt, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human hand and head movements are the most pervasive input modalities in
extended reality (XR) and are significant for a wide range of applications.
However, prior works on hand and head modelling in XR only explored a single
modality or focused on specific applications. We present HaHeAE - a novel
self-supervised method for learning generalisable joint representations of hand
and head movements in XR. At the core of our method is an autoencoder (AE) that
uses a graph convolutional network-based semantic encoder and a diffusion-based
stochastic encoder to learn the joint semantic and stochastic representations
of hand-head movements. It also features a diffusion-based decoder to
reconstruct the original signals. Through extensive evaluations on three public
XR datasets, we show that our method 1) significantly outperforms commonly used
self-supervised methods by up to 74.0% in terms of reconstruction quality and
is generalisable across users, activities, and XR environments, 2) enables new
applications, including interpretable hand-head cluster identification and
variable hand-head movement generation, and 3) can serve as an effective
feature extractor for downstream tasks. Together, these results demonstrate the
effectiveness of our method and underline the potential of self-supervised
methods for jointly modelling hand-head behaviours in extended reality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Link: https://zhiminghu.net/hu25_haheae</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Hierarchical World Models as Visual Whole-Body Humanoid Controllers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18418v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18418v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicklas Hansen, Jyothir S V, Vlad Sobal, <span class="highlight-author">Yann LeCun</span>, Xiaolong Wang, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole-body control for humanoids is challenging due to the high-dimensional
nature of the problem, coupled with the inherent instability of a bipedal
morphology. Learning from visual observations further exacerbates this
difficulty. In this work, we explore highly data-driven approaches to visual
whole-body humanoid control based on reinforcement learning, without any
simplifying assumptions, reward design, or skill primitives. Specifically, we
propose a hierarchical world model in which a high-level agent generates
commands based on visual observations for a low-level agent to execute, both of
which are trained with rewards. Our approach produces highly performant control
policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing
motions that are broadly preferred by humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and videos at https://nicklashansen.com/rlpuppeteer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for
  Embodied Interactive Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, Weiming Lu, Peng Li, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep thinking models have demonstrated remarkable
reasoning capabilities on mathematical and coding tasks. However, their
effectiveness in embodied domains which require continuous interaction with
environments through image action interleaved trajectories remains largely
-unexplored. We present Embodied Reasoner, a model that extends o1 style
reasoning to interactive embodied search tasks. Unlike mathematical reasoning
that relies primarily on logical deduction, embodied scenarios demand spatial
understanding, temporal reasoning, and ongoing self-reflection based on
interaction history. To address these challenges, we synthesize 9.3k coherent
Observation-Thought-Action trajectories containing 64k interactive images and
90k diverse thinking processes (analysis, spatial reasoning, reflection,
planning, and verification). We develop a three-stage training pipeline that
progressively enhances the model's capabilities through imitation learning,
self-exploration via rejection sampling, and self-correction through reflection
tuning. The evaluation shows that our model significantly outperforms those
advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and
Claude-3.7 by +9\%, 24\%, and +13\%. Analysis reveals our model exhibits fewer
repeated searches and logical inconsistencies, with particular advantages in
complex long-horizon tasks. Real-world environments also show our superiority
while exhibiting fewer repeated searches and logical inconsistency cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/zwq2018/embodied_reasoner Dataset:
  https://huggingface.co/datasets/zwq2018/embodied_reasoner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal-state Dynamics Estimation for Physics-based Human Motion Capture
  from Videos <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07795v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07795v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cuong Le, Viktor Johansson, Manon Kok, Bastian Wandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion capture from monocular videos has made significant progress in
recent years. However, modern approaches often produce temporal artifacts, e.g.
in form of jittery motion and struggle to achieve smooth and physically
plausible motions. Explicitly integrating physics, in form of internal forces
and exterior torques, helps alleviating these artifacts. Current
state-of-the-art approaches make use of an automatic PD controller to predict
torques and reaction forces in order to re-simulate the input kinematics, i.e.
the joint angles of a predefined skeleton. However, due to imperfect physical
models, these methods often require simplifying assumptions and extensive
preprocessing of the input kinematics to achieve good performance. To this end,
we propose a novel method to selectively incorporate the physics models with
the kinematics observations in an online setting, inspired by a neural
Kalman-filtering approach. We develop a control loop as a meta-PD controller to
predict internal joint torques and external reaction forces, followed by a
physics-based motion simulation. A recurrent neural network is introduced to
realize a Kalman filter that attentively balances the kinematics input and
simulated motion, resulting in an optimal-state dynamics prediction. We show
that this filtering step is crucial to provide an online supervision that helps
balancing the shortcoming of the respective input motions, thus being important
for not only capturing accurate global motion trajectories but also producing
physically plausible human poses. The proposed approach excels in the
physics-based human pose estimation task and demonstrates the physical
plausibility of the predictive dynamics, compared to state of the art. The code
is available on https://github.com/cuongle1206/OSDCap
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figure, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Call to Arms: AI Should be Critical for Social Media Analysis of
  Conflict Zones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00810v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00810v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afia Abedin, Abdul Bais, Cody Buntain, Laura Courchesne, Brian McQuinn, Matthew E. Taylor, Muhib Ullah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The massive proliferation of social media data represents a transformative
opportunity for conflict studies and for tracking the proliferation and use of
weaponry, as conflicts are increasingly documented in these online spaces. At
the same time, the scale and types of data available are problematic for
traditional open-source intelligence. This paper focuses on identifying
specific weapon systems and the insignias of the armed groups using them as
documented in the Ukraine war, as these tasks are critical to operational
intelligence and tracking weapon proliferation, especially given the scale of
international military aid given to Ukraine. The large scale of social media
makes manual assessment difficult, however, so this paper presents early work
that uses computer vision models to support this task. We demonstrate that
these models can both identify weapons embedded in images shared in social
media and how the resulting collection of military-relevant images and their
post times interact with the offline, real-world conflict. Not only can we then
track changes in the prevalence of images of tanks, land mines, military
trucks, etc., we find correlations among time series data associated with these
images and the daily fatalities in this conflict. This work shows substantial
opportunity for examining similar online documentation of conflict contexts,
and we also point to future avenues where computer vision can be further
improved for these open-source intelligence tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ State-of-the-Art Periorbital Distance Prediction and Disease
  Classification Using Periorbital Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18769v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18769v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George R. Nahass, Sasha Hubschman, Jeffrey C. Peterson, Ghasem Yazdanpanah, Nicholas Tomaras, Madison Cheung, Alex Palacios, Kevin Heinze, Chad A. Purnell, Pete Setabutr, Ann Q. Tran, Darvin Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Periorbital distances are critical markers for diagnosing and monitoring a
range of oculoplastic and craniofacial conditions. Manual measurement, however,
is subjective and prone to intergrader variability. Automated methods have been
developed but remain limited by standardized imaging requirements, small
datasets, and a narrow focus on individual measurements. We developed a
segmentation pipeline trained on a domain-specific dataset of healthy eyes and
compared its performance against the Segment Anything Model (SAM) and the prior
benchmark, PeriorbitAI. Segmentation accuracy was evaluated across multiple
disease classes and imaging conditions. We further investigated the use of
predicted periorbital distances as features for disease classification under
in-distribution (ID) and out-of-distribution (OOD) settings, comparing shallow
classifiers, CNNs, and fusion models. Our segmentation model achieved
state-of-the-art accuracy across all datasets, with error rates within
intergrader variability and superior performance relative to SAM and
PeriorbitAI. In classification tasks, models trained on periorbital distances
matched CNN performance on ID data (77--78\% accuracy) and substantially
outperformed CNNs under OOD conditions (63--68\% accuracy vs. 14\%). Fusion
models achieved the highest ID accuracy (80\%) but were sensitive to degraded
CNN features under OOD shifts. Segmentation-derived periorbital distances
provide robust, explainable features for disease classification and generalize
better under domain shift than CNN image classifiers. These results establish a
new benchmark for periorbital distance prediction and highlight the potential
of anatomy-based AI pipelines for real-world deployment in oculoplastic and
craniofacial care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh
  Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Shafiee Sarvestani, Sheyang Tang, Zhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mesh quality assessment (MQA) models play a critical role in the design,
optimization, and evaluation of mesh operation systems in a wide variety of
applications. Current MQA models, whether model-based methods using
topology-aware features or projection-based approaches working on rendered 2D
projections, often fail to capture the intricate interactions between texture
and 3D geometry. We introduce HybridMQA, a first-of-its-kind hybrid
full-reference colored MQA framework that integrates model-based and
projection-based approaches, capturing complex interactions between textural
information and 3D structures for enriched quality representations. Our method
employs graph learning to extract detailed 3D representations, which are then
projected to 2D using a novel feature rendering process that precisely aligns
them with colored projections. This enables the exploration of geometry-texture
interactions via cross-attention, producing comprehensive mesh quality
representations. Extensive experiments demonstrate HybridMQA's superior
performance across diverse datasets, highlighting its ability to effectively
leverage geometry-texture interactions for a thorough understanding of mesh
quality. Our implementation will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MGPATH: Vision-Language Model with Multi-Granular <span class="highlight-title">Prompt</span> Learning for
  Few-Shot WSI Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07409v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07409v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh-Tien Nguyen, Duy Minh Ho Nguyen, Nghiem Tuong Diep, Trung Quoc Nguyen, Nhat Ho, Jacqueline Michelle Metsch, Miriam Cindy Maurer, Daniel Sonntag, Hanibal Bohnenberger, Anne-Christin Hauschild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole slide pathology image classification presents challenges due to
gigapixel image sizes and limited annotation labels, hindering model
generalization. This paper introduces a prompt learning method to adapt large
vision-language models for few-shot pathology classification. We first extend
the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology
image tiles, into a vision-language model by adding adaptors and aligning it
with medical text encoders via contrastive learning on 923K image-text pairs.
The model is then used to extract visual features and text embeddings from
few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike
prior methods that combine prompts with frozen features using prefix embeddings
or self-attention, we propose multi-granular attention that compares
interactions between learnable prompts with individual image patches and groups
of them. This approach improves the model's ability to capture both
fine-grained details and broader context, enhancing its recognition of complex
patterns across sub-regions. To further improve accuracy, we leverage
(unbalanced) optimal transport-based visual-text distance to secure model
robustness by mitigating perturbations that might occur during the data
augmentation process. Empirical experiments on lung, kidney, and breast
pathology modalities validate the effectiveness of our approach; thereby, we
surpass several of the latest competitors and consistently improve performance
across diverse architectures, including CLIP, PLIP, and Prov-GigaPath
integrated PLIP. We release our implementations and pre-trained models at this
MGPATH.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ F$^3$Loc: Fusion and Filtering for Floorplan Localization <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changan Chen, Rui Wang, Christoph Vogel, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose an efficient data-driven solution to
self-localization within a floorplan. Floorplan data is readily available,
long-term persistent and inherently robust to changes in the visual appearance.
Our method does not require retraining per map and location or demand a large
database of images of the area of interest. We propose a novel probabilistic
model consisting of an observation and a novel temporal filtering module.
Operating internally with an efficient ray-based representation, the
observation module consists of a single and a multiview module to predict
horizontal depth from images and fuses their results to benefit from advantages
offered by either methodology. Our method operates on conventional consumer
hardware and overcomes a common limitation of competing methods that often
demand upright images. Our full system meets real-time requirements, while
outperforming the state-of-the-art by a significant margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figure, accepted to CVPR 2024 (fixed typo eq.8: s_x,s_y,
  s_phi -> x, y, phi)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDDA: Data Driven Attribution at LinkedIn 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Bencina, Erkut Aykutlug, Yue Chen, Zerui Zhang, Stephanie Sorenson, Shao Tang, Changshuai Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data Driven Attribution, which assigns conversion credits to marketing
interactions based on causal patterns learned from data, is the foundation of
modern marketing intelligence and vital to any marketing businesses and
advertising platform. In this paper, we introduce a unified transformer-based
attribution approach that can handle member-level data, aggregate-level data,
and integration of external macro factors. We detail the large scale
implementation of the approach at LinkedIn, showcasing significant impact. We
also share learning and insights that are broadly applicable to the marketing
and ad tech fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Predictive Optimization and Generation for Business AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyang Zhao, Olurotimi Seton, Himadeep Reddy Reddivari, Suvendu Jena, Shadow Zhao, Rachit Kumar, Changshuai Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sales process involves sales functions converting leads or opportunities
to customers and selling more products to existing customers. The optimization
of the sales process thus is key to success of any B2B business. In this work,
we introduce a principled approach to sales optimization and business AI,
namely the Causal Predictive Optimization and Generation, which includes three
layers: 1) prediction layer with causal ML 2) optimization layer with
constraint optimization and contextual bandit 3) serving layer with Generative
AI and feedback-loop for system enhancement. We detail the implementation and
deployment of the system in LinkedIn, showcasing significant wins over legacy
systems and sharing learning and insight broadly applicable to this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Pairwise Learning-To-Rank At Airbnb 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malay Haldar, Daochen Zha, Huiji Gao, Liwei He, Sanjeev Katariya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are three fundamental asks from a ranking algorithm: it should scale to
handle a large number of items, sort items accurately by their utility, and
impose a total order on the items for logical consistency. But here's the
catch-no algorithm can achieve all three at the same time. We call this
limitation the SAT theorem for ranking algorithms. Given the dilemma, how can
we design a practical system that meets user needs? Our current work at Airbnb
provides an answer, with a working solution deployed at scale. We start with
pairwise learning-to-rank (LTR) models-the bedrock of search ranking tech
stacks today. They scale linearly with the number of items ranked and perform
strongly on metrics like NDCG by learning from pairwise comparisons. They are
at a sweet spot of performance vs. cost, making them an ideal choice for
several industrial applications. However, they have a drawback-by ignoring
interactions between items, they compromise on accuracy. To improve accuracy,
we create a "true" pairwise LTR model-one that captures interactions between
items during pairwise comparisons. But accuracy comes at the expense of
scalability and total order, and we discuss strategies to counter these
challenges. For greater accuracy, we take each item in the search result, and
compare it against the rest of the items along two dimensions: (1) Superiority:
How strongly do searchers prefer the given item over the remaining ones? (2)
Similarity: How similar is the given item to all the other items? This forms
the basis of our "all-pairwise" LTR framework, which factors in interactions
across all items at once. Looking at items on the search result page all
together-superiority and similarity combined-gives us a deeper understanding of
what searchers truly want. We quantify the resulting improvements in searcher
experience through offline and online experiments at Airbnb.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Large Language Models in Multimodal Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejo Lopez-Avila, Jinhua Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal recommender systems (MRS) integrate heterogeneous user and item
data, such as text, images, and structured information, to enhance
recommendation performance. The emergence of large language models (LLMs)
introduces new opportunities for MRS by enabling semantic reasoning, in-context
learning, and dynamic input handling. Compared to earlier pre-trained language
models (PLMs), LLMs offer greater flexibility and generalisation capabilities
but also introduce challenges related to scalability and model accessibility.
This survey presents a comprehensive review of recent work at the intersection
of LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and
data adaptation techniques. We propose a novel taxonomy to characterise
integration patterns, identify transferable techniques from related
recommendation domains, provide an overview of evaluation metrics and datasets,
and point to possible future directions. We aim to clarify the emerging role of
LLMs in multimodal recommendation and support future research in this rapidly
evolving field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of International Collaborations with Highly Publishing
  Countries in Computer Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Gomez Espes, Michael Faerber, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper analyzes international collaborations in Computer Science,
focusing on three major players: China, the European Union, and the United
States. Drawing from a comprehensive literature review, we examine
collaboration patterns, research impact, retraction rates, and the role of the
Development Index in shaping research outcomes. Our findings show that while
China, the EU, and the US lead global research efforts, other regions are
narrowing the gap in publication volume. Collaborations involving these key
regions tend to have lower retraction rates, reflecting stronger adherence to
scientific standards. We also find that countries with a Very High Development
Index contribute to research with higher citation rates and fewer retractions.
Overall, this study highlights the value of international collaboration and the
importance of inclusive, ethical practices in advancing global research in
Computer Science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distance-aware Self-adaptive Graph Convolution for Fine-grained
  Hierarchical Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Huang, Yihong Chen, Wei Fan, Wei Zhou, Junhao Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Convolutional Networks (GCNs) are widely used to improve recommendation
accuracy and performance by effectively learning the representations of user
and item nodes. However, two major challenges remain: (1) the lack of further
optimization in the graph representation structure and (2) insufficient
attention given to the varying contributions of different convolutional
layers.This paper proposes SAGCN, a distance-based adaptive hierarchical
aggregation method that refines the aggregation process through differentiated
representation metrics. SAGCN introduces a detailed approach to multilayer
information aggregation and representation space optimization, enabling the
model to learn hierarchical embedding weights based on the distance between
hierarchical representations. This innovation allows for more precise
cross-layer information aggregation, improves the model's ability to capture
hierarchical embeddings, and optimizes the representation space structure.
Additionally, the objective loss function is refined to better align with
recommendation tasks.Extensive experiments conducted on four real-world
datasets demonstrate significant improvements, including over a 5% increase on
Yelp and a 5.58% increase in Recall@10 on the ML_1M dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GlobalMood: A cross-cultural benchmark for music emotion recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harin Lee, Elif Çelen, Peter Harrison, Manuel Anglada-Tort, Pol van Rijn, Minsu Park, Marc Schönwiesner, Nori Jacoby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human annotations of mood in music are essential for music generation and
recommender systems. However, existing datasets predominantly focus on Western
songs with mood terms derived from English, which may limit generalizability
across diverse linguistic and cultural backgrounds. To address this, we
introduce `GlobalMood', a novel cross-cultural benchmark dataset comprising
1,180 songs sampled from 59 countries, with large-scale annotations collected
from 2,519 individuals across five culturally and linguistically distinct
locations: U.S., France, Mexico, S. Korea, and Egypt. Rather than imposing
predefined mood categories, we implement a bottom-up, participant-driven
approach to organically elicit culturally specific music-related mood terms. We
then recruit another pool of human participants to collect 988,925 ratings for
these culture-specific descriptors. Our analysis confirms the presence of a
valence-arousal structure shared across cultures, yet also reveals significant
divergences in how certain mood terms, despite being dictionary equivalents,
are perceived cross-culturally. State-of-the-art multimodal models benefit
substantially from fine-tuning on our cross-culturally balanced dataset, as
evidenced by improved alignment with human evaluations - particularly in
non-English contexts. More broadly, our findings inform the ongoing debate on
the universality versus cultural specificity of emotional descriptors, and our
methodology can contribute to other multimodal and cross-lingual research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CXMArena: Unified <span class="highlight-title">Dataset</span> to benchmark performance in realistic CXM
  Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Garg, Kapil Sharma, Karan Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) hold immense potential for revolutionizing
Customer Experience Management (CXM), particularly in contact center
operations. However, evaluating their practical utility in complex operational
environments is hindered by data scarcity (due to privacy concerns) and the
limitations of current benchmarks. Existing benchmarks often lack realism,
failing to incorporate deep knowledge base (KB) integration, real-world noise,
or critical operational tasks beyond conversational fluency. To bridge this
gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset
specifically designed for evaluating AI in operational CXM contexts. Given the
diversity in possible contact center features, we have developed a scalable
LLM-powered pipeline that simulates the brand's CXM entities that form the
foundation of our datasets-such as knowledge articles including product
specifications, issue taxonomies, and contact center conversations. The
entities closely represent real-world distribution because of controlled noise
injection (informed by domain experts) and rigorous automated validation.
Building on this, we release CXMArena, which provides dedicated benchmarks
targeting five important operational tasks: Knowledge Base Refinement, Intent
Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with
Integrated Tools. Our baseline experiments underscore the benchmark's
difficulty: even state of the art embedding and generation models achieve only
68% accuracy on article search, while standard embedding methods yield a low F1
score of 0.3 for knowledge base refinement, highlighting significant challenges
for current models necessitating complex pipelines and solutions over
conventional techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FACTors: A New <span class="highlight-title">Dataset</span> for Studying the Fact-checking Ecosystem <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enes Altuncu, Can Başkent, Sanjay Bhattacherjee, Shujun Li, Dwaipayan Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our fight against false information is spearheaded by fact-checkers. They
investigate the veracity of claims and document their findings as fact-checking
reports. With the rapid increase in the amount of false information circulating
online, the use of automation in fact-checking processes aims to strengthen
this ecosystem by enhancing scalability. Datasets containing fact-checked
claims play a key role in developing such automated solutions. However, to the
best of our knowledge, there is no fact-checking dataset at the ecosystem
level, covering claims from a sufficiently long period of time and sourced from
a wide range of actors reflecting the entire ecosystem that admittedly follows
widely-accepted codes and principles of fact-checking. We present a new dataset
FACTors, the first to fill this gap by presenting ecosystem-level data on
fact-checking. It contains 118,112 claims from 117,993 fact-checking reports in
English (co-)authored by 1,953 individuals and published during the period of
1995-2025 by 39 fact-checking organisations that are active signatories of the
IFCN (International Fact-Checking Network) and/or EFCSN (European Fact-Checking
Standards Network). It contains 7,327 overlapping claims investigated by
multiple fact-checking organisations, corresponding to 2,977 unique claims. It
allows to conduct new ecosystem-level studies of the fact-checkers
(organisations and individuals). To demonstrate the usefulness of FACTors, we
present three example applications, including a first-of-its-kind statistical
analysis of the fact-checking ecosystem, examining the political inclinations
of the fact-checking organisations, and attempting to assign a credibility
score to each organisation based on the findings of the statistical analysis
and political leanings. Our methods for constructing FACTors are generic and
can be used to maintain a live dataset that can be updated dynamically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 48th International ACM SIGIR Conference on Research
  and Development in Information Retrieval (SIGIR '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Recommender Models and the Illusion of Progress: A Concerning
  Study of Reproducibility and a Conceptual Mismatch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Benigni, Maurizio Ferrari Dacrema, Dietmar Jannach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Countless new machine learning models are published every year and are
reported to significantly advance the state-of-the-art in \emph{top-n}
recommendation. However, earlier reproducibility studies indicate that progress
in this area may be quite limited. Specifically, various widespread
methodological issues, e.g., comparisons with untuned baseline models, have led
to an \emph{illusion of progress}. In this work, our goal is to examine whether
these problems persist in today's research. To this end, we aim to reproduce
the latest advancements reported from applying modern Denoising Diffusion
Probabilistic Models to recommender systems, focusing on four models published
at the top-ranked SIGIR conference in 2023 and 2024. Our findings are
concerning, revealing persistent methodological problems. Alarmingly, through
experiments, we find that the latest recommendation techniques based on
diffusion models, despite their computational complexity and substantial carbon
footprint, are consistently outperformed by simpler existing models.
Furthermore, we identify key mismatches between the characteristics of
diffusion models and those of the traditional \emph{top-n} recommendation task,
raising doubts about their suitability for recommendation. We also note that,
in the papers we analyze, the generative capabilities of these models are
constrained to a minimum. Overall, our results and continued methodological
issues call for greater scientific rigor and a disruptive change in the
research and publication culture in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scent of Knowledge: Optimizing Search-Enhanced Reasoning with
  Information Foraging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjin Qian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Focus, Merge, Rank: Improved Question Answering Based on Semi-structured
  Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derian Boer, Stephen Roth, Stefan Kramer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world settings, machine learning models and interactive systems
have access to both structured knowledge, e.g., knowledge graphs or tables, and
unstructured content, e.g., natural language documents. However, most rely on
either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking
unstructured content to nodes within structured data, thereby enabling new
strategies for knowledge access and use. In this work, we present
FocusedRetriever, a modular SKB-based framework for multi-hop question
answering. It integrates components (VSS-based entity search, LLM-based
generation of Cypher queries and pairwise re-ranking) in a way that enables it
to outperform state-of-the-art methods across all three STaRK benchmark test
sets, covering diverse domains and multiple performance metrics. The average
first-hit rate exceeds that of the second-best method by 25.7%.
FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to
extract relational facts and entity attributes from unstructured text, (2) node
set joins to filter answer candidates based on these extracted triplets and
constraints, (3) vector similarity search to retrieve and rank relevant
unstructured content, and (4) the contextual capabilities of LLMs to finally
rank the top-k answers. For generality, we only incorporate base LLMs in
FocusedRetriever in our evaluation. However, our analysis of intermediate
results highlights several opportunities for further upgrades including
finetuning. The source code is publicly available at
https://github.com/kramerlab/FocusedRetriever .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HMamba: Hyperbolic Mamba for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianru Zhang, Honggang Wen, Wei Yuan, Crystal Chen, Menglin Yang, Siu-Ming Yiu, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems have become a cornerstone of personalized
services, adept at modeling the temporal evolution of user preferences by
capturing dynamic interaction sequences. Existing approaches predominantly rely
on traditional models, including RNNs and Transformers. Despite their success
in local pattern recognition, Transformer-based methods suffer from quadratic
computational complexity and a tendency toward superficial attention patterns,
limiting their ability to infer enduring preference hierarchies in sequential
recommendation data. Recent advances in Mamba-based sequential models introduce
linear-time efficiency but remain constrained by Euclidean geometry, failing to
leverage the intrinsic hyperbolic structure of recommendation data. To bridge
this gap, we propose Hyperbolic Mamba, a novel architecture that unifies the
efficiency of Mamba's selective state space mechanism with hyperbolic
geometry's hierarchical representational power. Our framework introduces (1) a
hyperbolic selective state space that maintains curvature-aware sequence
modeling and (2) stabilized Riemannian operations to enable scalable training.
Experiments across four benchmarks demonstrate that Hyperbolic Mamba achieves
3-11% improvement while retaining Mamba's linear-time efficiency, enabling
real-world deployment. This work establishes a new paradigm for efficient,
hierarchy-aware sequential modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Display Content, Display Methods and Evaluation Methods of the HCI in
  Explainable Recommender Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqing Li, Yue Xu, Yuefeng Li, Yinghui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Recommender Systems (XRS) aim to provide users with
understandable reasons for the recommendations generated by these systems,
representing a crucial research direction in artificial intelligence (AI).
Recent research has increasingly focused on the algorithms, display, and
evaluation methodologies of XRS. While current research and reviews primarily
emphasize the algorithmic aspects, with fewer studies addressing the
Human-Computer Interaction (HCI) layer of XRS. Additionally, existing reviews
lack a unified taxonomy for XRS and there is insufficient attention given to
the emerging area of short video recommendations. In this study, we synthesize
existing literature and surveys on XRS, presenting a unified framework for its
research and development. The main contributions are as follows: 1) We adopt a
lifecycle perspective to systematically summarize the technologies and methods
used in XRS, addressing challenges posed by the diversity and complexity of
algorithmic models and explanation techniques. 2) For the first time, we
highlight the application of multimedia, particularly video-based explanations,
along with its potential, technical pathways, and challenges in XRS. 3) We
provide a structured overview of evaluation methods from both qualitative and
quantitative dimensions. These findings provide valuable insights for the
systematic design, progress, and testing of XRS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 Tables, 29 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Item Level Exploration Traffic Allocation in Large-scale Recommendation
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Wang, Junyi Jiao, Arnab Bhadury, Yaping Zhang, Mingyan Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper contributes to addressing the item cold start problem in
large-scale recommender systems, focusing on how to efficiently gain initial
visibility for newly ingested content. We propose an exploration system
designed to efficiently allocate impressions to these fresh items. Our approach
leverages a learned probabilistic model to predict an item's discoverability,
which then informs a scalable and adaptive traffic allocation strategy. This
system intelligently distributes exploration budgets, optimizing for the
long-term benefit of the recommendation platform. The impact is a demonstrably
more efficient cold-start process, leading to a significant increase in the
discoverability of new content and ultimately enriching the item corpus
available for exploitation, as evidenced by its successful deployment in a
large-scale production environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by the 18th ACM Recsys Large Recsys Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Goal-Conditioned Supervised Learning for Multi-Objective Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08911v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08911v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijun Li, Hilaf Hasson, Jing Hu, Joydeep Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective learning endeavors to concurrently optimize multiple
objectives using a single model, aiming to achieve high and balanced
performance across diverse objectives. However, this often entails a more
complex optimization problem, particularly when navigating potential conflicts
between objectives, leading to solutions with higher memory requirements and
computational complexity. This paper introduces a Multi-Objective
Goal-Conditioned Supervised Learning (MOGCSL) framework for automatically
learning to achieve multiple objectives from offline sequential data. MOGCSL
extends the conventional GCSL method to multi-objective scenarios by redefining
goals from one-dimensional scalars to multi-dimensional vectors. It benefits
from naturally eliminating the need for complex architectures and optimization
constraints. Moreover, MOGCSL effectively filters out uninformative or noisy
instances that fail to achieve desirable long-term rewards across multiple
objectives. We also introduces a novel goal-selection algorithm for MOGCSL to
model and identify "high" achievable goals for inference.
  While MOGCSL is quite general, we focus on its application to the next action
prediction problem in commercial-grade recommender systems. In this context,
any viable solution needs to be reasonably scalable and also be robust to large
amounts of noisy data that is characteristic of this application space. We show
that MOGCSL performs admirably on both counts by extensive experiments on
real-world recommendation datasets. Also, analysis and experiments are included
to explain its strength in discounting the noisier portions of training data in
recommender systems with multiple objectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OAEI-LLM-T: A TBox Benchmark <span class="highlight-title">Dataset</span> for Understanding Large Language
  Model Hallucinations in Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21813v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21813v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang, Jing Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations are often inevitable in downstream tasks using large language
models (LLMs). To tackle the substantial challenge of addressing hallucinations
for LLM-based ontology matching (OM) systems, we introduce a new benchmark
dataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the
Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of
ten different LLMs performing OM tasks. These OM-specific hallucinations are
organised into two primary categories and six sub-categories. We showcase the
usefulness of the dataset in constructing an LLM leaderboard for OM tasks and
for fine-tuning LLMs used in OM tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, 4 tables, 2 prompt templates</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Overlap Ratio in Defocused Electron Ptychography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirafshar Moshtaghpour, Angus I. Kirkland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Four-dimensional Scanning Transmission Electron Microscopy (4D STEM) with
data acquired using a defocused electron probe is a promising tool for
characterising complex biological specimens and materials through a phase
retrieval process known as Electron Ptychography (EP). The efficacy of 4D STEM
acquisition and the resulting quality of EP reconstruction depends on the
overlap ratio of adjacent illuminated areas. This paper demonstrates how the
overlap ratio impacts the data redundancy and the quality of the EP
reconstruction. We define two quantities as a function of the overlap ratio
that are independent of both the object and the EP algorithm. Subsequently, we
evaluate an EP algorithm for varying overlap ratios using simulated 4D STEM
datasets. Notably, a 40% or greater overlap ratio yields stable, high-quality
reconstructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deconstructing Jazz Piano Style Using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huw Cheston, Reuben Bance, Peter M. C. Harrison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artistic style has been studied for centuries, and recent advances in machine
learning create new possibilities for understanding it computationally.
However, ensuring that machine-learning models produce insights aligned with
the interests of practitioners and critics remains a significant challenge.
Here, we focus on musical style, which benefits from a rich theoretical and
mathematical analysis tradition. We train a variety of supervised-learning
models to identify 20 iconic jazz musicians across a carefully curated dataset
of 84 hours of recordings, and interpret their decision-making processes. Our
models include a novel multi-input architecture that enables four musical
domains (melody, harmony, rhythm, and dynamics) to be analysed separately.
These models enable us to address fundamental questions in music theory and
also advance the state-of-the-art in music performer identification (94%
accuracy across 20 classes). We release open-source implementations of our
models and an accompanying web application for exploring musical styles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper: 40 pages, 11 figures, 1 table; added information on training
  time + computation cost, corrections to Table 1. Supplementary material: 33
  pages, 48 figures, 6 tables; corrections to Table S.5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Signed Latent Factors for Spamming Activity Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.13814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.13814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuli Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the increasing trend of performing spamming activities (e.g., Web
spam, deceptive reviews, fake followers, etc.) on various online platforms to
gain undeserved benefits, spam detection has emerged as a hot research issue.
Previous attempts to combat spam mainly employ features related to metadata,
user behaviors, or relational ties. These studies have made considerable
progress in understanding and filtering spamming campaigns. However, this
problem remains far from fully solved. Almost all the proposed features focus
on a limited number of observed attributes or explainable phenomena, making it
difficult for existing methods to achieve further improvement. To broaden the
vision about solving the spam problem and address long-standing challenges
(class imbalance and graph incompleteness) in the spam detection area, we
propose a new attempt of utilizing signed latent factors to filter fraudulent
activities. The spam-contaminated relational datasets of multiple online
applications in this scenario are interpreted by the unified signed network.
Two competitive and highly dissimilar algorithms of latent factors mining (LFM)
models are designed based on multi-relational likelihoods estimation (LFM-MRLE)
and signed pairwise ranking (LFM-SPR), respectively. We then explore how to
apply the mined latent factors to spam detection tasks. Experiments on
real-world datasets of different kinds of Web applications (social media and
Web forum) indicate that LFM models outperform state-of-the-art baselines in
detecting spamming activities. By specifically manipulating experimental data,
the effectiveness of our methods in dealing with incomplete and imbalanced
challenges is validated.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WavReward: Spoken Dialogue Models With Generalist Reward Evaluators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengpeng Ji, Tianle Liang, Yangzhuo Li, Jialong Zuo, Minghui Fang, Jinzheng He, Yifu Chen, Zhengqing Liu, Ziyue Jiang, Xize Cheng, Siqi Zheng, Jin Xu, Junyang Lin, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered
significant attention in the speech domain. However, the evaluation of spoken
dialogue models' conversational performance has largely been overlooked. This
is primarily due to the intelligent chatbots convey a wealth of non-textual
information which cannot be easily measured using text-based language models
like ChatGPT. To address this gap, we propose WavReward, a reward feedback
model based on audio language models that can evaluate both the IQ and EQ of
spoken dialogue systems with speech input. Specifically, 1) based on audio
language models, WavReward incorporates the deep reasoning process and the
nonlinear reward mechanism for post-training. By utilizing multi-sample
feedback via the reinforcement learning algorithm, we construct a specialized
evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a
preference dataset used to train WavReward. ChatReward-30K includes both
comprehension and generation aspects of spoken dialogue models. These scenarios
span various tasks, such as text-based chats, nine acoustic attributes of
instruction chats, and implicit chats. WavReward outperforms previous
state-of-the-art evaluation models across multiple spoken dialogue scenarios,
achieving a substantial improvement about Qwen2.5-Omni in objective accuracy
from 55.1$\%$ to 91.5$\%$. In subjective A/B testing, WavReward also leads by a
margin of 83$\%$. Comprehensive ablation studies confirm the necessity of each
component of WavReward. All data and code will be publicly at
https://github.com/jishengpeng/WavReward after the paper is accepted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning-Based Prediction of Quality Shifts on Video Streaming
  Over 5G 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17938v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17938v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raza Ul Mustafa, Sesha Dassanayake, Noman Ashraf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Quality of Experience (QoE) is the users satisfaction while streaming a
video session over an over-the-top (OTT) platform like YouTube. QoE of YouTube
reflects the smooth streaming session without any buffering and quality shift
events. One of the most important factors nowadays affecting QoE of YouTube is
frequent shifts from higher to lower resolutions and vice versa. These shifts
ensure a smooth streaming session; however, it might get a lower mean opinion
score. For instance, dropping from 1080p to 480p during a video can preserve
continuity but might reduce the viewers enjoyment. Over time, OTT platforms are
looking for alternative ways to boost user experience instead of relying on
traditional Quality of Service (QoS) metrics such as bandwidth, latency, and
throughput. As a result, we look into the relationship between quality shifting
in YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our
findings state that these channel metrics positively correlate with shifts.
Thus, in real-time, OTT can only rely on them to predict video streaming
sessions into lower- and higher-resolution categories, thus providing more
resources to improve user experience. Using traditional Machine Learning (ML)
classifiers, we achieved an accuracy of 77-percent, while using only RSRP,
RSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency
networks promise enhanced streaming capabilities, the proposed methodology can
be used to improve OTT services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Multimedia Generated by Large AI Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00045v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00045v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large AI Models (LAIMs), particularly diffusion
models and large language models, has marked a new era where AI-generated
multimedia is increasingly integrated into various aspects of daily life.
Although beneficial in numerous fields, this content presents significant
risks, including potential misuse, societal disruptions, and ethical concerns.
Consequently, detecting multimedia generated by LAIMs has become crucial, with
a marked rise in related research. Despite this, there remains a notable gap in
systematic surveys that focus specifically on detecting LAIM-generated
multimedia. Addressing this, we provide the first survey to comprehensively
cover existing research on detecting multimedia (such as text, images, videos,
audio, and multimodal content) created by LAIMs. Specifically, we introduce a
novel taxonomy for detection methods, categorized by media modality, and
aligned with two perspectives: pure detection (aiming to enhance detection
performance) and beyond detection (adding attributes like generalizability,
robustness, and interpretability to detectors). Additionally, we have presented
a brief overview of generation mechanisms, public datasets, online detection
tools, and evaluation metrics to provide a valuable resource for researchers
and practitioners in this field. Most importantly, we offer a focused analysis
from a social media perspective to highlight their broader societal impact.
Furthermore, we identify current challenges in detection and propose directions
for future research that address unexplored, ongoing, and emerging issues in
detecting multimedia generated by LAIMs. Our aim for this survey is to fill an
academic gap and contribute to global AI security efforts, helping to ensure
the integrity of information in the digital realm. The project link is
https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh
  Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Shafiee Sarvestani, Sheyang Tang, Zhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mesh quality assessment (MQA) models play a critical role in the design,
optimization, and evaluation of mesh operation systems in a wide variety of
applications. Current MQA models, whether model-based methods using
topology-aware features or projection-based approaches working on rendered 2D
projections, often fail to capture the intricate interactions between texture
and 3D geometry. We introduce HybridMQA, a first-of-its-kind hybrid
full-reference colored MQA framework that integrates model-based and
projection-based approaches, capturing complex interactions between textural
information and 3D structures for enriched quality representations. Our method
employs graph learning to extract detailed 3D representations, which are then
projected to 2D using a novel feature rendering process that precisely aligns
them with colored projections. This enables the exploration of geometry-texture
interactions via cross-attention, producing comprehensive mesh quality
representations. Extensive experiments demonstrate HybridMQA's superior
performance across diverse datasets, highlighting its ability to effectively
leverage geometry-texture interactions for a thorough understanding of mesh
quality. Our implementation will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Text-to-Audio Generation with Adversarial Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Novack, Zach Evans, Zack Zukowski, Josiah Taylor, CJ Carr, Julian Parker, Adnan Al-Sinan, Gian Marco Iodice, Julian McAuley, Taylor Berg-Kirkpatrick, Jordi Pons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-audio systems, while increasingly performant, are slow at inference
time, thus making their latency unpractical for many creative applications. We
present Adversarial Relativistic-Contrastive (ARC) post-training, the first
adversarial acceleration algorithm for diffusion/flow models not based on
distillation. While past adversarial post-training methods have struggled to
compare against their expensive distillation counterparts, ARC post-training is
a simple procedure that (1) extends a recent relativistic adversarial
formulation to diffusion/flow post-training and (2) combines it with a novel
contrastive discriminator objective to encourage better prompt adherence. We
pair ARC post-training with a number optimizations to Stable Audio Open and
build a model capable of generating $\approx$12s of 44.1kHz stereo audio in
$\approx$75ms on an H100, and $\approx$7s on a mobile edge-device, the fastest
text-to-audio model to our knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-13T00:00:00Z">2025-05-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency,
  and Self-Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Kumar, Hwiyoon Kim, Jawahar Sai Nathani, Neil Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination, where large language models (LLMs) generate confident but
incorrect or irrelevant information, remains a key limitation in their
application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has
emerged as a promising method for improving multistep reasoning by guiding
models through intermediate steps. However, CoT alone does not fully address
the hallucination problem. In this work, we investigate how combining CoT with
retrieval-augmented generation (RAG), as well as applying self-consistency and
self-verification strategies, can reduce hallucinations and improve factual
accuracy. By incorporating external knowledge sources during reasoning and
enabling models to verify or revise their own outputs, we aim to generate more
accurate and coherent responses. We present a comparative evaluation of
baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification
techniques. Our results highlight the effectiveness of each method and identify
the most robust approach for minimizing hallucinations while preserving fluency
and reasoning depth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Meta <span class="highlight-title">Prompt</span> Engineering for Alignment with the Theory of Mind 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Baughman, Rahul Agarwal, Eduardo Morales, Gozde Akay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method of meta-prompting that jointly produces fluent text for
complex tasks while optimizing the similarity of neural states between a
human's mental expectation and a Large Language Model's (LLM) neural
processing. A technique of agentic reinforcement learning is applied, in which
an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,
how to produce content by interpreting the intended and unintended generated
text traits. To measure human mental beliefs around content production, users
modify long form AI-generated text articles before publication at the US Open
2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)
alignment problem by anticipating and including human edits within the creation
of text from an LLM. Throughout experimentation and by interpreting the results
of a live production system, the expectations of human content reviewers had
100% of alignment with AI 53.8% of the time with an average iteration count of
4.38. The geometric interpretation of content traits such as factualness,
novelty, repetitiveness, and relevancy over a Hilbert vector space combines
spatial volume (all trait importance) with vertices alignment (individual trait
relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an
increase in content quality by extending the coverage of tennis action. Our
work that was deployed at the US Open 2024 has been used across other live
events within sports and entertainment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ For <span class="highlight-title">GPT</span>-4 as with Humans: Information Structure Predicts Acceptability
  of Long-Distance Dependencies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicole Cuneo, Eleanor Graves, Supantho Rakshit, Adele E. Goldberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It remains debated how well any LM understands natural language or generates
reliable metalinguistic judgments. Moreover, relatively little work has
demonstrated that LMs can represent and respect subtle relationships between
form and function proposed by linguists. We here focus on a particular such
relationship established in recent work: English speakers' judgments about the
information structure of canonical sentences predicts independently collected
acceptability ratings on corresponding 'long distance dependency' [LDD]
constructions, across a wide array of base constructions and multiple types of
LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on
the same tasks used with humans and new extensions.Results reveal reliable
metalinguistic skill on the information structure and acceptability tasks,
replicating a striking interaction between the two, despite the zero-shot,
explicit nature of the tasks, and little to no chance of contamination [Studies
1a, 1b]. Study 2 manipulates the information structure of base sentences and
confirms a causal relationship: increasing the prominence of a constituent in a
context sentence increases the subsequent acceptability ratings on an LDD
construction. The findings suggest a tight relationship between natural and
GPT-4 generated English, and between information structure and syntax, which
begs for further exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A suite of LMs comprehend puzzle statements as well as humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adele E Goldberg, Supantho Rakshit, Jennifer Hu, Kyle Mahowald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent claims suggest that large language models (LMs) underperform humans in
comprehending minimally complex English statements (Dentella et al., 2024).
Here, we revisit those findings and argue that human performance was
overestimated, while LLM abilities were underestimated. Using the same stimuli,
we report a preregistered study comparing human responses in two conditions:
one allowed rereading (replicating the original study), and one that restricted
rereading (a more naturalistic comprehension test). Human accuracy dropped
significantly when rereading was restricted (73%), falling below that of
Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect
accuracy. Results further show that both humans and models are
disproportionately challenged by queries involving potentially reciprocal
actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than
model-specific deficits. Additional analyses using Llama-2-70B log
probabilities, a recoding of open-ended model responses, and grammaticality
ratings of other sentences reveal systematic underestimation of model
performance. We find that GPT-4o can align with either naive or expert
grammaticality judgments, depending on prompt framing. These findings
underscore the need for more careful experimental design and coding practices
in LLM evaluation, and they challenge the assumption that current models are
inherently weaker than humans at language comprehension.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prioritizing Image-Related Tokens Enhances Vision-Language <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyi Chen, Hao Peng, Tong Zhang, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In standard large vision-language models (LVLMs) pre-training, the model
typically maximizes the joint probability of the caption conditioned on the
image via next-token prediction (NTP); however, since only a small subset of
caption tokens directly relates to the visual content, this naive NTP
unintentionally fits the model to noise and increases the risk of
hallucination. We present PRIOR, a simple vision-language pre-training approach
that addresses this issue by prioritizing image-related tokens through
differential weighting in the NTP loss, drawing from the importance sampling
framework. PRIOR introduces a reference model-a text-only large language model
(LLM) trained on the captions without image inputs, to weight each token based
on its probability for LVLMs training. Intuitively, tokens that are directly
related to the visual inputs are harder to predict without the image and thus
receive lower probabilities from the text-only reference LLM. During training,
we implement a token-specific re-weighting term based on the importance scores
to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs
with visual encoders and LVLMs without visual encoders. We observe 19% and 8%
average relative improvement, respectively, on several vision-language
benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling
properties, as demonstrated by significantly higher scaling coefficients,
indicating greater potential for performance gains compared to NTP given
increasing compute and data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code will be available at https://github.com/Yangyi-Chen/PRIOR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ForeCite: Adapting <span class="highlight-title">Pre-Train</span>ed Language Models to Predict Future
  Citation Rates of Academic Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gavin Hull, Alex Bihlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the future citation rates of academic papers is an important step
toward the automation of research evaluation and the acceleration of scientific
progress. We present $\textbf{ForeCite}$, a simple but powerful framework to
append pre-trained causal language models with a linear head for average
monthly citation rate prediction. Adapting transformers for regression tasks,
ForeCite achieves a test correlation of $\rho = 0.826$ on a curated dataset of
900K+ biomedical papers published between 2000 and 2024, a 27-point improvement
over the previous state-of-the-art. Comprehensive scaling-law analysis reveals
consistent gains across model sizes and data volumes, while temporal holdout
experiments confirm practical robustness. Gradient-based saliency heatmaps
suggest a potentially undue reliance on titles and abstract texts. These
results establish a new state-of-the-art in forecasting the long-term influence
of academic research and lay the groundwork for the automated, high-fidelity
evaluation of scientific contributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Synthetic Data Evaluations of Language Models in Unsupervised
  Document Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Majurski, Cynthia Matuszek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) continue to advance, improving response quality and
coherence. Given Internet-scale training datasets, LMs have likely encountered
much of what users might ask them to generate in some form during their
training. A plethora of evaluation benchmarks have been constructed to assess
model quality, response appropriateness, and reasoning capabilities. However,
the human effort required for benchmark construction is limited and being
rapidly outpaced by the size and scope of the models under evaluation.
Additionally, having humans build a benchmark for every possible domain of
interest is impractical. Therefore, we propose a methodology for automating the
construction of fact-based synthetic data model evaluations grounded in
document populations. This work leverages those very same LMs to evaluate
domain-specific knowledge automatically, using only grounding documents (e.g.,
a textbook) as input. This synthetic data benchmarking approach corresponds
well with human curated questions with a Spearman ranking correlation of 0.96
and a benchmark evaluation Pearson accuracy correlation of 0.79. This novel
tool supports generating both multiple choice and open-ended synthetic data
questions to gain diagnostic insight of LM capability. We apply this
methodology to evaluate model performance on a recent relevant arXiv preprint,
discovering a surprisingly strong performance from Gemma3 models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Gains of LLMs With Humans in a World of LLMs Versus Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas McCullum, Pelagie Ami Agassi, Leo Anthony Celi, Daniel K. Ebner, Chrystinne Oliveira Fernandes, Rachel S. Hicklen, Mkliwa Koumbia, Lisa Soleymani Lehmann, David Restrepo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, a considerable research effort is devoted to comparing LLMs to a
group of human experts, where the term "expert" is often ill-defined or
variable, at best, in a state of constantly updating LLM releases. Without
proper safeguards in place, LLMs will threaten to cause harm to the established
structure of safe delivery of patient care which has been carefully developed
throughout history to keep the safety of the patient at the forefront. A key
driver of LLM innovation is founded on community research efforts which, if
continuing to operate under "humans versus LLMs" principles, will expedite this
trend. Therefore, research efforts moving forward must focus on effectively
characterizing the safe use of LLMs in clinical settings that persist across
the rapid development of novel LLM models. In this communication, we
demonstrate that rather than comparing LLMs to humans, there is a need to
develop strategies enabling efficient work of humans with LLMs in an almost
symbiotic manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Analytical Emotion Framework of Rumour Threads on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Xing, Boyang Sun, Kun Zhang, Preslav Nakov, Timothy Baldwin, Jey Han Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rumours in online social media pose significant risks to modern society,
motivating the need for better understanding of how they develop. We focus
specifically on the interface between emotion and rumours in threaded
discourses, building on the surprisingly sparse literature on the topic which
has largely focused on single aspect of emotions within the original rumour
posts themselves, and largely overlooked the comparative differences between
rumours and non-rumours. In this work, we take one step further to provide a
comprehensive analytical emotion framework with multi-aspect emotion detection,
contrasting rumour and non-rumour threads and provide both correlation and
causal analysis of emotions. We applied our framework on existing widely-used
rumour datasets to further understand the emotion dynamics in online social
media threads. Our framework reveals that rumours trigger more negative
emotions (e.g., anger, fear, pessimism), while non-rumours evoke more positive
ones. Emotions are contagious, rumours spread negativity, non-rumours spread
positivity. Causal analysis shows surprise bridges rumours and other emotions;
pessimism comes from sadness and fear, while optimism arises from joy and love.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICWSM 2025 MisD Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simulating and Analysing Human <span class="highlight-title">Survey</span> Responses with Large Language
  Models: A Case Study in Energy Stated Preference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Jacek Pawlak, Aruna Sivakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Survey research plays a crucial role in studies by capturing consumer
preferences and informing policy decisions. Stated preference (SP) surveys help
researchers understand how individuals make trade-offs in hypothetical,
potentially futuristic, scenarios. However, traditional methods are costly,
time-consuming, and affected by respondent fatigue and ethical constraints.
Large language models (LLMs) have shown remarkable capabilities in generating
human-like responses, prompting interest in their use in survey research. This
study investigates LLMs for simulating consumer choices in energy-related SP
surveys and explores their integration into data collection and analysis
workflows. Test scenarios were designed to assess the simulation performance of
several LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and
aggregated levels, considering prompt design, in-context learning (ICL),
chain-of-thought (CoT) reasoning, model types, integration with traditional
choice models, and potential biases. While LLMs achieve accuracy above random
guessing, performance remains insufficient for practical simulation use.
Cloud-based LLMs do not consistently outperform smaller local models.
DeepSeek-R1 achieves the highest average accuracy (77%) and outperforms
non-reasoning LLMs in accuracy, factor identification, and choice distribution
alignment. Previous SP choices are the most effective input; longer prompts
with more factors reduce accuracy. Mixed logit models can support LLM prompt
refinement. Reasoning LLMs show potential in data analysis by indicating factor
significance, offering a qualitative complement to statistical models. Despite
limitations, pre-trained LLMs offer scalability and require minimal historical
data. Future work should refine prompts, further explore CoT reasoning, and
investigate fine-tuning techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Principled Data Selection for Alignment: The Hidden Risks of Difficult
  Examples <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengqian Gao, Haonan Li, Liu Liu, Zeke Xie, Peilin Zhao, Zhiqiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The alignment of large language models (LLMs) often assumes that using more
clean data yields better outcomes, overlooking the match between model capacity
and example difficulty. Challenging this, we propose a new principle:
Preference data vary in difficulty, and overly difficult examples hinder
alignment, by exceeding the model's capacity. Through systematic
experimentation, we validate this principle with three key findings: (1)
preference examples vary in difficulty, as evidenced by consistent learning
orders across alignment runs; (2) overly difficult examples significantly
degrade performance across four LLMs and two datasets; and (3) the capacity of
a model dictates its threshold for handling difficult examples, underscoring a
critical relationship between data selection and model capacity. Building on
this principle, we introduce Selective DPO, which filters out overly difficult
examples. This simple adjustment improves alignment performance by 9-16% in win
rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a
series of DPO variants with different algorithmic adjustments. Together, these
results illuminate the importance of aligning data difficulty with model
capacity, offering a transformative perspective for improving alignment
strategies in LLMs. Code is available at
https://github.com/glorgao/SelectiveDPO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Securing RAG: A Risk Assessment and Mitigation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Ammann, Sara Ott, Christoph R. Landolt, Marco P. Lehmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) has emerged as the de facto industry
standard for user-facing NLP applications, offering the ability to integrate
data without re-training or fine-tuning Large Language Models (LLMs). This
capability enhances the quality and accuracy of responses but also introduces
novel security and privacy challenges, particularly when sensitive data is
integrated. With the rapid adoption of RAG, securing data and services has
become a critical priority. This paper first reviews the vulnerabilities of RAG
pipelines, and outlines the attack surface from data pre-processing and data
storage management to integration with LLMs. The identified risks are then
paired with corresponding mitigations in a structured overview. In a second
step, the paper develops a framework that combines RAG-specific security
considerations, with existing general security guidelines, industry standards,
and best practices. The proposed framework aims to guide the implementation of
robust, compliant, secure, and trustworthy RAG systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, Sara Ott and Lukas Ammann contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interest Changes: Considering User Interest Life Cycle in Recommendation
  System <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinjiang Cai, Jiangpan Hou, Yangping Zhu, Yuan Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recommendation systems, user interests are always in a state of constant
flux. Typically, a user interest experiences a emergent phase, a stable phase,
and a declining phase, which are referred to as the "user interest life-cycle".
Recent papers on user interest modeling have primarily focused on how to
compute the correlation between the target item and user's historical
behaviors, without thoroughly considering the life-cycle features of user
interest. In this paper, we propose an effective method called Deep Interest
Life-cycle Network (DILN), which not only captures the interest life-cycle
features efficiently, but can also be easily integrated to existing ranking
models. DILN contains two key components: Interest Life-cycle Encoder Module
constructs historical activity histograms of the user interest and then encodes
them into dense representation. Interest Life-cycle Fusion Module injects the
encoded dense representation into multiple expert networks, with the aim of
enabling the specific phase of interest life-cycle to activate distinct
experts. Online A/B testing reveals that DILN achieves significant improvements
of +0.38% in CTR, +1.04% in CVR and +0.25% in duration per user, which
demonstrates its effectiveness. In addition, DILN inherently increase the
exposure of users' emergent and stable interests while decreasing the exposure
of declining interests. DILN has been deployed on the Lofter App.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lost in Transliteration: Bridging the Script Gap in Neural IR <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Chari, Iadh Ounis, Sean MacAvaney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most human languages use scripts other than the Latin alphabet. Search users
in these languages often formulate their information needs in a transliterated
-- usually Latinized -- form for ease of typing. For example, Greek speakers
might use Greeklish, and Arabic speakers might use Arabizi. This paper shows
that current search systems, including those that use multilingual dense
embeddings such as BGE-M3, do not generalise to this setting, and their
performance rapidly deteriorates when exposed to transliterated queries. This
creates a ``script gap" between the performance of the same queries when
written in their native or transliterated form. We explore whether adapting the
popular ``translate-train" paradigm to transliterations can enhance the
robustness of multilingual Information Retrieval (IR) methods and bridge the
gap between native and transliterated scripts. By exploring various
combinations of non-Latin and Latinized query text for training, we investigate
whether we can enhance the capacity of existing neural retrieval techniques and
enable them to apply to this important setting. We show that by further
fine-tuning IR models on an even mixture of native and Latinized text, they can
perform this cross-script matching at nearly the same performance as when the
query was formulated in the native script. Out-of-domain evaluation and further
qualitative analysis show that transliterations can also cause queries to lose
some of their nuances, motivating further research in this direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 tables. paper accepted at the Short Paper track of The
  48th International ACM SIGIR Conference on Research and Development in
  Information Retrieval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TikTok Search Recommendations: Governance and Research Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taylor Annabell, Robert Gorwa, Rebecca Scharlach, Jacob van de Kerkhof, Thales Bertaglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Like other social media, TikTok is embracing its use as a search engine,
developing search products to steer users to produce searchable content and
engage in content discovery. Their recently developed product search
recommendations are preformulated search queries recommended to users on
videos. However, TikTok provides limited transparency about how search
recommendations are generated and moderated, despite requirements under
regulatory frameworks like the European Union's Digital Services Act. By
suggesting that the platform simply aggregates comments and common searches
linked to videos, it sidesteps responsibility and issues that arise from
contextually problematic recommendations, reigniting long-standing concerns
about platform liability and moderation. This position paper addresses the
novelty of search recommendations on TikTok by highlighting the challenges that
this feature poses for platform governance and offering a computational
research agenda, drawing on preliminary qualitative analysis. It sets out the
need for transparency in platform documentation, data access and research to
study search recommendations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in The 1st international Workshop on Computational
  Approaches to Content Moderation and Platform Governance (COMPASS), held at
  ICWSM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperbolic Contrastive Learning with Model-augmentation for
  Knowledge-aware Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyin Sun, Chen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benefiting from the effectiveness of graph neural networks (GNNs) and
contrastive learning, GNN-based contrastive learning has become mainstream for
knowledge-aware recommendation. However, most existing contrastive
learning-based methods have difficulties in effectively capturing the
underlying hierarchical structure within user-item bipartite graphs and
knowledge graphs. Moreover, they commonly generate positive samples for
contrastive learning by perturbing the graph structure, which may lead to a
shift in user preference learning. To overcome these limitations, we propose
hyperbolic contrastive learning with model-augmentation for knowledge-aware
recommendation. To capture the intrinsic hierarchical graph structures, we
first design a novel Lorentzian knowledge aggregation mechanism, which enables
more effective representations of users and items. Then, we propose three
model-level augmentation techniques to assist Hyperbolic contrastive learning.
Different from the classical structure-level augmentation (e.g., edge
dropping), the proposed model-augmentations can avoid preference shifts between
the augmented positive pair. Finally, we conduct extensive experiments to
demonstrate the superiority (maximum improvement of $11.03\%$) of proposed
methods over existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph RAG for Legal Norms: A Hierarchical and Temporal Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hudson de Martim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article proposes an adaptation of Graph Retrieval Augmented Generation
(Graph RAG) specifically designed for the analysis and comprehension of legal
norms, which are characterized by their predefined hierarchical structure,
extensive network of internal and external references and multiple temporal
versions. By combining structured knowledge graphs with contextually enriched
text segments, Graph RAG offers a promising solution to address the inherent
complexity and vast volume of legal data. The integration of hierarchical
structure and temporal evolution into knowledge graphs - along with the concept
of comprehensive Text Units - facilitates the construction of richer,
interconnected representations of legal knowledge. Through a detailed analysis
of Graph RAG and its application to legal norm datasets, this article aims to
advance the field of Artificial Intelligence applied to Law, creating
opportunities for more effective systems in legal research, legislative
analysis, and decision support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alleviating LLM-based Generative Retrieval Hallucination in Alipay
  Search <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21098v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21098v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yedan Shen, Kaixin Wu, Yuechen Ding, Jingyuan Wen, Hong Liu, Mingjie Zhong, Zhouhan Lin, Jia Xu, Linjian Mo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval (GR) has revolutionized document retrieval with the
advent of large language models (LLMs), and LLM-based GR is gradually being
adopted by the industry. Despite its remarkable advantages and potential,
LLM-based GR suffers from hallucination and generates documents that are
irrelevant to the query in some instances, severely challenging its credibility
in practical applications. We thereby propose an optimized GR framework
designed to alleviate retrieval hallucination, which integrates knowledge
distillation reasoning in model training and incorporate decision agent to
further improve retrieval precision. Specifically, we employ LLMs to assess and
reason GR retrieved query-document (q-d) pairs, and then distill the reasoning
data as transferred knowledge to the GR model. Moreover, we utilize a decision
agent as post-processing to extend the GR retrieved documents through retrieval
model and select the most relevant ones from multi perspectives as the final
generative retrieval result. Extensive offline experiments on real-world
datasets and online A/B tests on Fund Search and Insurance Search in Alipay
demonstrate our framework's superiority and effectiveness in improving search
quality and conversion gains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Popularity Bias Amplification in Recommender Systems
  Employed in the Entertainment Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04752v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04752v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Kowald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become an integral part of our daily online
experience by analyzing past user behavior to suggest relevant content in
entertainment domains such as music, movies, and books. Today, they are among
the most widely used applications of AI and machine learning. Consequently,
regulations and guidelines for trustworthy AI, such as the European AI Act,
which addresses issues like bias and fairness, are highly relevant to the
design, development, and evaluation of recommender systems. One particularly
important type of bias in this context is popularity bias, which results in the
unfair underrepresentation of less popular content in recommendation lists.
This work summarizes our research on investigating the amplification of
popularity bias in recommender systems within the entertainment sector.
Analyzing datasets from three entertainment domains, music, movies, and anime,
we demonstrate that an item's recommendation frequency is positively correlated
with its popularity. As a result, user groups with little interest in popular
content receive less accurate recommendations compared to those who prefer
widely popular items. Furthermore, this work contributes to a better
understanding of the connection between recommendation accuracy, calibration
quality of algorithms, and popularity bias amplification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EWAF'25, summarizes fairness and popularity bias research
  presented in Dr. Kowald's habilitation:
  https://domkowald.github.io/documents/others/2024habilitation_recsys.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clinically inspired enhance Explainability and Interpretability of an
  AI-Tool for BCC diagnosis based on expert annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iván Matas, Carmen Serrano, Francisca Silva, Amalia Serrano, Tomás Toledo-Pastrana, Begoña Acha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An AI tool has been developed to provide interpretable support for the
diagnosis of BCC via teledermatology, thus speeding up referrals and optimizing
resource utilization. The interpretability is provided in two ways: on the one
hand, the main BCC dermoscopic patterns are found in the image to justify the
BCC/Non BCC classification. Secondly, based on the common visual XAI Grad-CAM,
a clinically inspired visual explanation is developed where the relevant
features for diagnosis are located. Since there is no established ground truth
for BCC dermoscopic features, a standard reference is inferred from the
diagnosis of four dermatologists using an Expectation Maximization (EM) based
algorithm. The results demonstrate significant improvements in classification
accuracy and interpretability, positioning this approach as a valuable tool for
early BCC detection and referral to dermatologists. The BCC/non-BCC
classification achieved an accuracy rate of 90%. For Clinically-inspired XAI
results, the detection of BCC patterns useful to clinicians reaches 99%
accuracy. As for the Clinically-inspired Visual XAI results, the mean of the
Grad-CAM normalized value within the manually segmented clinical features is
0.57, while outside this region it is 0.16. This indicates that the model
struggles to accurately identify the regions of the BCC patterns. These results
prove the ability of the AI tool to provide a useful explanation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 4 tables, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From raw affiliations to organization identifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myrto Kallipoliti, Serafeim Chatzopoulos, Miriam Baglioni, Eleni Adamidi, Paris Koloveas, Thanasis Vergoulis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate affiliation matching, which links affiliation strings to
standardized organization identifiers, is critical for improving research
metadata quality, facilitating comprehensive bibliometric analyses, and
supporting data interoperability across scholarly knowledge bases. Existing
approaches fail to handle the complexity of affiliation strings that often
include mentions of multiple organizations or extraneous information. In this
paper, we present AffRo, a novel approach designed to address these challenges,
leveraging advanced parsing and disambiguation techniques. We also introduce
AffRoDB, an expert-curated dataset to systematically evaluate affiliation
matching algorithms, ensuring robust benchmarking. Results demonstrate the
effectiveness of AffRp in accurately identifying organizations from complex
affiliation strings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-level Distributionally Robust Optimization for Large Language
  Model-based Dense Retrieval <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyuan Ma, Yongliang Ma, Xing Wu, Zhenpeng Su, Ming Zhou, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous
heterogeneous fine-tuning collections from different domains. However, the
discussion about its training data distribution is still minimal. Previous
studies rely on empirically assigned dataset choices or sampling ratios, which
inevitably lead to sub-optimal retrieval performances. In this paper, we
propose a new task-level Distributionally Robust Optimization (tDRO) algorithm
for LLM-DR fine-tuning, targeted at improving the universal domain
generalization ability by end-to-end reweighting the data distribution of each
task. The tDRO parameterizes the domain weights and updates them with scaled
domain gradients. The optimized weights are then transferred to the LLM-DR
fine-tuning to train more robust retrievers. Experiments show optimal
improvements in large-scale retrieval benchmarks and reduce up to 30% dataset
usage after applying our optimization algorithm with a series of
different-sized LLM-DR models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI25. Source code is available at
  https://github.com/ma787639046/tdro</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlippedRAG: Black-Box Opinion Manipulation Adversarial Attacks to
  Retrieval-Augmented Generation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02968v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02968v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Chen, Jiawei Liu, Yuyang Gong, Miaokun Chen, Haotan Liu, Qikai Cheng, Fan Zhang, Wei Lu, Xiaozhong Liu, Xiaofeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) enriches LLMs by dynamically retrieving
external knowledge, reducing hallucinations and satisfying real-time
information needs. While existing research mainly targets RAG's performance and
efficiency, emerging studies highlight critical security concerns. Yet, current
adversarial approaches remain limited, mostly addressing white-box scenarios or
heuristic black-box attacks without fully investigating vulnerabilities in the
retrieval phase. Additionally, prior works mainly focus on factoid QA tasks,
their attacks lack complexity and can be easily corrected by advanced LLMs. In
this paper, we investigate a more realistic and critical threat scenario:
adversarial attacks intended for opinion manipulation against black-box RAG
models, particularly on controversial topics. Specifically, we propose
FlippedRAG, a transfer-based adversarial attack against black-box RAG systems.
We first demonstrate that the underlying retriever of a black-box RAG system
can be reverse-engineered, enabling us to train a surrogate retriever.
Leveraging the surrogate retriever, we further craft target poisoning triggers,
altering vary few documents to effectively manipulate both retrieval and
subsequent generation. Extensive empirical results show that FlippedRAG
substantially outperforms baseline methods, improving the average attack
success rate by 16.7%. FlippedRAG achieves on average a 50% directional shift
in the opinion polarity of RAG-generated responses, ultimately causing a
notable 20% shift in user cognition. Furthermore, we evaluate the performance
of several potential defensive measures, concluding that existing mitigation
strategies remain insufficient against such sophisticated manipulation attacks.
These results highlight an urgent need for developing innovative defensive
solutions to ensure the security and trustworthiness of RAG systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2407.13757</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing User Interest based on Stream Clustering and Memory Networks
  in Large-Scale Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13238v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13238v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Liu, Nian Wang, Cong Xu, Ming Zhao, Bin Wang, Yi Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RSs) provide personalized recommendation service based
on user interest, which are widely used in various platforms. However, there
are lots of users with sparse interest due to lacking consumption behaviors,
which leads to poor recommendation results for them. This problem is widespread
in large-scale RSs and is particularly difficult to address. To solve this
challenging problem, we propose an innovative solution called User Interest
Enhancement (UIE). UIE enhances user interest including user profile and user
history behavior sequences by leveraging the enhancement vectors and
personalized enhancement vectors generated based on dynamic streaming
clustering of similar users and items from multiple perspectives, which are
stored and updated in memory networks. UIE not only remarkably improves model
performance for users with sparse interest, but also delivers notable gains for
other users. As an end-to-end solution, UIE is easy to implement on top of
existing ranking models. Furthermore, we extend our approach to long-tail items
using similar methods, which also yields excellent improvements. We conduct
extensive offline and online experiments in an industrial RS. The results
demonstrate that UIE substantially outperforms other existing approaches,
especially for users with sparse interest. UIE has been deployed in several
large-scale RSs at Tencent since 2022, which was made public on 21 May 2024. In
addition, UIE-based methods have also been successfully applied in candidate
generation, pre-ranking, and context-DNN stages. Multiple teams have developed
solutions based on UIE, focusing on updating clustering algorithms and
attention mechanisms. As far as we know, UIE has been applied in multiple RSs,
advertising systems and search engines. The thoughts of UIE, dynamic streaming
clustering and similarity enhancement, have inspired subsequent relevant works.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Accessible and Safe Live Streaming Using Distributed Content
  Filtering with MoQ <span class="chip">ICME 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew C. Freeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Live video streaming is increasingly popular on social media platforms. With
the growth of live streaming comes an increased need for robust content
moderation to remove dangerous, illegal, or otherwise objectionable content.
Whereas video on demand distribution enables offline content analysis, live
streaming imposes restrictions on latency for both analysis and distribution.
In this paper, we present extensions to the in-progress Media Over QUIC
Transport protocol that enable real-time content moderation in one-to-many
video live streams. Importantly, our solution removes only the video segments
that contain objectionable content, allowing playback resumption as soon as the
stream conforms to content policies again. Content analysis tasks may be
transparently distributed to arbitrary client devices. We implement and
evaluate our system in the context of light strobe removal for photosensitive
viewers, finding that streaming clients experience an increased latency of only
one group-of-pictures duration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the ICME 2025 LIVES workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models for Computer-Aided Design: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Licheng Zhang, Bach Le, Naveed Akhtar, Siew-Kei Lam, Tuan Ngo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have seen rapid advancements in recent years,
with models like ChatGPT and DeepSeek, showcasing their remarkable capabilities
across diverse domains. While substantial research has been conducted on LLMs
in various fields, a comprehensive review focusing on their integration with
Computer-Aided Design (CAD) remains notably absent. CAD is the industry
standard for 3D modeling and plays a vital role in the design and development
of products across different industries. As the complexity of modern designs
increases, the potential for LLMs to enhance and streamline CAD workflows
presents an exciting frontier. This article presents the first systematic
survey exploring the intersection of LLMs and CAD. We begin by outlining the
industrial significance of CAD, highlighting the need for AI-driven innovation.
Next, we provide a detailed overview of the foundation of LLMs. We also examine
both closed-source LLMs as well as publicly available models. The core of this
review focuses on the various applications of LLMs in CAD, providing a taxonomy
of six key areas where these models are making considerable impact. Finally, we
propose several promising future directions for further advancements, which
offer vast opportunities for innovation and are poised to shape the future of
CAD technology. Github:
https://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FMNV: A <span class="highlight-title">Dataset</span> of Media-Published News Videos for Fake News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.07687v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.07687v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Wang, Zhong Qian, Peifeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  News media, particularly video-based platforms, have become deeply embed-ded
in daily life, concurrently amplifying the risks of misinformation
dissem-ination. Consequently, multimodal fake news detection has garnered
signifi-cant research attention. However, existing datasets predominantly
comprise user-generated videos characterized by crude editing and limited
public en-gagement, whereas professionally crafted fake news videos
disseminated by media outlets-often politically or virally motivated-pose
substantially greater societal harm. To address this gap, we construct FMNV, a
novel da-taset exclusively composed of news videos published by media
organizations. Through empirical analysis of existing datasets and our curated
collection, we categorize fake news videos into four distinct types. Building
upon this taxonomy, we employ Large Language Models (LLMs) to automatically
generate deceptive content by manipulating authentic media-published news
videos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream
architecture that integrates spatio-temporal motion features from a 3D
ResNeXt-101 backbone and static visual semantics from CLIP. The two streams are
fused via an attention-based mechanism, while co-attention modules refine the
visual, textual, and audio features for effective multi-modal aggregation.
Comparative experiments demonstrate both the generali-zation capability of FMNV
across multiple baselines and the superior detec-tion efficacy of FMNVD. This
work establishes critical benchmarks for de-tecting high-impact fake news in
media ecosystems while advancing meth-odologies for cross-modal inconsistency
analysis. Our dataset is available in https://github.com/DennisIW/FMNV.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-12T00:00:00Z">2025-05-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reproducibility, Replicability, and Insights into Visual Document
  Retrieval with Late Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingfen Qiao, Jia-Huei Ju, Xinyu Ma, Evangelos Kanoulas, Andrew Yates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Document Retrieval (VDR) is an emerging research area that focuses on
encoding and retrieving document images directly, bypassing the dependence on
Optical Character Recognition (OCR) for document search. A recent advance in
VDR was introduced by ColPali, which significantly improved retrieval
effectiveness through a late interaction mechanism. ColPali's approach
demonstrated substantial performance gains over existing baselines that do not
use late interaction on an established benchmark. In this study, we investigate
the reproducibility and replicability of VDR methods with and without late
interaction mechanisms by systematically evaluating their performance across
multiple pre-trained vision-language models. Our findings confirm that late
interaction yields considerable improvements in retrieval effectiveness;
however, it also introduces computational inefficiencies during inference.
Additionally, we examine the adaptability of VDR models to textual inputs and
assess their robustness across text-intensive datasets within the proposed
benchmark, particularly when scaling the indexing mechanism. Furthermore, our
research investigates the specific contributions of late interaction by looking
into query-patch matching in the context of visual document retrieval. We find
that although query tokens cannot explicitly match image patches as in the text
retrieval scenario, they tend to match the patch contains visually similar
tokens or their surrounding patches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Retrieval-Augmented Generation for Chemistry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianrui Zhong, Bowen Jin, Siru Ouyang, Yanzhen Shen, Qiao Jin, Yin Fang, Zhiyong Lu, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has emerged as a powerful framework for
enhancing large language models (LLMs) with external knowledge, particularly in
scientific domains that demand specialized and dynamic information. Despite its
promise, the application of RAG in the chemistry domain remains underexplored,
primarily due to the lack of high-quality, domain-specific corpora and
well-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a
comprehensive benchmark designed to systematically assess the effectiveness of
RAG across a diverse set of chemistry-related tasks. The accompanying chemistry
corpus integrates heterogeneous knowledge sources, including scientific
literature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia
entries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG
toolkit that supports five retrieval algorithms and eight LLMs. Using
ChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain
-- achieving an average relative improvement of 17.4% over direct inference
methods. We further conduct in-depth analyses on retriever architectures,
corpus selection, and the number of retrieved passages, culminating in
practical recommendations to guide future research and deployment of RAG
systems in the chemistry domain. The code and data is available at
https://chemrag.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Reproducible Biomedical Question Answering using Retrieval
  Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linus Stuhlmann, Michael Alexander Saxer, Jonathan Fürst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical question-answering (QA) systems require effective retrieval and
generation components to ensure accuracy, efficiency, and scalability. This
study systematically examines a Retrieval-Augmented Generation (RAG) system for
biomedical QA, evaluating retrieval strategies and response time trade-offs. We
first assess state-of-the-art retrieval methods, including BM25, BioBERT,
MedCPT, and a hybrid approach, alongside common data stores such as
Elasticsearch, MongoDB, and FAISS, on a ~10% subset of PubMed (2.4M documents)
to measure indexing efficiency, retrieval latency, and retriever performance in
the end-to-end RAG system. Based on these insights, we deploy the final RAG
system on the full 24M PubMed corpus, comparing different retrievers' impact on
overall performance. Evaluations of the retrieval depth show that retrieving 50
documents with BM25 before reranking with MedCPT optimally balances accuracy
(0.90), recall (0.90), and response time (1.91s). BM25 retrieval time remains
stable (82ms), while MedCPT incurs the main computational cost. These results
highlight previously not well-known trade-offs in retrieval depth, efficiency,
and scalability for biomedical QA. With open-source code, the system is fully
reproducible and extensible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SDS25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KAQG: A Knowledge-Graph-Enhanced RAG for Difficulty-Controlled Question
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching Han Chen, Ming Fang Shiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  KAQG introduces a decisive breakthrough for Retrieval-Augmented Generation
(RAG) by explicitly tackling the two chronic weaknesses of current pipelines:
transparent multi-step reasoning and fine-grained cognitive difficulty control.
This transforms RAG from a passive retriever into an accountable generator of
calibrated exam items. Technically, the framework fuses knowledge graphs, RAG
retrieval, and educational assessment theory into a single pipeline. Domain
passages are parsed into a structured graph; graph-aware retrieval feeds fact
chains to an LLM; and an assessment layer governed by Bloom's Taxonomy levels
and Item Response Theory (IRT) transforms those chains into psychometrically
sound questions. This cross-disciplinary marriage yields two scholarly
contributions: it shows how semantic graph contexts guide LLM reasoning paths,
and it operationalizes difficulty metrics within the generation process,
producing items whose IRT parameters match expert benchmarks. Every module,
from KG construction scripts to the multi-agent reasoning scheduler and the
automatic IRT validator, is openly released on GitHub. This enables peer
laboratories to replicate experiments, benchmark against baselines, and extend
individual components without licensing barriers. Its reproducible design paves
the way for rigorous ablation studies, cross-domain transfer experiments, and
shared leaderboards on multi-step reasoning benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Injecting Knowledge Graphs into Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erica Coppolillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) remains a key challenge for symbolic reasoning. Existing
methods mainly rely on prompt engineering or fine-tuning, which lose structural
fidelity or incur high computational costs. Building on recent encoding
techniques which integrate graph embeddings within the LLM input as tokens, we
extend this paradigm to the KG domain by leveraging Knowledge Graph Embedding
(KGE) models, thus enabling graph-aware reasoning. Our approach is
model-agnostic, resource-efficient, and compatible with any LLMs. Extensive
experimentation on synthetic and real-world datasets shows that our method
improves reasoning performance over established baselines, further achieving
the best trade-off in terms of accuracy and efficiency against state-of-the-art
LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRADA: Graph-based Reranker against Adversarial Documents Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjie Zheng, Aryo Pradipta Gema, Giwon Hong, Xuanli He, Pasquale Minervini, Youcheng Sun, Qiongkai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large
language models (LLMs) by integrating external knowledge from retrieved
documents, thereby overcoming the limitations of models' static intrinsic
knowledge. However, these systems are susceptible to adversarial attacks that
manipulate the retrieval process by introducing documents that are adversarial
yet semantically similar to the query. Notably, while these adversarial
documents resemble the query, they exhibit weak similarity to benign documents
in the retrieval set. Thus, we propose a simple yet effective Graph-based
Reranking against Adversarial Document Attacks (GRADA) framework aiming at
preserving retrieval quality while significantly reducing the success of
adversaries. Our study evaluates the effectiveness of our approach through
experiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b,
Llama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with
results from the Natural Questions dataset demonstrating up to an 80% reduction
in attack success rates while maintaining minimal loss in accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why Uncertainty Estimation Methods Fall Short in RAG: An Axiomatic
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are valued for their strong performance across
various tasks, but they also produce inaccurate or misleading outputs.
Uncertainty Estimation (UE) quantifies the model's confidence and helps users
assess response reliability. However, existing UE methods have not been
thoroughly examined in scenarios like Retrieval-Augmented Generation (RAG),
where the input prompt includes non-parametric knowledge. This paper shows that
current UE methods cannot reliably assess correctness in the RAG setting. We
further propose an axiomatic framework to identify deficiencies in existing
methods and guide the development of improved approaches. Our framework
introduces five constraints that an effective UE method should meet after
incorporating retrieved documents into the LLM's prompt. Experimental results
reveal that no existing UE method fully satisfies all the axioms, explaining
their suboptimal performance in RAG. We further introduce a simple yet
effective calibration function based on our framework, which not only satisfies
more axioms than baseline methods but also improves the correlation between
uncertainty estimates and correctness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-driven SpatioTemporal Graph KANsformer for Medical Examination
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianan Li, Yangtao Zhou, Zhifu Zhao, Qinglan Huang, Jian Qi, Xiao He, Hua Chu, Fu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems in AI-based medical diagnostics and treatment
constitute a critical component of AI in healthcare. Although some studies have
explored this area and made notable progress, healthcare recommendation systems
remain in their nascent stage. And these researches mainly target the treatment
process such as drug or disease recommendations. In addition to the treatment
process, the diagnostic process, particularly determining which medical
examinations are necessary to evaluate the condition, also urgently requires
intelligent decision support. To bridge this gap, we first formalize the task
of medical examination recommendations. Compared to traditional
recommendations, the medical examination recommendation involves more complex
interactions. This complexity arises from two folds: 1) The historical medical
records for examination recommendations are heterogeneous and redundant, which
makes the recommendation results susceptible to noise. 2) The correlation
between the medical history of patients is often irregular, making it
challenging to model spatiotemporal dependencies. Motivated by the above
observation, we propose a novel Diffusion-driven SpatioTemporal Graph
KANsformer for Medical Examination Recommendation (DST-GKAN) with a two-stage
learning paradigm to solve the above challenges. In the first stage, we exploit
a task-adaptive diffusion model to distill recommendation-oriented information
by reducing the noises in heterogeneous medical data. In the second stage, a
spatiotemporal graph KANsformer is proposed to simultaneously model the complex
spatial and temporal relationships. Moreover, to facilitate the medical
examination recommendation research, we introduce a comprehensive dataset. The
experimental results demonstrate the state-of-the-art performance of the
proposed method compared to various competitive baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QUPID: Quantified Understanding for Enhanced Performance, Insights, and
  Decisions in Korean Search Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ohjoon Kwon, Changsu Lee, Jihye Back, Lim Sun Suk, Inho Kang, Donghyeon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been widely used for relevance assessment
in information retrieval. However, our study demonstrates that combining two
distinct small language models (SLMs) with different architectures can
outperform LLMs in this task. Our approach -- QUPID -- integrates a generative
SLM with an embedding-based SLM, achieving higher relevance judgment accuracy
while reducing computational costs compared to state-of-the-art LLM solutions.
This computational efficiency makes QUPID highly scalable for real-world search
systems processing millions of queries daily. In experiments across diverse
document types, our method demonstrated consistent performance improvements
(Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x
faster inference times. Furthermore, when integrated into production search
pipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how
architectural diversity in model combinations can significantly enhance both
search relevance and operational efficiency in information retrieval systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DARLR: Dual-Agent Offline Reinforcement Learning for Recommender Systems
  with Dynamic Reward <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhang, Ruihong Qiu, Xuwei Xu, Jiajun Liu, Sen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-based offline reinforcement learning (RL) has emerged as a promising
approach for recommender systems, enabling effective policy learning by
interacting with frozen world models. However, the reward functions in these
world models, trained on sparse offline logs, often suffer from inaccuracies.
Specifically, existing methods face two major limitations in addressing this
challenge: (1) deterministic use of reward functions as static look-up tables,
which propagates inaccuracies during policy learning, and (2) static
uncertainty designs that fail to effectively capture decision risks and
mitigate the impact of these inaccuracies. In this work, a dual-agent
framework, DARLR, is proposed to dynamically update world models to enhance
recommendation policies. To achieve this, a \textbf{\textit{selector}} is
introduced to identify reference users by balancing similarity and diversity so
that the \textbf{\textit{recommender}} can aggregate information from these
users and iteratively refine reward estimations for dynamic reward shaping.
Further, the statistical features of the selected users guide the dynamic
adaptation of an uncertainty penalty to better align with evolving
recommendation requirements. Extensive experiments on four benchmark datasets
demonstrate the superior performance of DARLR, validating its effectiveness.
The code is available at https://github.com/ArronDZhang/DARLR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Generative Re-ranking Model for List-level Multi-objective
  Optimization at Taobao 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Meng, Cheng Guo, Yi Cao, Tong Liu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  E-commerce recommendation systems aim to generate ordered lists of items for
customers, optimizing multiple business objectives, such as clicks, conversions
and Gross Merchandise Volume (GMV). Traditional multi-objective optimization
methods like formulas or Learning-to-rank (LTR) models take effect at
item-level, neglecting dynamic user intent and contextual item interactions.
List-level multi-objective optimization in the re-ranking stage can overcome
this limitation, but most current re-ranking models focus more on accuracy
improvement with context. In addition, re-ranking is faced with the challenges
of time complexity and diversity. In light of this, we propose a novel
end-to-end generative re-ranking model named Sequential Ordered Regression
Transformer-Generator (SORT-Gen) for the less-studied list-level
multi-objective optimization problem. Specifically, SORT-Gen is divided into
two parts: 1)Sequential Ordered Regression Transformer innovatively uses
Transformer and ordered regression to accurately estimate multi-objective
values for variable-length sub-lists. 2)Mask-Driven Fast Generation Algorithm
combines multi-objective candidate queues, efficient item selection and
diversity mechanism into model inference, providing a fast online list
generation method. Comprehensive online experiments demonstrate that SORT-Gen
brings +4.13% CLCK and +8.10% GMV for Baiyibutie, a notable Mini-app of Taobao.
Currently, SORT-Gen has been successfully deployed in multiple scenarios of
Taobao App, serving for a vast number of users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for
  Few-Shot Knowledge Graph Completion <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongho Kim, Chanyeong Heo, Jaehee Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KGs), composed of triples in the form of (head, relation,
tail) and consisting of entities and relations, play a key role in information
retrieval systems such as question answering, entity search, and
recommendation. In real-world KGs, although many entities exist, the relations
exhibit a long-tail distribution, which can hinder information retrieval
performance. Previous few-shot knowledge graph completion studies focused
exclusively on the positive triple information that exists in the graph or,
when negative triples were incorporated, used them merely as a signal to
indicate incorrect triples. To overcome this limitation, we propose
Relation-Based Conditional Diffusion with Attention Pooling (ReCDAP). First,
negative triples are generated by randomly replacing the tail entity in the
support set. By conditionally incorporating positive information in the KG and
non-existent negative information into the diffusion process, the model
separately estimates the latent distributions for positive and negative
relations. Moreover, including an attention pooler enables the model to
leverage the differences between positive and negative cases explicitly.
Experiments on two widely used datasets demonstrate that our method outperforms
existing approaches, achieving state-of-the-art performance. The code is
available at https://github.com/hou27/ReCDAP-FKGC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025, 5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>ing vs. Fine-tuning: A Reproducibility Study on Dense Retrieval
  Knowledge Acquisition <span class="chip">SIGIR-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Yao, Shuai Wang, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers utilize pre-trained backbone language models (e.g., BERT,
LLaMA) that are fine-tuned via contrastive learning to perform the task of
encoding text into sense representations that can be then compared via a
shallow similarity operation, e.g. inner product. Recent research has
questioned the role of fine-tuning vs. that of pre-training within dense
retrievers, specifically arguing that retrieval knowledge is primarily gained
during pre-training, meaning knowledge not acquired during pre-training cannot
be sub-sequentially acquired via fine-tuning. We revisit this idea here as the
claim was only studied in the context of a BERT-based encoder using DPR as
representative dense retriever. We extend the previous analysis by testing
other representation approaches (comparing the use of CLS tokens with that of
mean pooling), backbone architectures (encoder-only BERT vs. decoder-only
LLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Our
study confirms that in DPR tuning, pre-trained knowledge underpins retrieval
performance, with fine-tuning primarily adjusting neuron activation rather than
reorganizing knowledge. However, this pattern does not hold universally, such
as in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure full
reproducibility and make our implementation publicly available at
https://github.com/ielab/DenseRetriever-Knowledge-Acquisition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in SIGIR-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reassessing Large Language Model Boolean Query Generation for Systematic
  <span class="highlight-title">Review</span>s <span class="chip">SIGIR-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systematic reviews are comprehensive literature reviews that address highly
focused research questions and represent the highest form of evidence in
medicine. A critical step in this process is the development of complex Boolean
queries to retrieve relevant literature. Given the difficulty of manually
constructing these queries, recent efforts have explored Large Language Models
(LLMs) to assist in their formulation. One of the first studies,Wang et al.,
investigated ChatGPT for this task, followed by Staudinger et al., which
evaluated multiple LLMs in a reproducibility study. However, the latter
overlooked several key aspects of the original work, including (i) validation
of generated queries, (ii) output formatting constraints, and (iii) selection
of examples for chain-of-thought (Guided) prompting. As a result, its findings
diverged significantly from the original study. In this work, we systematically
reproduce both studies while addressing these overlooked factors. Our results
show that query effectiveness varies significantly across models and prompt
designs, with guided query formulation benefiting from well-chosen seed
studies. Overall, prompt design and model selection are key drivers of
successful query formulation. Our findings provide a clearer understanding of
LLMs' potential in Boolean query generation and highlight the importance of
model- and prompt-specific optimisations. The complex nature of systematic
reviews adds to challenges in both developing and reproducing methods but also
highlights the importance of reproducibility studies in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in SIGIR-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Integrated Layered Attention (AILA) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.22742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.22742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Claster, Suhas KM, Dhairya Gundechia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Adaptive Integrated Layered Attention (AILA), a neural network
architecture that combines dense skip connections with different mechanisms for
adaptive feature reuse across network layers. We evaluate AILA on three
challenging tasks: price forecasting for various commodities and indices (S&P
500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the
CIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In
all cases, AILA matches strong deep learning baselines (LSTMs, Transformers,
and ResNets), achieving it at a fraction of the training and inference time.
Notably, we implement and test two versions of the model - AILA-Architecture 1,
which uses simple linear layers as the connection mechanism between layers, and
AILA-Architecture 2, which implements an attention mechanism to selectively
focus on outputs from previous layers. Both architectures are applied in a
single-task learning setting, with each model trained separately for individual
tasks. Results confirm that AILA's adaptive inter-layer connections yield
robust gains by flexibly reusing pertinent features at multiple network depths.
The AILA approach thus presents an extension to existing architectures,
improving long-range sequence modeling, image recognition with optimised
computational speed, and SOTA classification performance in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Generative AI Techniques in Government: A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunyi Liu, Mengzhe Geng, Rebecca Hart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The swift progress of Generative Artificial intelligence (GenAI), notably
Large Language Models (LLMs), is reshaping the digital landscape. Recognizing
this transformative potential, the National Research Council of Canada (NRC)
launched a pilot initiative to explore the integration of GenAI techniques into
its daily operation for performance excellence, where 22 projects were launched
in May 2024. Within these projects, this paper presents the development of the
intelligent agent Pubbie as a case study, targeting the automation of
performance measurement, data management and insight reporting at the NRC.
Cutting-edge techniques are explored, including LLM orchestration and semantic
embedding via RoBERTa, while strategic fine-tuning and few-shot learning
approaches are incorporated to infuse domain knowledge at an affordable cost.
The user-friendly interface of Pubbie allows general government users to input
queries in natural language and easily upload or download files with a simple
button click, greatly reducing manual efforts and accessibility barriers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In submission to IEEE Intelligent Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Brain Passage Retrieval -- An Investigation of EEG Query
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06695v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06695v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niall McGuire, Yashar Moshfeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information Retrieval (IR) systems primarily rely on users' ability to
translate their internal information needs into (text) queries. However, this
translation process is often uncertain and cognitively demanding, leading to
queries that incompletely or inaccurately represent users' true needs. This
challenge is particularly acute for users with ill-defined information needs or
physical impairments that limit traditional text input, where the gap between
cognitive intent and query expression becomes even more pronounced. Recent
neuroscientific studies have explored Brain-Machine Interfaces (BMIs) as a
potential solution, aiming to bridge the gap between users' cognitive semantics
and their search intentions. However, current approaches attempting to decode
explicit text queries from brain signals have shown limited effectiveness in
learning robust brain-to-text representations, often failing to capture the
nuanced semantic information present in brain patterns. To address these
limitations, we propose BPR (Brain Passage Retrieval), a novel framework that
eliminates the need for intermediate query translation by enabling direct
retrieval of relevant passages from users' brain signals. Our approach
leverages dense retrieval architectures to map EEG signals and text passages
into a shared semantic space. Through comprehensive experiments on the ZuCo
dataset, we demonstrate that BPR achieves up to 8.81% improvement in
precision@5 over existing EEG-to-text baselines, while maintaining
effectiveness across 30 participants. Our ablation studies reveal the critical
role of hard negative sampling and specialised brain encoders in achieving
robust cross-modal alignment. These results establish the viability of direct
brain-to-passage retrieval and provide a foundation for developing more natural
interfaces between users' cognitive states and IR systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Push and Pull: A Framework for Measuring Attentional Agency on Digital
  Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Wojtowicz, Shrey Jain, Nicholas Vincent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a framework for measuring attentional agency, which we define as a
user's ability to allocate attention according to their own desires, goals, and
intentions on digital platforms that use statistical learning to prioritize
informational content. Such platforms extend people's limited powers of
attention by extrapolating their preferences to large collections of previously
unconsidered informational objects. However, platforms typically also allow
users to influence the attention of other users in various ways. We introduce a
formal framework for measuring how much a given platform empowers each user to
both pull information into their own attention and push information into the
attention of others. We also use these definitions to clarify the implications
of generative foundation models and other recent advances in AI for the
structure and efficiency of digital platforms. We conclude with a set of
possible strategies for better understanding and reshaping attentional agency
online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Process-Supervised LLM Recommenders via Flow-guided Tuning <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07377v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07377v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongming Gao, Mengyao Gao, Chenxiao Fan, Shuai Yuan, Wentao Shi, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) are increasingly adapted for
recommendation systems via supervised fine-tuning (SFT), this approach
amplifies popularity bias due to its likelihood maximization objective,
compromising recommendation diversity and fairness. To address this, we present
Flow-guided fine-tuning recommender (Flower), which replaces SFT with a
Generative Flow Network (GFlowNet) framework that enacts process supervision
through token-level reward propagation. Flower's key innovation lies in
decomposing item-level rewards into constituent token rewards, enabling direct
alignment between token generation probabilities and their reward signals. This
mechanism achieves three critical advancements: (1) popularity bias mitigation
and fairness enhancement through empirical distribution matching, (2)
preservation of diversity through GFlowNet's proportional sampling, and (3)
flexible integration of personalized preferences via adaptable token rewards.
Experiments demonstrate Flower's superior distribution-fitting capability and
its significant advantages over traditional SFT in terms of accuracy, fairness,
and diversity, highlighting its potential to improve LLM-based recommendation
systems. The implementation is available via
https://github.com/MrPeach0301/Flower
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From <span class="highlight-title">Prompt</span>ing to Alignment: A Generative Framework for Query
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10208v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10208v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erxue Min, Hsiu-Yuan Huang, Min Yang, Xihong Yang, Xin Jia, Yunfang Wu, Hengyi Cai, Junfeng Wang, Shuaiqiang Wang, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern search systems, search engines often suggest relevant queries to
users through various panels or components, helping refine their information
needs. Traditionally, these recommendations heavily rely on historical search
logs to build models, which suffer from cold-start or long-tail issues.
Furthermore, tasks such as query suggestion, completion or clarification are
studied separately by specific design, which lacks generalizability and hinders
adaptation to novel applications. Despite recent attempts to explore the use of
LLMs for query recommendation, these methods mainly rely on the inherent
knowledge of LLMs or external sources like few-shot examples, retrieved
documents, or knowledge bases, neglecting the importance of the calibration and
alignment with user feedback, thus limiting their practical utility. To address
these challenges, we first propose a general Generative Query Recommendation
(GQR) framework that aligns LLM-based query generation with user preference.
Specifically, we unify diverse query recommendation tasks by a universal prompt
framework, leveraging the instruct-following capability of LLMs for effective
generation. Secondly, we align LLMs with user feedback via presenting a
CTR-alignment framework, which involves training a query-wise CTR predictor as
a process reward model and employing list-wise preference alignment to maximize
the click probability of the generated query list. Furthermore, recognizing the
inconsistency between LLM knowledge and proactive search intents arising from
the separation of user-initiated queries from models, we align LLMs with user
initiative via retrieving co-occurrence queries as side information when
historical logs are available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ROLeR: Effective Reward Shaping in Offline Reinforcement Learning for
  Recommender Systems <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhang, Ruihong Qiu, Jiajun Liu, Sen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) is an effective tool for real-world
recommender systems with its capacity to model the dynamic interest of users
and its interactive nature. Most existing offline RL recommender systems focus
on model-based RL through learning a world model from offline data and building
the recommendation policy by interacting with this model. Although these
methods have made progress in the recommendation performance, the effectiveness
of model-based offline RL methods is often constrained by the accuracy of the
estimation of the reward model and the model uncertainties, primarily due to
the extreme discrepancy between offline logged data and real-world data in user
interactions with online platforms. To fill this gap, a more accurate reward
model and uncertainty estimation are needed for the model-based RL methods. In
this paper, a novel model-based Reward Shaping in Offline Reinforcement
Learning for Recommender Systems, ROLeR, is proposed for reward and uncertainty
estimation in recommendation systems. Specifically, a non-parametric reward
shaping method is designed to refine the reward model. In addition, a flexible
and more representative uncertainty penalty is designed to fit the needs of
recommendation systems. Extensive experiments conducted on four benchmark
datasets showcase that ROLeR achieves state-of-the-art performance compared
with existing baselines. The source code can be downloaded at
https://github.com/ArronDZhang/ROLeR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for
  Scientific Videos and Podcasts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Wittenborg, Constantin Sebastian Tremel, Niklas Stehr, Oliver Karras, Markus Stocker, Sören Auer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Democratic societies need accessible, reliable information. Videos and
Podcasts have established themselves as the medium of choice for civic
dissemination, but also as carriers of misinformation. The emerging Science
Communication Knowledge Infrastructure (SciCom KI) curating non-textual media
is still fragmented and not adequately equipped to scale against the content
flood. Our work sets out to support the SciCom KI with a central, collaborative
platform, the SciCom Wiki, to facilitate FAIR (findable, accessible,
interoperable, reusable) media representation and the fact-checking of their
content, particularly for videos and podcasts. Building an open-source service
system centered around Wikibase, we survey requirements from 53 stakeholders,
refine these in 11 interviews, and evaluate our prototype based on these
requirements with another 14 participants. To address the most requested
feature, fact-checking, we developed a neurosymbolic computational
fact-checking approach, converting heterogenous media into knowledge graphs.
This increases machine-readability and allows comparing statements against
equally represented ground-truth. Our computational fact-checking tool was
iteratively evaluated through 10 expert interviews, a public user survey with
43 participants verified the necessity and usability of our tool. Overall, our
findings identified several needs to systematically support the SciCom KI. The
SciCom Wiki, as a FAIR digital library complementing our neurosymbolic
computational fact-checking framework, was found suitable to address the raised
requirements. Further, we identified that the SciCom KI is severely
underdeveloped regarding FAIR knowledge and related systems facilitating its
collaborative creation and curation. Our system can provide a central knowledge
node, yet a collaborative effort is required to scale against the imminent
(mis-)information flood.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures, submitted to TPDL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning
  in The DCASE 2025 Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Han Huck Yang, Sreyan Ghosh, Qing Wang, Jaeyeon Kim, Hengyi Hong, Sonal Kumar, Guirui Zhong, Zhifeng Kong, S Sakshi, Vaibhavi Lokegaonkar, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha, Gunhee Kim, Jun Du, Rafael Valle, Bryan Catanzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Task 5 of the DCASE 2025 Challenge: an Audio Question Answering
(AQA) benchmark spanning multiple domains of sound understanding. This task
defines three QA subsets (Bioacoustics, Temporal Soundscapes, and Complex QA)
to test audio-language models on interactive question-answering over diverse
acoustic scenes. We describe the dataset composition (from marine mammal calls
to soundscapes and complex real-world clips), the evaluation protocol (top-1
accuracy with answer-shuffling robustness), and baseline systems
(Qwen2-Audio-7B, AudioFlamingo 2, Gemini-2-Flash). Preliminary results on the
development set are compared, showing strong variation across models and
subsets. This challenge aims to advance the audio understanding and reasoning
capabilities of audio-language models toward human-level acuity, which are
crucial for enabling AI agents to perceive and interact about the world
effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. DCASE 2025 Audio QA Challenge:
  https://dcase.community/challenge2025/task-audio-question-answering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmoVLM-KD: Fusing Distilled Expertise with Vision-Language Models for
  Visual Emotion Analysis <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SangEun Lee, Yubeen Lee, Eunil Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual emotion analysis, which has gained considerable attention in the field
of affective computing, aims to predict the dominant emotions conveyed by an
image. Despite advancements in visual emotion analysis with the emergence of
vision-language models, we observed that instruction-tuned vision-language
models and conventional vision models exhibit complementary strengths in visual
emotion analysis, as vision-language models excel in certain cases, whereas
vision models perform better in others. This finding highlights the need to
integrate these capabilities to enhance the performance of visual emotion
analysis. To bridge this gap, we propose EmoVLM-KD, an instruction-tuned
vision-language model augmented with a lightweight module distilled from
conventional vision models. Instead of deploying both models simultaneously,
which incurs high computational costs, we transfer the predictive patterns of a
conventional vision model into the vision-language model using a knowledge
distillation framework. Our approach first fine-tunes a vision-language model
on emotion-specific instruction data and then attaches a distilled module to
its visual encoder while keeping the vision-language model frozen. Predictions
from the vision language model and the distillation module are effectively
balanced by a gate module, which subsequently generates the final outcome.
Extensive experiments show that EmoVLM-KD achieves state-of-the-art performance
on multiple visual emotion analysis benchmark datasets, outperforming the
existing methods while maintaining computational efficiency. The code is
available in https://github.com/sange1104/EmoVLM-KD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Workshop and Competition on Affective & Behavior Analysis
  in-the-wild (ABAW), CVPR 2025, 10 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-11T00:00:00Z">2025-05-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Distillation for Enhancing Walmart E-commerce Search Relevance
  Using Large Language Models <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongwei Shang, Nguyen Vo, Nitin Yadav, Tian Zhang, Ajit Puthenputhussery, Xunfan Cai, Shuyi Chen, Prijith Chandran, Changsung Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the products displayed in e-commerce search results are relevant to
users queries is crucial for improving the user experience. With their advanced
semantic understanding, deep learning models have been widely used for
relevance matching in search tasks. While large language models (LLMs) offer
superior ranking capabilities, it is challenging to deploy LLMs in real-time
systems due to the high-latency requirements. To leverage the ranking power of
LLMs while meeting the low-latency demands of production systems, we propose a
novel framework that distills a high performing LLM into a more efficient,
low-latency student model. To help the student model learn more effectively
from the teacher model, we first train the teacher LLM as a classification
model with soft targets. Then, we train the student model to capture the
relevance margin between pairs of products for a given query using mean squared
error loss. Instead of using the same training data as the teacher model, we
significantly expand the student model dataset by generating unlabeled data and
labeling it with the teacher model predictions. Experimental results show that
the student model performance continues to improve as the size of the augmented
training data increases. In fact, with enough augmented data, the student model
can outperform the teacher model. The student model has been successfully
deployed in production at Walmart.com with significantly positive metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, published at WWWW'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Reinforcement Learning Framework for Application-Specific TCP
  Congestion-Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinming Xing, Muhammad Shahzad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Congestion Control (CC) module plays a critical role in the Transmission
Control Protocol (TCP), ensuring the stability and efficiency of network data
transmission. The CC approaches that are commonly used these days employ
heuristics-based rules to adjust the sending rate. Due to their
heuristics-based nature, these approaches are not only unable to adapt to
changing network conditions but are also agnostic to the diverse requirements
that different applications often have. Recently, several learning-based CC
approaches have been proposed to adapt to changing network conditions.
Unfortunately, they are not designed to take application requirements into
account. Prior heuristics-based as well as learning-based CC approaches focus
on achieving a singular objective, which is often to maximize throughput, even
though a lot of applications care more about latency, packet losses, jitter,
and different combinations of various network metrics. Motivated by this, we
propose a Deep Reinforcement Learning (DRL) based CC framework, namely ASC,
which allows any application to specify any arbitrary objectives that the
network traffic of that application should achieve and is able to swiftly adapt
to the changes in the objectives of the applications as well as to the changes
in the network conditions. Our ASC framework further employs a client-server
architecture that serves two purposes: 1) it makes ASC highly scalable in terms
of the arrival and departure of TCP connections, and 2) it makes ASC very
lightweight for the nodes maintaining the TCP connections. We implemented and
extensively evaluated ASC in a variety of settings. Our results show that it
can not only achieve various objectives but also outperforms prior approaches
even in the specific objectives that those approaches were designed to achieve.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NetSight: Graph Attention Based Traffic Forecasting in Computer Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinming Xing, Guoheng Sun, Hui Sun, Linchao Pan, Shakir Mahmood, Xuanhao Luo, Muhammad Shahzad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The traffic in today's networks is increasingly influenced by the
interactions among network nodes as well as by the temporal fluctuations in the
demands of the nodes. Traditional statistical prediction methods are becoming
obsolete due to their inability to address the non-linear and dynamic
spatio-temporal dependencies present in today's network traffic. The most
promising direction of research today is graph neural networks (GNNs) based
prediction approaches that are naturally suited to handle graph-structured
data. Unfortunately, the state-of-the-art GNN approaches separate the modeling
of spatial and temporal information, resulting in the loss of important
information about joint dependencies. These GNN based approaches further do not
model information at both local and global scales simultaneously, leaving
significant room for improvement. To address these challenges, we propose
NetSight. NetSight learns joint spatio-temporal dependencies simultaneously at
both global and local scales from the time-series of measurements of any given
network metric collected at various nodes in a network. Using the learned
information, NetSight can then accurately predict the future values of the
given network metric at those nodes in the network. We propose several new
concepts and techniques in the design of NetSight, such as spatio-temporal
adjacency matrix and node normalization. Through extensive evaluations and
comparison with prior approaches using data from two large real-world networks,
we show that NetSight significantly outperforms all prior state-of-the-art
approaches. We will release the source code and data used in the evaluation of
NetSight on the acceptance of this paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Web Page Classification using LLMs for Crawling Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuichi Sasazawa, Yasuhiro Sogawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A web crawler is a system designed to collect web pages, and efficient
crawling of new pages requires appropriate algorithms. While website features
such as XML sitemaps and the frequency of past page updates provide important
clues for accessing new pages, their universal application across diverse
conditions is challenging. In this study, we propose a method to efficiently
collect new pages by classifying web pages into two types, "Index Pages" and
"Content Pages," using a large language model (LLM), and leveraging the
classification results to select index pages as starting points for accessing
new pages. We construct a dataset with automatically annotated web page types
and evaluate our approach from two perspectives: the page type classification
performance and coverage of new pages. Experimental results demonstrate that
the LLM-based method outperformed baseline methods in both evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Distracting Effect: Understanding Irrelevant Passages in RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Amiraz, Florin Cuconasu, Simone Filice, Zohar Karnin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A well-known issue with Retrieval Augmented Generation (RAG) is that
retrieved passages that are irrelevant to the query sometimes distract the
answer-generating LLM, causing it to provide an incorrect response. In this
paper, we shed light on this core issue and formulate the distracting effect of
a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the
distracting effect of a passage and demonstrate its robustness across LLMs.
  Our research introduces novel methods for identifying and using hard
distracting passages to improve RAG systems. By fine-tuning LLMs with these
carefully selected distracting passages, we achieve up to a 7.5% increase in
answering accuracy compared to counterparts fine-tuned on conventional RAG
datasets. Our contribution is two-fold: first, we move beyond the simple binary
classification of irrelevant passages as either completely unrelated vs.
distracting, and second, we develop and analyze multiple methods for finding
hard distracting passages. To our knowledge, no other research has provided
such a comprehensive framework for identifying and utilizing hard distracting
passages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incremental Analysis of Legacy Applications Using Knowledge Graphs for
  Application Modernization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saravanan Krishnan, Amith Singhee, Keerthi Narayan Raghunath, Alex Mathai, Atul Kumar, David Wenk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industries such as banking, telecom, and airlines - o6en have large so6ware
systems that are several decades old. Many of these systems are written in old
programming languages such as COBOL, PL/1, Assembler, etc. In many cases, the
documentation is not updated, and those who developed/designed these systems
are no longer around. Understanding these systems for either modernization or
even regular maintenance has been a challenge. An extensive application may
have natural boundaries based on its code dependencies and architecture. There
are also other logical boundaries in an enterprise setting driven by business
functions, data domains, etc. Due to these complications, the system architects
generally plan their modernization across these logical boundaries in parts,
thereby adopting an incremental approach for the modernization journey of the
entire system. In this work, we present a so6ware system analysis tool that
allows a subject ma=er expert (SME) or system architect to analyze a large
so6ware system incrementally. We analyze the source code and other artifacts
(such as data schema) to create a knowledge graph using a customizable
ontology/schema. Entities and relations in our ontology can be defined for any
combination of programming languages and platforms. Using this knowledge graph,
the analyst can then define logical boundaries around dependent Entities (e.g.
Programs, Transactions, Database Tables etc.). Our tool then presents different
views showcasing the dependencies from the newly defined boundary to/from the
other logical groups of the system. This exercise is repeated interactively to
1) Identify the Entities and groupings of interest for a modernization task and
2) Understand how a change in one part of the system may affect the other
parts. To validate the efficacy of our tool, we provide an initial study of our
system on two client applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Split-then-Join Approach to Abstractive Summarization for Very Long
  Documents in a Low Resource Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lhuqita Fazry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $\texttt{BIGBIRD-PEGASUS}$ model achieves $\textit{state-of-the-art}$ on
abstractive text summarization for long documents. However it's capacity still
limited to maximum of $4,096$ tokens, thus caused performance degradation on
summarization for very long documents. Common method to deal with the issue is
to truncate the documents. In this reasearch, we'll use different approach.
We'll use the pretrained $\texttt{BIGBIRD-PEGASUS}$ model by fine tuned the
model on other domain dataset. First, we filter out all documents which length
less than $20,000$ tokens to focus on very long documents. To prevent domain
shifting problem and overfitting on transfer learning due to small dataset, we
augment the dataset by splitting document-summary training pair into parts, to
fit the document into $4,096$ tokens. Source code available on
$\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Recommendations using Fine-Tuned LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prabhdeep Cheema, Erhan Guven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As digital media platforms strive to meet evolving user expectations,
delivering highly personalized and intuitive movies and media recommendations
has become essential for attracting and retaining audiences. Traditional
systems often rely on keyword-based search and recommendation techniques, which
limit users to specific keywords and a combination of keywords. This paper
proposes an approach that generates synthetic datasets by modeling real-world
user interactions, creating complex chat-style data reflective of diverse
preferences. This allows users to express more information with complex
preferences, such as mood, plot details, and thematic elements, in addition to
conventional criteria like genre, title, and actor-based searches. In today's
search space, users cannot write queries like ``Looking for a fantasy movie
featuring dire wolves, ideally set in a harsh frozen world with themes of
loyalty and survival.''
  Building on these contributions, we evaluate synthetic datasets for diversity
and effectiveness in training and benchmarking models, particularly in areas
often absent from traditional datasets. This approach enhances personalization
and accuracy by enabling expressive and natural user queries. It establishes a
foundation for the next generation of conversational AI-driven search and
recommendation systems in digital entertainment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented at IEEE CAI 2025. This version includes minor
  clarifications and formatting updates</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models
  to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via
  Cross-Modal Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xilin Jiang, Junkai Wu, Vishal Choudhari, Nima Mesgarani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio large language models (LLMs) are considered experts at recognizing
sound objects, yet their performance relative to LLMs in other sensory
modalities, such as visual or audio-visual LLMs, and to humans using their
ears, eyes, or both remains unexplored. To investigate this, we systematically
evaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio,
Qwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of
different classes from audio-only, silent video, or sounded video inputs. We
uncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the
sensory discrepancy between human ears and eyes. To reduce this gap, we
introduce a cross-modal distillation framework, where an LLM in one modality
serves as the teacher and another as the student, with knowledge transfer in
sound classes predicted as more challenging to the student by a heuristic
model. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice
versa, leads to notable improvements, particularly in challenging classes. This
work highlights the sensory gap in LLMs from a human-aligned perspective and
proposes a principled approach to enhancing modality-specific perception in
multimodal LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio <span class="highlight-title">Transformer</span>s <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.00335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.00335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Verma, Jonathan Berger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past two decades, CNN architectures have produced compelling models
of sound perception and cognition, learning hierarchical organizations of
features. Analogous to successes in computer vision, audio feature
classification can be optimized for a particular task of interest, over a wide
variety of datasets and labels. In fact similar architectures designed for
image understanding have proven effective for acoustic scene analysis. Here we
propose applying Transformer based architectures without convolutional layers
to raw audio signals. On a standard dataset of Free Sound 50K,comprising of 200
categories, our model outperforms convolutional models to produce state of the
art results. This is significant as unlike in natural language processing and
computer vision, we do not perform unsupervised pre-training for outperforming
convolutional architectures. On the same training set, with respect mean
aver-age precision benchmarks, we show a significant improvement. We further
improve the performance of Transformer architectures by using techniques such
as pooling inspired from convolutional net-work designed in the past few years.
In addition, we also show how multi-rate signal processing ideas inspired from
wavelets, can be applied to the Transformer embeddings to improve the results.
We also show how our models learns a non-linear non constant band-width
filter-bank, which shows an adaptable time frequency front end representation
for the task of audio understanding, different from other tasks e.g. pitch
estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures; Under review WASPAA 2021; Typo Fixes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Image Captioning Hallucinations in Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Zhao, Chengcui Zhang, Runlin Zhang, Tianyang Wang, Xi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in vision-language models (VLMs) hinder reliability and
real-world applicability, usually stemming from distribution shifts between
pretraining data and test samples. Existing solutions, such as retraining or
fine-tuning on additional data, demand significant computational resources and
labor-intensive data collection, while ensemble-based methods incur additional
costs by introducing auxiliary VLMs. To address these challenges, we propose a
novel test-time adaptation framework using reinforcement learning to mitigate
hallucinations during inference without retraining or any auxiliary VLMs. By
updating only the learnable parameters in the layer normalization of the
language model (approximately 0.003% of the model parameters), our method
reduces distribution shifts between test samples and pretraining samples. A
CLIP-based hallucination evaluation model is proposed to provide dual rewards
to VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in
hallucination rates on LLaVA and InstructBLIP, respectively. Our approach
outperforms state-of-the-art baselines with a 68.3% improvement in
hallucination mitigation, demonstrating its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03603v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03603v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Z. Zhou, Y. B. Wang, J. F. Wu, T. Hu, J. N. Zhang, Z. J. Li, Y. Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven human animation technology is widely used in human-computer
interaction, and the emergence of diffusion models has further advanced its
development. Currently, most methods rely on multi-stage generation and
intermediate representations, resulting in long inference time and issues with
generation quality in specific foreground regions and audio-motion consistency.
These shortcomings are primarily due to the lack of localized fine-grained
supervised guidance. To address above challenges, we propose PAHA, an
end-to-end audio-driven upper-body human animation framework with diffusion
model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts
Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss
weights based on pose confidence scores, effectively improving visual quality.
PCE constructs and trains diffusion-based regional audio-visual classifiers to
improve the consistency of motion and co-speech audio. Afterwards, we design
two novel inference guidance methods for the foregoing classifiers, Sequential
Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality
respectively. Additionally, we build CNAS, the first public Chinese News Anchor
Speech dataset, to advance research and validation in this field. Extensive
experimental results and user studies demonstrate that PAHA significantly
outperforms existing methods in audio-motion alignment and video-related
evaluations. The codes and CNAS dataset will be released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-10T00:00:00Z">2025-05-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development of a WAZOBIA-Named Entity Recognition System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. E Emedem, I. E Onyenwe, E. G Onyedinma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition NER is very crucial for various natural language
processing applications, including information extraction, machine translation,
and sentiment analysis. Despite the ever-increasing interest in African
languages within computational linguistics, existing NER systems focus mainly
on English, European, and a few other global languages, leaving a significant
gap for under-resourced languages. This research presents the development of a
WAZOBIA-NER system tailored for the three most prominent Nigerian languages:
Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilation
of annotated datasets for each language, addressing data scarcity and
linguistic diversity challenges. Exploring the state-of-the-art machine
learning technique, Conditional Random Fields (CRF) and deep learning models
such as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder
Representation from Transformers (Bert) and fine-tune with a Recurrent Neural
Network (RNN), the study evaluates the effectiveness of these approaches in
recognizing three entities: persons, organizations, and locations. The system
utilizes optical character recognition (OCR) technology to convert textual
images into machine-readable text, thereby enabling the Wazobia system to
accept both input text and textual images for extraction purposes. The system
achieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in
F1-score, and 0.9301 in accuracy. The model's evaluation was conducted across
three languages, with precision, recall, F1-score, and accuracy as key
assessment metrics. The Wazobia-NER system demonstrates that it is feasible to
build robust NER tools for under-resourced African languages using current NLP
frameworks and transfer learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OMGM: Orchestrate Multiple Granularities and Modalities for Efficient
  Multimodal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Yang, Jingjing Fu, Rui Wang, Jinyu Wang, Lei Song, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language retrieval-augmented generation (RAG) has become an effective
approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which
requires external knowledge beyond the visual content presented in images. The
effectiveness of Vision-language RAG systems hinges on multimodal retrieval,
which is inherently challenging due to the diverse modalities and knowledge
granularities in both queries and knowledge bases. Existing methods have not
fully tapped into the potential interplay between these elements. We propose a
multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that
harmonizes multiple granularities and modalities to enhance efficacy. Our
system begins with a broad initial search aligning knowledge granularity for
cross-modal retrieval, followed by a multimodal fusion reranking to capture the
nuanced multimodal information for top entity selection. A text reranker then
filters out the most relevant fine-grained section for augmented generation.
Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our
method achieves state-of-the-art retrieval performance and highly competitive
answering results, underscoring its effectiveness in advancing KB-VQA systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, 17 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic
  Modeling in Social Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqin Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of rapid development of social media, social recommendation
systems as hybrid recommendation systems have been widely applied. Existing
methods capture interest similarity between users to filter out
interest-irrelevant relations in social networks that inevitably decrease
recommendation accuracy, however, limited research has a focus on the mutual
influence of semantic information between the social network and the user-item
interaction network for further improving social recommendation. To address
these issues, we introduce a social \underline{r}ecommendation model with
ro\underline{bu}st g\underline{r}aph denoisin\underline{g}-augmentation fusion
and multi-s\underline{e}mantic Modeling(Burger). Specifically, we firstly
propose to construct a social tensor in order to smooth the training process of
the model. Then, a graph convolutional network and a tensor convolutional
network are employed to capture user's item preference and social preference,
respectively. Considering the different semantic information in the user-item
interaction network and the social network, a bi-semantic coordination loss is
proposed to model the mutual influence of semantic information. To alleviate
the interference of interest-irrelevant relations on multi-semantic modeling,
we further use Bayesian posterior probability to mine potential social
relations to replace social noise. Finally, the sliding window mechanism is
utilized to update the social tensor as the input for the next iteration.
Extensive experiments on three real datasets show Burger has a superior
performance compared with the state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context
  RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Woosang Lim, Zekun Li, Gyuwan Kim, Sungyoung Ji, HyeonJung Kim, Kyuri Choi, Jin Hyuk Lim, Kyungpyo Park, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context (LC) Large Language Models (LLMs) combined with
Retrieval-Augmented Generation (RAG) hold strong potential for complex
multi-hop and large-document tasks. However, existing RAG systems often suffer
from imprecise retrieval, incomplete context coverage under constrained context
windows, and fragmented information caused by suboptimal context construction.
We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical
retrieval framework that compresses and partitions documents into
coarse-to-fine granularities, then adaptively merges relevant contexts through
chunk- and document-level expansions in real time. By starting from the
finest-level retrieval and progressively incorporating higher-level and broader
context, MacRAG constructs effective query-specific long contexts, optimizing
both precision and coverage. Evaluations on the challenging LongBench
expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm that MacRAG
consistently surpasses baseline RAG pipelines on single- and multi-step
generation with Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish
MacRAG as an efficient, scalable solution for real-world long-context,
multi-hop reasoning. Our code is available at
https://github.com/Leezekun/MacRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust Recommendation: A <span class="highlight-title">Review</span> and an Adversarial Robustness
  Evaluation Library 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Cheng, Xiaowen Huang, Jitao Sang, Jian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, recommender system has achieved significant success. However, due
to the openness of recommender systems, they remain vulnerable to malicious
attacks. Additionally, natural noise in training data and issues such as data
sparsity can also degrade the performance of recommender systems. Therefore,
enhancing the robustness of recommender systems has become an increasingly
important research topic. In this survey, we provide a comprehensive overview
of the robustness of recommender systems. Based on our investigation, we
categorize the robustness of recommender systems into adversarial robustness
and non-adversarial robustness. In the adversarial robustness, we introduce the
fundamental principles and classical methods of recommender system adversarial
attacks and defenses. In the non-adversarial robustness, we analyze
non-adversarial robustness from the perspectives of data sparsity, natural
noise, and data imbalance. Additionally, we summarize commonly used datasets
and evaluation metrics for evaluating the robustness of recommender systems.
Finally, we also discuss the current challenges in the field of recommender
system robustness and potential future research directions. Additionally, to
facilitate fair and efficient evaluation of attack and defense methods in
adversarial robustness, we propose an adversarial robustness evaluation
library--ShillingREC, and we conduct evaluations of basic attack models and
recommendation models. ShillingREC project is released at
https://github.com/chengleileilei/ShillingREC.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General
  Vision-Language Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Huang, Qing Li, Chuan Yan, Zebang Cheng, Yurong Huang, Xiang Li, Bin Li, Xiaohui Wang, Zheng Lian, Xiaojiang Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion understanding in videos aims to accurately recognize and interpret
individuals' emotional states by integrating contextual, visual, textual, and
auditory cues. While Large Multimodal Models (LMMs) have demonstrated
significant progress in general vision-language (VL) tasks, their performance
in emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on
emotion-related tasks often leads to catastrophic forgetting, hindering their
ability to generalize across diverse tasks. To address these challenges, we
present Emotion-Qwen, a tailored multimodal framework designed to enhance both
emotion understanding and general VL reasoning. Emotion-Qwen incorporates a
sophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm,
which dynamically routes inputs to balance emotion-specific and general-purpose
processing. The model is pre-trained in a three-stage pipeline on large-scale
general and emotional image datasets to support robust multimodal
representations. Furthermore, we construct the Video Emotion Reasoning (VER)
dataset, comprising more than 40K bilingual video clips with fine-grained
descriptive annotations, to further enrich Emotion-Qwen's emotional reasoning
capability. Experimental results demonstrate that Emotion-Qwen achieves
state-of-the-art performance on multiple emotion recognition benchmarks, while
maintaining competitive results on general VL tasks. Code and models are
available at https://anonymous.4open.science/r/Emotion-Qwen-Anonymous.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Discrete and Continuous: A Multimodal Strategy for Complex
  Emotion Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiehui Jia, Huan Zhang, Jinhua Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of human-computer interaction, accurately recognizing and
interpreting human emotions is crucial yet challenging due to the complexity
and subtlety of emotional expressions. This study explores the potential for
detecting a rich and flexible range of emotions through a multimodal approach
which integrates facial expressions, voice tones, and transcript from video
clips. We propose a novel framework that maps variety of emotions in a
three-dimensional Valence-Arousal-Dominance (VAD) space, which could reflect
the fluctuations and positivity/negativity of emotions to enable a more variety
and comprehensive representation of emotional states. We employed K-means
clustering to transit emotions from traditional discrete categorization to a
continuous labeling system and built a classifier for emotion recognition upon
this system. The effectiveness of the proposed model is evaluated using the
MER2024 dataset, which contains culturally consistent video clips from Chinese
movies and TV series, annotated with both discrete and open-vocabulary emotion
labels. Our experiment successfully achieved the transformation between
discrete and continuous models, and the proposed model generated a more diverse
and comprehensive set of emotion vocabulary while maintaining strong accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-09T00:00:00Z">2025-05-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tweedie Regression for Video Recommendation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zheng, Qiang Chen, Chenglei Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern recommendation systems aim to increase click-through rates (CTR) for
better user experience, through commonly treating ranking as a classification
task focused on predicting CTR. However, there is a gap between this method and
the actual objectives of businesses across different sectors. In video
recommendation services, the objective of video on demand (VOD) extends beyond
merely encouraging clicks, but also guiding users to discover their true
interests, leading to increased watch time. And longer users watch time will
leads to more revenue through increased chances of presenting online display
advertisements. This research addresses the issue by redefining the problem
from classification to regression, with a focus on maximizing revenue through
user viewing time. Due to the lack of positive labels on recommendation, the
study introduces Tweedie Loss Function, which is better suited in this scenario
than the traditional mean square error loss. The paper also provides insights
on how Tweedie process capture users diverse interests. Our offline simulation
and online A/B test revealed that we can substantially enhance our core
business objectives: user engagement in terms of viewing time and,
consequently, revenue. Additionally, we provide a theoretical comparison
between the Tweedie Loss and the commonly employed viewing time weighted
Logloss, highlighting why Tweedie Regression stands out as an efficient
solution. We further outline a framework for designing a loss function that
focuses on a singular objective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICMI 2025 IEEE 4th International Conference on Computing and Machine
  Intelligence April 05-06, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Millions of Tweets to Actionable Insights: Leveraging LLMs for User
  Profiling <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vahid Rahimzadeh, Ali Hamzehpour, Azadeh Shakery, Masoud Asadpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media user profiling through content analysis is crucial for tasks
like misinformation detection, engagement prediction, hate speech monitoring,
and user behavior modeling. However, existing profiling techniques, including
tweet summarization, attribute-based profiling, and latent representation
learning, face significant limitations: they often lack transferability,
produce non-interpretable features, require large labeled datasets, or rely on
rigid predefined categories that limit adaptability. We introduce a novel large
language model (LLM)-based approach that leverages domain-defining statements,
which serve as key characteristics outlining the important pillars of a domain
as foundations for profiling. Our two-stage method first employs
semi-supervised filtering with a domain-specific knowledge base, then generates
both abstractive (synthesized descriptions) and extractive (representative
tweet selections) user profiles. By harnessing LLMs' inherent knowledge with
minimal human validation, our approach is adaptable across domains while
reducing the need for large labeled datasets. Our method generates
interpretable natural language user profiles, condensing extensive user data
into a scale that unlocks LLMs' reasoning and knowledge capabilities for
downstream social network tasks. We contribute a Persian political Twitter (X)
dataset and an LLM-based evaluation framework with human validation.
Experimental results show our method significantly outperforms state-of-the-art
LLM-based and traditional methods by 9.8%, demonstrating its effectiveness in
creating flexible, adaptable, and interpretable user profiles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MisD @ AAAI ICWSM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Multi-Hop Semantic Paths for Recommendation in Heterogeneous
  Information Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongye Zheng, Yue Xing, Lipeng Zhu, Xu Han, Junliang Du, Wanyu Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on the problem of path modeling in heterogeneous
information networks and proposes a multi-hop path-aware recommendation
framework. The method centers on multi-hop paths composed of various types of
entities and relations. It models user preferences through three stages: path
selection, semantic representation, and attention-based fusion. In the path
selection stage, a path filtering mechanism is introduced to remove redundant
and noisy information. In the representation learning stage, a sequential
modeling structure is used to jointly encode entities and relations, preserving
the semantic dependencies within paths. In the fusion stage, an attention
mechanism assigns different weights to each path to generate a global user
interest representation. Experiments conducted on real-world datasets such as
Amazon-Book show that the proposed method significantly outperforms existing
recommendation models across multiple evaluation metrics, including HR@10,
Recall@10, and Precision@10. The results confirm the effectiveness of multi-hop
paths in capturing high-order interaction semantics and demonstrate the
expressive modeling capabilities of the framework in heterogeneous
recommendation scenarios. This method provides both theoretical and practical
value by integrating structural information modeling in heterogeneous networks
with recommendation algorithm design. It offers a more expressive and flexible
paradigm for learning user preferences in complex data environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-Effective, Low Latency Vector Search with Azure Cosmos DB 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitish Upreti, Krishnan Sundaram, Hari Sudan Sundar, Samer Boshra, Balachandar Perumalswamy, Shivam Atri, Martin Chisholm, Revti Raman Singh, Greg Yang, Subramanyam Pattipaka, Tamara Hass, Nitesh Dudhey, James Codella, Mark Hildebrand, Magdalen Manohar, Jack Moffitt, Haiyang Xu, Naren Datha, Suryansh Gupta, Ravishankar Krishnaswamy, Prashant Gupta, Abhishek Sahu, Ritika Mor, Santosh Kulkarni, Hemeswari Varada, Sudhanshu Barthwal, Amar Sagare, Dinesh Billa, Zishan Fu, Neil Deshpande, Shaun Cooper, Kevin Pilch, Simon Moreno, Aayush Kataria, Vipul Vishal, Harsha Vardhan Simhadri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector indexing enables semantic search over diverse corpora and has become
an important interface to databases for both users and AI agents. Efficient
vector search requires deep optimizations in database systems. This has
motivated a new class of specialized vector databases that optimize for vector
search quality and cost. Instead, we argue that a scalable, high-performance,
and cost-efficient vector search system can be built inside a cloud-native
operational database like Azure Cosmos DB while leveraging the benefits of a
distributed database such as high availability, durability, and scale. We do
this by deeply integrating DiskANN, a state-of-the-art vector indexing library,
inside Azure Cosmos DB NoSQL. This system uses a single vector index per
partition stored in existing index trees, and kept in sync with underlying
data. It supports < 20ms query latency over an index spanning 10 million of
vectors, has stable recall over updates, and offers nearly 15x and 41x lower
query cost compared to Zilliz and Pinecone serverless enterprise products. It
also scales out to billions of vectors via automatic partitioning. This
convergent design presents a point in favor of integrating vector indices into
operational databases in the context of recent debates on specialized vector
databases, and offers a template for vector indexing in other databases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document Attribution: Examining Citation Relationships using Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vipula Rawte, Ryan A. Rossi, Franck Dernoncourt, Nedim Lipka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) are increasingly applied to document-based
tasks - such as document summarization, question answering, and information
extraction - where user requirements focus on retrieving information from
provided documents rather than relying on the model's parametric knowledge,
ensuring the trustworthiness and interpretability of these systems has become a
critical concern. A central approach to addressing this challenge is
attribution, which involves tracing the generated outputs back to their source
documents. However, since LLMs can produce inaccurate or imprecise responses,
it is crucial to assess the reliability of these citations.
  To tackle this, our work proposes two techniques. (1) A zero-shot approach
that frames attribution as a straightforward textual entailment task. Our
method using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the
best baseline of ID and OOD sets of AttributionBench, respectively. (2) We also
explore the role of the attention mechanism in enhancing the attribution
process. Using a smaller LLM, flan-t5-small, the F1 scores outperform the
baseline across almost all layers except layer 4 and layers 8 through 11.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ir_explain: a Python Library of Explainable IR Methods <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18546v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18546v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Saha, Harsh Agarwal, V Venktesh, Avishek Anand, Swastik Mohanty, Debapriyo Majumdar, Mandar Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent advancements in Neural Ranking Models have resulted in
significant improvements over traditional statistical retrieval models, it is
generally acknowledged that the use of large neural architectures and the
application of complex language models in Information Retrieval (IR) have
reduced the transparency of retrieval methods. Consequently, Explainability and
Interpretability have emerged as important research topics in IR. Several
axiomatic and post-hoc explanation methods, as well as approaches that attempt
to be interpretable-by-design, have been proposed. This article presents
\irexplain, an open-source Python library that implements a variety of
well-known techniques for Explainable IR (ExIR) within a common, extensible
framework. \irexplain supports the three standard categories of post-hoc
explanations, namely pointwise, pairwise, and listwise explanations. The
library is designed to make it easy to reproduce state-of-the-art ExIR
baselines on standard test collections, as well as to explore new approaches to
explaining IR models and methods. To facilitate adoption, \irexplain is
well-integrated with widely-used toolkits such as Pyserini and \irdatasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear as a Resources and Reproducibility Track paper in Proc. ACM
  SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rec-R1: Bridging Generative Large Language Models and User-Centric
  Recommendation Systems via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.24289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.24289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Lin, Tian Wang, Kun Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Rec-R1, a general reinforcement learning framework that bridges
large language models (LLMs) with recommendation systems through closed-loop
optimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1
directly optimizes LLM generation using feedback from a fixed black-box
recommendation model, without relying on synthetic SFT data from proprietary
models such as GPT-4o. This avoids the substantial cost and effort required for
data distillation. To verify the effectiveness of Rec-R1, we evaluate it on two
representative tasks: product search and sequential recommendation.
Experimental results demonstrate that Rec-R1 not only consistently outperforms
prompting- and SFT-based methods, but also achieves significant gains over
strong discriminative baselines, even when used with simple retrievers such as
BM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,
unlike SFT, which often impairs instruction-following and reasoning. These
findings suggest Rec-R1 as a promising foundation for continual task-specific
adaptation without catastrophic forgetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20698v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20698v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saron Samuel, Dan DeGenaro, Jimena Guallar-Blasco, Kate Sanders, Oluwaseun Eisape, Tanner Spendlove, Arun Reddy, Alexander Martin, Andrew Yates, Eugene Yang, Cameron Carpenter, David Etter, Efsun Kayi, Matthew Wiesner, Kenton Murray, Reno Kriz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Videos inherently contain multiple modalities, including visual events, text
overlays, sounds, and speech, all of which are important for retrieval.
However, state-of-the-art multimodal language models like VAST and LanguageBind
are built on vision-language models (VLMs), and thus overly prioritize visual
signals. Retrieval benchmarks further reinforce this bias by focusing on visual
queries and neglecting other modalities. We create a search system MMMORRF that
extracts text and features from both visual and audio modalities and integrates
them with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is
both effective and efficient, demonstrating practicality in searching videos
based on users' information needs instead of visual descriptive queries. We
evaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed
for more targeted information needs, and find that it improves nDCG@20 by 81%
over leading multimodal encoders and 37% over single-modality retrieval,
demonstrating the value of integrating diverse modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image
  Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.04938v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.04938v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Zhang, Shuai Guo, Chenxi Sun, Yuchen Zhu, Jinhai Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, deformable medical image registration techniques have made
significant progress. However, existing models still lack efficiency in
parallel extraction of coarse and fine-grained features. To address this, we
construct a new pyramid registration network based on feature and deformation
field (FF-PNet). For coarse-grained feature extraction, we design a Residual
Feature Fusion Module (RFFM), for fine-grained image deformation, we propose a
Residual Deformation Field Fusion Module (RDFFM). Through the parallel
operation of these two modules, the model can effectively handle complex image
deformations. It is worth emphasizing that the encoding stage of FF-PNet only
employs traditional convolutional neural networks without any attention
mechanisms or multilayer perceptrons, yet it still achieves remarkable
improvements in registration accuracy, fully demonstrating the superior feature
decoding capabilities of RFFM and RDFFM. We conducted extensive experiments on
the LPBA and OASIS datasets. The results show our network consistently
outperforms popular methods in metrics like the Dice Similarity Coefficient.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can <span class="highlight-title">Prompt</span>ing LLMs Unlock Hate Speech Detection across Languages? A
  Zero-shot and Few-shot Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faeze Ghorbanpour, Daryna Dementieva, Alexander Fraser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite growing interest in automated hate speech detection, most existing
approaches overlook the linguistic diversity of online content. Multilingual
instruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ
offer promising capabilities across languages, but their effectiveness in
identifying hate speech through zero-shot and few-shot prompting remains
underexplored. This work evaluates LLM prompting-based detection across eight
non-English languages, utilizing several prompting techniques and comparing
them to fine-tuned encoder models. We show that while zero-shot and few-shot
prompting lag behind fine-tuned encoder models on most of the real-world
evaluation sets, they achieve better generalization on functional tests for
hate speech detection. Our study also reveals that prompt design plays a
critical role, with each language often requiring customized prompting
techniques to maximize performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiating Emigration from Return Migration of Scholars Using
  Name-Based Nationality Detection Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faeze Ghorbanpour, Thiago Zordan Malaguth, Aliakbar Akbaritabar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most web and digital trace data do not include information about an
individual's nationality due to privacy concerns. The lack of data on
nationality can create challenges for migration research. It can lead to a
left-censoring issue since we are uncertain about the migrant's country of
origin. Once we observe an emigration event, if we know the nationality, we can
differentiate it from return migration. We propose methods to detect the
nationality with the least available data, i.e., full names. We use the
detected nationality in comparison with the country of academic origin, which
is a common approach in studying the migration of researchers. We gathered 2.6
million unique name-nationality pairs from Wikipedia and categorized them into
families of nationalities with three granularity levels to use as our training
data. Using a character-based machine learning model, we achieved a weighted F1
score of 84% for the broadest and 67% for the most granular, country-level
categorization. In our empirical study, we used the trained and tested model to
assign nationality to 8+ million scholars' full names in Scopus data. Our
results show that using the country of first publication as a proxy for
nationality underestimates the size of return flows, especially for countries
with a more diverse academic workforce, such as the USA, Australia, and Canada.
We found that around 48% of emigration from the USA was return migration once
we used the country of name origin, in contrast to 33% based on academic
origin. In the most recent period, 79% of scholars whose affiliation has
consistently changed from the USA to China, and are considered emigrants, have
Chinese names in contrast to 41% with a Chinese academic origin. Our proposed
methods for addressing left-censoring issues are beneficial for other research
that uses digital trace data to study migration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear @ ICWSM 2025. The link to the camera-ready paper
  will be added soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual and Auditory Aesthetic Preferences Across Cultures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harin Lee, Eline Van Geert, Elif Celen, Raja Marjieh, Pol van Rijn, Minsu Park, Nori Jacoby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on how humans perceive aesthetics in shapes, colours, and music has
predominantly focused on Western populations, limiting our understanding of how
cultural environments shape aesthetic preferences. We present a large-scale
cross-cultural study examining aesthetic preferences across five distinct
modalities extensively explored in the literature: shape, curvature, colour,
musical harmony and melody. We gather 401,403 preference judgements from 4,835
participants across 10 countries, systematically sampling two-dimensional
parameter spaces for each modality. The findings reveal both universal patterns
and cultural variations. Preferences for shape and curvature cross-culturally
demonstrate a consistent preference for symmetrical forms. While colour
preferences are categorically consistent, ratio-like preferences vary across
cultures. Musical harmony shows strong agreement in interval relationships
despite differing regions of preference within the broad frequency spectrum,
while melody shows the highest cross-cultural variation. These results suggest
that aesthetic preferences emerge from an interplay between shared perceptual
mechanisms and cultural learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented at CogSci 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A vector quantized masked autoencoder for audiovisual speech emotion
  recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03568v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03568v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir Sadok, Simon Leglaive, Renaud Séguier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important challenge in emotion recognition is to develop methods that can
leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV
model, a self-supervised multimodal model that leverages masked autoencoders to
learn representations of audiovisual speech without labels. The model includes
vector quantized variational autoencoders that compress raw audio and visual
speech data into discrete tokens. The audiovisual speech tokens are used to
train a multimodal masked autoencoder that consists of an encoder-decoder
architecture with attention mechanisms. The model is designed to extract both
local (i.e., at the frame level) and global (i.e., at the sequence level)
representations of audiovisual speech. During self-supervised pre-training, the
VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual
speech, for the task of reconstructing randomly masked audiovisual speech
tokens and with a contrastive learning strategy. During this pre-training, the
encoder learns to extract a representation of audiovisual speech that can be
subsequently leveraged for emotion recognition. During the supervised
fine-tuning stage, a small classification model is trained on top of the
VQ-MAE-AV encoder for an emotion recognition task. The proposed approach
achieves state-of-the-art emotion recognition results across several datasets
in both controlled and in-the-wild conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, https://samsad35.github.io/VQ-MAE-AudioVisual/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Persistence of Backdoor-based Watermarks for Neural Networks: A
  Comprehensive Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02704v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02704v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Tu Ngo, Chuan Song Heng, Nandish Chattopadhyay, Anupam Chattopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) have gained considerable traction in recent years
due to the unparalleled results they gathered. However, the cost behind
training such sophisticated models is resource intensive, resulting in many to
consider DNNs to be intellectual property (IP) to model owners. In this era of
cloud computing, high-performance DNNs are often deployed all over the internet
so that people can access them publicly. As such, DNN watermarking schemes,
especially backdoor-based watermarks, have been actively developed in recent
years to preserve proprietary rights. Nonetheless, there lies much uncertainty
on the robustness of existing backdoor watermark schemes, towards both
adversarial attacks and unintended means such as fine-tuning neural network
models. One reason for this is that no complete guarantee of robustness can be
assured in the context of backdoor-based watermark. In this paper, we
extensively evaluate the persistence of recent backdoor-based watermarks within
neural networks in the scenario of fine-tuning, we propose/develop a novel
data-driven idea to restore watermark after fine-tuning without exposing the
trigger set. Our empirical results show that by solely introducing training
data after fine-tuning, the watermark can be restored if model parameters do
not shift dramatically during fine-tuning. Depending on the types of trigger
samples used, trigger accuracy can be reinstated to up to 100%. Our study
further explores how the restoration process works using loss landscape
visualization, as well as the idea of introducing training data in fine-tuning
stage to alleviate watermark vanishing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-05-17T05:27:49.476609187Z">
            2025-05-17 05:27:49 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
